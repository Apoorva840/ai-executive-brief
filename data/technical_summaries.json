[
  {
    "title": "RAGdb: A Zero-Dependency, Embeddable Architecture for Multimodal Retrieval-Augmented Generation on the Edge",
    "url": "https://arxiv.org/abs/2602.22217",
    "source": "Arxiv AI",
    "score": 8,
    "what_happened": "arXiv:2602.22217v1 Announce Type: cross Abstract: Retrieval-Augmented Generation (RAG) has established itself as the standard paradigm for grounding Large Language Models (LLMs) in domain-specific, up-to-date data. However, the prevailing architecture for RAG has evolved into a complex, distributed stack requiring cloud-hosted vector databases, hea",
    "technical_takeaway": null,
    "primary_risk": null,
    "primary_opportunity": null,
    "who_should_care": null
  },
  {
    "title": "Even frontier LLMs from GPT-5 onward lose up to 33% accuracy when you chat too long",
    "url": "https://the-decoder.com/even-frontier-llms-from-gpt-5-onward-lose-up-to-33-accuracy-when-you-chat-too-long/",
    "source": "The Decoder",
    "score": 8,
    "what_happened": "Even with newer models like GPT-5.2 and Claude 4.6, AI chatbots still give worse answers the longer a conversation goes on. The article Even frontier LLMs from GPT-5 onward lose up to 33% accuracy when you chat too long appeared first on The Decoder.",
    "technical_takeaway": null,
    "primary_risk": null,
    "primary_opportunity": null,
    "who_should_care": null
  },
  {
    "title": "Current language model training leaves large parts of the internet on the table",
    "url": "https://the-decoder.com/current-language-model-training-leaves-large-parts-of-the-internet-on-the-table/",
    "source": "The Decoder",
    "score": 7,
    "what_happened": "Large language models learn from web data, but which pages actually make it into training sets depends heavily on a seemingly mundane choice: the HTML extractor. Researchers at Apple, Stanford, and the University of Washington found that three common extraction tools pull surprisingly different content from the same web pages. The article Current l",
    "technical_takeaway": null,
    "primary_risk": null,
    "primary_opportunity": null,
    "who_should_care": null
  },
  {
    "title": "Anthropic Hits Back After US Military Labels It a ‘Supply Chain Risk’",
    "url": "https://www.wired.com/story/anthropic-supply-chain-risk-shockwaves-silicon-valley/",
    "source": "Wired AI",
    "score": 6,
    "what_happened": "Anthropic says it would be “legally unsound” for the Pentagon to blacklist its technology after talks over military use of its artificial intelligence models broke down.",
    "technical_takeaway": null,
    "primary_risk": null,
    "primary_opportunity": null,
    "who_should_care": null
  },
  {
    "title": "ViT-Linearizer: Distilling Quadratic Knowledge into Linear-Time Vision Models",
    "url": "https://arxiv.org/abs/2504.00037",
    "source": "Arxiv AI",
    "score": 8,
    "what_happened": "arXiv:2504.00037v2 Announce Type: replace-cross Abstract: Vision Transformers (ViTs) have delivered remarkable progress through global self-attention, yet their quadratic complexity can become prohibitive for high-resolution inputs. In this work, we present ViT-Linearizer, a cross-architecture distillation framework that transfers rich ViT represen",
    "technical_takeaway": null,
    "primary_risk": null,
    "primary_opportunity": null,
    "who_should_care": null
  }
]