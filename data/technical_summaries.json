[
  {
    "title": "Dynamic Expert Quantization for Scalable Mixture-of-Experts Inference",
    "url": "https://arxiv.org/abs/2511.15015",
    "source": "Arxiv AI",
    "score": 8,
    "what_happened": "arXiv:2511.15015v3 Announce Type: replace-cross Abstract: Mixture-of-Experts (MoE) has become a practical architecture for scaling LLM capacity while keeping per-token compute modest, but deploying MoE models on a single, memory-limited GPU remains difficult because expert weights dominate the HBM footprint. Existing expert offloading and prefetchi",
    "technical_takeaway": null,
    "primary_risk": null,
    "primary_opportunity": null,
    "who_should_care": null
  },
  {
    "title": "Understanding LLM Evaluator Behavior: A Structured Multi-Evaluator Framework for Merchant Risk Assessment",
    "url": "https://arxiv.org/abs/2602.05110",
    "source": "Arxiv AI",
    "score": 7,
    "what_happened": "arXiv:2602.05110v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly used as evaluators of reasoning quality, yet their reliability and bias in payments-risk settings remain poorly understood. We introduce a structured multi-evaluator framework for assessing LLM reasoning in Merchant Category Code (MCC)-based merchant risk",
    "technical_takeaway": null,
    "primary_risk": null,
    "primary_opportunity": null,
    "who_should_care": null
  },
  {
    "title": "Hallucination-Resistant Security Planning with a Large Language Model",
    "url": "https://arxiv.org/abs/2602.05279",
    "source": "Arxiv AI",
    "score": 7,
    "what_happened": "arXiv:2602.05279v1 Announce Type: new Abstract: Large language models (LLMs) are promising tools for supporting security management tasks, such as incident response planning. However, their unreliability and tendency to hallucinate remain significant challenges. In this paper, we address these challenges by introducing a principled framework for us",
    "technical_takeaway": null,
    "primary_risk": null,
    "primary_opportunity": null,
    "who_should_care": null
  },
  {
    "title": "ProAct: Agentic Lookahead in Interactive Environments",
    "url": "https://arxiv.org/abs/2602.05327",
    "source": "Arxiv AI",
    "score": 7,
    "what_happened": "arXiv:2602.05327v1 Announce Type: new Abstract: Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a",
    "technical_takeaway": null,
    "primary_risk": null,
    "primary_opportunity": null,
    "who_should_care": null
  },
  {
    "title": "AgentXRay: White-Boxing Agentic Systems via Workflow Reconstruction",
    "url": "https://arxiv.org/abs/2602.05353",
    "source": "Arxiv AI",
    "score": 7,
    "what_happened": "arXiv:2602.05353v1 Announce Type: new Abstract: Large Language Models have shown strong capabilities in complex problem solving, yet many agentic systems remain difficult to interpret and control due to opaque internal workflows. While some frameworks offer explicit architectures for collaboration, many deployed agentic systems operate as black box",
    "technical_takeaway": null,
    "primary_risk": null,
    "primary_opportunity": null,
    "who_should_care": null
  }
]