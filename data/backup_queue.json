[
  {
    "title": "TriGen: NPU Architecture for End-to-End Acceleration of Large Language Models based on SW-HW Co-Design",
    "summary": "arXiv:2602.12962v1 Announce Type: cross Abstract: Recent studies have extensively explored NPU architectures for accelerating AI inference in on-device environments, which are inherently resource-constrained. Meanwhile, transformer-based large language models (LLMs) have become dominant, with rapidly increasing model sizes but low degree of paramet",
    "url": "https://arxiv.org/abs/2602.12962",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Intent-Driven Smart Manufacturing Integrating Knowledge Graphs and Large Language Models",
    "summary": "arXiv:2602.12419v1 Announce Type: new Abstract: The increasing complexity of smart manufacturing environments demands interfaces that can translate high-level human intents into machine-executable actions. This paper presents a unified framework that integrates instruction-tuned Large Language Models (LLMs) with ontology-aligned Knowledge Graphs (K",
    "url": "https://arxiv.org/abs/2602.12419",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents",
    "summary": "arXiv:2602.12876v1 Announce Type: new Abstract: Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain li",
    "url": "https://arxiv.org/abs/2602.12876",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "A Lightweight LLM Framework for Disaster Humanitarian Information Classification",
    "summary": "arXiv:2602.12284v1 Announce Type: cross Abstract: Timely classification of humanitarian information from social media is critical for effective disaster response. However, deploying large language models (LLMs) for this task faces challenges in resource-constrained emergency settings. This paper develops a lightweight, cost-effective framework for ",
    "url": "https://arxiv.org/abs/2602.12284",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "RankLLM: Weighted Ranking of LLMs by Quantifying Question Difficulty",
    "summary": "arXiv:2602.12424v1 Announce Type: cross Abstract: Benchmarks establish a standardized evaluation framework to systematically assess the performance of large language models (LLMs), facilitating objective comparisons and driving advancements in the field. However, existing benchmarks fail to differentiate question difficulty, limiting their ability ",
    "url": "https://arxiv.org/abs/2602.12424",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Self-EvolveRec: Self-Evolving Recommender Systems with LLM-based Directional Feedback",
    "summary": "arXiv:2602.12612v1 Announce Type: cross Abstract: Traditional methods for automating recommender system design, such as Neural Architecture Search (NAS), are often constrained by a fixed search space defined by human priors, limiting innovation to pre-defined operators. While recent LLM-driven code evolution frameworks shift fixed search space targ",
    "url": "https://arxiv.org/abs/2602.12612",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Vision Token Reduction via Attention-Driven Self-Compression for Efficient Multimodal Large Language Models",
    "summary": "arXiv:2602.12618v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) incur significant computational cost from processing numerous vision tokens through all LLM layers. Prior pruning methods operate either before the LLM, limiting generality due to diverse encoder-projector designs or within the LLM using heuristics that are i",
    "url": "https://arxiv.org/abs/2602.12618",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Artic: AI-oriented Real-time Communication for MLLM Video Assistant",
    "summary": "arXiv:2602.12641v1 Announce Type: cross Abstract: AI Video Assistant emerges as a new paradigm for Real-time Communication (RTC), where one peer is a Multimodal Large Language Model (MLLM) deployed in the cloud. This makes interaction between humans and AI more intuitive, akin to chatting with a real person. However, a fundamental mismatch exists b",
    "url": "https://arxiv.org/abs/2602.12641",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "RADAR: Revealing Asymmetric Development of Abilities in MLLM Pre-training",
    "summary": "arXiv:2602.12892v1 Announce Type: cross Abstract: Pre-trained Multi-modal Large Language Models (MLLMs) provide a knowledge-rich foundation for post-training by leveraging their inherent perception and reasoning capabilities to solve complex tasks. However, the lack of an efficient evaluation framework impedes the diagnosis of their performance bot",
    "url": "https://arxiv.org/abs/2602.12892",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Know More, Know Clearer: A Meta-Cognitive Framework for Knowledge Augmentation in Large Language Models",
    "summary": "arXiv:2602.12996v1 Announce Type: cross Abstract: Knowledge augmentation has significantly enhanced the performance of Large Language Models (LLMs) in knowledge-intensive tasks. However, existing methods typically operate on the simplistic premise that model performance equates with internal knowledge, overlooking the knowledge-confidence gaps that",
    "url": "https://arxiv.org/abs/2602.12996",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Buy versus Build an LLM: A Decision Framework for Governments",
    "summary": "arXiv:2602.13033v1 Announce Type: cross Abstract: Large Language Models (LLMs) represent a new frontier of digital infrastructure that can support a wide range of public-sector applications, from general purpose citizen services to specialized and sensitive state functions. When expanding AI access, governments face a set of strategic choices over ",
    "url": "https://arxiv.org/abs/2602.13033",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Asynchronous Verified Semantic Caching for Tiered LLM Architectures",
    "summary": "arXiv:2602.13165v1 Announce Type: cross Abstract: Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential for reducing inference cost and latency. Production deployments typically use a tiered static-dynamic design: a static cache of curated, offline vetted responses ",
    "url": "https://arxiv.org/abs/2602.13165",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Difficulty-Aware Agentic Orchestration for Query-Specific Multi-Agent Workflows",
    "summary": "arXiv:2509.11079v5 Announce Type: replace Abstract: Large Language Model (LLM)-based agentic systems have shown strong capabilities across various tasks. However, existing multi-agent frameworks often rely on static or task-level workflows, which either over-process simple queries or underperform on complex ones, while also neglecting the efficienc",
    "url": "https://arxiv.org/abs/2509.11079",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "LTSM-Bundle: A Toolbox and Benchmark on Large Language Models for Time Series Forecasting",
    "summary": "arXiv:2406.14045v3 Announce Type: replace-cross Abstract: Time Series Forecasting (TSF) has long been a challenge in time series analysis. Inspired by the success of Large Language Models (LLMs), researchers are now developing Large Time Series Models (LTSMs)-universal transformer-based models that use autoregressive prediction-to improve TSF. Howe",
    "url": "https://arxiv.org/abs/2406.14045",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders",
    "summary": "arXiv:2507.03262v4 Announce Type: replace-cross Abstract: Recent multimodal large language models (MLLMs) increasingly integrate multiple vision encoders to improve performance on various benchmarks, assuming that diverse pretraining objectives yield complementary visual signals. However, we show this assumption often fails in practice. Through sys",
    "url": "https://arxiv.org/abs/2507.03262",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  }
]