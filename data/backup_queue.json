[
  {
    "title": "ViT-Linearizer: Distilling Quadratic Knowledge into Linear-Time Vision Models",
    "summary": "arXiv:2504.00037v2 Announce Type: replace-cross Abstract: Vision Transformers (ViTs) have delivered remarkable progress through global self-attention, yet their quadratic complexity can become prohibitive for high-resolution inputs. In this work, we present ViT-Linearizer, a cross-architecture distillation framework that transfers rich ViT represen",
    "url": "https://arxiv.org/abs/2504.00037",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "Agentic AI for Intent-driven Optimization in Cell-free O-RAN",
    "summary": "arXiv:2602.22539v1 Announce Type: new Abstract: Agentic artificial intelligence (AI) is emerging as a key enabler for autonomous radio access networks (RANs), where multiple large language model (LLM)-based agents reason and collaborate to achieve operator-defined intents. The open RAN (O-RAN) architecture enables the deployment and coordination of",
    "url": "https://arxiv.org/abs/2602.22539",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety",
    "summary": "arXiv:2602.22557v1 Announce Type: new Abstract: Current safety mechanisms for Large Language Models (LLMs) rely heavily on static, fine-tuned classifiers that suffer from adaptation rigidity, the inability to enforce new governance rules without expensive retraining. To address this, we introduce CourtGuard, a retrieval-augmented multi-agent framew",
    "url": "https://arxiv.org/abs/2602.22557",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks",
    "summary": "arXiv:2602.22808v1 Announce Type: new Abstract: Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy throu",
    "url": "https://arxiv.org/abs/2602.22808",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning",
    "summary": "arXiv:2602.22963v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is spa",
    "url": "https://arxiv.org/abs/2602.22963",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "SPM-Bench: Benchmarking Large Language Models for Scanning Probe Microscopy",
    "summary": "arXiv:2602.22971v1 Announce Type: new Abstract: As LLMs achieved breakthroughs in general reasoning, their proficiency in specialized scientific domains reveals pronounced gaps in existing benchmarks due to data contamination, insufficient complexity, and prohibitive human labor costs. Here we present SPM-Bench, an original, PhD-level multimodal be",
    "url": "https://arxiv.org/abs/2602.22971",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "To Deceive is to Teach? Forging Perceptual Robustness via Adversarial Reinforcement Learning",
    "summary": "arXiv:2602.22227v1 Announce Type: cross Abstract: Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) exhibit perceptual fragility when confronted with visually complex scenes. This weakness stems from a reliance on finite training datasets, which are prohibitively expensive to scale and impose a ceiling on model robustn",
    "url": "https://arxiv.org/abs/2602.22227",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "Deep Sequence Modeling with Quantum Dynamics: Language as a Wave Function",
    "summary": "arXiv:2602.22255v1 Announce Type: cross Abstract: We introduce a sequence modeling framework in which the latent state is a complex-valued wave function evolving on a finite-dimensional Hilbert space under a learned, time-dependent Hamiltonian. Unlike standard recurrent architectures that rely on gating mechanisms to suppress competing hypotheses, ",
    "url": "https://arxiv.org/abs/2602.22255",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "Decoding the Hook: A Multimodal LLM Framework for Analyzing the Hooking Period of Video Ads",
    "summary": "arXiv:2602.22299v1 Announce Type: cross Abstract: Video-based ads are a vital medium for brands to engage consumers, with social media platforms leveraging user data to optimize ad delivery and boost engagement. A crucial but under-explored aspect is the 'hooking period', the first three seconds that capture viewer attention and influence engagemen",
    "url": "https://arxiv.org/abs/2602.22299",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "Ruyi2 Technical Report",
    "summary": "arXiv:2602.22543v1 Announce Type: cross Abstract: Large Language Models (LLMs) face significant challenges regarding deployment costs and latency, necessitating adaptive computing strategies. Building upon the AI Flow framework, we introduce Ruyi2 as an evolution of our adaptive model series designed for efficient variable-depth computation. While ",
    "url": "https://arxiv.org/abs/2602.22543",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "ContextRL: Enhancing MLLM's Knowledge Discovery Efficiency with Context-Augmented RL",
    "summary": "arXiv:2602.22623v1 Announce Type: cross Abstract: We propose ContextRL, a novel framework that leverages context augmentation to overcome these bottlenecks. Specifically, to enhance Identifiability, we provide the reward model with full reference solutions as context, enabling fine-grained process verification to filter out false positives (samples",
    "url": "https://arxiv.org/abs/2602.22623",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "IMMACULATE: A Practical LLM Auditing Framework via Verifiable Computation",
    "summary": "arXiv:2602.22700v1 Announce Type: cross Abstract: Commercial large language models are typically deployed as black-box API services, requiring users to trust providers to execute inference correctly and report token usage honestly. We present IMMACULATE, a practical auditing framework that detects economically motivated deviations-such as model sub",
    "url": "https://arxiv.org/abs/2602.22700",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs",
    "summary": "arXiv:2602.22716v1 Announce Type: cross Abstract: 3D Large Vision-Language Models (3D LVLMs) built upon Large Language Models (LLMs) have achieved remarkable progress across various multimodal tasks. However, their inherited position-dependent modeling mechanism, Rotary Position Embedding (RoPE), remains suboptimal for 3D multimodal understanding. ",
    "url": "https://arxiv.org/abs/2602.22716",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "LLM4AD: A Platform for Algorithm Design with Large Language Model",
    "summary": "arXiv:2412.17287v2 Announce Type: replace Abstract: We introduce LLM4AD, a unified Python platform for algorithm design (AD) with large language models (LLMs). LLM4AD is a generic framework with modularized blocks for search methods, algorithm design tasks, and LLM interface. The platform integrates numerous key methods and supports a wide range of",
    "url": "https://arxiv.org/abs/2412.17287",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices",
    "summary": "arXiv:2602.21858v2 Announce Type: replace Abstract: Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomousl",
    "url": "https://arxiv.org/abs/2602.21858",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  }
]