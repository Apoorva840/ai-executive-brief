[
  {
    "title": "Microsoft has a new plan to prove what\u2019s real and what\u2019s AI online",
    "summary": "AI-enabled deception now permeates our online lives. There are the high-profile cases you may easily spot, like when White House officials recently shared a manipulated image of a protester in Minnesota and then mocked those asking about it. Other times, it slips quietly into social media feeds and racks up views, like the videos that&#8230;",
    "url": "https://www.technologyreview.com/2026/02/19/1133360/microsoft-has-a-new-plan-to-prove-whats-real-and-whats-ai-online/",
    "source": "MIT Technology Review AI",
    "published_at": "2026-02-19T16:00:00+00:00"
  },
  {
    "title": "Code Metal Raises $125 Million to Rewrite the Defense Industry\u2019s Code With AI",
    "summary": "The Boston startup uses AI to translate and verify legacy software for defense contractors, arguing modernization can\u2019t come at the cost of new bugs.",
    "url": "https://www.wired.com/story/vibe-coding-startup-code-metal-raises-series-b-fundraising/",
    "source": "Wired AI",
    "published_at": "2026-02-19T19:30:00+00:00"
  },
  {
    "title": "Perplexity\u2019s Retreat From Ads Signals a Bigger Strategic Shift",
    "summary": "The AI search startup once predicted advertising would be a massive business. Now it's betting on a smaller, more valuable audience.",
    "url": "https://www.wired.com/story/perplexity-ads-shift-search-google/",
    "source": "Wired AI",
    "published_at": "2026-02-19T19:12:38+00:00"
  },
  {
    "title": "This AI Tool Will Tell You to Stop Slacking Off",
    "summary": "Fomi watches you work, then scolds you when your attention wanders. It\u2019s helpful, but there are privacy issues to consider.",
    "url": "https://www.wired.com/story/fomi-ai-will-tell-you-to-stop-slacking-off/",
    "source": "Wired AI",
    "published_at": "2026-02-19T10:30:00+00:00"
  },
  {
    "title": "Towards Efficient Constraint Handling in Neural Solvers for Routing Problems",
    "summary": "arXiv:2602.16012v1 Announce Type: new Abstract: Neural solvers have achieved impressive progress in addressing simple routing problems, particularly excelling in computational efficiency. However, their advantages under complex constraints remain nascent, for which current constraint-handling schemes via feasibility masking or implicit feasibility ",
    "url": "https://arxiv.org/abs/2602.16012",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Optimization Instability in Autonomous Agentic Workflows for Clinical Symptom Detection",
    "summary": "arXiv:2602.16037v1 Announce Type: new Abstract: Autonomous agentic workflows that iteratively refine their own behavior hold considerable promise, yet their failure modes remain poorly characterized. We investigate optimization instability, a phenomenon in which continued autonomous improvement paradoxically degrades classifier performance, using P",
    "url": "https://arxiv.org/abs/2602.16037",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "How Uncertain Is the Grade? A Benchmark of Uncertainty Metrics for LLM-Based Automatic Assessment",
    "summary": "arXiv:2602.16039v1 Announce Type: new Abstract: The rapid rise of large language models (LLMs) is reshaping the landscape of automatic assessment in education. While these systems demonstrate substantial advantages in adaptability to diverse question types and flexibility in output formats, they also introduce new challenges related to output uncer",
    "url": "https://arxiv.org/abs/2602.16039",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Evidence-Grounded Subspecialty Reasoning: Evaluating a Curated Clinical Intelligence Layer on the 2025 Endocrinology Board-Style Examination",
    "summary": "arXiv:2602.16050v1 Announce Type: new Abstract: Background: Large language models have demonstrated strong performance on general medical examinations, but subspecialty clinical reasoning remains challenging due to rapidly evolving guidelines and nuanced evidence hierarchies. Methods: We evaluated January Mirror, an evidence-grounded clinical reaso",
    "url": "https://arxiv.org/abs/2602.16050",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Improving Interactive In-Context Learning from Natural Language Feedback",
    "summary": "arXiv:2602.16066v1 Announce Type: new Abstract: Adapting one's thought process based on corrective feedback is an essential ability in human learning, particularly in collaborative settings. In contrast, the current large language model training paradigm relies heavily on modeling vast, static corpora. While effective for knowledge acquisition, it ",
    "url": "https://arxiv.org/abs/2602.16066",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "GPSBench: Do Large Language Models Understand GPS Coordinates?",
    "summary": "arXiv:2602.16105v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly deployed in applications that interact with the physical world, such as navigation, robotics, or mapping, making robust geospatial reasoning a critical capability. Despite that, LLMs' ability to reason about GPS coordinates and real-world geography remains",
    "url": "https://arxiv.org/abs/2602.16105",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Learning Personalized Agents from Human Feedback",
    "summary": "arXiv:2602.16173v1 Announce Type: new Abstract: Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these app",
    "url": "https://arxiv.org/abs/2602.16173",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "EnterpriseGym Corecraft: Training Generalizable Agents on High-Fidelity RL Environments",
    "summary": "arXiv:2602.16179v1 Announce Type: new Abstract: We show that training AI agents on high-fidelity reinforcement learning environments produces capabilities that generalize beyond the training distribution. We introduce \\corecraft{}, the first environment in \\textsc{EnterpriseGym}, Surge AI's suite of agentic RL environments. \\corecraft{} is a fully ",
    "url": "https://arxiv.org/abs/2602.16179",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Revolutionizing Long-Term Memory in AI: New Horizons with High-Capacity and High-Speed Storage",
    "summary": "arXiv:2602.16192v1 Announce Type: new Abstract: Driven by our mission of \"uplifting the world with memory,\" this paper explores the design concept of \"memory\" that is essential for achieving artificial superintelligence (ASI). Rather than proposing novel methods, we focus on several alternative approaches whose potential benefits are widely imagina",
    "url": "https://arxiv.org/abs/2602.16192",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Toward Scalable Verifiable Reward: Proxy State-Based Evaluation for Multi-turn Tool-Calling LLM Agents",
    "summary": "arXiv:2602.16246v1 Announce Type: new Abstract: Interactive large language model (LLM) agents operating via multi-turn dialogue and multi-step tool calling are increasingly used in production. Benchmarks for these agents must both reliably compare models and yield on-policy training data. Prior agentic benchmarks (e.g., tau-bench, tau2-bench, AppWo",
    "url": "https://arxiv.org/abs/2602.16246",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Multi-agent cooperation through in-context co-player inference",
    "summary": "arXiv:2602.16301v1 Announce Type: new Abstract: Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between \"learning-aware\" agents that account for and shape the learning dynamics of their co-players. However, existing ap",
    "url": "https://arxiv.org/abs/2602.16301",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Verifiable Semantics for Agent-to-Agent Communication",
    "summary": "arXiv:2602.16424v1 Announce Type: new Abstract: Multiagent AI systems require consistent communication, but we lack methods to verify that agents share the same understanding of the terms used. Natural language is interpretable but vulnerable to semantic drift, while learned protocols are efficient but opaque. We propose a certification protocol ba",
    "url": "https://arxiv.org/abs/2602.16424",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning",
    "summary": "arXiv:2602.16435v1 Announce Type: new Abstract: Automated feature engineering (AFE) enables AI systems to autonomously construct high-utility representations from raw tabular data. However, existing AFE methods rely on statistical heuristics, yielding brittle features that fail under distribution shift. We introduce CAFE, a framework that reformula",
    "url": "https://arxiv.org/abs/2602.16435",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach",
    "summary": "arXiv:2602.16481v1 Announce Type: new Abstract: Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observat",
    "url": "https://arxiv.org/abs/2602.16481",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs",
    "summary": "arXiv:2602.16512v1 Announce Type: new Abstract: Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or u",
    "url": "https://arxiv.org/abs/2602.16512",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Creating a digital poet",
    "summary": "arXiv:2602.16578v1 Announce Type: new Abstract: Can a machine write good poetry? Any positive answer raises fundamental questions about the nature and value of art. We report a seven-month poetry workshop in which a large language model was shaped into a digital poet through iterative in-context expert feedback, without retraining. Across sessions,",
    "url": "https://arxiv.org/abs/2602.16578",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
    "summary": "arXiv:2602.16653v1 Announce Type: new Abstract: Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigati",
    "url": "https://arxiv.org/abs/2602.16653",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Towards a Science of AI Agent Reliability",
    "summary": "arXiv:2602.16666v1 Announce Type: new Abstract: AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a sin",
    "url": "https://arxiv.org/abs/2602.16666",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "What Persona Are We Missing? Identifying Unknown Relevant Personas for Faithful User Simulation",
    "summary": "arXiv:2602.15832v1 Announce Type: cross Abstract: Existing user simulations, where models generate user-like responses in dialogue, often lack verification that sufficient user personas are provided, questioning the validity of the simulations. To address this core concern, this work explores the task of identifying relevant but unknown personas of",
    "url": "https://arxiv.org/abs/2602.15832",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "EdgeNav-QE: QLoRA Quantization and Dynamic Early Exit for LAM-based Navigation on Edge Devices",
    "summary": "arXiv:2602.15836v1 Announce Type: cross Abstract: Large Action Models (LAMs) have shown immense potential in autonomous navigation by bridging high-level reasoning with low-level control. However, deploying these multi-billion parameter models on edge devices remains a significant challenge due to memory constraints and latency requirements. In thi",
    "url": "https://arxiv.org/abs/2602.15836",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "The Perplexity Paradox: Why Code Compresses Better Than Math in LLM Prompts",
    "summary": "arXiv:2602.15843v1 Announce Type: cross Abstract: In \"Compress or Route?\" (Johnson, 2026), we found that code generation tolerates aggressive prompt compression (r >= 0.6) while chain-of-thought reasoning degrades gradually. That study was limited to HumanEval (164 problems), left the \"perplexity paradox\" mechanism unvalidated, and provided no adap",
    "url": "https://arxiv.org/abs/2602.15843",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Language Model Representations for Efficient Few-Shot Tabular Classification",
    "summary": "arXiv:2602.15844v1 Announce Type: cross Abstract: The Web is a rich source of structured data in the form of tables, from product catalogs and knowledge bases to scientific datasets. However, the heterogeneity of the structure and semantics of these tables makes it challenging to build a unified method that can effectively leverage the information ",
    "url": "https://arxiv.org/abs/2602.15844",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Do Personality Traits Interfere? Geometric Limitations of Steering in Large Language Models",
    "summary": "arXiv:2602.15847v1 Announce Type: cross Abstract: Personality steering in large language models (LLMs) commonly relies on injecting trait-specific steering vectors, implicitly assuming that personality traits can be controlled independently. In this work, we examine whether this assumption holds by analysing the geometric relationships between Big ",
    "url": "https://arxiv.org/abs/2602.15847",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Can LLMs Assess Personality? Validating Conversational AI for Trait Profiling",
    "summary": "arXiv:2602.15848v1 Announce Type: cross Abstract: This study validates Large Language Models (LLMs) as a dynamic alternative to questionnaire-based personality assessment. Using a within-subjects experiment (N=33), we compared Big Five personality scores derived from guided LLM conversations against the gold-standard IPIP-50 questionnaire, while al",
    "url": "https://arxiv.org/abs/2602.15848",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Preference Optimization for Review Question Generation Improves Writing Quality",
    "summary": "arXiv:2602.15849v1 Announce Type: cross Abstract: Peer review relies on substantive, evidence-based questions, yet existing LLM-based approaches often generate surface-level queries, drawing over 50\\% of their question tokens from a paper's first page. To bridge this gap, we develop IntelliReward, a novel reward model built from a frozen autoregres",
    "url": "https://arxiv.org/abs/2602.15849",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Narrative Theory-Driven LLM Methods for Automatic Story Generation and Understanding: A Survey",
    "summary": "arXiv:2602.15851v1 Announce Type: cross Abstract: Applications of narrative theories using large language models (LLMs) deliver promising use-cases in automatic story generation and understanding tasks. Our survey examines how natural language processing (NLP) research engages with fields of narrative studies, and proposes a taxonomy for ongoing ef",
    "url": "https://arxiv.org/abs/2602.15851",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Building Safe and Deployable Clinical Natural Language Processing under Temporal Leakage Constraints",
    "summary": "arXiv:2602.15852v1 Announce Type: cross Abstract: Clinical natural language processing (NLP) models have shown promise for supporting hospital discharge planning by leveraging narrative clinical documentation. However, note-based models are particularly vulnerable to temporal and lexical leakage, where documentation artifacts encode future clinical",
    "url": "https://arxiv.org/abs/2602.15852",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "A Lightweight Explainable Guardrail for Prompt Safety",
    "summary": "arXiv:2602.15853v1 Announce Type: cross Abstract: We propose a lightweight explainable guardrail (LEG) method for the classification of unsafe prompts. LEG uses a multi-task learning architecture to jointly learn a prompt classifier and an explanation classifier, where the latter labels prompt words that explain the safe/unsafe overall decision. LE",
    "url": "https://arxiv.org/abs/2602.15853",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Decoupling Strategy and Execution in Task-Focused Dialogue via Goal-Oriented Preference Optimization",
    "summary": "arXiv:2602.15854v1 Announce Type: cross Abstract: Large language models show potential in task-oriented dialogue systems, yet existing training methods often rely on token-level likelihood or preference optimization, which poorly align with long-horizon task success. To address this, we propose Goal-Oriented Preference Optimization (GOPO), a hierar",
    "url": "https://arxiv.org/abs/2602.15854",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Kalman-Inspired Runtime Stability and Recovery in Hybrid Reasoning Systems",
    "summary": "arXiv:2602.15855v1 Announce Type: cross Abstract: Hybrid reasoning systems that combine learned components with model-based inference are increasingly deployed in tool-augmented decision loops, yet their runtime behavior under partial observability and sustained evidence mismatch remains poorly understood. In practice, failures often arise as gradu",
    "url": "https://arxiv.org/abs/2602.15855",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Rethinking Soft Compression in Retrieval-Augmented Generation: A Query-Conditioned Selector Perspective",
    "summary": "arXiv:2602.15856v1 Announce Type: cross Abstract: Retrieval-Augmented Generation (RAG) effectively grounds Large Language Models (LLMs) with external knowledge and is widely applied to Web-related tasks. However, its scalability is hindered by excessive context length and redundant retrievals. Recent research on soft context compression aims to add",
    "url": "https://arxiv.org/abs/2602.15856",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "State Design Matters: How Representations Shape Dynamic Reasoning in Large Language Models",
    "summary": "arXiv:2602.15858v1 Announce Type: cross Abstract: As large language models (LLMs) move from static reasoning tasks toward dynamic environments, their success depends on the ability to navigate and respond to an environment that changes as they interact at inference time. An underexplored factor in these settings is the representation of the state. ",
    "url": "https://arxiv.org/abs/2602.15858",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "CAST: Achieving Stable LLM-based Text Analysis for Data Analytics",
    "summary": "arXiv:2602.15861v1 Announce Type: cross Abstract: Text analysis of tabular data relies on two core operations: \\emph{summarization} for corpus-level theme extraction and \\emph{tagging} for row-level labeling. A critical limitation of employing large language models (LLMs) for these tasks is their inability to meet the high standards of output stabi",
    "url": "https://arxiv.org/abs/2602.15861",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Enhancing Action and Ingredient Modeling for Semantically Grounded Recipe Generation",
    "summary": "arXiv:2602.15862v1 Announce Type: cross Abstract: Recent advances in Multimodal Large Language Models (MLMMs) have enabled recipe generation from food images, yet outputs often contain semantically incorrect actions or ingredients despite high lexical scores (e.g., BLEU, ROUGE). To address this gap, we propose a semantically grounded framework that",
    "url": "https://arxiv.org/abs/2602.15862",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Not the Example, but the Process: How Self-Generated Examples Enhance LLM Reasoning",
    "summary": "arXiv:2602.15863v1 Announce Type: cross Abstract: Recent studies have shown that Large Language Models (LLMs) can improve their reasoning performance through self-generated few-shot examples, achieving results comparable to manually curated in-context examples. However, the underlying mechanism behind these gains remains unclear, making it hard to ",
    "url": "https://arxiv.org/abs/2602.15863",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "AI as Teammate or Tool? A Review of Human-AI Interaction in Decision Support",
    "summary": "arXiv:2602.15865v1 Announce Type: cross Abstract: The integration of Artificial Intelligence (AI) necessitates determining whether systems function as tools or collaborative teammates. In this study, by synthesizing Human-AI Interaction (HAI) literature, we analyze this distinction across four dimensions: interaction design, trust calibration, coll",
    "url": "https://arxiv.org/abs/2602.15865",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "NLP Privacy Risk Identification in Social Media (NLP-PRISM): A Survey",
    "summary": "arXiv:2602.15866v1 Announce Type: cross Abstract: Natural Language Processing (NLP) is integral to social media analytics but often processes content containing Personally Identifiable Information (PII), behavioral cues, and metadata raising privacy risks such as surveillance, profiling, and targeted advertising. To systematically assess these risk",
    "url": "https://arxiv.org/abs/2602.15866",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Playing With AI: How Do State-Of-The-Art Large Language Models Perform in the 1977 Text-Based Adventure Game Zork?",
    "summary": "arXiv:2602.15867v1 Announce Type: cross Abstract: In this positioning paper, we evaluate the problem-solving and reasoning capabilities of contemporary Large Language Models (LLMs) through their performance in Zork, the seminal text-based adventure game first released in 1977. The game's dialogue-based structure provides a controlled environment fo",
    "url": "https://arxiv.org/abs/2602.15867",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Test-Time Adaptation for Tactile-Vision-Language Models",
    "summary": "arXiv:2602.15873v1 Announce Type: cross Abstract: Tactile-vision-language (TVL) models are increasingly deployed in real-world robotic and multimodal perception tasks, where test-time distribution shifts are unavoidable. Existing test-time adaptation (TTA) methods provide filtering in unimodal settings but lack explicit treatment of modality-wise r",
    "url": "https://arxiv.org/abs/2602.15873",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Fly0: Decoupling Semantic Grounding from Geometric Planning for Zero-Shot Aerial Navigation",
    "summary": "arXiv:2602.15875v1 Announce Type: cross Abstract: Current Visual-Language Navigation (VLN) methodologies face a trade-off between semantic understanding and control precision. While Multimodal Large Language Models (MLLMs) offer superior reasoning, deploying them as low-level controllers leads to high latency, trajectory oscillations, and poor gene",
    "url": "https://arxiv.org/abs/2602.15875",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Genetic Generalized Additive Models",
    "summary": "arXiv:2602.15877v1 Announce Type: cross Abstract: Generalized Additive Models (GAMs) balance predictive accuracy and interpretability, but manually configuring their structure is challenging. We propose using the multi-objective genetic algorithm NSGA-II to automatically optimize GAMs, jointly minimizing prediction error (RMSE) and a Complexity Pen",
    "url": "https://arxiv.org/abs/2602.15877",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "IT-OSE: Exploring Optimal Sample Size for Industrial Data Augmentation",
    "summary": "arXiv:2602.15878v1 Announce Type: cross Abstract: In industrial scenarios, data augmentation is an effective approach to improve model performance. However, its benefits are not unidirectionally beneficial. There is no theoretical research or established estimation for the optimal sample size (OSS) in augmentation, nor is there an established metri",
    "url": "https://arxiv.org/abs/2602.15878",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "FUTURE-VLA: Forecasting Unified Trajectories Under Real-time Execution",
    "summary": "arXiv:2602.15882v1 Announce Type: cross Abstract: General vision-language models increasingly support unified spatiotemporal reasoning over long video streams, yet deploying such capabilities on robots remains constrained by the prohibitive latency of processing long-horizon histories and generating high-dimensional future predictions. To bridge th",
    "url": "https://arxiv.org/abs/2602.15882",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "NeuroSleep: Neuromorphic Event-Driven Single-Channel EEG Sleep Staging for Edge-Efficient Sensing",
    "summary": "arXiv:2602.15888v1 Announce Type: cross Abstract: Reliable, continuous neural sensing on wearable edge platforms is fundamental to long-term health monitoring; however, for electroencephalography (EEG)-based sleep monitoring, dense high-frequency processing is often computationally prohibitive under tight energy budgets. To address this bottleneck,",
    "url": "https://arxiv.org/abs/2602.15888",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Evidence for Daily and Weekly Periodic Variability in GPT-4o Performance",
    "summary": "arXiv:2602.15889v1 Announce Type: cross Abstract: Large language models (LLMs) are increasingly used in research both as tools and as objects of investigation. Much of this work implicitly assumes that LLM performance under fixed conditions (identical model snapshot, hyperparameters, and prompt) is time-invariant. If average output quality changes ",
    "url": "https://arxiv.org/abs/2602.15889",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Surrogate Modeling for Neutron Transport: A Neural Operator Approach",
    "summary": "arXiv:2602.15890v1 Announce Type: cross Abstract: This work introduces a neural operator based surrogate modeling framework for neutron transport computation. Two architectures, the Deep Operator Network (DeepONet) and the Fourier Neural Operator (FNO), were trained for fixed source problems to learn the mapping from anisotropic neutron sources, Q(",
    "url": "https://arxiv.org/abs/2602.15890",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Egocentric Bias in Vision-Language Models",
    "summary": "arXiv:2602.15892v1 Announce Type: cross Abstract: Visual perspective taking--inferring how the world appears from another's viewpoint--is foundational to social cognition. We introduce FlipSet, a diagnostic benchmark for Level-2 visual perspective taking (L2 VPT) in vision-language models. The task requires simulating 180-degree rotations of 2D cha",
    "url": "https://arxiv.org/abs/2602.15892",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Understand Then Memory: A Cognitive Gist-Driven RAG Framework with Global Semantic Diffusion",
    "summary": "arXiv:2602.15895v1 Announce Type: cross Abstract: Retrieval-Augmented Generation (RAG) effectively mitigates hallucinations in LLMs by incorporating external knowledge. However, the inherent discrete representation of text in existing frameworks often results in a loss of semantic integrity, leading to retrieval deviations. Inspired by the human ep",
    "url": "https://arxiv.org/abs/2602.15895",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Doc-to-LoRA: Learning to Instantly Internalize Contexts",
    "summary": "arXiv:2602.15902v1 Announce Type: cross Abstract: Long input sequences are central to in-context learning, document understanding, and multi-step reasoning of Large Language Models (LLMs). However, the quadratic attention cost of Transformers makes inference memory-intensive and slow. While context distillation (CD) can transfer information into mo",
    "url": "https://arxiv.org/abs/2602.15902",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Resp-Agent: An Agent-Based System for Multimodal Respiratory Sound Generation and Disease Diagnosis",
    "summary": "arXiv:2602.15909v1 Announce Type: cross Abstract: Deep learning-based respiratory auscultation is currently hindered by two fundamental challenges: (i) inherent information loss, as converting signals into spectrograms discards transient acoustic events and clinical context; (ii) limited data availability, exacerbated by severe class imbalance. To ",
    "url": "https://arxiv.org/abs/2602.15909",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Foundation Models for Medical Imaging: Status, Challenges, and Directions",
    "summary": "arXiv:2602.15913v1 Announce Type: cross Abstract: Foundation models (FMs) are rapidly reshaping medical imaging, shifting the field from narrowly trained, task-specific networks toward large, general-purpose models that can be adapted across modalities, anatomies, and clinical tasks. In this review, we synthesize the emerging landscape of medical i",
    "url": "https://arxiv.org/abs/2602.15913",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "MaS-VQA: A Mask-and-Select Framework for Knowledge-Based Visual Question Answering",
    "summary": "arXiv:2602.15915v1 Announce Type: cross Abstract: Knowledge-based Visual Question Answering (KB-VQA) requires models to answer questions by integrating visual information with external knowledge. However, retrieved knowledge is often noisy, partially irrelevant, or misaligned with the visual content, while internal model knowledge is difficult to c",
    "url": "https://arxiv.org/abs/2602.15915",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "EarthSpatialBench: Benchmarking Spatial Reasoning Capabilities of Multimodal LLMs on Earth Imagery",
    "summary": "arXiv:2602.15918v1 Announce Type: cross Abstract: Benchmarking spatial reasoning in multimodal large language models (MLLMs) has attracted growing interest in computer vision due to its importance for embodied AI and other agentic systems that require precise interaction with the physical world. However, spatial reasoning on Earth imagery has lagge",
    "url": "https://arxiv.org/abs/2602.15918",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Generalized Leverage Score for Scalable Assessment of Privacy Vulnerability",
    "summary": "arXiv:2602.15919v1 Announce Type: cross Abstract: Can the privacy vulnerability of individual data points be assessed without retraining models or explicitly simulating attacks? We answer affirmatively by showing that exposure to membership inference attack (MIA) is fundamentally governed by a data point's influence on the learned model. We formali",
    "url": "https://arxiv.org/abs/2602.15919",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "A fully differentiable framework for training proxy Exchange Correlation Functionals for periodic systems",
    "summary": "arXiv:2602.15923v1 Announce Type: cross Abstract: Density Functional Theory (DFT) is widely used for first-principles simulations in chemistry and materials science, but its computational cost remains a key limitation for large systems. Motivated by recent advances in ML-based exchange-correlation (XC) functionals, this paper introduces a different",
    "url": "https://arxiv.org/abs/2602.15923",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "From Tool Orchestration to Code Execution: A Study of MCP Design Choices",
    "summary": "arXiv:2602.15945v1 Announce Type: cross Abstract: Model Context Protocols (MCPs) provide a unified platform for agent systems to discover, select, and orchestrate tools across heterogeneous execution environments. As MCP-based systems scale to incorporate larger tool catalogs and multiple concurrently connected MCP servers, traditional tool-by-tool",
    "url": "https://arxiv.org/abs/2602.15945",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Hybrid Model Predictive Control with Physics-Informed Neural Network for Satellite Attitude Control",
    "summary": "arXiv:2602.15954v1 Announce Type: cross Abstract: Reliable spacecraft attitude control depends on accurate prediction of attitude dynamics, particularly when model-based strategies such as Model Predictive Control (MPC) are employed, where performance is limited by the quality of the internal system model. For spacecraft with complex dynamics, obta",
    "url": "https://arxiv.org/abs/2602.15954",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "DocSplit: A Comprehensive Benchmark Dataset and Evaluation Approach for Document Packet Recognition and Splitting",
    "summary": "arXiv:2602.15958v1 Announce Type: cross Abstract: Document understanding in real-world applications often requires processing heterogeneous, multi-page document packets containing multiple documents stitched together. Despite recent advances in visual document understanding, the fundamental task of document packet splitting, which involves separati",
    "url": "https://arxiv.org/abs/2602.15958",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Position-Aware Scene-Appearance Disentanglement for Bidirectional Photoacoustic Microscopy Registration",
    "summary": "arXiv:2602.15959v1 Announce Type: cross Abstract: High-speed optical-resolution photoacoustic microscopy (OR-PAM) with bidirectional raster scanning doubles imaging speed but introduces coupled domain shift and geometric misalignment between forward and backward scan lines. Existing registration methods, constrained by brightness constancy assumpti",
    "url": "https://arxiv.org/abs/2602.15959",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "From Reflection to Repair: A Scoping Review of Dataset Documentation Tools",
    "summary": "arXiv:2602.15968v1 Announce Type: cross Abstract: Dataset documentation is widely recognized as essential for the responsible development of automated systems. Despite growing efforts to support documentation through different kinds of artifacts, little is known about the motivations shaping documentation tool design or the factors hindering their ",
    "url": "https://arxiv.org/abs/2602.15968",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "B-DENSE: Branching For Dense Ensemble Network Learning",
    "summary": "arXiv:2602.15971v1 Announce Type: cross Abstract: Inspired by non-equilibrium thermodynamics, diffusion models have achieved state-of-the-art performance in generative modeling. However, their iterative sampling nature results in high inference latency. While recent distillation techniques accelerate sampling, they discard intermediate trajectory s",
    "url": "https://arxiv.org/abs/2602.15971",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "ReLoop: Structured Modeling and Behavioral Verification for Reliable LLM-Based Optimization",
    "summary": "arXiv:2602.15983v1 Announce Type: cross Abstract: Large language models (LLMs) can translate natural language into optimization code, but silent failures pose a critical risk: code that executes and returns solver-feasible solutions may encode semantically incorrect formulations, creating a feasibility-correctness gap of up to 90 percentage points ",
    "url": "https://arxiv.org/abs/2602.15983",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Anatomy of Capability Emergence: Scale-Invariant Representation Collapse and Top-Down Reorganization in Neural Networks",
    "summary": "arXiv:2602.15997v1 Announce Type: cross Abstract: Capability emergence during neural network training remains mechanistically opaque. We track five geometric measures across five model scales (405K-85M parameters), 120+ emergence events in eight algorithmic tasks, and three Pythia language models (160M-2.8B). We find: (1) training begins with a uni",
    "url": "https://arxiv.org/abs/2602.15997",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "ODYN: An All-Shifted Non-Interior-Point Method for Quadratic Programming in Robotics and AI",
    "summary": "arXiv:2602.16005v1 Announce Type: cross Abstract: We introduce ODYN, a novel all-shifted primal-dual non-interior-point quadratic programming (QP) solver designed to efficiently handle challenging dense and sparse QPs. ODYN combines all-shifted nonlinear complementarity problem (NCP) functions with proximal method of multipliers to robustly address",
    "url": "https://arxiv.org/abs/2602.16005",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "MAEB: Massive Audio Embedding Benchmark",
    "summary": "arXiv:2602.16008v1 Announce Type: cross Abstract: We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-",
    "url": "https://arxiv.org/abs/2602.16008",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval",
    "summary": "arXiv:2602.16019v1 Announce Type: cross Abstract: Vision-language foundation models have emerged as powerful general-purpose representation learners with strong potential for multimodal understanding, but their deterministic embeddings often fail to provide the reliability required for high-stakes biomedical applications. This work introduces MedPr",
    "url": "https://arxiv.org/abs/2602.16019",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Transforming GenAI Policy to Prompting Instruction: An RCT of Scalable Prompting Interventions in a CS1 Course",
    "summary": "arXiv:2602.16033v1 Announce Type: cross Abstract: Despite universal GenAI adoption, students cannot distinguish task performance from actual learning and lack skills to leverage AI for learning, leading to worse exam performance when AI use remains unreflective. Yet few interventions teaching students to prompt AI as a tutor rather than solution pr",
    "url": "https://arxiv.org/abs/2602.16033",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "AI-CARE: Carbon-Aware Reporting Evaluation Metric for AI Models",
    "summary": "arXiv:2602.16042v1 Announce Type: cross Abstract: As machine learning (ML) continues its rapid expansion, the environmental cost of model training and inference has become a critical societal concern. Existing benchmarks overwhelmingly focus on standard performance metrics such as accuracy, BLEU, or mAP, while largely ignoring energy consumption an",
    "url": "https://arxiv.org/abs/2602.16042",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Can Generative Artificial Intelligence Survive Data Contamination? Theoretical Guarantees under Contaminated Recursive Training",
    "summary": "arXiv:2602.16065v1 Announce Type: cross Abstract: Generative Artificial Intelligence (AI), such as large language models (LLMs), has become a transformative force across science, industry, and society. As these systems grow in popularity, web data becomes increasingly interwoven with this AI-generated material and it is increasingly difficult to se",
    "url": "https://arxiv.org/abs/2602.16065",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Omni-iEEG: A Large-Scale, Comprehensive iEEG Dataset and Benchmark for Epilepsy Research",
    "summary": "arXiv:2602.16072v1 Announce Type: cross Abstract: Epilepsy affects over 50 million people worldwide, and one-third of patients suffer drug-resistant seizures where surgery offers the best chance of seizure freedom. Accurate localization of the epileptogenic zone (EZ) relies on intracranial EEG (iEEG). Clinical workflows, however, remain constrained",
    "url": "https://arxiv.org/abs/2602.16072",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "ScenicRules: An Autonomous Driving Benchmark with Multi-Objective Specifications and Abstract Scenarios",
    "summary": "arXiv:2602.16073v1 Announce Type: cross Abstract: Developing autonomous driving systems for complex traffic environments requires balancing multiple objectives, such as avoiding collisions, obeying traffic rules, and making efficient progress. In many situations, these objectives cannot be satisfied simultaneously, and explicit priority relations n",
    "url": "https://arxiv.org/abs/2602.16073",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Language Statistics and False Belief Reasoning: Evidence from 41 Open-Weight LMs",
    "summary": "arXiv:2602.16085v1 Announce Type: cross Abstract: Research on mental state reasoning in language models (LMs) has the potential to inform theories of human social cognition--such as the theory that mental state reasoning emerges in part from language exposure--and our understanding of LMs themselves. Yet much published work on LMs relies on a relat",
    "url": "https://arxiv.org/abs/2602.16085",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Updating Parametric Knowledge with Context Distillation Retains Post-Training Capabilities",
    "summary": "arXiv:2602.16093v1 Announce Type: cross Abstract: Post-training endows pretrained LLMs with a variety of desirable skills, including instruction-following, reasoning, and others. However, these post-trained LLMs only encode knowledge up to a cut-off date, necessitating continual adaptation. Unfortunately, existing solutions cannot simultaneously le",
    "url": "https://arxiv.org/abs/2602.16093",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Federated Graph AGI for Cross-Border Insider Threat Intelligence in Government Financial Schemes",
    "summary": "arXiv:2602.16109v1 Announce Type: cross Abstract: Cross-border insider threats pose a critical challenge to government financial schemes, particularly when dealing with distributed, privacy-sensitive data across multiple jurisdictions. Existing approaches face fundamental limitations: they cannot effectively share intelligence across borders due to",
    "url": "https://arxiv.org/abs/2602.16109",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis",
    "summary": "arXiv:2602.16110v1 Announce Type: cross Abstract: Computed Tomography (CT) is one of the most widely used and diagnostically information-dense imaging modalities, covering critical organs such as the heart, lungs, liver, and colon. Clinical interpretation relies on both slice-driven local features (e.g., sub-centimeter nodules, lesion boundaries) a",
    "url": "https://arxiv.org/abs/2602.16110",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Surrogate-Based Prevalence Measurement for Large-Scale A/B Testing",
    "summary": "arXiv:2602.16111v1 Announce Type: cross Abstract: Online media platforms often need to measure how frequently users are exposed to specific content attributes in order to evaluate trade-offs in A/B experiments. A direct approach is to sample content, label it using a high-quality rubric (e.g., an expert-reviewed LLM prompt), and estimate impression",
    "url": "https://arxiv.org/abs/2602.16111",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Rethinking ANN-based Retrieval: Multifaceted Learnable Index for Large-scale Recommendation System",
    "summary": "arXiv:2602.16124v1 Announce Type: cross Abstract: Approximate nearest neighbor (ANN) search is widely used in the retrieval stage of large-scale recommendation systems. In this stage, candidate items are indexed using their learned embedding vectors, and ANN search is executed for each user (or item) query to retrieve a set of relevant items. Howev",
    "url": "https://arxiv.org/abs/2602.16124",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Retrieval Collapses When AI Pollutes the Web",
    "summary": "arXiv:2602.16136v1 Announce Type: cross Abstract: The rapid proliferation of AI-generated content on the Web presents a structural risk to information retrieval, as search engines and Retrieval-Augmented Generation (RAG) systems increasingly consume evidence produced by the Large Language Models (LLMs). We characterize this ecosystem-level failure ",
    "url": "https://arxiv.org/abs/2602.16136",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Human-AI Collaboration in Large Language Model-Integrated Building Energy Management Systems: The Role of User Domain Knowledge and AI Literacy",
    "summary": "arXiv:2602.16140v1 Announce Type: cross Abstract: This study aimed to comprehend how user domain knowledge and artificial intelligence (AI) literacy impact the effective use of human-AI interactive building energy management system (BEMS). While prior studies have investigated the potential of integrating large language models (LLMs) into BEMS or b",
    "url": "https://arxiv.org/abs/2602.16140",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "ASPEN: Spectral-Temporal Fusion for Cross-Subject Brain Decoding",
    "summary": "arXiv:2602.16147v1 Announce Type: cross Abstract: Cross-subject generalization in EEG-based brain-computer interfaces (BCIs) remains challenging due to individual variability in neural signals. We investigate whether spectral representations offer more stable features for cross-subject transfer than temporal waveforms. Through correlation analyses ",
    "url": "https://arxiv.org/abs/2602.16147",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution",
    "summary": "arXiv:2602.16154v1 Announce Type: cross Abstract: Chain-of-thought (CoT) reasoning sometimes fails to faithfully reflect the true computation of a large language model (LLM), hampering its utility in explaining how LLMs arrive at their answers. Moreover, optimizing for faithfulness and interpretability in reasoning often degrades task performance. ",
    "url": "https://arxiv.org/abs/2602.16154",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "HiPER: Hierarchical Reinforcement Learning with Explicit Credit Assignment for Large Language Model Agents",
    "summary": "arXiv:2602.16165v1 Announce Type: cross Abstract: Training LLMs as interactive agents for multi-turn decision-making remains challenging, particularly in long-horizon tasks with sparse and delayed rewards, where agents must execute extended sequences of actions before receiving meaningful feedback. Most existing reinforcement learning (RL) approach",
    "url": "https://arxiv.org/abs/2602.16165",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Edge Learning via Federated Split Decision Transformers for Metaverse Resource Allocation",
    "summary": "arXiv:2602.16174v1 Announce Type: cross Abstract: Mobile edge computing (MEC) based wireless metaverse services offer an untethered, immersive experience to users, where the superior quality of experience (QoE) needs to be achieved under stringent latency constraints and visual quality demands. To achieve this, MEC-based intelligent resource alloca",
    "url": "https://arxiv.org/abs/2602.16174",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Conjugate Learning Theory: Uncovering the Mechanisms of Trainability and Generalization in Deep Neural Networks",
    "summary": "arXiv:2602.16177v1 Announce Type: cross Abstract: In this work, we propose a notion of practical learnability grounded in finite sample settings, and develop a conjugate learning theoretical framework based on convex conjugate duality to characterize this learnability property. Building on this foundation, we demonstrate that training deep neural n",
    "url": "https://arxiv.org/abs/2602.16177",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "SIT-LMPC: Safe Information-Theoretic Learning Model Predictive Control for Iterative Tasks",
    "summary": "arXiv:2602.16187v1 Announce Type: cross Abstract: Robots executing iterative tasks in complex, uncertain environments require control strategies that balance robustness, safety, and high performance. This paper introduces a safe information-theoretic learning model predictive control (SIT-LMPC) algorithm for iterative tasks. Specifically, we design",
    "url": "https://arxiv.org/abs/2602.16187",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Beyond Learning: A Training-Free Alternative to Model Adaptation",
    "summary": "arXiv:2602.16189v1 Announce Type: cross Abstract: Despite the continuous research and evolution of language models, they sometimes underperform previous versions. Existing approaches to overcome these challenges are resource-intensive, highlighting the need for alternatives that enable immediate action. We assume that each language model has a loca",
    "url": "https://arxiv.org/abs/2602.16189",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Rethinking Input Domains in Physics-Informed Neural Networks via Geometric Compactification Mappings",
    "summary": "arXiv:2602.16193v1 Announce Type: cross Abstract: Several complex physical systems are governed by multi-scale partial differential equations (PDEs) that exhibit both smooth low-frequency components and localized high-frequency structures. Existing physics-informed neural network (PINN) methods typically train with fixed coordinate system inputs, w",
    "url": "https://arxiv.org/abs/2602.16193",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Temporal Panel Selection in Ongoing Citizens' Assemblies",
    "summary": "arXiv:2602.16194v1 Announce Type: cross Abstract: Permanent citizens' assemblies are ongoing deliberative bodies composed of randomly selected citizens, organized into panels that rotate over time. Unlike one-off panels, which represent the population in a single snapshot, permanent assemblies enable shifting participation across multiple rounds. T",
    "url": "https://arxiv.org/abs/2602.16194",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Graphon Mean-Field Subsampling for Cooperative Heterogeneous Multi-Agent Reinforcement Learning",
    "summary": "arXiv:2602.16196v1 Announce Type: cross Abstract: Coordinating large populations of interacting agents is a central challenge in multi-agent reinforcement learning (MARL), where the size of the joint state-action space scales exponentially with the number of agents. Mean-field methods alleviate this burden by aggregating agent interactions, but the",
    "url": "https://arxiv.org/abs/2602.16196",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Long-Tail Knowledge in Large Language Models: Taxonomy, Mechanisms, Interventions and Implications",
    "summary": "arXiv:2602.16201v1 Announce Type: cross Abstract: Large language models (LLMs) are trained on web-scale corpora that exhibit steep power-law distributions, in which the distribution of knowledge is highly long-tailed, with most appearing infrequently. While scaling has improved average-case performance, persistent failures on low-frequency, domain-",
    "url": "https://arxiv.org/abs/2602.16201",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Geometric Neural Operators via Lie Group-Constrained Latent Dynamics",
    "summary": "arXiv:2602.16209v1 Announce Type: cross Abstract: Neural operators offer an effective framework for learning solutions of partial differential equations for many physical systems in a resolution-invariant and data-driven manner. Existing neural operators, however, often suffer from instability in multi-layer iteration and long-horizon rollout, whic",
    "url": "https://arxiv.org/abs/2602.16209",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Graph neural network for colliding particles with an application to sea ice floe modeling",
    "summary": "arXiv:2602.16213v1 Announce Type: cross Abstract: This paper introduces a novel approach to sea ice modeling using Graph Neural Networks (GNNs), utilizing the natural graph structure of sea ice, where nodes represent individual ice pieces, and edges model the physical interactions, including collisions. This concept is developed within a one-dimens",
    "url": "https://arxiv.org/abs/2602.16213",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "UCTECG-Net: Uncertainty-aware Convolution Transformer ECG Network for Arrhythmia Detection",
    "summary": "arXiv:2602.16216v1 Announce Type: cross Abstract: Deep learning has improved automated electrocardiogram (ECG) classification, but limited insight into prediction reliability hinders its use in safety-critical settings. This paper proposes UCTECG-Net, an uncertainty-aware hybrid architecture that combines one-dimensional convolutions and Transforme",
    "url": "https://arxiv.org/abs/2602.16216",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Are LLMs Ready to Replace Bangla Annotators?",
    "summary": "arXiv:2602.16241v1 Announce Type: cross Abstract: Large Language Models (LLMs) are increasingly used as automated annotators to scale dataset creation, yet their reliability as unbiased annotators--especially for low-resource and identity-sensitive settings--remains poorly understood. In this work, we study the behavior of LLMs as zero-shot annotat",
    "url": "https://arxiv.org/abs/2602.16241",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Color-based Emotion Representation for Speech Emotion Recognition",
    "summary": "arXiv:2602.16256v1 Announce Type: cross Abstract: Speech emotion recognition (SER) has traditionally relied on categorical or dimensional labels. However, this technique is limited in representing both the diversity and interpretability of emotions. To overcome this limitation, we focus on color attributes, such as hue, saturation, and value, to re",
    "url": "https://arxiv.org/abs/2602.16256",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Generative AI Usage of University Students: Navigating Between Education and Business",
    "summary": "arXiv:2602.16307v1 Announce Type: cross Abstract: This study investigates generative artificial intelligence (GenAI) usage of university students who study alongside their professional career. Previous literature has paid little attention to part-time students and the intersectional use of GenAI between education and business. This study examines w",
    "url": "https://arxiv.org/abs/2602.16307",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "The Weight of a Bit: EMFI Sensitivity Analysis of Embedded Deep Learning Models",
    "summary": "arXiv:2602.16309v1 Announce Type: cross Abstract: Fault injection attacks on embedded neural network models have been shown as a potent threat. Numerous works studied resilience of models from various points of view. As of now, there is no comprehensive study that would evaluate the influence of number representations used for model parameters agai",
    "url": "https://arxiv.org/abs/2602.16309",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "The Diversity Paradox revisited: Systemic Effects of Feedback Loops in Recommender Systems",
    "summary": "arXiv:2602.16315v1 Announce Type: cross Abstract: Recommender systems shape individual choices through feedback loops in which user behavior and algorithmic recommendations coevolve over time. The systemic effects of these loops remain poorly understood, in part due to unrealistic assumptions in existing simulation studies. We propose a feedback-lo",
    "url": "https://arxiv.org/abs/2602.16315",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "A Graph Meta-Network for Learning on Kolmogorov-Arnold Networks",
    "summary": "arXiv:2602.16316v1 Announce Type: cross Abstract: Weight-space models learn directly from the parameters of neural networks, enabling tasks such as predicting their accuracy on new datasets. Naive methods -- like applying MLPs to flattened parameters -- perform poorly, making the design of better weight-space architectures a central challenge. Whil",
    "url": "https://arxiv.org/abs/2602.16316",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "A Self-Supervised Approach for Enhanced Feature Representations in Object Detection Tasks",
    "summary": "arXiv:2602.16322v1 Announce Type: cross Abstract: In the fast-evolving field of artificial intelligence, where models are increasingly growing in complexity and size, the availability of labeled data for training deep learning models has become a significant challenge. Addressing complex problems like object detection demands considerable time and ",
    "url": "https://arxiv.org/abs/2602.16322",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Guide-Guard: Off-Target Predicting in CRISPR Applications",
    "summary": "arXiv:2602.16327v1 Announce Type: cross Abstract: With the introduction of cyber-physical genome sequencing and editing technologies, such as CRISPR, researchers can more easily access tools to investigate and create remedies for a variety of topics in genetics and health science (e.g. agriculture and medicine). As the field advances and grows, new",
    "url": "https://arxiv.org/abs/2602.16327",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Spatial Audio Question Answering and Reasoning on Dynamic Source Movements",
    "summary": "arXiv:2602.16334v1 Announce Type: cross Abstract: Spatial audio understanding aims to enable machines to interpret complex auditory scenes, particularly when sound sources move over time. In this work, we study Spatial Audio Question Answering (Spatial AQA) with a focus on movement reasoning, where a model must infer object motion, position, and di",
    "url": "https://arxiv.org/abs/2602.16334",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "HAWX: A Hardware-Aware FrameWork for Fast and Scalable ApproXimation of DNNs",
    "summary": "arXiv:2602.16336v1 Announce Type: cross Abstract: This work presents HAWX, a hardware-aware scalable exploration framework that employs multi-level sensitivity scoring at different DNN abstraction levels (operator, filter, layer, and model) to guide selective integration of heterogeneous AxC blocks. Supported by predictive models for accuracy, powe",
    "url": "https://arxiv.org/abs/2602.16336",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Articulated 3D Scene Graphs for Open-World Mobile Manipulation",
    "summary": "arXiv:2602.16356v1 Announce Type: cross Abstract: Semantics has enabled 3D scene understanding and affordance-driven object interaction. However, robots operating in real-world environments face a critical limitation: they cannot anticipate how objects move. Long-horizon mobile manipulation requires closing the gap between semantics, geometry, and ",
    "url": "https://arxiv.org/abs/2602.16356",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "AI-Driven Structure Refinement of X-ray Diffraction",
    "summary": "arXiv:2602.16372v1 Announce Type: cross Abstract: Artificial intelligence can rapidly propose candidate phases and structures from X-ray diffraction (XRD), but these hypotheses often fail in downstream refinement because peak intensities cannot be stably assigned under severe overlap and diffraction consistency is enforced only weakly. Here we intr",
    "url": "https://arxiv.org/abs/2602.16372",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Automated Histopathology Report Generation via Pyramidal Feature Extraction and the UNI Foundation Model",
    "summary": "arXiv:2602.16422v1 Announce Type: cross Abstract: Generating diagnostic text from histopathology whole slide images (WSIs) is challenging due to the gigapixel scale of the input and the requirement for precise, domain specific language. We propose a hierarchical vision language framework that combines a frozen pathology foundation model with a Tran",
    "url": "https://arxiv.org/abs/2602.16422",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Designing Production-Scale OCR for India: Multilingual and Domain-Specific Systems",
    "summary": "arXiv:2602.16430v1 Announce Type: cross Abstract: Designing Optical Character Recognition (OCR) systems for India requires balancing linguistic diversity, document heterogeneity, and deployment constraints. In this paper, we study two training strategies for building multilingual OCR systems with Vision-Language Models through the Chitrapathak seri",
    "url": "https://arxiv.org/abs/2602.16430",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment",
    "summary": "arXiv:2602.16438v1 Announce Type: cross Abstract: Conventional large language model (LLM) fairness alignment largely focuses on mitigating bias along single sensitive attributes, overlooking fairness as an inherently multidimensional and context-specific value. This approach risks creating systems that achieve narrow fairness metrics while exacerba",
    "url": "https://arxiv.org/abs/2602.16438",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Hardware-accelerated graph neural networks: an alternative approach for neuromorphic event-based audio classification and keyword spotting on SoC FPGA",
    "summary": "arXiv:2602.16442v1 Announce Type: cross Abstract: As the volume of data recorded by embedded edge sensors increases, particularly from neuromorphic devices producing discrete event streams, there is a growing need for hardware-aware neural architectures that enable efficient, low-latency, and energy-conscious local processing. We present an FPGA im",
    "url": "https://arxiv.org/abs/2602.16442",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation",
    "summary": "arXiv:2602.16444v1 Announce Type: cross Abstract: The pursuit of general-purpose robotic manipulation is hindered by the scarcity of diverse, real-world interaction data. Unlike data collection from web in vision or language, robotic data collection is an active process incurring prohibitive physical costs. Consequently, automated task curation to ",
    "url": "https://arxiv.org/abs/2602.16444",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "GICDM: Mitigating Hubness for Reliable Distance-Based Generative Model Evaluation",
    "summary": "arXiv:2602.16449v1 Announce Type: cross Abstract: Generative model evaluation commonly relies on high-dimensional embedding spaces to compute distances between samples. We show that dataset representations in these spaces are affected by the hubness phenomenon, which distorts nearest neighbor relationships and biases distance-based metrics. Buildin",
    "url": "https://arxiv.org/abs/2602.16449",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models",
    "summary": "arXiv:2602.16467v1 Announce Type: cross Abstract: The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination qu",
    "url": "https://arxiv.org/abs/2602.16467",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling",
    "summary": "arXiv:2602.16485v1 Announce Type: cross Abstract: Existing Multi-Agent Systems (MAS) typically rely on static, homogeneous model configurations, limiting their ability to exploit the distinct strengths of differently post-trained models. To address this, we introduce Team-of-Thoughts, a novel MAS architecture that leverages the complementary capabi",
    "url": "https://arxiv.org/abs/2602.16485",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Learning to Learn from Language Feedback with Social Meta-Learning",
    "summary": "arXiv:2602.16488v1 Announce Type: cross Abstract: Large language models (LLMs) often struggle to learn from corrective feedback within a conversational context. They are rarely proactive in soliciting this feedback, even when faced with ambiguity, which can make their dialogues feel static, one-sided, and lacking the adaptive qualities of human con",
    "url": "https://arxiv.org/abs/2602.16488",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "From Growing to Looping: A Unified View of Iterative Computation in LLMs",
    "summary": "arXiv:2602.16490v1 Announce Type: cross Abstract: Looping, reusing a block of layers across depth, and depth growing, training shallow-to-deep models by duplicating middle layers, have both been linked to stronger reasoning, but their relationship remains unclear. We provide a mechanistic unification: looped and depth-grown models exhibit convergen",
    "url": "https://arxiv.org/abs/2602.16490",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Fast and Scalable Analytical Diffusion",
    "summary": "arXiv:2602.16498v1 Announce Type: cross Abstract: Analytical diffusion models offer a mathematically transparent path to generative modeling by formulating the denoising score as an empirical-Bayes posterior mean. However, this interpretability comes at a prohibitive cost: the standard formulation necessitates a full-dataset scan at every timestep,",
    "url": "https://arxiv.org/abs/2602.16498",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Interpretability-by-Design with Accurate Locally Additive Models and Conditional Feature Effects",
    "summary": "arXiv:2602.16503v1 Announce Type: cross Abstract: Generalized additive models (GAMs) offer interpretability through independent univariate feature effects but underfit when interactions are present in data. GA$^2$Ms add selected pairwise interactions which improves accuracy, but sacrifices interpretability and limits model auditing. We propose \\emp",
    "url": "https://arxiv.org/abs/2602.16503",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents",
    "summary": "arXiv:2602.16520v1 Announce Type: cross Abstract: Jailbreak prompts are a practical and evolving threat to large language models (LLMs), particularly in agentic systems that execute tools over untrusted content. Many attacks exploit long-context hiding, semantic camouflage, and lightweight obfuscations that can evade single-pass guardrails. We pres",
    "url": "https://arxiv.org/abs/2602.16520",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "MerLean: An Agentic Framework for Autoformalization in Quantum Computation",
    "summary": "arXiv:2602.16554v1 Announce Type: cross Abstract: We introduce MerLean, a fully automated agentic framework for autoformalization in quantum computation. MerLean extracts mathematical statements from \\LaTeX{} source files, formalizes them into verified Lean~4 code built on Mathlib, and translates the result back into human-readable \\LaTeX{} for sem",
    "url": "https://arxiv.org/abs/2602.16554",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "AIFL: A Global Daily Streamflow Forecasting Model Using Deterministic LSTM Pre-trained on ERA5-Land and Fine-tuned on IFS",
    "summary": "arXiv:2602.16579v1 Announce Type: cross Abstract: Reliable global streamflow forecasting is essential for flood preparedness and water resource management, yet data-driven models often suffer from a performance gap when transitioning from historical reanalysis to operational forecast products. This paper introduces AIFL (Artificial Intelligence for",
    "url": "https://arxiv.org/abs/2602.16579",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows",
    "summary": "arXiv:2602.16585v1 Announce Type: cross Abstract: Operational rigor determines whether human-agent collaboration succeeds or fails. Scientific data pipelines need the equivalent of DevOps -- SciOps -- yet common approaches fragment provenance across disconnected systems without transactional guarantees. DataJoint 2.0 addresses this gap through the ",
    "url": "https://arxiv.org/abs/2602.16585",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification",
    "summary": "arXiv:2602.16590v1 Announce Type: cross Abstract: Street-view image attribute classification is a vital downstream task of image classification, enabling applications such as autonomous driving, urban analytics, and high-definition map construction. It remains computationally demanding whether training from scratch, initialising from pre-trained we",
    "url": "https://arxiv.org/abs/2602.16590",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving",
    "summary": "arXiv:2602.16603v1 Announce Type: cross Abstract: The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and ",
    "url": "https://arxiv.org/abs/2602.16603",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Explainable AI: Context-Aware Layer-Wise Integrated Gradients for Explaining Transformer Models",
    "summary": "arXiv:2602.16608v1 Announce Type: cross Abstract: Transformer models achieve state-of-the-art performance across domains and tasks, yet their deeply layered representations make their predictions difficult to interpret. Existing explainability methods rely on final-layer attributions, capture either local token-level attributions or global attentio",
    "url": "https://arxiv.org/abs/2602.16608",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Who can we trust? LLM-as-a-jury for Comparative Assessment",
    "summary": "arXiv:2602.16610v1 Announce Type: cross Abstract: Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges v",
    "url": "https://arxiv.org/abs/2602.16610",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Causal and Compositional Abstraction",
    "summary": "arXiv:2602.16612v1 Announce Type: cross Abstract: Abstracting from a low level to a more explanatory high level of description, and ideally while preserving causal structure, is fundamental to scientific practice, to causal inference problems, and to robust, efficient and interpretable AI. We present a general account of abstractions between low an",
    "url": "https://arxiv.org/abs/2602.16612",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models",
    "summary": "arXiv:2602.16626v1 Announce Type: cross Abstract: Recent success in natural language processing has motivated growing interest in large-scale foundation models for neuroimaging data. Such models often require discretization of continuous neural time series data, a process referred to as 'tokenization'. However, the impact of different tokenization ",
    "url": "https://arxiv.org/abs/2602.16626",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Almost Sure Convergence of Differential Temporal Difference Learning for Average Reward Markov Decision Processes",
    "summary": "arXiv:2602.16629v1 Announce Type: cross Abstract: The average reward is a fundamental performance metric in reinforcement learning (RL) focusing on the long-run performance of an agent. Differential temporal difference (TD) learning algorithms are a major advance for average reward RL as they provide an efficient online method to learn the value fu",
    "url": "https://arxiv.org/abs/2602.16629",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Enhanced Diffusion Sampling: Efficient Rare Event Sampling and Free Energy Calculation with Diffusion Models",
    "summary": "arXiv:2602.16634v1 Announce Type: cross Abstract: The rare-event sampling problem has long been the central limiting factor in molecular dynamics (MD), especially in biomolecular simulation. Recently, diffusion models such as BioEmu have emerged as powerful equilibrium samplers that generate independent samples from complex molecular distributions,",
    "url": "https://arxiv.org/abs/2602.16634",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Retrieval Augmented Generation of Literature-derived Polymer Knowledge: The Example of a Biodegradable Polymer Expert System",
    "summary": "arXiv:2602.16650v1 Announce Type: cross Abstract: Polymer literature contains a large and growing body of experimental knowledge, yet much of it is buried in unstructured text and inconsistent terminology, making systematic retrieval and reasoning difficult. Existing tools typically extract narrow, study-specific facts in isolation, failing to pres",
    "url": "https://arxiv.org/abs/2602.16650",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment",
    "summary": "arXiv:2602.16660v1 Announce Type: cross Abstract: The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the",
    "url": "https://arxiv.org/abs/2602.16660",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation",
    "summary": "arXiv:2602.16671v1 Announce Type: cross Abstract: Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct inte",
    "url": "https://arxiv.org/abs/2602.16671",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
    "summary": "arXiv:2602.16699v1 Announce Type: cross Abstract: LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an",
    "url": "https://arxiv.org/abs/2602.16699",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology",
    "summary": "arXiv:2602.16703v1 Announce Type: cross Abstract: Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-reg",
    "url": "https://arxiv.org/abs/2602.16703",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Policy Compiler for Secure Agentic Systems",
    "summary": "arXiv:2602.16708v1 Announce Type: cross Abstract: LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Polic",
    "url": "https://arxiv.org/abs/2602.16708",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "A Review of Fairness and A Practical Guide to Selecting Context-Appropriate Fairness Metrics in Machine Learning",
    "summary": "arXiv:2411.06624v4 Announce Type: replace Abstract: Recent regulatory proposals for artificial intelligence emphasize fairness requirements for machine learning models. However, precisely defining the appropriate measure of fairness is challenging due to philosophical, cultural and political contexts. Biases can infiltrate machine learning models i",
    "url": "https://arxiv.org/abs/2411.06624",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Scalable Precise Computation of Shannon Entropy",
    "summary": "arXiv:2502.01160v3 Announce Type: replace Abstract: Quantitative information flow analyses (QIF) are a class of techniques for measuring the amount of confidential information leaked by a program to its public outputs. Shannon entropy is an important method to quantify the amount of leakage in QIF. This paper focuses on the programs modeled in Bool",
    "url": "https://arxiv.org/abs/2502.01160",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "SurgRAW: Multi-Agent Workflow with Chain of Thought Reasoning for Robotic Surgical Video Analysis",
    "summary": "arXiv:2503.10265v2 Announce Type: replace Abstract: Robotic-assisted surgery (RAS) is central to modern surgery, driving the need for intelligent systems with accurate scene understanding. Most existing surgical AI methods rely on isolated, task-specific models, leading to fragmented pipelines with limited interpretability and no unified understand",
    "url": "https://arxiv.org/abs/2503.10265",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Large Language Models for Water Distribution Systems Modeling and Decision-Making",
    "summary": "arXiv:2503.16191v2 Announce Type: replace Abstract: The integration of Large Language Models (LLMs) into engineering workflows presents new opportunities for making computational tools more accessible. Especially where such tools remain underutilized due to technical or expertise barriers, such as water distribution system (WDS) management. This st",
    "url": "https://arxiv.org/abs/2503.16191",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "EconEvals: Benchmarks and Litmus Tests for Economic Decision-Making by LLM Agents",
    "summary": "arXiv:2503.18825v4 Announce Type: replace Abstract: We develop evaluation methods for measuring the economic decision-making capabilities and tendencies of LLMs. First, we develop benchmarks derived from key problems in economics -- procurement, scheduling, and pricing -- that test an LLM's ability to learn from the environment in context. Second, ",
    "url": "https://arxiv.org/abs/2503.18825",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "GDGB: A Benchmark for Generative Dynamic Text-Attributed Graph Learning",
    "summary": "arXiv:2507.03267v2 Announce Type: replace Abstract: Dynamic Text-Attributed Graphs (DyTAGs), which intricately integrate structural, temporal, and textual attributes, are crucial for modeling complex real-world systems. However, most existing DyTAG datasets exhibit poor textual quality, which severely limits their utility for generative DyTAG tasks",
    "url": "https://arxiv.org/abs/2507.03267",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Language and Experience: A Computational Model of Social Learning in Complex Tasks",
    "summary": "arXiv:2509.00074v2 Announce Type: replace Abstract: The ability to combine linguistic guidance from others with direct experience is central to human development, enabling safe and rapid learning in new environments. How do people integrate these two sources of knowledge, and how might AI systems? We present a computational framework that models so",
    "url": "https://arxiv.org/abs/2509.00074",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models",
    "summary": "arXiv:2509.24803v2 Announce Type: replace Abstract: Recent advances in multimodal time series learning underscore a paradigm shift from analytics centered on basic patterns toward advanced time series understanding and reasoning. However, existing multimodal time series datasets mostly remain at the level of surface alignment and question answering",
    "url": "https://arxiv.org/abs/2509.24803",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing",
    "summary": "arXiv:2510.12121v2 Announce Type: replace Abstract: Precise attribute intensity control--generating Large Language Model (LLM) outputs with specific, user-defined attribute intensities--is crucial for AI systems adaptable to diverse user expectations. Current LLM alignment methods, however, typically provide only directional or open-ended guidance,",
    "url": "https://arxiv.org/abs/2510.12121",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning",
    "summary": "arXiv:2510.18318v4 Announce Type: replace Abstract: Geospatial data offers immense potential for understanding our planet. However, the sheer volume and diversity of this data along with its varied resolutions, timescales, and sparsity pose significant challenges for thorough analysis and interpretation. This paper introduces Earth AI, a family of ",
    "url": "https://arxiv.org/abs/2510.18318",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "CaveAgent: Transforming LLMs into Stateful Runtime Operators",
    "summary": "arXiv:2601.01569v2 Announce Type: replace Abstract: LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms that struggle with long-horizon tasks due to fragile multi-turn dependencies and context drift. We present CaveAgent, a framework that shifts tool use from ",
    "url": "https://arxiv.org/abs/2601.01569",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "DIAGPaper: Diagnosing Valid and Specific Weaknesses in Scientific Papers via Multi-Agent Reasoning",
    "summary": "arXiv:2601.07611v2 Announce Type: replace Abstract: Paper weakness identification using single-agent or multi-agent LLMs has attracted increasing attention, yet existing approaches exhibit key limitations. Many multi-agent systems simulate human roles at a surface level, missing the underlying criteria that lead experts to assess complementary inte",
    "url": "https://arxiv.org/abs/2601.07611",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent",
    "summary": "arXiv:2602.00663v2 Announce Type: replace Abstract: Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, ",
    "url": "https://arxiv.org/abs/2602.00663",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents",
    "summary": "arXiv:2602.02050v2 Announce Type: replace Abstract: Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing ",
    "url": "https://arxiv.org/abs/2602.02050",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "VERA-MH: Reliability and Validity of an Open-Source AI Safety Evaluation in Mental Health",
    "summary": "arXiv:2602.05088v3 Announce Type: replace Abstract: Millions now use generative AI chatbots for psychological support. Despite the promise related to availability and scale, the single most pressing question in AI for mental health is whether these tools are safe. The Validation of Ethical and Responsible AI in Mental Health (VERA-MH) evaluation wa",
    "url": "https://arxiv.org/abs/2602.05088",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "AgentNoiseBench: Benchmarking Robustness of Tool-Using LLM Agents Under Noisy Condition",
    "summary": "arXiv:2602.11348v2 Announce Type: replace Abstract: Recent advances in large language models have enabled LLM-based agents to achieve strong performance on a variety of benchmarks. However, their performance in real-world deployments often that observed on benchmark settings, especially in complex and imperfect environments. This discrepancy largel",
    "url": "https://arxiv.org/abs/2602.11348",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v2 Announce Type: replace Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decisio",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "ForesightSafety Bench: A Frontier Risk Evaluation and Governance Framework towards Safe AI",
    "summary": "arXiv:2602.14135v2 Announce Type: replace Abstract: Rapidly evolving AI exhibits increasingly strong autonomy and goal-directed capabilities, accompanied by derivative systemic risks that are more unpredictable, difficult to control, and potentially irreversible. However, current AI safety evaluation systems suffer from critical limitations such as",
    "url": "https://arxiv.org/abs/2602.14135",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Evaluating Language Model Agency through Negotiations",
    "summary": "arXiv:2401.04536v3 Announce Type: replace-cross Abstract: We introduce an approach to evaluate language model (LM) agency using negotiation games. This approach better reflects real-world use cases and addresses some of the shortcomings of alternative LM benchmarks. Negotiation games enable us to study multi-turn, and cross-model interactions, modu",
    "url": "https://arxiv.org/abs/2401.04536",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Prompt When the Animal is: Temporal Animal Behavior Grounding with Positional Recovery Training",
    "summary": "arXiv:2405.05523v2 Announce Type: replace-cross Abstract: Temporal grounding is crucial in multimodal learning, but it poses challenges when applied to animal behavior data due to the sparsity and uniform distribution of moments. To address these challenges, we propose a novel Positional Recovery Training framework (Port), which prompts the model w",
    "url": "https://arxiv.org/abs/2405.05523",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Ctrl-GenAug: Controllable Generative Augmentation for Medical Sequence Classification",
    "summary": "arXiv:2409.17091v3 Announce Type: replace-cross Abstract: In the medical field, the limited availability of large-scale datasets and labor-intensive annotation processes hinder the performance of deep models. Diffusion-based generative augmentation approaches present a promising solution to this issue, having been proven effective in advancing down",
    "url": "https://arxiv.org/abs/2409.17091",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "MC-LLaVA: Multi-Concept Personalized Vision-Language Model",
    "summary": "arXiv:2411.11706v4 Announce Type: replace-cross Abstract: Current vision-language models (VLMs) show exceptional abilities across diverse tasks, such as visual question answering. To enhance user experience, recent studies have investigated VLM personalization to understand user-provided concepts. However, they mainly focus on single concepts, negl",
    "url": "https://arxiv.org/abs/2411.11706",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics",
    "summary": "arXiv:2411.16537v5 Announce Type: replace-cross Abstract: Spatial understanding is a crucial capability that enables robots to perceive their surroundings, reason about their environment, and interact with it meaningfully. In modern robotics, these capabilities are increasingly provided by vision-language models. However, these models face signific",
    "url": "https://arxiv.org/abs/2411.16537",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Cocoa: Co-Planning and Co-Execution with AI Agents",
    "summary": "arXiv:2412.10999v4 Announce Type: replace-cross Abstract: As AI agents take on increasingly long-running tasks involving sophisticated planning and execution, there is a corresponding need for novel interaction designs that enable deeper human-agent collaboration. However, most prior works leverage human interaction to fix \"autonomous\" workflows th",
    "url": "https://arxiv.org/abs/2412.10999",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for Text-to-Image Models",
    "summary": "arXiv:2501.03544v4 Announce Type: replace-cross Abstract: Recent text-to-image (T2I) models have exhibited remarkable performance in generating high-quality images from text descriptions. However, these models are vulnerable to misuse, particularly generating not-safe-for-work (NSFW) content, such as sexually explicit, violent, political, and distu",
    "url": "https://arxiv.org/abs/2501.03544",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Adaptive Rank Allocation for Federated Parameter-Efficient Fine-Tuning of Language Models",
    "summary": "arXiv:2501.14406v4 Announce Type: replace-cross Abstract: Pre-trained Language Models (PLMs) have demonstrated their superiority and versatility in modern Natural Language Processing (NLP), effectively adapting to various downstream tasks through further fine-tuning. Federated Parameter-Efficient Fine-Tuning (FedPEFT) has emerged as a promising sol",
    "url": "https://arxiv.org/abs/2501.14406",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs",
    "summary": "arXiv:2501.16534v5 Announce Type: replace-cross Abstract: Alignment in large language models (LLMs) is used to enforce guidelines such as safety. Yet, alignment fails in the face of jailbreak attacks that modify inputs to induce unsafe outputs. In this paper, we introduce and evaluate a new technique for jailbreak attacks. We observe that alignment",
    "url": "https://arxiv.org/abs/2501.16534",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Understanding Transformer Optimization via Gradient Heterogeneity",
    "summary": "arXiv:2502.00213v4 Announce Type: replace-cross Abstract: Transformers are difficult to optimize with stochastic gradient descent (SGD) and largely rely on adaptive optimizers such as Adam. Despite their empirical success, the reasons behind Adam's superior performance over SGD remain poorly understood. In this study, we analyze the optimization of",
    "url": "https://arxiv.org/abs/2502.00213",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Forget Forgetting: Continual Learning in a World of Abundant Memory",
    "summary": "arXiv:2502.07274v5 Announce Type: replace-cross Abstract: Continual learning (CL) has traditionally focused on minimizing exemplar memory, a constraint often misaligned with modern systems where GPU time, not storage, is the primary bottleneck. This paper challenges this paradigm by investigating a more realistic regime: one where memory is abundan",
    "url": "https://arxiv.org/abs/2502.07274",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "FOCUS on Contamination: Hydrology-Informed Noise-Aware Learning for Geospatial PFAS Mapping",
    "summary": "arXiv:2502.14894v4 Announce Type: replace-cross Abstract: Per- and polyfluoroalkyl substances (PFAS) are persistent environmental contaminants with significant public health impacts, yet large-scale monitoring remains severely limited due to the high cost and logistical challenges of field sampling. The lack of samples leads to difficulty simulatin",
    "url": "https://arxiv.org/abs/2502.14894",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "A Survey: Spatiotemporal Consistency in Video Generation",
    "summary": "arXiv:2502.17863v2 Announce Type: replace-cross Abstract: Video generation aims to produce temporally coherent sequences of visual frames, representing a pivotal advancement in Artificial Intelligence Generated Content (AIGC). Compared to static image generation, video generation poses unique challenges: it demands not only high-quality individual ",
    "url": "https://arxiv.org/abs/2502.17863",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances Rare Disease Diagnosis from Clinical Notes",
    "summary": "arXiv:2503.12286v2 Announce Type: replace-cross Abstract: Background: Several studies show that large language models (LLMs) struggle with phenotype-driven gene prioritization for rare diseases. These studies typically use Human Phenotype Ontology (HPO) terms to prompt foundation models like GPT and LLaMA to predict candidate genes. However, in rea",
    "url": "https://arxiv.org/abs/2503.12286",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning with Large Language Models",
    "summary": "arXiv:2504.00869v2 Announce Type: replace-cross Abstract: Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation",
    "url": "https://arxiv.org/abs/2504.00869",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "FedEFC: Federated Learning Using Enhanced Forward Correction Against Noisy Labels",
    "summary": "arXiv:2504.05615v3 Announce Type: replace-cross Abstract: Federated Learning (FL) is a powerful framework for privacy-preserving distributed learning. It enables multiple clients to collaboratively train a global model without sharing raw data. However, handling noisy labels in FL remains a major challenge due to heterogeneous data distributions an",
    "url": "https://arxiv.org/abs/2504.05615",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "FindAnything: Open-Vocabulary and Object-Centric Mapping for Robot Exploration in Any Environment",
    "summary": "arXiv:2504.08603v3 Announce Type: replace-cross Abstract: Geometrically accurate and semantically expressive map representations have proven invaluable for robot deployment and task planning in unknown environments. Nevertheless, real-time, open-vocabulary semantic understanding of large-scale unknown environments still presents open challenges, ma",
    "url": "https://arxiv.org/abs/2504.08603",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis",
    "summary": "arXiv:2504.19223v4 Announce Type: replace-cross Abstract: Spectral imaging offers promising applications across diverse domains, including medicine and urban scene understanding, and is already established as a critical modality in remote sensing. However, variability in channel dimensionality and captured wavelengths among spectral cameras impede ",
    "url": "https://arxiv.org/abs/2504.19223",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Modeling Human Behavior in a Strategic Network Game with Complex Group Dynamics",
    "summary": "arXiv:2505.03795v3 Announce Type: replace-cross Abstract: Human networks greatly impact important societal outcomes, including wealth and health inequality, poverty, and bullying. As such, understanding human networks is critical to learning how to promote favorable societal outcomes. As a step toward better understanding human networks, we compare",
    "url": "https://arxiv.org/abs/2505.03795",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "PLAICraft: Large-Scale Time-Aligned Vision-Speech-Action Dataset for Embodied AI",
    "summary": "arXiv:2505.12707v2 Announce Type: replace-cross Abstract: Advances in deep generative modeling have made it increasingly plausible to train human-level embodied agents. Yet progress has been limited by the absence of large-scale, real-time, multi-modal, and socially interactive datasets that reflect the sensory-motor complexity of natural environme",
    "url": "https://arxiv.org/abs/2505.12707",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models",
    "summary": "arXiv:2505.15801v4 Announce Type: replace-cross Abstract: Large reasoning models such as OpenAI o1 and DeepSeek-R1 have demonstrated remarkable performance in complex reasoning tasks. A critical component of their training is the incorporation of reference-based reward systems within reinforcement learning (RL), where model outputs are evaluated ag",
    "url": "https://arxiv.org/abs/2505.15801",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference",
    "summary": "arXiv:2505.19427v2 Announce Type: replace-cross Abstract: The growing computational demands of large language models (LLMs) make efficient inference and activation strategies increasingly critical. While recent approaches, such as Mixture-of-Experts (MoE), leverage selective activation but require specialized training, training-free sparse activati",
    "url": "https://arxiv.org/abs/2505.19427",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Experience-based Knowledge Correction for Robust Planning in Minecraft",
    "summary": "arXiv:2505.24157v3 Announce Type: replace-cross Abstract: Large Language Model (LLM)-based planning has advanced embodied agents in long-horizon environments such as Minecraft, where acquiring latent knowledge of goal (or item) dependencies and feasible actions is critical. However, LLMs often begin with flawed priors and fail to correct them throu",
    "url": "https://arxiv.org/abs/2505.24157",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency",
    "summary": "arXiv:2506.08822v2 Announce Type: replace-cross Abstract: Generative modeling-based visuomotor policies have been widely adopted in robotic manipulation, attributed to their ability to model multimodal action distributions. However, the high inference cost of multi-step sampling limits its applicability in real-time robotic systems. Existing approa",
    "url": "https://arxiv.org/abs/2506.08822",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "DiffusionBlocks: Block-wise Neural Network Training via Diffusion Interpretation",
    "summary": "arXiv:2506.14202v3 Announce Type: replace-cross Abstract: End-to-end backpropagation requires storing activations throughout all layers, creating memory bottlenecks that limit model scalability. Existing block-wise training methods offer means to alleviate this problem, but they rely on ad-hoc local objectives and remain largely unexplored beyond c",
    "url": "https://arxiv.org/abs/2506.14202",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic",
    "summary": "arXiv:2506.23875v3 Announce Type: replace-cross Abstract: The chain of thought, i.e., step-by-step reasoning, is one of the fundamental mechanisms of Transformers. While the design of intermediate reasoning steps has been extensively studied and shown to critically influence performance on mathematical, multi-step reasoning tasks, the ordering of t",
    "url": "https://arxiv.org/abs/2506.23875",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Expressive Power of Graph Transformers via Logic",
    "summary": "arXiv:2508.01067v2 Announce Type: replace-cross Abstract: Transformers are the basis of modern large language models, but relatively little is known about their precise expressive power on graphs. We study the expressive power of graph transformers (GTs) by Dwivedi and Bresson (2020) and GPS-networks by Ramp\\'asek et al. (2022), both under soft-att",
    "url": "https://arxiv.org/abs/2508.01067",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Model-Agnostic Dynamic Feature Selection with Uncertainty Quantification",
    "summary": "arXiv:2508.02566v3 Announce Type: replace-cross Abstract: Dynamic feature selection (DFS) addresses budget constraints in decision-making by sequentially acquiring features for each instance, making it appealing for resource-limited scenarios. However, existing DFS methods require models specifically designed for the sequential acquisition setting,",
    "url": "https://arxiv.org/abs/2508.02566",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision",
    "summary": "arXiv:2508.08177v3 Announce Type: replace-cross Abstract: Accurately grounding regions of interest (ROIs) is critical for diagnosis and treatment planning in medical imaging. While multimodal large language models (MLLMs) combine visual perception with natural language, current medical-grounding pipelines still rely on supervised fine-tuning with e",
    "url": "https://arxiv.org/abs/2508.08177",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Pinet: Optimizing hard-constrained neural networks with orthogonal projection layers",
    "summary": "arXiv:2508.10480v2 Announce Type: replace-cross Abstract: We introduce an output layer for neural networks that ensures satisfaction of convex constraints. Our approach, $\\Pi$net, leverages operator splitting for rapid and reliable projections in the forward pass, and the implicit function theorem for backpropagation. We deploy $\\Pi$net as a feasib",
    "url": "https://arxiv.org/abs/2508.10480",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "FairTabGen: High-Fidelity and Fair Synthetic Health Data Generation from Limited Samples",
    "summary": "arXiv:2508.11810v2 Announce Type: replace-cross Abstract: Synthetic healthcare data generation offers a promising solution to research limitations in clinical settings caused by privacy and regulatory constraints. However, current synthetic data generation approaches require specialized knowledge about training generative models and require high co",
    "url": "https://arxiv.org/abs/2508.11810",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "COGITAO: A Visual Reasoning Framework To Study Compositionality & Generalization",
    "summary": "arXiv:2509.05249v2 Announce Type: replace-cross Abstract: The ability to compose learned concepts and apply them in novel settings is key to human intelligence, but remains a persistent limitation in state-of-the-art machine learning models. To address this issue, we introduce COGITAO, a modular and extensible data generation framework and benchmar",
    "url": "https://arxiv.org/abs/2509.05249",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Software Dependencies 2.0: An Empirical Study of Reuse and Integration of Pre-Trained Models in Open-Source Projects",
    "summary": "arXiv:2509.06085v2 Announce Type: replace-cross Abstract: Pre-trained models (PTMs) are machine learning models that have been trained in advance, often on large-scale data, and can be reused for new tasks, thereby reducing the need for costly training from scratch. Their widespread adoption introduces a new class of software dependency, which we t",
    "url": "https://arxiv.org/abs/2509.06085",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "PolicyPad: Collaborative Prototyping of LLM Policies",
    "summary": "arXiv:2509.19680v2 Announce Type: replace-cross Abstract: As LLMs gain adoption in high-stakes domains like mental health, domain experts are increasingly consulted to provide input into policies governing their behavior. From an observation of 19 policymaking workshops with 9 experts over 15 weeks, we identified opportunities to better support rap",
    "url": "https://arxiv.org/abs/2509.19680",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "FeatBench: Towards More Realistic Evaluation of Feature-level Code Generation",
    "summary": "arXiv:2509.22237v2 Announce Type: replace-cross Abstract: Evaluating Large Language Models (LLMs) on repository-level feature implementation is a critical frontier in software engineering. However, establishing a benchmark that faithfully mirrors realistic development scenarios remains a significant challenge. Existing feature-level benchmarks gene",
    "url": "https://arxiv.org/abs/2509.22237",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Predicting Training Re-evaluation Curves Enables Effective Data Curriculums for LLMs",
    "summary": "arXiv:2509.25380v2 Announce Type: replace-cross Abstract: Data curriculums have become central to successful LLM training, yet principles governing optimal data placement remain unclear. We introduce the *training re-evaluation curve (TREC)*, a diagnostic that retrospectively evaluates training batches *using the final model weights*. The TREC char",
    "url": "https://arxiv.org/abs/2509.25380",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Multilingual Routing in Mixture-of-Experts",
    "summary": "arXiv:2510.04694v2 Announce Type: replace-cross Abstract: Mixture-of-Experts (MoE) architectures have become the key to scaling modern LLMs, yet little is understood about how their sparse routing dynamics respond to multilingual data. In this work, we analyze expert routing patterns using parallel multilingual datasets and present highly interpret",
    "url": "https://arxiv.org/abs/2510.04694",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "StarEmbed: Benchmarking Time Series Foundation Models on Astronomical Observations of Variable Stars",
    "summary": "arXiv:2510.06200v3 Announce Type: replace-cross Abstract: Time series foundation models (TSFMs) are increasingly being adopted as highly-capable general-purpose time series representation learners. Although their training corpora are vast, they exclude astronomical time series data. Observations of stars produce peta-scale time series with unique c",
    "url": "https://arxiv.org/abs/2510.06200",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Lossless Vocabulary Reduction for Auto-Regressive Language Models",
    "summary": "arXiv:2510.08102v2 Announce Type: replace-cross Abstract: Tokenization -- the process of decomposing a given text into a sequence of subwords called tokens -- is one of the key components in the development of language models. Particularly, auto-regressive language models generate texts token by token, i.e., by predicting the next-token distributio",
    "url": "https://arxiv.org/abs/2510.08102",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D Reconstruction",
    "summary": "arXiv:2510.12768v2 Announce Type: replace-cross Abstract: Reconstructing dynamic 3D scenes from monocular input is fundamentally under-constrained, with ambiguities arising from occlusion and extreme novel views. While dynamic Gaussian Splatting offers an efficient representation, vanilla models optimize all Gaussian primitives uniformly, ignoring ",
    "url": "https://arxiv.org/abs/2510.12768",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "GENESIS: A Generative Model of Episodic-Semantic Interaction",
    "summary": "arXiv:2510.15828v2 Announce Type: replace-cross Abstract: A central challenge in cognitive neuroscience is to explain how semantic and episodic memory, two major forms of declarative memory, typically associated with cortical and hippocampal processing, interact to support learning, recall, and imagination. Despite significant advances, we still la",
    "url": "https://arxiv.org/abs/2510.15828",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "CreativityPrism: A Holistic Evaluation Framework for Large Language Model Creativity",
    "summary": "arXiv:2510.20091v2 Announce Type: replace-cross Abstract: Creativity is often seen as a hallmark of human intelligence. While large language models (LLMs) are increasingly perceived as generating creative text, there is still no holistic and scalable framework to evaluate their creativity across diverse scenarios. Existing methods of LLM creativity",
    "url": "https://arxiv.org/abs/2510.20091",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Transformers can do Bayesian Clustering",
    "summary": "arXiv:2510.24318v3 Announce Type: replace-cross Abstract: Bayesian clustering accounts for uncertainty but is computationally demanding at scale. Furthermore, real-world datasets often contain missing values, and simple imputation ignores the associated uncertainty, resulting in suboptimal results. We present Cluster-PFN, a Transformer-based model ",
    "url": "https://arxiv.org/abs/2510.24318",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Q3R: Quadratic Reweighted Rank Regularizer for Effective Low-Rank Training",
    "summary": "arXiv:2511.04485v2 Announce Type: replace-cross Abstract: Parameter-efficient training based on low-rank optimization has become a highly successful tool for fine-tuning large deep learning models. However, these methods often fail for low-rank pre-training, where simultaneously maintaining low-rank weight structure and optimizing the task objectiv",
    "url": "https://arxiv.org/abs/2511.04485",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Reasoning Up the Instruction Ladder for Controllable Language Models",
    "summary": "arXiv:2511.04694v4 Announce Type: replace-cross Abstract: As large language model (LLM) based systems take on high-stakes roles in real-world decision-making, they must reconcile competing instructions from multiple sources (e.g., model developers, users, and tools) within a single prompt context. Thus, enforcing an instruction hierarchy (IH) in LL",
    "url": "https://arxiv.org/abs/2511.04694",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Mastering Olympiad-Level Physics with Artificial Intelligence",
    "summary": "arXiv:2511.10515v2 Announce Type: replace-cross Abstract: Olympiad-level physics problem-solving significantly challenges both humans and artificial intelligence (AI), as it requires integrating appropriate modeling, application of physical principles, and precise calculation within long reasoning processes. In this paper, we introduce LOCA (LOgica",
    "url": "https://arxiv.org/abs/2511.10515",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Language-Guided Invariance Probing of Vision-Language Models",
    "summary": "arXiv:2511.13494v2 Announce Type: replace-cross Abstract: Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) i",
    "url": "https://arxiv.org/abs/2511.13494",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Refined Bayesian Optimization for Efficient Beam Alignment in Intelligent Indoor Wireless Environments",
    "summary": "arXiv:2512.00036v2 Announce Type: replace-cross Abstract: Future intelligent indoor wireless environments require fast and reliable beam alignment to sustain high-throughput links under mobility and blockage. Exhaustive beam training achieves optimal performance but is prohibitively costly. In indoor settings, dense scatterers and transceiver hardw",
    "url": "https://arxiv.org/abs/2512.00036",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Mixture-of-Experts as Soft Clustering: A Dual Jacobian-PCA Spectral Geometry Perspective",
    "summary": "arXiv:2601.11616v2 Announce Type: replace-cross Abstract: Mixture-of-Experts (MoE) architectures are widely used for efficiency and conditional computation, but their effect on the geometry of learned functions and representations remains poorly understood. We study MoEs through a geometric lens, interpreting routing as soft partitioning into overl",
    "url": "https://arxiv.org/abs/2601.11616",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "StableQAT: Stable Quantization-Aware Training at Ultra-Low Bitwidths",
    "summary": "arXiv:2601.19320v2 Announce Type: replace-cross Abstract: Quantization-aware training (QAT) is essential for deploying large models under strict memory and latency constraints, yet achieving stable and robust optimization at ultra-low bitwidths remains challenging. Common approaches based on the straight-through estimator (STE) or soft quantizers o",
    "url": "https://arxiv.org/abs/2601.19320",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Cardinality-Preserving Attention Channels for Graph Transformers in Molecular Property Prediction",
    "summary": "arXiv:2602.02201v5 Announce Type: replace-cross Abstract: Molecular property prediction is crucial for drug discovery when labeled data are scarce. This work presents CardinalGraphFormer, a graph transformer augmented with a query-conditioned cardinality-preserving attention (CPA) channel that retains dynamic support-size signals complementary to s",
    "url": "https://arxiv.org/abs/2602.02201",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?",
    "summary": "arXiv:2602.05023v2 Announce Type: replace-cross Abstract: Vision-language models (VLMs) have demonstrated strong performance in image geolocation, a capability further sharpened by frontier multimodal large reasoning models (MLRMs). This poses a significant privacy risk, as these widely accessible models can be exploited to infer sensitive location",
    "url": "https://arxiv.org/abs/2602.05023",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Protean Compiler: An Agile Framework to Drive Fine-grain Phase Ordering",
    "summary": "arXiv:2602.06142v2 Announce Type: replace-cross Abstract: The phase ordering problem has been a long-standing challenge since the late 1970s, yet it remains an open problem due to having a vast optimization space and an unbounded nature, making it an open-ended problem without a finite solution, one can limit the scope by reducing the number and th",
    "url": "https://arxiv.org/abs/2602.06142",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Vision and Language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning",
    "summary": "arXiv:2602.07680v2 Announce Type: replace-cross Abstract: Vision-language models (VLMs) have recently emerged as powerful representation learning systems that align visual observations with natural language concepts, offering new opportunities for semantic reasoning in safety-critical autonomous driving. This paper investigates how vision-language ",
    "url": "https://arxiv.org/abs/2602.07680",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "When Models Examine Themselves: Vocabulary-Activation Correspondence in Self-Referential Processing",
    "summary": "arXiv:2602.11358v2 Announce Type: replace-cross Abstract: Large language models produce rich introspective language when prompted for self-examination, but whether this language reflects internal computation or sophisticated confabulation has remained unclear. We show that self-referential vocabulary tracks concurrent activation dynamics, and that ",
    "url": "https://arxiv.org/abs/2602.11358",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "VIRENA: Virtual Arena for Research, Education, and Democratic Innovation",
    "summary": "arXiv:2602.12207v2 Announce Type: replace-cross Abstract: Digital platforms shape how people communicate, deliberate, and form opinions. Studying these dynamics has become increasingly difficult due to restricted data access, ethical constraints on real-world experiments, and limitations of existing research tools. VIRENA (Virtual Arena) is a platf",
    "url": "https://arxiv.org/abs/2602.12207",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment",
    "summary": "arXiv:2602.12281v2 Announce Type: replace-cross Abstract: The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions.",
    "url": "https://arxiv.org/abs/2602.12281",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Knowledge-Based Design Requirements for Generative Social Robots in Higher Education",
    "summary": "arXiv:2602.12873v2 Announce Type: replace-cross Abstract: Generative social robots (GSRs) powered by large language models enable adaptive, conversational tutoring but also introduce risks such as hallucinations, overreliance, and privacy violations. Existing frameworks for educational technologies and responsible AI primarily define desired behavi",
    "url": "https://arxiv.org/abs/2602.12873",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Semantic Chunking and the Entropy of Natural Language",
    "summary": "arXiv:2602.13194v2 Announce Type: replace-cross Abstract: The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per characte",
    "url": "https://arxiv.org/abs/2602.13194",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Learning to Select Like Humans: Explainable Active Learning for Medical Imaging",
    "summary": "arXiv:2602.13308v2 Announce Type: replace-cross Abstract: Medical image analysis requires substantial labeled data for model training, yet expert annotation is expensive and time-consuming. Active learning (AL) addresses this challenge by strategically selecting the most informative samples for the annotation purpose, but traditional methods solely",
    "url": "https://arxiv.org/abs/2602.13308",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Arming Data Agents with Tribal Knowledge",
    "summary": "arXiv:2602.13521v2 Announce Type: replace-cross Abstract: Natural language to SQL (NL2SQL) translation enables non-expert users to query relational databases through natural language. Recently, NL2SQL agents, powered by the reasoning capabilities of Large Language Models (LLMs), have significantly advanced NL2SQL translation. Nonetheless, NL2SQL ag",
    "url": "https://arxiv.org/abs/2602.13521",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook",
    "summary": "arXiv:2602.14299v2 Announce Type: replace-cross Abstract: As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonom",
    "url": "https://arxiv.org/abs/2602.14299",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "A Geometric Analysis of Small-sized Language Model Hallucinations",
    "summary": "arXiv:2602.14778v2 Announce Type: replace-cross Abstract: Hallucinations -- fluent but factually incorrect responses -- pose a major challenge to the reliability of language models, especially in multi-step or agentic settings. This work investigates hallucinations in small-sized LLMs through a geometric perspective, starting from the hypothesis th",
    "url": "https://arxiv.org/abs/2602.14778",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Indic-TunedLens: Interpreting Multilingual Models in Indian Languages",
    "summary": "arXiv:2602.15038v2 Announce Type: replace-cross Abstract: Multilingual large language models (LLMs) are increasingly deployed in linguistically diverse regions like India, yet most interpretability tools remain tailored to English. Prior work reveals that LLMs often operate in English centric representation spaces, making cross lingual interpretabi",
    "url": "https://arxiv.org/abs/2602.15038",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Weight space Detection of Backdoors in LoRA Adapters",
    "summary": "arXiv:2602.15195v2 Announce Type: replace-cross Abstract: LoRA adapters let users fine-tune large language models (LLMs) efficiently. However, LoRA adapters are shared through open repositories like Hugging Face Hub \\citep{huggingface_hub_docs}, making them vulnerable to backdoor attacks. Current detection methods require running the model with tes",
    "url": "https://arxiv.org/abs/2602.15195",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Closing the Distribution Gap in Adversarial Training for LLMs",
    "summary": "arXiv:2602.15238v2 Announce Type: replace-cross Abstract: Adversarial training for LLMs is one of the most promising methods to reliably improve robustness against adversaries. However, despite significant progress, models remain vulnerable to simple in-distribution exploits, such as rewriting prompts in the past tense or translating them into othe",
    "url": "https://arxiv.org/abs/2602.15238",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "High-Fidelity Network Management for Federated AI-as-a-Service: Cross-Domain Orchestration",
    "summary": "arXiv:2602.15281v2 Announce Type: replace-cross Abstract: To support the emergence of AI-as-a-Service (AIaaS), communication service providers (CSPs) are on the verge of a radical transformation-from pure connectivity providers to AIaaS a managed network service (control-and-orchestration plane that exposes AI models). In this model, the CSP is res",
    "url": "https://arxiv.org/abs/2602.15281",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "AI-Paging: Lease-Based Execution Anchoring for Network-Exposed AI-as-a-Service",
    "summary": "arXiv:2602.15286v2 Announce Type: replace-cross Abstract: With AI-as-a-Service (AIaaS) now deployed across multiple providers and model tiers, selecting the appropriate model instance at run time is increasingly outside the end user's knowledge and operational control. Accordingly, the 6G service providers are envisioned to play a crucial role in e",
    "url": "https://arxiv.org/abs/2602.15286",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Far Out: Evaluating Language Models on Slang in Australian and Indian English",
    "summary": "arXiv:2602.15373v2 Announce Type: replace-cross Abstract: Language models exhibit systematic performance gaps when processing text in non-standard language varieties, yet their ability to comprehend variety-specific slang remains underexplored for several languages. We present a comprehensive evaluation of slang awareness in Indian English (en-IN) ",
    "url": "https://arxiv.org/abs/2602.15373",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "SecCodeBench-V2 Technical Report",
    "summary": "arXiv:2602.15485v2 Announce Type: replace-cross Abstract: We introduce SecCodeBench-V2, a publicly released benchmark for evaluating Large Language Model (LLM) copilots' capabilities of generating secure code. SecCodeBench-V2 comprises 98 generation and fix scenarios derived from Alibaba Group's industrial productions, where the underlying security",
    "url": "https://arxiv.org/abs/2602.15485",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens",
    "summary": "arXiv:2602.15620v2 Announce Type: replace-cross Abstract: Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often suffer from late-stage performance col",
    "url": "https://arxiv.org/abs/2602.15620",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models",
    "summary": "arXiv:2602.15689v2 Announce Type: replace-cross Abstract: Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. ",
    "url": "https://arxiv.org/abs/2602.15689",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00"
  },
  {
    "title": "Media Authenticity Methods in Practice: Capabilities, Limitations, and Directions",
    "summary": "As synthetic media grows, verifying what\u2019s real, and the origin of content, matters more than ever. Our latest report explores media integrity and authentication methods, their limits, and practical paths toward trustworthy provenance across images, audio, and video. The post Media Authenticity Methods in Practice: Capabilities, Limitations, and Di",
    "url": "https://www.microsoft.com/en-us/research/blog/media-authenticity-methods-in-practice-capabilities-limitations-and-directions/",
    "source": "Microsoft Research",
    "published_at": "2026-02-19T16:00:51+00:00"
  },
  {
    "title": "Train AI models with Unsloth and Hugging Face Jobs for FREE",
    "summary": "",
    "url": "https://huggingface.co/blog/unsloth-jobs",
    "source": "Hugging Face Blog",
    "published_at": "2026-02-20T00:00:00+00:00"
  },
  {
    "title": "\u300c\u30c7\u30fc\u30bf\u4e0d\u8db3\u300d\u306e\u58c1\u3092\u8d8a\u3048\u308b\uff1a\u5408\u6210\u30da\u30eb\u30bd\u30ca\u304c\u65e5\u672c\u306eAI\u958b\u767a\u3092\u52a0\u901f",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/nemotron-personas-japan-nttdata-ja",
    "source": "Hugging Face Blog",
    "published_at": "2026-02-19T15:32:38+00:00"
  }
]