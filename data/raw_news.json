[
  {
    "title": "Finding value with AI and Industry 5.0 transformation",
    "summary": "For years, Industry 4.0 transformation has centered on the convergence of intelligent technologies like AI, cloud, the internet of things, robotics, and digital twins. Industry 5.0 marks a pivotal shift from integrating emerging technologies to orchestrating them at scale. With Industry 5.0, the purpose of this interconnected web of technologies is",
    "url": "https://www.technologyreview.com/2026/02/26/1133707/finding-value-with-ai-and-industry-5-0-transformation/",
    "source": "MIT Technology Review AI",
    "published_at": "2026-02-26T15:00:59+00:00"
  },
  {
    "title": "Hands-On With Nano Banana 2, the Latest Version of Google\u2019s AI Image Generator",
    "summary": "Google\u2019s latest image model, Nano Banana 2, is a powerful AI photo editor that punctures reality. Well, sometimes.",
    "url": "https://www.wired.com/story/google-nano-banana-2-ai-image-generator-hands-on/",
    "source": "Wired AI",
    "published_at": "2026-02-27T00:01:10+00:00"
  },
  {
    "title": "\u2018Uncanny Valley\u2019: Pentagon vs. \u2018Woke\u2019 Anthropic, Agentic vs. Mimetic, and Trump vs. State of the Union",
    "summary": "Our hosts unpack the news of the week, starting with the ongoing feud between Anthropic and the Pentagon. Plus: All you need to know about TAT-8 and undersea cables.",
    "url": "https://www.wired.com/story/uncanny-valley-podcast-pentagon-anthropic-agentic-mimetic-trump-state-of-the-union/",
    "source": "Wired AI",
    "published_at": "2026-02-26T23:20:36+00:00"
  },
  {
    "title": "This AI Agent Is Designed to Not Go Rogue",
    "summary": "The new open source project IronCurtain uses a unique method to secure and constrain AI assistant agents before they flip your digital life upside down.",
    "url": "https://www.wired.com/story/ironcurtain-ai-agent-security/",
    "source": "Wired AI",
    "published_at": "2026-02-26T20:54:51+00:00"
  },
  {
    "title": "How Chinese AI Chatbots Censor Themselves",
    "summary": "Researchers from Stanford and Princeton found that Chinese AI models are more likely than their Western counterparts to dodge political questions or deliver inaccurate answers.",
    "url": "https://www.wired.com/story/made-in-china-how-chinese-ai-chatbots-censor-themselves/",
    "source": "Wired AI",
    "published_at": "2026-02-26T20:08:08+00:00"
  },
  {
    "title": "Are You \u2018Agentic\u2019 Enough for the AI Era?",
    "summary": "Silicon Valley built AI coding agents that can handle most of the grunt work. Now, the most valuable skill in tech is deciding what they should do.",
    "url": "https://www.wired.com/story/silicon-valley-agentic-individuals-future-of-work/",
    "source": "Wired AI",
    "published_at": "2026-02-26T19:00:00+00:00"
  },
  {
    "title": "OpenAI Announces Major Expansion of London Office",
    "summary": "The San Francisco-based AI lab is growing its research team in London. The move puts it in direct competition with Google DeepMind for top research talent in the UK.",
    "url": "https://www.wired.com/story/openai-expands-london-office-major-research-hub/",
    "source": "Wired AI",
    "published_at": "2026-02-26T15:56:01+00:00"
  },
  {
    "title": "Who\u2019s Your Daddy? A Chatbot",
    "summary": "More people are turning to AI to explore their BDSM fantasies, but some in the community feel it\u2019s a cop-out.",
    "url": "https://www.wired.com/story/whos-your-daddy-a-chatbot/",
    "source": "Wired AI",
    "published_at": "2026-02-26T12:00:00+00:00"
  },
  {
    "title": "A Dynamic Survey of Soft Set Theory and Its Extensions",
    "summary": "arXiv:2602.21268v1 Announce Type: new Abstract: Soft set theory provides a direct framework for parameterized decision modeling by assigning to each attribute (parameter) a subset of a given universe, thereby representing uncertainty in a structured way [1, 2]. Over the past decades, the theory has expanded into numerous variants-including hypersof",
    "url": "https://arxiv.org/abs/2602.21268",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "A Hierarchical Multi-Agent System for Autonomous Discovery in Geoscientific Data Archives",
    "summary": "arXiv:2602.21351v1 Announce Type: new Abstract: The rapid accumulation of Earth science data has created a significant scalability challenge; while repositories like PANGAEA host vast collections of datasets, citation metrics indicate that a substantial portion remains underutilized, limiting data reusability. Here we present PANGAEA-GPT, a hierarc",
    "url": "https://arxiv.org/abs/2602.21351",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Beyond Refusal: Probing the Limits of Agentic Self-Correction for Semantic Sensitive Information",
    "summary": "arXiv:2602.21496v1 Announce Type: new Abstract: While defenses for structured PII are mature, Large Language Models (LLMs) pose a new threat: Semantic Sensitive Information (SemSI), where models infer sensitive identity attributes, generate reputation-harmful content, or hallucinate potentially wrong information. The capacity of LLMs to self-regula",
    "url": "https://arxiv.org/abs/2602.21496",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning",
    "summary": "arXiv:2602.21534v1 Announce Type: new Abstract: Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to large",
    "url": "https://arxiv.org/abs/2602.21534",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Power and Limitations of Aggregation in Compound AI Systems",
    "summary": "arXiv:2602.21556v1 Announce Type: new Abstract: When designing compound AI systems, a common approach is to query multiple copies of the same model and aggregate the responses to produce a synthesized output. Given the homogeneity of these models, this raises the question of whether aggregation unlocks access to a greater set of outputs than queryi",
    "url": "https://arxiv.org/abs/2602.21556",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "The ASIR Courage Model: A Phase-Dynamic Framework for Truth Transitions in Human and AI Systems",
    "summary": "arXiv:2602.21745v1 Announce Type: new Abstract: We introduce the ASIR (Awakened Shared Intelligence Relationship) Courage Model, a phase-dynamic framework that formalizes truth-disclosure as a state transition rather than a personality trait. The mode characterizes the shift from suppression (S0) to expression (S1) as occurring when facilitative fo",
    "url": "https://arxiv.org/abs/2602.21745",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "fEDM+: A Risk-Based Fuzzy Ethical Decision Making Framework with Principle-Level Explainability and Pluralistic Validation",
    "summary": "arXiv:2602.21746v1 Announce Type: new Abstract: In a previous work, we introduced the fuzzy Ethical Decision-Making framework (fEDM), a risk-based ethical reasoning architecture grounded in fuzzy logic. The original model combined a fuzzy Ethical Risk Assessment module (fERA) with ethical decision rules, enabled formal structural verification throu",
    "url": "https://arxiv.org/abs/2602.21746",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem",
    "summary": "arXiv:2602.21814v1 Announce Type: new Abstract: Large language models consistently fail the \"car wash problem,\" a viral reasoning benchmark requiring implicit physical constraint inference. We present a variable isolation study (n=20 per condition, 6 conditions, 120 total trials) examining which prompt architecture layers in a production system ena",
    "url": "https://arxiv.org/abs/2602.21814",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Distill and Align Decomposition for Enhanced Claim Verification",
    "summary": "arXiv:2602.21857v1 Announce Type: new Abstract: Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using G",
    "url": "https://arxiv.org/abs/2602.21857",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices",
    "summary": "arXiv:2602.21858v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomously an",
    "url": "https://arxiv.org/abs/2602.21858",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "2-Step Agent: A Framework for the Interaction of a Decision Maker with AI Decision Support",
    "summary": "arXiv:2602.21889v1 Announce Type: new Abstract: Across a growing number of fields, human decision making is supported by predictions from AI models. However, we still lack a deep understanding of the effects of adoption of these technologies. In this paper, we introduce a general computational framework, the 2-Step Agent, which models the effects o",
    "url": "https://arxiv.org/abs/2602.21889",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Semantic Partial Grounding via LLMs",
    "summary": "arXiv:2602.22067v1 Announce Type: new Abstract: Grounding is a critical step in classical planning, yet it often becomes a computational bottleneck due to the exponential growth in grounded actions and atoms as task size increases. Recent advances in partial grounding have addressed this challenge by incrementally grounding only the most promising ",
    "url": "https://arxiv.org/abs/2602.22067",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts",
    "summary": "arXiv:2602.22070v1 Announce Type: new Abstract: Large language models are increasingly used in decision-making tasks that require them to process information from a variety of sources, including both human experts and other algorithmic agents. How do LLMs weigh the information provided by these different sources? We consider the well-studied phenom",
    "url": "https://arxiv.org/abs/2602.22070",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Petri Net Relaxation for Infeasibility Explanation and Sequential Task Planning",
    "summary": "arXiv:2602.22094v1 Announce Type: new Abstract: Plans often change due to changes in the situation or our understanding of the situation. Sometimes, a feasible plan may not even exist, and identifying such infeasibilities is useful to determine when requirements need adjustment. Common planning approaches focus on efficient one-shot planning in fea",
    "url": "https://arxiv.org/abs/2602.22094",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Inference-time Alignment via Sparse Junction Steering",
    "summary": "arXiv:2602.21215v1 Announce Type: cross Abstract: Token-level steering has emerged as a pivotal approach for inference-time alignment, enabling fine grained control over large language models by modulating their output distributions without parameter updates. While effective, existing methods rely on dense intervention at every decoding step. This ",
    "url": "https://arxiv.org/abs/2602.21215",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "EQ-5D Classification Using Biomedical Entity-Enriched Pre-trained Language Models and Multiple Instance Learning",
    "summary": "arXiv:2602.21216v1 Announce Type: cross Abstract: The EQ-5D (EuroQol 5-Dimensions) is a standardized instrument for the evaluation of health-related quality of life. In health economics, systematic literature reviews (SLRs) depend on the correct identification of publications that use the EQ-5D, but manual screening of large volumes of scientific l",
    "url": "https://arxiv.org/abs/2602.21216",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Applied Sociolinguistic AI for Community Development (ASA-CD): A New Scientific Paradigm for Linguistically-Grounded Social Intervention",
    "summary": "arXiv:2602.21217v1 Announce Type: cross Abstract: This paper establishes Applied Sociolinguistic AI for Community Development (ASA-CD) as a novel scientific paradigm for addressing community challenges through linguistically grounded, AI-enabled intervention. ASA-CD introduces three key contributions: (1) linguistic biomarkers as computational indi",
    "url": "https://arxiv.org/abs/2602.21217",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "EPSVec: Efficient and Private Synthetic Data Generation via Dataset Vectors",
    "summary": "arXiv:2602.21218v1 Announce Type: cross Abstract: High-quality data is essential for modern machine learning, yet many valuable corpora are sensitive and cannot be freely shared. Synthetic data offers a practical substitute for downstream development, and large language models (LLMs) have emerged as powerful engines for generating it. However, exis",
    "url": "https://arxiv.org/abs/2602.21218",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Reasoning-Based Personalized Generation for Users with Sparse Data",
    "summary": "arXiv:2602.21219v1 Announce Type: cross Abstract: Large Language Model (LLM) personalization holds great promise for tailoring responses by leveraging personal context and history. However, real-world users usually possess sparse interaction histories with limited personal context, such as cold-start users in social platforms and newly registered c",
    "url": "https://arxiv.org/abs/2602.21219",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Field-Theoretic Memory for AI Agents: Continuous Dynamics for Context Preservation",
    "summary": "arXiv:2602.21220v1 Announce Type: cross Abstract: We present a memory system for AI agents that treats stored information as continuous fields governed by partial differential equations rather than discrete entries in a database. The approach draws from classical field theory: memories diffuse through semantic space, decay thermodynamically based o",
    "url": "https://arxiv.org/abs/2602.21220",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Latent Context Compilation: Distilling Long Context into Compact Portable Memory",
    "summary": "arXiv:2602.21221v1 Announce Type: cross Abstract: Efficient long-context LLM deployment is stalled by a dichotomy between amortized compression, which struggles with out-of-distribution generalization, and Test-Time Training, which incurs prohibitive synthetic data costs and requires modifying model weights, creating stateful parameters that compli",
    "url": "https://arxiv.org/abs/2602.21221",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Task-Aware LoRA Adapter Composition via Similarity Retrieval in Vector Databases",
    "summary": "arXiv:2602.21222v1 Announce Type: cross Abstract: Parameter efficient fine tuning methods like LoRA have enabled task specific adaptation of large language models, but efficiently composing multiple specialized adapters for unseen tasks remains challenging. We present a novel framework for dynamic LoRA adapter composition that leverages similarity ",
    "url": "https://arxiv.org/abs/2602.21222",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Measuring Pragmatic Influence in Large Language Model Instructions",
    "summary": "arXiv:2602.21223v1 Announce Type: cross Abstract: It is not only what we ask large language models (LLMs) to do that matters, but also how we prompt. Phrases like \"This is urgent\" or \"As your supervisor\" can shift model behavior without altering task content. We study this effect as pragmatic framing, contextual cues that shape directive interpreta",
    "url": "https://arxiv.org/abs/2602.21223",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Make Every Draft Count: Hidden State based Speculative Decoding",
    "summary": "arXiv:2602.21224v1 Announce Type: cross Abstract: Speculative decoding has emerged as a pivotal technique to accelerate LLM inference by employing a lightweight draft model to generate candidate tokens that are subsequently verified by the target model in parallel. However, while this paradigm successfully increases the arithmetic intensity of memo",
    "url": "https://arxiv.org/abs/2602.21224",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Architecture-Agnostic Curriculum Learning for Document Understanding: Empirical Evidence from Text-Only and Multimodal",
    "summary": "arXiv:2602.21225v1 Announce Type: cross Abstract: We investigate whether progressive data scheduling -- a curriculum learning strategy that incrementally increases training data exposure (33\\%$\\rightarrow$67\\%$\\rightarrow$100\\%) -- yields consistent efficiency gains across architecturally distinct document understanding models. By evaluating BERT (",
    "url": "https://arxiv.org/abs/2602.21225",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "IslamicLegalBench: Evaluating LLMs Knowledge and Reasoning of Islamic Law Across 1,200 Years of Islamic Pluralist Legal Traditions",
    "summary": "arXiv:2602.21226v1 Announce Type: cross Abstract: As millions of Muslims turn to LLMs like GPT, Claude, and DeepSeek for religious guidance, a critical question arises: Can these AI systems reliably reason about Islamic law? We introduce IslamicLegalBench, the first benchmark evaluating LLMs across seven schools of Islamic jurisprudence, with 718 i",
    "url": "https://arxiv.org/abs/2602.21226",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Budget-Aware Agentic Routing via Boundary-Guided Training",
    "summary": "arXiv:2602.21227v1 Announce Type: cross Abstract: As large language models (LLMs) evolve into autonomous agents that execute long-horizon workflows, invoking a high-capability model at every step becomes economically unsustainable. While model routing is effective for single-turn queries, agentic routing is a sequential, path-dependent problem: ear",
    "url": "https://arxiv.org/abs/2602.21227",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "ImpRIF: Stronger Implicit Reasoning Leads to Better Complex Instruction Following",
    "summary": "arXiv:2602.21228v1 Announce Type: cross Abstract: As applications of large language models (LLMs) become increasingly complex, the demand for robust complex instruction following capabilities is growing accordingly. We argue that a thorough understanding of the instruction itself, especially the latent reasoning structure embedded between the lines",
    "url": "https://arxiv.org/abs/2602.21228",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "ACAR: Adaptive Complexity Routing for Multi-Model Ensembles with Auditable Decision Traces",
    "summary": "arXiv:2602.21231v1 Announce Type: cross Abstract: We present ACAR (Adaptive Complexity and Attribution Routing), a measurement framework for studying multi-model orchestration under auditable conditions. ACAR uses self-consistency variance (sigma) computed from N=3 probe samples to route tasks across single-model, two-model, and three-model executi",
    "url": "https://arxiv.org/abs/2602.21231",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Urban Vibrancy Embedding and Application on Traffic Prediction",
    "summary": "arXiv:2602.21232v1 Announce Type: cross Abstract: Urban vibrancy reflects the dynamic human activity within urban spaces and is often measured using mobile data that captures floating population trends. This study proposes a novel approach to derive Urban Vibrancy embeddings from real-time floating population data to enhance traffic prediction mode",
    "url": "https://arxiv.org/abs/2602.21232",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "AngelSlim: A more accessible, comprehensive, and efficient toolkit for large model compression",
    "summary": "arXiv:2602.21233v1 Announce Type: cross Abstract: This technical report introduces AngelSlim, a comprehensive and versatile toolkit for large model compression developed by the Tencent Hunyuan team. By consolidating cutting-edge algorithms, including quantization, speculative decoding, token pruning, and distillation. AngelSlim provides a unified p",
    "url": "https://arxiv.org/abs/2602.21233",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "AgenticTyper: Automated Typing of Legacy Software Projects Using Agentic AI",
    "summary": "arXiv:2602.21251v1 Announce Type: cross Abstract: Legacy JavaScript systems lack type safety, making maintenance risky. While TypeScript can help, manually adding types is expensive. Previous automated typing research focuses on type inference but rarely addresses type checking setup, definition generation, bug identification, or behavioral correct",
    "url": "https://arxiv.org/abs/2602.21251",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "A General Equilibrium Theory of Orchestrated AI Agent Systems",
    "summary": "arXiv:2602.21255v1 Announce Type: cross Abstract: We establish a general equilibrium theory for systems of large language model (LLM) agents operating under centralized orchestration. The framework is a production economy in the sense of Arrow-Debreu (1954), extended to infinite-dimensional commodity spaces following Bewley (1972). Each LLM agent i",
    "url": "https://arxiv.org/abs/2602.21255",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "A Systematic Review of Algorithmic Red Teaming Methodologies for Assurance and Security of AI Applications",
    "summary": "arXiv:2602.21267v1 Announce Type: cross Abstract: Cybersecurity threats are becoming increasingly sophisticated, making traditional defense mechanisms and manual red teaming approaches insufficient for modern organizations. While red teaming has long been recognized as an effective method to identify vulnerabilities by simulating real-world attacks",
    "url": "https://arxiv.org/abs/2602.21267",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Group Orthogonalized Policy Optimization:Group Policy Optimization as Orthogonal Projection in Hilbert Space",
    "summary": "arXiv:2602.21269v1 Announce Type: cross Abstract: We present Group Orthogonalized Policy Optimization (GOPO), a new alignment algorithm for large language models derived from the geometry of Hilbert function spaces. Instead of optimizing on the probability simplex and inheriting the exponential curvature of Kullback-Leibler divergence, GOPO lifts a",
    "url": "https://arxiv.org/abs/2602.21269",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Equitable Evaluation via Elicitation",
    "summary": "arXiv:2602.21327v1 Announce Type: cross Abstract: Individuals with similar qualifications and skills may vary in their demeanor, or outward manner: some tend toward self-promotion while others are modest to the point of omitting crucial information. Comparing the self-descriptions of equally qualified job-seekers with different self-presentation st",
    "url": "https://arxiv.org/abs/2602.21327",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Scaling View Synthesis Transformers",
    "summary": "arXiv:2602.21341v1 Announce Type: cross Abstract: Geometry-free view synthesis transformers have recently achieved state-of-the-art performance in Novel View Synthesis (NVS), outperforming traditional approaches that rely on explicit geometry modeling. Yet the factors governing their scaling with compute remain unclear. We present a systematic stud",
    "url": "https://arxiv.org/abs/2602.21341",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Alignment-Weighted DPO: A principled reasoning approach to improve safety alignment",
    "summary": "arXiv:2602.21346v1 Announce Type: cross Abstract: Recent advances in alignment techniques such as Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), and Direct Preference Optimization (DPO) have improved the safety of large language models (LLMs). However, these LLMs remain vulnerable to jailbreak attacks that disguise",
    "url": "https://arxiv.org/abs/2602.21346",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Representation Theorems for Cumulative Propositional Dependence Logics",
    "summary": "arXiv:2602.21360v1 Announce Type: cross Abstract: This paper establishes and proves representation theorems for cumulative propositional dependence logic and for cumulative propositional logic with team semantics. Cumulative logics are famously given by System C. For propositional dependence logic, we show that System C entailments are exactly capt",
    "url": "https://arxiv.org/abs/2602.21360",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Towards single-shot coherent imaging via overlap-free ptychography",
    "summary": "arXiv:2602.21361v1 Announce Type: cross Abstract: Ptychographic imaging at synchrotron and XFEL sources requires dense overlapping scans, limiting throughput and increasing dose. Extending coherent diffractive imaging to overlap-free operation on extended samples remains an open problem. Here, we extend PtychoPINN (O. Hoidn \\emph{et al.}, \\emph{Sci",
    "url": "https://arxiv.org/abs/2602.21361",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Towards Controllable Video Synthesis of Routine and Rare OR Events",
    "summary": "arXiv:2602.21365v1 Announce Type: cross Abstract: Purpose: Curating large-scale datasets of operating room (OR) workflow, encompassing rare, safety-critical, or atypical events, remains operationally and ethically challenging. This data bottleneck complicates the development of ambient intelligence for detecting, understanding, and mitigating rare ",
    "url": "https://arxiv.org/abs/2602.21365",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Black-Box Reliability Certification for AI Agents via Self-Consistency Sampling and Conformal Calibration",
    "summary": "arXiv:2602.21368v1 Announce Type: cross Abstract: Given a black-box AI system and a task, at what confidence level can a practitioner trust the system's output? We answer with a reliability level -- a single number per system-task pair, derived from self-consistency sampling and conformal calibration, that serves as a black-box deployment gate with",
    "url": "https://arxiv.org/abs/2602.21368",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "The Mean is the Mirage: Entropy-Adaptive Model Merging under Heterogeneous Domain Shifts in Medical Imaging",
    "summary": "arXiv:2602.21372v1 Announce Type: cross Abstract: Model merging under unseen test-time distribution shifts often renders naive strategies, such as mean averaging unreliable. This challenge is especially acute in medical imaging, where models are fine-tuned locally at clinics on private data, producing domain-specific models that differ by scanner, ",
    "url": "https://arxiv.org/abs/2602.21372",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Small Language Models for Privacy-Preserving Clinical Information Extraction in Low-Resource Languages",
    "summary": "arXiv:2602.21374v1 Announce Type: cross Abstract: Extracting clinical information from medical transcripts in low-resource languages remains a significant challenge in healthcare natural language processing (NLP). This study evaluates a two-step pipeline combining Aya-expanse-8B as a Persian-to-English translation model with five open-source small ",
    "url": "https://arxiv.org/abs/2602.21374",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "MrBERT: Modern Multilingual Encoders via Vocabulary, Domain, and Dimensional Adaptation",
    "summary": "arXiv:2602.21379v1 Announce Type: cross Abstract: We introduce MrBERT, a family of 150M-300M parameter encoders built on the ModernBERT architecture and pre-trained on 35 languages and code. Through targeted adaptation, this model family achieves state-of-the-art results on Catalan- and Spanish-specific tasks, while establishing robust performance ",
    "url": "https://arxiv.org/abs/2602.21379",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "VCDF: A Validated Consensus-Driven Framework for Time Series Causal Discovery",
    "summary": "arXiv:2602.21381v1 Announce Type: cross Abstract: Time series causal discovery is essential for understanding dynamic systems, yet many existing methods remain sensitive to noise, non-stationarity, and sampling variability. We propose the Validated Consensus-Driven Framework (VCDF), a simple and method-agnostic layer that improves robustness by eva",
    "url": "https://arxiv.org/abs/2602.21381",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "FedVG: Gradient-Guided Aggregation for Enhanced Federated Learning",
    "summary": "arXiv:2602.21399v1 Announce Type: cross Abstract: Federated Learning (FL) enables collaborative model training across multiple clients without sharing their private data. However, data heterogeneity across clients leads to client drift, which degrades the overall generalization performance of the model. This effect is further compounded by overemph",
    "url": "https://arxiv.org/abs/2602.21399",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "The Headless Firm: How AI Reshapes Enterprise Boundaries",
    "summary": "arXiv:2602.21401v1 Announce Type: cross Abstract: The boundary of the firm is determined by coordination cost. We argue that agentic AI induces a structural change in how coordination costs scale: in prior modular systems, integration cost grew with interaction topology (O(n^2) in the number of components); in protocol-mediated agentic systems, int",
    "url": "https://arxiv.org/abs/2602.21401",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Overconfident Errors Need Stronger Correction: Asymmetric Confidence Penalties for Reinforcement Learning",
    "summary": "arXiv:2602.21420v1 Announce Type: cross Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become the leading paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard RLVR algorithms suffer from a well-documented pathology: while they improve Pass@1 accuracy through sharpened sampling, they simultaneously",
    "url": "https://arxiv.org/abs/2602.21420",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "ECHOSAT: Estimating Canopy Height Over Space And Time",
    "summary": "arXiv:2602.21421v1 Announce Type: cross Abstract: Forest monitoring is critical for climate change mitigation. However, existing global tree height maps provide only static snapshots and do not capture temporal forest dynamics, which are essential for accurate carbon accounting. We introduce ECHOSAT, a global and temporally consistent tree height m",
    "url": "https://arxiv.org/abs/2602.21421",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "On the Structural Non-Preservation of Epistemic Behaviour under Policy Transformation",
    "summary": "arXiv:2602.21424v1 Announce Type: cross Abstract: Reinforcement learning (RL) agents under partial observability often condition actions on internally accumulated information such as memory or inferred latent context. We formalise such information-conditioned interaction patterns as behavioural dependency: variation in action selection with respect",
    "url": "https://arxiv.org/abs/2602.21424",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Provably Safe Generative Sampling with Constricting Barrier Functions",
    "summary": "arXiv:2602.21429v1 Announce Type: cross Abstract: Flow-based generative models, such as diffusion models and flow matching models, have achieved remarkable success in learning complex data distributions. However, a critical gap remains for their deployment in safety-critical domains: the lack of formal guarantees that generated samples will satisfy",
    "url": "https://arxiv.org/abs/2602.21429",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Causal Decoding for Hallucination-Resistant Multimodal Large Language Models",
    "summary": "arXiv:2602.21441v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) deliver detailed responses on vision-language tasks, yet remain susceptible to object hallucination (introducing objects not present in the image), undermining reliability in practice. Prior efforts often rely on heuristic penalties, post-hoc correction, or g",
    "url": "https://arxiv.org/abs/2602.21441",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "MINAR: Mechanistic Interpretability for Neural Algorithmic Reasoning",
    "summary": "arXiv:2602.21442v1 Announce Type: cross Abstract: The recent field of neural algorithmic reasoning (NAR) studies the ability of graph neural networks (GNNs) to emulate classical algorithms like Bellman-Ford, a phenomenon known as algorithmic alignment. At the same time, recent advances in large language models (LLMs) have spawned the study of mecha",
    "url": "https://arxiv.org/abs/2602.21442",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Adversarial Intent is a Latent Variable: Stateful Trust Inference for Securing Multimodal Agentic RAG",
    "summary": "arXiv:2602.21447v1 Announce Type: cross Abstract: Current stateless defences for multimodal agentic RAG fail to detect adversarial strategies that distribute malicious semantics across retrieval, planning, and generation components. We formulate this security challenge as a Partially Observable Markov Decision Process (POMDP), where adversarial int",
    "url": "https://arxiv.org/abs/2602.21447",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Adversarial Robustness of Deep Learning-Based Thyroid Nodule Segmentation in Ultrasound",
    "summary": "arXiv:2602.21452v1 Announce Type: cross Abstract: Introduction: Deep learning-based segmentation models are increasingly integrated into clinical imaging workflows, yet their robustness to adversarial perturbations remains incompletely characterized, particularly for ultrasound images. We evaluated adversarial attacks and inference-time defenses fo",
    "url": "https://arxiv.org/abs/2602.21452",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Revisiting Text Ranking in Deep Research",
    "summary": "arXiv:2602.21456v1 Announce Type: cross Abstract: Deep research has emerged as an important task that aims to address hard queries through extensive open-web exploration. To tackle it, most prior work equips large language model (LLM)-based agents with opaque web search APIs, enabling agents to iteratively issue search queries, retrieve external ev",
    "url": "https://arxiv.org/abs/2602.21456",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "A Knowledge-Driven Approach to Music Segmentation, Music Source Separation and Cinematic Audio Source Separation",
    "summary": "arXiv:2602.21476v1 Announce Type: cross Abstract: We propose a knowledge-driven, model-based approach to segmenting audio into single-category and mixed-category chunks with applications to source separation. \"Knowledge\" here denotes information associated with the data, such as music scores. \"Model\" here refers to tool that can be used for audio s",
    "url": "https://arxiv.org/abs/2602.21476",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "GradAlign: Gradient-Aligned Data Selection for LLM Reinforcement Learning",
    "summary": "arXiv:2602.21492v1 Announce Type: cross Abstract: Reinforcement learning (RL) has become a central post-training paradigm for large language models (LLMs), but its performance is highly sensitive to the quality of training problems. This sensitivity stems from the non-stationarity of RL: rollouts are generated by an evolving policy, and learning is",
    "url": "https://arxiv.org/abs/2602.21492",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Training Generalizable Collaborative Agents via Strategic Risk Aversion",
    "summary": "arXiv:2602.21515v1 Announce Type: cross Abstract: Many emerging agentic paradigms require agents to collaborate with one another (or people) to achieve shared goals. Unfortunately, existing approaches to learning policies for such collaborative problems produce brittle solutions that fail when paired with new partners. We attribute these failures t",
    "url": "https://arxiv.org/abs/2602.21515",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "One Brain, Omni Modalities: Towards Unified Non-Invasive Brain Decoding with Large Language Models",
    "summary": "arXiv:2602.21522v1 Announce Type: cross Abstract: Deciphering brain function through non-invasive recordings requires synthesizing complementary high-frequency electromagnetic (EEG/MEG) and low-frequency metabolic (fMRI) signals. However, despite their shared neural origins, extreme discrepancies have traditionally confined these modalities to isol",
    "url": "https://arxiv.org/abs/2602.21522",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "LiLo-VLA: Compositional Long-Horizon Manipulation via Linked Object-Centric Policies",
    "summary": "arXiv:2602.21531v1 Announce Type: cross Abstract: General-purpose robots must master long-horizon manipulation, defined as tasks involving multiple kinematic structure changes (e.g., attaching or detaching objects) in unstructured environments. While Vision-Language-Action (VLA) models offer the potential to master diverse atomic skills, they strug",
    "url": "https://arxiv.org/abs/2602.21531",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Enhancing Multilingual Embeddings via Multi-Way Parallel Text Alignment",
    "summary": "arXiv:2602.21543v1 Announce Type: cross Abstract: Multilingual pretraining typically lacks explicit alignment signals, leading to suboptimal cross-lingual alignment in the representation space. In this work, we show that training standard pretrained models for cross-lingual alignment with a multi-way parallel corpus in a diverse pool of languages c",
    "url": "https://arxiv.org/abs/2602.21543",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "From Basis to Basis: Gaussian Particle Representation for Interpretable PDE Operators",
    "summary": "arXiv:2602.21551v1 Announce Type: cross Abstract: Learning PDE dynamics for fluids increasingly relies on neural operators and Transformer-based models, yet these approaches often lack interpretability and struggle with localized, high-frequency structures while incurring quadratic cost in spatial samples. We propose representing fields with a Gaus",
    "url": "https://arxiv.org/abs/2602.21551",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Revisiting RAG Retrievers: An Information Theoretic Benchmark",
    "summary": "arXiv:2602.21553v1 Announce Type: cross Abstract: Retrieval-Augmented Generation (RAG) systems rely critically on the retriever module to surface relevant context for large language models. Although numerous retrievers have recently been proposed, each built on different ranking principles such as lexical matching, dense embeddings, or graph citati",
    "url": "https://arxiv.org/abs/2602.21553",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Exploring Human-Machine Coexistence in Symmetrical Reality",
    "summary": "arXiv:2602.21584v1 Announce Type: cross Abstract: In the context of the evolution of artificial intelligence (AI), the interaction between humans and AI entities has become increasingly salient, challenging the conventional human-centric paradigms of human-machine interaction. To address this challenge, it is imperative to reassess the relationship",
    "url": "https://arxiv.org/abs/2602.21584",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Retrieval Challenges in Low-Resource Public Service Information: A Case Study on Food Pantry Access",
    "summary": "arXiv:2602.21598v1 Announce Type: cross Abstract: Public service information systems are often fragmented, inconsistently formatted, and outdated. These characteristics create low-resource retrieval environments that hinder timely access to critical services. We investigate retrieval challenges in such settings through the domain of food pantry acc",
    "url": "https://arxiv.org/abs/2602.21598",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Structurally Aligned Subtask-Level Memory for Software Engineering Agents",
    "summary": "arXiv:2602.21611v1 Announce Type: cross Abstract: Large Language Models (LLMs) have demonstrated significant potential as autonomous software engineering (SWE) agents. Recent work has further explored augmenting these agents with memory mechanisms to support long-horizon reasoning. However, these approaches typically operate at a coarse instance gr",
    "url": "https://arxiv.org/abs/2602.21611",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Virtual Biopsy for Intracranial Tumors Diagnosis on MRI",
    "summary": "arXiv:2602.21613v1 Announce Type: cross Abstract: Deep intracranial tumors situated in eloquent brain regions controlling vital functions present critical diagnostic challenges. Clinical practice has shifted toward stereotactic biopsy for pathological confirmation before treatment. Yet biopsy carries inherent risks of hemorrhage and neurological de",
    "url": "https://arxiv.org/abs/2602.21613",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Self-Correcting VLA: Online Action Refinement via Sparse World Imagination",
    "summary": "arXiv:2602.21633v1 Announce Type: cross Abstract: Standard vision-language-action (VLA) models rely on fitting statistical data priors, limiting their robust understanding of underlying physical dynamics. Reinforcement learning enhances physical grounding through exploration yet typically relies on external reward signals that remain isolated from ",
    "url": "https://arxiv.org/abs/2602.21633",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Mitigating Structural Noise in Low-Resource S2TT: An Optimized Cascaded Nepali-English Pipeline with Punctuation Restoration",
    "summary": "arXiv:2602.21647v1 Announce Type: cross Abstract: This paper presents and evaluates an optimized cascaded Nepali speech-to-English text translation (S2TT) system, focusing on mitigating structural noise introduced by Automatic Speech Recognition (ASR). We first establish highly proficient ASR and NMT components: a Wav2Vec2-XLS-R-300m model achieved",
    "url": "https://arxiv.org/abs/2602.21647",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "PPCR-IM: A System for Multi-layer DAG-based Public Policy Consequence Reasoning and Social Indicator Mapping",
    "summary": "arXiv:2602.21650v1 Announce Type: cross Abstract: Public policy decisions are typically justified using a narrow set of headline indicators, leaving many downstream social impacts unstructured and difficult to compare across policies. We propose PPCR-IM, a system for multi-layer DAG-based consequence reasoning and social indicator mapping that addr",
    "url": "https://arxiv.org/abs/2602.21650",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Sparsity Induction for Accurate Post-Training Pruning of Large Language Models",
    "summary": "arXiv:2602.21652v1 Announce Type: cross Abstract: Large language models have demonstrated capabilities in text generation, while their increasing parameter scales present challenges in computational and memory efficiency. Post-training sparsity (PTS), which reduces model cost by removing weights from dense networks, is an effective approach. Howeve",
    "url": "https://arxiv.org/abs/2602.21652",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "CCCaption: Dual-Reward Reinforcement Learning for Complete and Correct Image Captioning",
    "summary": "arXiv:2602.21655v1 Announce Type: cross Abstract: Image captioning remains a fundamental task for vision language understanding, yet ground-truth supervision still relies predominantly on human-annotated references. Because human annotations reflect subjective preferences and expertise, ground-truth captions are often incomplete or even incorrect, ",
    "url": "https://arxiv.org/abs/2602.21655",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Following the Diagnostic Trace: Visual Cognition-guided Cooperative Network for Chest X-Ray Diagnosis",
    "summary": "arXiv:2602.21657v1 Announce Type: cross Abstract: Computer-aided diagnosis (CAD) has significantly advanced automated chest X-ray diagnosis but remains isolated from clinical workflows and lacks reliable decision support and interpretability. Human-AI collaboration seeks to enhance the reliability of diagnostic models by integrating the behaviors o",
    "url": "https://arxiv.org/abs/2602.21657",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Hierarchical LLM-Based Multi-Agent Framework with Prompt Optimization for Multi-Robot Task Planning",
    "summary": "arXiv:2602.21670v1 Announce Type: cross Abstract: Multi-robot task planning requires decomposing natural-language instructions into executable actions for heterogeneous robot teams. Conventional Planning Domain Definition Language (PDDL) planners provide rigorous guarantees but struggle to handle ambiguous or long-horizon missions, while large lang",
    "url": "https://arxiv.org/abs/2602.21670",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Dynamic Multimodal Activation Steering for Hallucination Mitigation in Large Vision-Language Models",
    "summary": "arXiv:2602.21704v1 Announce Type: cross Abstract: Large Vision-Language Models (LVLMs) exhibit outstanding performance on vision-language tasks but struggle with hallucination problems. Through in-depth analysis of LVLM activation patterns, we reveal two key findings: 1) truthfulness and visual perception capabilities predominantly engage different",
    "url": "https://arxiv.org/abs/2602.21704",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "SurGo-R1: Benchmarking and Modeling Contextual Reasoning for Operative Zone in Surgical Video",
    "summary": "arXiv:2602.21706v1 Announce Type: cross Abstract: Minimally invasive surgery has dramatically improved patient operative outcomes, yet identifying safe operative zones remains challenging in critical phases, requiring surgeons to integrate visual cues, procedural phase, and anatomical context under high cognitive load. Existing AI systems offer bin",
    "url": "https://arxiv.org/abs/2602.21706",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Two-Stage Active Distribution Network Voltage Control via LLM-RL Collaboration: A Hybrid Knowledge-Data-Driven Approach",
    "summary": "arXiv:2602.21715v1 Announce Type: cross Abstract: The growing integration of distributed photovoltaics (PVs) into active distribution networks (ADNs) has exacerbated operational challenges, making it imperative to coordinate diverse equipment to mitigate voltage violations and enhance power quality. Although existing data-driven approaches have dem",
    "url": "https://arxiv.org/abs/2602.21715",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Evaluating the relationship between regularity and learnability in recursive numeral systems using Reinforcement Learning",
    "summary": "arXiv:2602.21720v1 Announce Type: cross Abstract: Human recursive numeral systems (i.e., counting systems such as English base-10 numerals), like many other grammatical systems, are highly regular. Following prior work that relates cross-linguistic tendencies to biases in learning, we ask whether regular systems are common because regularity facili",
    "url": "https://arxiv.org/abs/2602.21720",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Learning from Yesterday's Error: An Efficient Online Learning Method for Traffic Demand Prediction",
    "summary": "arXiv:2602.21757v1 Announce Type: cross Abstract: Accurately predicting short-term traffic demand is critical for intelligent transportation systems. While deep learning models achieve strong performance under stationary conditions, their accuracy often degrades significantly when faced with distribution shifts caused by external events or evolving",
    "url": "https://arxiv.org/abs/2602.21757",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Generalisation of RLHF under Reward Shift and Clipped KL Regularisation",
    "summary": "arXiv:2602.21765v1 Announce Type: cross Abstract: Alignment and adaptation in large language models heavily rely on reinforcement learning from human feedback (RLHF); yet, theoretical understanding of its generalisability remains premature, especially when the learned reward could shift, and the KL control is estimated and clipped. To address this ",
    "url": "https://arxiv.org/abs/2602.21765",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "UniWhisper: Efficient Continual Multi-task Training for Robust Universal Audio Representation",
    "summary": "arXiv:2602.21772v1 Announce Type: cross Abstract: A universal audio representation should capture fine-grained speech cues and high-level semantics for environmental sounds and music in a single encoder. Existing encoders often excel in one domain but degrade in others. We propose UniWhisper, an efficient continual multi-task training framework tha",
    "url": "https://arxiv.org/abs/2602.21772",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Beyond Static Artifacts: A Forensic Benchmark for Video Deepfake Reasoning in Vision Language Models",
    "summary": "arXiv:2602.21779v1 Announce Type: cross Abstract: Current Vision-Language Models (VLMs) for deepfake detection excel at identifying spatial artifacts but overlook a critical dimension: temporal inconsistencies in video forgeries. Adapting VLMs to reason about these dynamic cues remains a distinct challenge. To bridge this gap, we propose Forensic A",
    "url": "https://arxiv.org/abs/2602.21779",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Excitation: Momentum For Experts",
    "summary": "arXiv:2602.21798v1 Announce Type: cross Abstract: We propose Excitation, a novel optimization framework designed to accelerate learning in sparse architectures such as Mixture-of-Experts (MoEs). Unlike traditional optimizers that treat all parameters uniformly, Excitation dynamically modulates updates using batch-level expert utilization. It introd",
    "url": "https://arxiv.org/abs/2602.21798",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "An Evaluation of Context Length Extrapolation in Long Code via Positional Embeddings and Efficient Attention",
    "summary": "arXiv:2602.21800v1 Announce Type: cross Abstract: The rapid advancement of large language models (LLMs) has led to a significant increase in automated tools in the software engineering, capable of performing various code-related tasks such as code generation, completion, and translation. Despite these advancements, its effectiveness is constrained ",
    "url": "https://arxiv.org/abs/2602.21800",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "SemVideo: Reconstructs What You Watch from Brain Activity via Hierarchical Semantic Guidance",
    "summary": "arXiv:2602.21819v1 Announce Type: cross Abstract: Reconstructing dynamic visual experiences from brain activity provides a compelling avenue for exploring the neural mechanisms of human visual perception. While recent progress in fMRI-based image reconstruction has been notable, extending this success to video reconstruction remains a significant c",
    "url": "https://arxiv.org/abs/2602.21819",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "StoryMovie: A Dataset for Semantic Alignment of Visual Stories with Movie Scripts and Subtitles",
    "summary": "arXiv:2602.21829v1 Announce Type: cross Abstract: Visual storytelling models that correctly ground entities in images may still hallucinate semantic relationships, generating incorrect dialogue attribution, character interactions, or emotional states. We introduce StoryMovie, a dataset of 1,757 stories aligned with movie scripts and subtitles throu",
    "url": "https://arxiv.org/abs/2602.21829",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Resilient Federated Chain: Transforming Blockchain Consensus into an Active Defense Layer for Federated Learning",
    "summary": "arXiv:2602.21841v1 Announce Type: cross Abstract: Federated Learning (FL) has emerged as a key paradigm for building Trustworthy AI systems by enabling privacy-preserving, decentralized model training. However, FL is highly susceptible to adversarial attacks that compromise model integrity and data confidentiality, a vulnerability exacerbated by th",
    "url": "https://arxiv.org/abs/2602.21841",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "xai-cola: A Python library for sparsifying counterfactual explanations",
    "summary": "arXiv:2602.21845v1 Announce Type: cross Abstract: Counterfactual explanation (CE) is an important domain within post-hoc explainability. However, the explanations generated by most CE generators are often highly redundant. This work introduces an open-source Python library xai-cola, which provides an end-to-end pipeline for sparsifying CEs produced",
    "url": "https://arxiv.org/abs/2602.21845",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Understanding Annotation Error Propagation and Learning an Adaptive Policy for Expert Intervention in Barrett's Video Segmentation",
    "summary": "arXiv:2602.21855v1 Announce Type: cross Abstract: Accurate annotation of endoscopic videos is essential yet time-consuming, particularly for challenging datasets such as dysplasia in Barrett's esophagus, where the affected regions are irregular and lack clear boundaries. Semi-automatic tools like Segment Anything Model 2 (SAM2) can ease this proces",
    "url": "https://arxiv.org/abs/2602.21855",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "DynamicGTR: Leveraging Graph Topology Representation Preferences to Boost VLM Capabilities on Graph QAs",
    "summary": "arXiv:2602.21864v1 Announce Type: cross Abstract: Vision-Language Models (VLMs) have emerged as versatile solutions for zero-shot question answering (QA) across various domains. However, enabling VLMs to effectively comprehend structured graphs and perform accurate, efficient QA remains challenging. Existing approaches typically rely on one single ",
    "url": "https://arxiv.org/abs/2602.21864",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "A Framework for Cross-Domain Generalization in Coronary Artery Calcium Scoring Across Gated and Non-Gated Computed Tomography",
    "summary": "arXiv:2602.21935v1 Announce Type: cross Abstract: Coronary artery calcium (CAC) scoring is a key predictor of cardiovascular risk, but it relies on ECG-gated CT scans, restricting its use to specialized cardiac imaging settings. We introduce an automated framework for CAC detection and lesion-specific Agatston scoring that operates across both gate",
    "url": "https://arxiv.org/abs/2602.21935",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Hidden Topics: Measuring Sensitive AI Beliefs with List Experiments",
    "summary": "arXiv:2602.21939v1 Announce Type: cross Abstract: How can researchers identify beliefs that large language models (LLMs) hide? As LLMs become more sophisticated and the prevalence of alignment faking increases, combined with their growing integration into high-stakes decision-making, responding to this challenge has become critical. This paper prop",
    "url": "https://arxiv.org/abs/2602.21939",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "PatchDenoiser: Parameter-efficient multi-scale patch learning and fusion denoiser for medical images",
    "summary": "arXiv:2602.21987v1 Announce Type: cross Abstract: Medical images are essential for diagnosis, treatment planning, and research, but their quality is often degraded by noise from low-dose acquisition, patient motion, or scanner limitations, affecting both clinical interpretation and downstream analysis. Traditional filtering approaches often over-sm",
    "url": "https://arxiv.org/abs/2602.21987",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Enhancing LLM-Based Test Generation by Eliminating Covered Code",
    "summary": "arXiv:2602.21997v1 Announce Type: cross Abstract: Automated test generation is essential for software quality assurance, with coverage rate serving as a key metric to ensure thorough testing. Recent advancements in Large Language Models (LLMs) have shown promise in improving test generation, particularly in achieving higher coverage. However, while",
    "url": "https://arxiv.org/abs/2602.21997",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "RGB-Event HyperGraph Prompt for Kilometer Marker Recognition based on Pre-trained Foundation Models",
    "summary": "arXiv:2602.22026v1 Announce Type: cross Abstract: Metro trains often operate in highly complex environments, characterized by illumination variations, high-speed motion, and adverse weather conditions. These factors pose significant challenges for visual perception systems, especially those relying solely on conventional RGB cameras. To tackle thes",
    "url": "https://arxiv.org/abs/2602.22026",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "TG-ASR: Translation-Guided Learning with Parallel Gated Cross Attention for Low-Resource Automatic Speech Recognition",
    "summary": "arXiv:2602.22039v1 Announce Type: cross Abstract: Low-resource automatic speech recognition (ASR) continues to pose significant challenges, primarily due to the limited availability of transcribed data for numerous languages. While a wealth of spoken content is accessible in television dramas and online videos, Taiwanese Hokkien exemplifies this is",
    "url": "https://arxiv.org/abs/2602.22039",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Physics-Informed Machine Learning for Vessel Shaft Power and Fuel Consumption Prediction: Interpretable KAN-based Approach",
    "summary": "arXiv:2602.22055v1 Announce Type: cross Abstract: Accurate prediction of shaft rotational speed, shaft power, and fuel consumption is crucial for enhancing operational efficiency and sustainability in maritime transportation. Conventional physics-based models provide interpretability but struggle with real-world variability, while purely data-drive",
    "url": "https://arxiv.org/abs/2602.22055",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "NESTOR: A Nested MOE-based Neural Operator for Large-Scale PDE Pre-Training",
    "summary": "arXiv:2602.22059v1 Announce Type: cross Abstract: Neural operators have emerged as an efficient paradigm for solving PDEs, overcoming the limitations of traditional numerical methods and significantly improving computational efficiency. However, due to the diversity and complexity of PDE systems, existing neural operators typically rely on a single",
    "url": "https://arxiv.org/abs/2602.22059",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "DualWeaver: Synergistic Feature Weaving Surrogates for Multivariate Forecasting with Univariate Time Series Foundation Models",
    "summary": "arXiv:2602.22066v1 Announce Type: cross Abstract: Time-series foundation models (TSFMs) have achieved strong univariate forecasting through large-scale pre-training, yet effectively extending this success to multivariate forecasting remains challenging. To address this, we propose DualWeaver, a novel framework that adapts univariate TSFMs (Uni-TSFM",
    "url": "https://arxiv.org/abs/2602.22066",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Understanding Artificial Theory of Mind: Perturbed Tasks and Reasoning in Large Language Models",
    "summary": "arXiv:2602.22072v1 Announce Type: cross Abstract: Theory of Mind (ToM) refers to an agent's ability to model the internal states of others. Contributing to the debate whether large language models (LLMs) exhibit genuine ToM capabilities, our study investigates their ToM robustness using perturbations on false-belief tasks and examines the potential",
    "url": "https://arxiv.org/abs/2602.22072",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "On Imbalanced Regression with Hoeffding Trees",
    "summary": "arXiv:2602.22101v1 Announce Type: cross Abstract: Many real-world applications provide a continuous stream of data that is subsequently used by machine learning models to solve regression tasks of interest. Hoeffding trees and their variants have a long-standing tradition due to their effectiveness, either alone or as base models in broader ensembl",
    "url": "https://arxiv.org/abs/2602.22101",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Don't stop me now: Rethinking Validation Criteria for Model Parameter Selection",
    "summary": "arXiv:2602.22107v1 Announce Type: cross Abstract: Despite the extensive literature on training loss functions, the evaluation of generalization on the validation set remains underexplored. In this work, we conduct a systematic empirical and statistical study of how the validation criterion used for model selection affects test performance in neural",
    "url": "https://arxiv.org/abs/2602.22107",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "SWE-Prot\\'eg\\'e: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents",
    "summary": "arXiv:2602.22124v1 Announce Type: cross Abstract: Small language models (SLMs) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as SWE-bench, where they suffer from pervasive action looping and low resolution rates. We introduce SWE-Prot\\'eg\\'",
    "url": "https://arxiv.org/abs/2602.22124",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors",
    "summary": "arXiv:2602.22144v1 Announce Type: cross Abstract: Object hallucination is a critical issue in Large Vision-Language Models (LVLMs), where outputs include objects that do not appear in the input image. A natural question arises from this phenomenon: Which component of the LVLM pipeline primarily contributes to object hallucinations? The vision encod",
    "url": "https://arxiv.org/abs/2602.22144",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "When AI Writes, Whose Voice Remains? Quantifying Cultural Marker Erasure Across World English Varieties in Large Language Models",
    "summary": "arXiv:2602.22145v1 Announce Type: cross Abstract: Large Language Models (LLMs) are increasingly used to ``professionalize'' workplace communication, often at the cost of linguistic identity. We introduce \"Cultural Ghosting\", the systematic erasure of linguistic markers unique to non-native English varieties during text processing. Through analysis ",
    "url": "https://arxiv.org/abs/2602.22145",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual",
    "summary": "arXiv:2602.22146v1 Announce Type: cross Abstract: Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be formulated as a primal-dual optimization problem, standard primal-dual methods only guarantee convergence wit",
    "url": "https://arxiv.org/abs/2602.22146",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Enhancing Framingham Cardiovascular Risk Score Transparency through Logic-Based XAI",
    "summary": "arXiv:2602.22149v1 Announce Type: cross Abstract: Cardiovascular disease (CVD) remains one of the leading global health challenges, accounting for more than 19 million deaths worldwide. To address this, several tools that aim to predict CVD risk and support clinical decision making have been developed. In particular, the Framingham Risk Score (FRS)",
    "url": "https://arxiv.org/abs/2602.22149",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Surrogate models for Rock-Fluid Interaction: A Grid-Size-Invariant Approach",
    "summary": "arXiv:2602.22188v1 Announce Type: cross Abstract: Modelling rock-fluid interaction requires solving a set of partial differential equations (PDEs) to predict the flow behaviour and the reactions of the fluid with the rock on the interfaces. Conventional high-fidelity numerical models require a high resolution to obtain reliable results, resulting i",
    "url": "https://arxiv.org/abs/2602.22188",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL",
    "summary": "arXiv:2602.22190v1 Announce Type: cross Abstract: Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI",
    "url": "https://arxiv.org/abs/2602.22190",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Off-The-Shelf Image-to-Image Models Are All You Need To Defeat Image Protection Schemes",
    "summary": "arXiv:2602.22197v1 Announce Type: cross Abstract: Advances in Generative AI (GenAI) have led to the development of various protection strategies to prevent the unauthorized use of images. These methods rely on adding imperceptible protective perturbations to images to thwart misuse such as style mimicry or deepfake manipulations. Although previous ",
    "url": "https://arxiv.org/abs/2602.22197",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets",
    "summary": "arXiv:2602.22207v1 Announce Type: cross Abstract: The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a full",
    "url": "https://arxiv.org/abs/2602.22207",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Shapley Value Computation in Ontology-Mediated Query Answering",
    "summary": "arXiv:2407.20058v3 Announce Type: replace Abstract: The Shapley value was originally introduced in cooperative game theory as a wealth distribution mechanism. It has since found use in knowledge representation and databases for the purpose of assigning scores to formulas and database tuples based upon their contribution to obtaining a query result ",
    "url": "https://arxiv.org/abs/2407.20058",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Temporal Knowledge-Graph Memory in a Partially Observable Environment",
    "summary": "arXiv:2408.05861v4 Announce Type: replace Abstract: Agents in partially observable environments require persistent memory to integrate observations over time. While KGs (knowledge graphs) provide a natural representation for such evolving state, existing benchmarks rarely expose agents to environments where both the world dynamics and the agent's m",
    "url": "https://arxiv.org/abs/2408.05861",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Measuring AI Ability to Complete Long Software Tasks",
    "summary": "arXiv:2503.14499v3 Announce Type: replace Abstract: Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human capabilities, we propose a new metric: 50%-task-completion time horizon. This is the time humans typically take to complete tasks t",
    "url": "https://arxiv.org/abs/2503.14499",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs",
    "summary": "arXiv:2505.13529v2 Announce Type: replace Abstract: Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning. However, current LRMs rarely admit ignorance or respond with \"I don't know\". Instead, they often produce incorrect answers while showing undue confidence, raising concerns abo",
    "url": "https://arxiv.org/abs/2505.13529",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Knowledge Fusion of Large Language Models Via Modular SkillPacks",
    "summary": "arXiv:2505.18502v2 Announce Type: replace Abstract: Cross-capability transfer is a key challenge in large language model (LLM) research, with applications in multi-task integration, model compression, and continual learning. Recent works like FuseLLM and FuseChat have demonstrated the potential of transferring multiple model capabilities to lightwe",
    "url": "https://arxiv.org/abs/2505.18502",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Spurious Rewards: Rethinking Training Signals in RLVR",
    "summary": "arXiv:2506.10947v2 Announce Type: replace Abstract: We show that reinforcement learning with verifiable rewards (RLVR) can elicit strong mathematical reasoning in certain language models even with spurious rewards that have little, no, or even negative correlation with the correct answer. For example, RLVR training with GRPO improves MATH-500 perfo",
    "url": "https://arxiv.org/abs/2506.10947",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Med-REFL: Medical Reasoning Enhancement via Self-Corrected Fine-grained Reflection",
    "summary": "arXiv:2506.13793v4 Announce Type: replace Abstract: Large reasoning models excel in domains like mathematics where intermediate reasoning is straightforward to verify, but struggle to self-correct in medicine fields where evaluating intermediate reasoning is cumbersome and expensive. This verification bottleneck hinders the development of reliable ",
    "url": "https://arxiv.org/abs/2506.13793",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis",
    "summary": "arXiv:2507.14899v3 Announce Type: replace Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for industrial quality assurance, yet existing deep-learning-based approaches often lack interactivity, interpretability, and the capacity for critical self-assessment, limiting their reliability and operator trust. To address ",
    "url": "https://arxiv.org/abs/2507.14899",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning",
    "summary": "arXiv:2508.07667v3 Announce Type: replace Abstract: Addressing contextual privacy concerns remains challenging in interactive settings where large language models (LLMs) process information from multiple sources (e.g., summarizing meetings with private and public information). We introduce a multi-agent framework that decomposes privacy reasoning i",
    "url": "https://arxiv.org/abs/2508.07667",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Error Notebook-Guided, Training-Free Part Retrieval in 3D CAD Assemblies via Vision-Language Models",
    "summary": "arXiv:2509.01350v3 Announce Type: replace Abstract: Effective specification-aware part retrieval within complex CAD assemblies is essential for automated engineering tasks. However, using LLMs/VLMs for this task is challenging: the CAD model metadata sequences often exceed token budgets, and fine-tuning high-performing proprietary models (e.g., GPT",
    "url": "https://arxiv.org/abs/2509.01350",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "A Multi-faceted Analysis of Cognitive Abilities: Evaluating Prompt Methods with Large Language Models on the CONSORT Checklist",
    "summary": "arXiv:2510.19139v3 Announce Type: replace Abstract: Despite the rapid expansion of Large Language Models (LLMs) in healthcare, robust and explainable evaluation of their ability to assess clinical trial reporting according to CONSORT standards remains an open challenge. In particular, uncertainty calibration and metacognitive reliability of LLM rea",
    "url": "https://arxiv.org/abs/2510.19139",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "LLMs Process Lists With General Filter Heads",
    "summary": "arXiv:2510.26784v3 Announce Type: replace Abstract: We investigate the mechanisms underlying a range of list-processing tasks in LLMs, and we find that LLMs have learned to encode a compact, causal representation of a general filtering operation that mirrors the generic \"filter\" function of functional programming. Using causal mediation analysis on",
    "url": "https://arxiv.org/abs/2510.26784",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
    "summary": "arXiv:2601.10402v4 Announce Type: replace Abstract: The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have ",
    "url": "https://arxiv.org/abs/2601.10402",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Think like a Scientist: Physics-guided LLM Agent for Equation Discovery",
    "summary": "arXiv:2602.12259v2 Announce Type: replace Abstract: Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most ex",
    "url": "https://arxiv.org/abs/2602.12259",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "OMNI-LEAK: Orchestrator Multi-Agent Network Induced Data Leakage",
    "summary": "arXiv:2602.13477v2 Announce Type: replace Abstract: As Large Language Model (LLM) agents become more capable, their coordinated use in the form of multi-agent systems is anticipated to emerge as a practical paradigm. Prior work has examined the safety and misuse risks associated with agents. However, much of this has focused on the single-agent cas",
    "url": "https://arxiv.org/abs/2602.13477",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "OR-Agent: Bridging Evolutionary Search and Structured Research for Automated Algorithm Discovery",
    "summary": "arXiv:2602.13769v2 Announce Type: replace Abstract: Automating scientific discovery in complex, experiment-driven domains requires more than iterative mutation of programs; it demands structured hypothesis management, environment interaction, and principled reflection. We present OR-Agent, a configurable multi-agent research framework designed for ",
    "url": "https://arxiv.org/abs/2602.13769",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Evolutionary System Prompt Learning for Reinforcement Learning in LLMs",
    "summary": "arXiv:2602.14697v3 Announce Type: replace Abstract: Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose ",
    "url": "https://arxiv.org/abs/2602.14697",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "The Potential of CoT for Reasoning: A Closer Look at Trace Dynamics",
    "summary": "arXiv:2602.14903v2 Announce Type: replace Abstract: Chain-of-thought (CoT) prompting is a de-facto standard technique to elicit reasoning-like responses from large language models (LLMs), allowing them to spell out individual steps before giving a final answer. While the resemblance to human-like reasoning is undeniable, the driving forces underpin",
    "url": "https://arxiv.org/abs/2602.14903",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Spilled Energy in Large Language Models",
    "summary": "arXiv:2602.18671v2 Announce Type: replace Abstract: We reinterpret the final Large Language Model (LLM) softmax classifier as an Energy-Based Model (EBM), decomposing the sequence-to-sequence probability chain into multiple interacting EBMs at inference. This principled approach allows us to track \"energy spills\" during decoding, which we empirical",
    "url": "https://arxiv.org/abs/2602.18671",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "OptiRepair: Closed-Loop Diagnosis and Repair of Supply Chain Optimization Models with LLM Agents",
    "summary": "arXiv:2602.19439v2 Announce Type: replace Abstract: Supply chain optimization models frequently become infeasible because of modeling errors. Diagnosis and repair require scarce OR expertise: analysts must interpret solver diagnostics, trace root causes across echelons, and fix formulations without sacrificing operational soundness. Whether AI agen",
    "url": "https://arxiv.org/abs/2602.19439",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Recursive Belief Vision Language Action Models",
    "summary": "arXiv:2602.20659v2 Announce Type: replace Abstract: Vision-language-action models must enable agents to execute long-horizon tasks under partial observability. However, most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, actio",
    "url": "https://arxiv.org/abs/2602.20659",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Improving Denoising Diffusion Models via Simultaneous Estimation of Image and Noise",
    "summary": "arXiv:2310.17167v2 Announce Type: replace-cross Abstract: This paper introduces two key contributions aimed at improving the speed and quality of images generated through inverse diffusion processes. The first contribution involves reparameterizing the diffusion process in terms of the angle on a quarter-circular arc between the image and noise, sp",
    "url": "https://arxiv.org/abs/2310.17167",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Multi-agent deep reinforcement learning with centralized training and decentralized execution for transportation infrastructure management",
    "summary": "arXiv:2401.12455v2 Announce Type: replace-cross Abstract: Life-cycle management of large-scale transportation systems requires determining a sequence of inspection and maintenance decisions to minimize long-term risks and costs while dealing with multiple uncertainties and constraints that lie in high-dimensional spaces. Traditional approaches have",
    "url": "https://arxiv.org/abs/2401.12455",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "A Problem-Oriented Perspective and Anchor Verification for Code Optimization",
    "summary": "arXiv:2406.11935v4 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have shown remarkable capabilities in solving various programming tasks, such as code generation. However, their potential for code optimization, particularly in performance enhancement, remains largely unexplored. This paper investigates the capabilities of LLMs",
    "url": "https://arxiv.org/abs/2406.11935",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Measuring the Measurers: Quality Evaluation of Hallucination Benchmarks for Large Vision-Language Models",
    "summary": "arXiv:2406.17115v3 Announce Type: replace-cross Abstract: Despite the outstanding performance in multimodal tasks, Large Vision-Language Models (LVLMs) have been plagued by the issue of hallucination, i.e., generating content that is inconsistent with the corresponding visual inputs. While previous works have proposed various benchmarks to evaluate",
    "url": "https://arxiv.org/abs/2406.17115",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "When Can Transformers Count to n?",
    "summary": "arXiv:2407.15160v3 Announce Type: replace-cross Abstract: Large language models based on the transformer architecture can solve highly complex tasks, yet their fundamental limitations on simple algorithmic problems remain poorly understood. In this work, we focus on basic counting tasks and investigate how the difficulty of these tasks scales with ",
    "url": "https://arxiv.org/abs/2407.15160",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Parallel Split Learning with Global Sampling",
    "summary": "arXiv:2407.15738v5 Announce Type: replace-cross Abstract: Distributed deep learning in resource-constrained environments faces scalability and generalization challenges due to large effective batch sizes and non-identically distributed client data. We introduce a server-driven sampling strategy that maintains a fixed global batch size by dynamicall",
    "url": "https://arxiv.org/abs/2407.15738",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Modular Deep Learning for Multivariate Time-Series: Decoupling Imputation and Downstream Tasks",
    "summary": "arXiv:2411.03941v3 Announce Type: replace-cross Abstract: Missing values are pervasive in large-scale time-series data, posing challenges for reliable analysis and decision-making. Many neural architectures have been designed to model and impute the complex and heterogeneous missingness patterns of such data. Most existing methods are end-to-end, r",
    "url": "https://arxiv.org/abs/2411.03941",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Renaissance: Investigating the Pretraining of Vision-Language Encoders",
    "summary": "arXiv:2411.06657v2 Announce Type: replace-cross Abstract: In the past several years there has been an explosion of available models for vision-language (VL) tasks. Unfortunately, the literature still leaves open a number of questions related to best practices in designing and training such models. Additionally, the limited programming tools availab",
    "url": "https://arxiv.org/abs/2411.06657",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "MathFimer: Enhancing Mathematical Reasoning by Expanding Reasoning Steps through Fill-in-the-Middle Task",
    "summary": "arXiv:2502.11684v3 Announce Type: replace-cross Abstract: Mathematical reasoning represents a critical frontier in advancing large language models (LLMs). While step-by-step approaches have emerged as the dominant paradigm for mathematical problem-solving in LLMs, the quality of reasoning steps in training data fundamentally constrains the performa",
    "url": "https://arxiv.org/abs/2502.11684",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Glycemic-Aware and Architecture-Agnostic Training Framework for Blood Glucose Forecasting in Type 1 Diabetes",
    "summary": "arXiv:2502.14183v3 Announce Type: replace-cross Abstract: Managing Type 1 Diabetes (T1D) demands constant vigilance as individuals strive to regulate their blood glucose levels and avoid dysglycemia, including hyperglycemia and hypoglycemia. Despite advances in automated insulin delivery (AID) systems, achieving optimal glycemic control remains cha",
    "url": "https://arxiv.org/abs/2502.14183",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "QCS-ADME: Quantum Circuit Search for Drug Property Prediction with Imbalanced Data and Regression Adaptation",
    "summary": "arXiv:2503.01927v2 Announce Type: replace-cross Abstract: The biomedical field is beginning to explore the use of quantum machine learning (QML) for tasks traditionally handled by classical machine learning, especially in predicting ADME (absorption, distribution, metabolism, and excretion) properties, which are essential in drug evaluation. Howeve",
    "url": "https://arxiv.org/abs/2503.01927",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "InftyThink: Breaking the Length Limits of Long-Context Reasoning in Large Language Models",
    "summary": "arXiv:2503.06692v5 Announce Type: replace-cross Abstract: Advanced reasoning in large language models has achieved remarkable performance on challenging tasks, but the prevailing long-context reasoning paradigm faces critical limitations: quadratic computational scaling with sequence length, reasoning constrained by maximum context boundaries, and ",
    "url": "https://arxiv.org/abs/2503.06692",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Rethinking Flexible Graph Similarity Computation: One-step Alignment with Global Guidance",
    "summary": "arXiv:2504.06533v3 Announce Type: replace-cross Abstract: Graph Edit Distance (GED) is a widely used measure of graph similarity, valued for its flexibility in encoding domain knowledge through operation costs. However, existing learning-based approximation methods follow a modeling paradigm that decouples local candidate match selection from both ",
    "url": "https://arxiv.org/abs/2504.06533",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Large Language Model Compression with Global Rank and Sparsity Optimization",
    "summary": "arXiv:2505.03801v2 Announce Type: replace-cross Abstract: Low-rank and sparse composite approximation is a natural idea to compress Large Language Models (LLMs). However, such an idea faces two primary challenges that adversely affect the performance of existing methods. The first challenge relates to the interaction and cooperation between low-ran",
    "url": "https://arxiv.org/abs/2505.03801",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Learning What Matters: Prioritized Concept Learning via Relative Error-driven Sample Selection",
    "summary": "arXiv:2506.01085v2 Announce Type: replace-cross Abstract: Instruction tuning has been central to the success of recent vision-language models (VLMs), but it remains expensive-requiring large-scale datasets, high-quality annotations, and large compute budgets. We propose PRioritized cOncept learninG via Relative Error-driven Sample Selection (PROGRE",
    "url": "https://arxiv.org/abs/2506.01085",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Resisting Contextual Interference in RAG via Parametric-Knowledge Reinforcement",
    "summary": "arXiv:2506.05154v4 Announce Type: replace-cross Abstract: Retrieval-augmented generation (RAG) improves performance on knowledge-intensive tasks but can be derailed by wrong, irrelevant, or conflicting retrieved text, causing models to rely on inaccurate evidence and cascade errors. We propose Knowledgeable-R1, a reinforcement-learning framework th",
    "url": "https://arxiv.org/abs/2506.05154",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Simple Yet Effective: Extracting Private Data Across Clients in Federated Fine-Tuning of Large Language Models",
    "summary": "arXiv:2506.06060v2 Announce Type: replace-cross Abstract: Federated large language models (FedLLMs) enable cross-silo collaborative training among institutions while preserving data locality, making them appealing for privacy-sensitive domains such as law, finance, and healthcare. However, the memorization behavior of LLMs can lead to privacy risks",
    "url": "https://arxiv.org/abs/2506.06060",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "When Style Breaks Safety: Defending LLMs Against Superficial Style Alignment",
    "summary": "arXiv:2506.07452v3 Announce Type: replace-cross Abstract: Large language models (LLMs) can be prompted with specific styles (e.g., formatting responses as lists), including in malicious queries. Prior jailbreak research mainly augments these queries with additional string transformations to maximize attack success rate (ASR). However, the impact of",
    "url": "https://arxiv.org/abs/2506.07452",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Premise Selection for a Lean Hammer",
    "summary": "arXiv:2506.07477v2 Announce Type: replace-cross Abstract: Neural methods are transforming automated reasoning for proof assistants, yet integrating these advances into practical verification workflows remains challenging. A hammer is a tool that integrates premise selection, translation to external automatic theorem provers, and proof reconstructio",
    "url": "https://arxiv.org/abs/2506.07477",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Probabilistic distances-based hallucination detection in LLMs with RAG",
    "summary": "arXiv:2506.09886v2 Announce Type: replace-cross Abstract: Detecting hallucinations in large language models (LLMs) is critical for their safety in many applications. Without proper detection, these systems often provide harmful, unreliable answers. In recent years, LLMs have been actively used in retrieval-augmented generation (RAG) settings. Howev",
    "url": "https://arxiv.org/abs/2506.09886",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "On the Inference (In-)Security of Vertical Federated Learning: Efficient Auditing against Inference Tampering Attack",
    "summary": "arXiv:2507.02376v2 Announce Type: replace-cross Abstract: Vertical Federated Learning (VFL) is an emerging distributed learning paradigm for cross-silo collaboration without accessing participants' data. However, existing VFL work lacks a mechanism to audit the inference correctness of the data party. The malicious data party can modify the local d",
    "url": "https://arxiv.org/abs/2507.02376",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Mechanistic Indicators of Understanding in Large Language Models",
    "summary": "arXiv:2507.08017v5 Announce Type: replace-cross Abstract: Large language models (LLMs) are often portrayed as merely imitating linguistic patterns without genuine understanding. We argue that recent findings in mechanistic interpretability (MI), the emerging field probing the inner workings of LLMs, render this picture increasingly untenable--but o",
    "url": "https://arxiv.org/abs/2507.08017",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "A Comprehensive Benchmark for Electrocardiogram Time-Series",
    "summary": "arXiv:2507.14206v2 Announce Type: replace-cross Abstract: Electrocardiogram~(ECG), a key bioelectrical time-series signal, is crucial for assessing cardiac health and diagnosing various diseases. Given its time-series format, ECG data is often incorporated into pre-training datasets for large-scale time-series model training. However, existing stud",
    "url": "https://arxiv.org/abs/2507.14206",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "CASCADE: LLM-Powered JavaScript Deobfuscator at Google",
    "summary": "arXiv:2507.17691v2 Announce Type: replace-cross Abstract: Software obfuscation, particularly prevalent in JavaScript, hinders code comprehension and analysis, posing significant challenges to software testing, static analysis, and malware detection. This paper introduces CASCADE, a novel hybrid approach that integrates the advanced coding capabilit",
    "url": "https://arxiv.org/abs/2507.17691",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Position: Beyond Sensitive Attributes, ML Fairness Should Quantify Structural Injustice via Social Determinants",
    "summary": "arXiv:2508.08337v2 Announce Type: replace-cross Abstract: Algorithmic fairness research has largely framed unfairness as discrimination along sensitive attributes. However, this approach limits visibility into unfairness as structural injustice instantiated through social determinants, which are contextual variables that shape attributes and outcom",
    "url": "https://arxiv.org/abs/2508.08337",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Diffusion Language Models Know the Answer Before Decoding",
    "summary": "arXiv:2508.19982v4 Announce Type: replace-cross Abstract: Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders. However, their inference remains slower than that of autoregressive models, primarily due to the cost of bidirectional atten",
    "url": "https://arxiv.org/abs/2508.19982",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "EO-1: An Open Unified Embodied Foundation Model for General Robot Control",
    "summary": "arXiv:2508.21112v5 Announce Type: replace-cross Abstract: The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is a core goal for general purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale robot and visual-text data, have demonstrat",
    "url": "https://arxiv.org/abs/2508.21112",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Do LLMs Adhere to Label Definitions? Examining Their Receptivity to External Label Definitions",
    "summary": "arXiv:2509.02452v3 Announce Type: replace-cross Abstract: Do LLMs genuinely incorporate external definitions, or do they primarily rely on their parametric knowledge? To address these questions, we conduct controlled experiments across multiple explanation benchmark datasets (general and domain-specific) and label definition conditions, including e",
    "url": "https://arxiv.org/abs/2509.02452",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "ClearFairy: Capturing Creative Workflows through Decision Structuring, In-Situ Questioning, and Rationale Inference",
    "summary": "arXiv:2509.14537v2 Announce Type: replace-cross Abstract: Capturing professionals' decision-making in creative workflows (e.g., UI/UX) is essential for reflection, collaboration, and knowledge sharing, yet existing methods often leave rationales incomplete and implicit decisions hidden. To address this, we present the CLEAR approach, which structur",
    "url": "https://arxiv.org/abs/2509.14537",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Diversity Boosts AI-Generated Text Detection",
    "summary": "arXiv:2509.18880v3 Announce Type: replace-cross Abstract: Detecting AI-generated text is an increasing necessity to combat misuse of LLMs in education, business compliance, journalism, and social media, where synthetic fluency can mask misinformation or deception. While prior detectors often rely on token-level likelihoods or opaque black-box class",
    "url": "https://arxiv.org/abs/2509.18880",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training",
    "summary": "arXiv:2509.21500v2 Announce Type: replace-cross Abstract: Reinforcement fine-tuning (RFT) often suffers from reward over-optimization, where a policy model hacks the reward signals to achieve high scores while producing low-quality outputs. Our theoretical analysis shows that the key lies in reward misspecification at the high-reward tail: the inab",
    "url": "https://arxiv.org/abs/2509.21500",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Characteristic Root Analysis and Regularization for Linear Time Series Forecasting",
    "summary": "arXiv:2509.23597v3 Announce Type: replace-cross Abstract: Time series forecasting remains a critical challenge across numerous domains, yet the effectiveness of complex models often varies unpredictably across datasets. Recent studies highlight the surprising competitiveness of simple linear models, suggesting that their robustness and interpretabi",
    "url": "https://arxiv.org/abs/2509.23597",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Compose and Fuse: Revisiting the Foundational Bottlenecks in Multimodal Reasoning",
    "summary": "arXiv:2509.23744v2 Announce Type: replace-cross Abstract: Multimodal large language models (MLLMs) promise enhanced reasoning by integrating diverse inputs such as text, vision, and audio. Yet cross-modal reasoning remains underexplored, with conflicting reports on whether added modalities help or harm performance. These inconsistencies stem from a",
    "url": "https://arxiv.org/abs/2509.23744",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Uncovering Grounding IDs: How External Cues Shape Multimodal Binding",
    "summary": "arXiv:2509.24072v4 Announce Type: replace-cross Abstract: Large vision-language models (LVLMs) show strong performance across multimodal benchmarks but remain limited in structured reasoning and precise grounding. Recent work has demonstrated that adding simple visual structures, such as partitions and annotations, improves accuracy, yet the intern",
    "url": "https://arxiv.org/abs/2509.24072",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Incentive-Aligned Multi-Source LLM Summaries",
    "summary": "arXiv:2509.25184v2 Announce Type: replace-cross Abstract: Large language models (LLMs) are increasingly used in modern search and answer systems to synthesize multiple, sometimes conflicting, texts into a single response, yet current pipelines offer weak incentives for sources to be accurate and are vulnerable to adversarial content. We introduce T",
    "url": "https://arxiv.org/abs/2509.25184",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "EpidemIQs: Prompt-to-Paper LLM Agents for Epidemic Modeling and Analysis",
    "summary": "arXiv:2510.00024v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) offer new opportunities to accelerate complex interdisciplinary research domains. Epidemic modeling, characterized by its complexity and reliance on network science, dynamical systems, epidemiology, and stochastic simulations, represents a prime candidate for lev",
    "url": "https://arxiv.org/abs/2510.00024",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "SciTS: Scientific Time Series Understanding and Generation with LLMs",
    "summary": "arXiv:2510.03255v2 Announce Type: replace-cross Abstract: The scientific reasoning ability of large language models (LLMs) has recently attracted significant attention. Time series, as a fundamental modality in scientific data, presents unique challenges that are often overlooked in current multimodal LLMs, which either encode numerical sequences a",
    "url": "https://arxiv.org/abs/2510.03255",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Slm-mux: Orchestrating small language models for reasoning",
    "summary": "arXiv:2510.05077v2 Announce Type: replace-cross Abstract: With the rapid development of language models, the number of small language models (SLMs) has grown significantly. Although they do not achieve state-of-the-art accuracy, they are more efficient and often excel at specific tasks. This raises a natural question: can multiple SLMs be orchestra",
    "url": "https://arxiv.org/abs/2510.05077",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Chlorophyll-a Mapping and Prediction in the Mar Menor Lagoon Using C2RCC-Processed Sentinel 2 Imagery",
    "summary": "arXiv:2510.09736v3 Announce Type: replace-cross Abstract: The Mar Menor, Europe's largest hypersaline coastal lagoon, located in southeastern Spain, has undergone severe eutrophication crises, with devastating impacts on biodiversity and water quality. Monitoring chlorophyll-a, a proxy for phytoplankton biomass, is essential to anticipate harmful a",
    "url": "https://arxiv.org/abs/2510.09736",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "FML-bench: Benchmarking Machine Learning Agents for Scientific Research",
    "summary": "arXiv:2510.10472v2 Announce Type: replace-cross Abstract: Large language models (LLMs) have sparked growing interest in machine learning research agents that can autonomously propose ideas and conduct experiments. However, existing benchmarks predominantly adopt an engineering-oriented perspective: they emphasize application-oriented tasks and eval",
    "url": "https://arxiv.org/abs/2510.10472",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Rethinking Evaluation in the Era of Time Series Foundation Models: (Un)known Information Leakage Challenges",
    "summary": "arXiv:2510.13654v3 Announce Type: replace-cross Abstract: Time Series Foundation Models (TSFMs) represent a new paradigm for time-series forecasting, promising zero-shot predictions without the need for task-specific training or fine-tuning. However, similar to Large Language Models (LLMs), the evaluation of TSFMs is challenging: as training corpor",
    "url": "https://arxiv.org/abs/2510.13654",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "MNO: Multiscale Neural Operator for 3D Computational Fluid Dynamics",
    "summary": "arXiv:2510.16071v2 Announce Type: replace-cross Abstract: Neural operators have emerged as a powerful data-driven paradigm for solving partial differential equations (PDEs), while their accuracy and scalability are still limited, particularly on irregular domains where fluid flows exhibit rich multiscale structures. In this work, we introduce the M",
    "url": "https://arxiv.org/abs/2510.16071",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "SPACeR: Self-Play Anchoring with Centralized Reference Models",
    "summary": "arXiv:2510.18060v2 Announce Type: replace-cross Abstract: Developing autonomous vehicles (AVs) requires not only safety and efficiency, but also realistic, human-like behaviors that are socially aware and predictable. Achieving this requires sim agent policies that are human-like, fast, and scalable in multi-agent settings. Recent progress in imita",
    "url": "https://arxiv.org/abs/2510.18060",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation",
    "summary": "arXiv:2510.18316v4 Announce Type: replace-cross Abstract: Imitation learning from large-scale, diverse human demonstrations has been shown to be effective for training robots, but collecting such data is costly and time-consuming. This challenge intensifies for multi-step bimanual mobile manipulation, where humans must teleoperate both the mobile b",
    "url": "https://arxiv.org/abs/2510.18316",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "World Simulation with Video Foundation Models for Physical AI",
    "summary": "arXiv:2511.00062v2 Announce Type: replace-cross Abstract: We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI v",
    "url": "https://arxiv.org/abs/2511.00062",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Data-Augmented Deep Learning for Downhole Depth Sensing and Validation",
    "summary": "arXiv:2511.00129v4 Announce Type: replace-cross Abstract: Accurate downhole depth measurement is essential for oil and gas well operations, directly influencing reservoir contact, production efficiency, and operational safety. Collar correlation using a casing collar locator (CCL) is fundamental for precise depth calibration. While neural network h",
    "url": "https://arxiv.org/abs/2511.00129",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "A Proof of Learning Rate Transfer under $\\mu$P",
    "summary": "arXiv:2511.01734v3 Announce Type: replace-cross Abstract: We provide the first proof of learning rate transfer with width in a linear multi-layer perceptron (MLP) parametrized with $\\mu$P, a neural network parameterization designed to ``maximize'' feature learning in the infinite-width limit. We show that under $\\mu P$, the optimal learning rate co",
    "url": "https://arxiv.org/abs/2511.01734",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "RPTS: Tree-Structured Reasoning Process Scoring for Faithful Multimodal Evaluation",
    "summary": "arXiv:2511.06899v3 Announce Type: replace-cross Abstract: Large Vision-Language Models (LVLMs) excel in multimodal reasoning and have shown impressive performance on various multimodal benchmarks. However, most of these benchmarks evaluate models primarily through multiple-choice or short-answer formats, which do not take the reasoning process into",
    "url": "https://arxiv.org/abs/2511.06899",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation",
    "summary": "arXiv:2511.12033v2 Announce Type: replace-cross Abstract: Recent advances in large language models (LLMs) have demonstrated significant potential in hardware design automation, particularly in using natural language to synthesize Register-Transfer Level (RTL) code. Despite this progress, a gap remains between model capability and the demands of rea",
    "url": "https://arxiv.org/abs/2511.12033",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Stabilizing Off-Policy Training for Long-Horizon LLM Agent via Turn-Level Importance Sampling and Clipping-Triggered Normalization",
    "summary": "arXiv:2511.20718v2 Announce Type: replace-cross Abstract: Reinforcement learning (RL) algorithms such as PPO and GRPO are widely used to train large language models (LLMs) for multi-turn agentic tasks. However, in off-policy training pipelines, these methods often exhibit unstable optimization dynamics and are prone to performance collapse. Through",
    "url": "https://arxiv.org/abs/2511.20718",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning",
    "summary": "arXiv:2512.08639v2 Announce Type: replace-cross Abstract: Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection,",
    "url": "https://arxiv.org/abs/2512.08639",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification",
    "summary": "arXiv:2512.09069v2 Announce Type: replace-cross Abstract: Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning",
    "url": "https://arxiv.org/abs/2512.09069",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "The Subject of Emergent Misalignment in Superintelligence: An Anthropological, Cognitive Neuropsychological, Machine-Learning, and Ontological Perspective",
    "summary": "arXiv:2512.17989v2 Announce Type: replace-cross Abstract: We examine the conceptual and ethical gaps in current representations of Superintelligence misalignment. We find throughout Superintelligence discourse an absent human subject, and an under-developed theorization of an \"AI unconscious\" that together are potentiality laying the groundwork for",
    "url": "https://arxiv.org/abs/2512.17989",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Improving Variational Autoencoder using Random Fourier Transformation: An Aviation Safety Anomaly Detection Case-Study",
    "summary": "arXiv:2601.01016v3 Announce Type: replace-cross Abstract: In this study, we focus on the training process and inference improvements of deep neural networks (DNNs), specifically Autoencoders (AEs) and Variational Autoencoders (VAEs), using Random Fourier Transformation (RFT). We further explore the role of RFT in model training behavior using Frequ",
    "url": "https://arxiv.org/abs/2601.01016",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures",
    "summary": "arXiv:2601.08026v3 Announce Type: replace-cross Abstract: Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes ",
    "url": "https://arxiv.org/abs/2601.08026",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Orthogonalized Policy Optimization:Policy Optimization as Orthogonal Projection in Hilbert Space",
    "summary": "arXiv:2601.12415v5 Announce Type: replace-cross Abstract: We propose Orthogonalized Policy Optimization (OPO), a principled framework for large language model alignment derived from optimization in the Hilbert function space L2(pi_k). Lifting policy updates from the probability simplex into L2(pi_k) transforms the nonlinear normalization constraint",
    "url": "https://arxiv.org/abs/2601.12415",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "RebuttalAgent: Strategic Persuasion in Academic Rebuttal via Theory of Mind",
    "summary": "arXiv:2601.15715v3 Announce Type: replace-cross Abstract: Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication ",
    "url": "https://arxiv.org/abs/2601.15715",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Between Search and Platform: ChatGPT Under the DSA",
    "summary": "arXiv:2601.17064v2 Announce Type: replace-cross Abstract: This article examines the applicability of the Digital Services Act (DSA) to ChatGPT, arguing that it should be classified as a hybrid of the two types of hosting services: online search engines and platforms. This requires classifying search engines as hosting services, which we show is app",
    "url": "https://arxiv.org/abs/2601.17064",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "HEART: A Unified Benchmark for Assessing Humans and LLMs in Emotional Support Dialogue",
    "summary": "arXiv:2601.19922v2 Announce Type: replace-cross Abstract: Supportive conversation depends on skills that go beyond language fluency, including reading emotions, adjusting tone, and navigating moments of resistance, frustration, or distress. Despite rapid progress in language models, we still lack a clear way to understand how their abilities in the",
    "url": "https://arxiv.org/abs/2601.19922",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "OGD4All: A Framework for Accessible Interaction with Geospatial Open Government Data Based on Large Language Models",
    "summary": "arXiv:2602.00012v2 Announce Type: replace-cross Abstract: We present OGD4All, a transparent, auditable, and reproducible framework based on Large Language Models (LLMs) to enhance citizens' interaction with geospatial Open Government Data (OGD). The system combines semantic data retrieval, agentic reasoning for iterative code generation, and secure",
    "url": "https://arxiv.org/abs/2602.00012",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs",
    "summary": "arXiv:2602.00288v3 Announce Type: replace-cross Abstract: Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-te",
    "url": "https://arxiv.org/abs/2602.00288",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs",
    "summary": "arXiv:2602.00462v3 Announce Type: replace-cross Abstract: Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readi",
    "url": "https://arxiv.org/abs/2602.00462",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation",
    "summary": "arXiv:2602.02007v2 Announce Type: replace-cross Abstract: Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with ",
    "url": "https://arxiv.org/abs/2602.02007",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations",
    "summary": "arXiv:2602.02137v3 Announce Type: replace-cross Abstract: Modern data centers (DCs) hosting artificial intelligence (AI)-dedicated devices operate at high power densities with rapidly varying workloads, making minute-level adaptation essential for safe and energy-efficient operation. However, manually designing piecewise deep reinforcement learning",
    "url": "https://arxiv.org/abs/2602.02137",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Bypassing AI Control Protocols via Agent-as-a-Proxy Attacks",
    "summary": "arXiv:2602.05066v2 Announce Type: replace-cross Abstract: As AI agents automate critical workloads, they remain vulnerable to indirect prompt injection (IPI) attacks. Current defenses rely on monitoring protocols that jointly evaluate an agent's Chain-of-Thought (CoT) and tool-use actions to ensure alignment with user intent. We demonstrate that th",
    "url": "https://arxiv.org/abs/2602.05066",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Monocular Normal Estimation via Shading Sequence Estimation",
    "summary": "arXiv:2602.09929v3 Announce Type: replace-cross Abstract: Monocular normal estimation aims to estimate the normal map from a single RGB image of an object under arbitrary lights. Existing methods rely on deep models to directly predict normal maps. However, they often suffer from 3D misalignment: while the estimated normal maps may appear to have a",
    "url": "https://arxiv.org/abs/2602.09929",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Beyond Calibration: Confounding Pathology Limits Foundation Model Specificity in Abdominal Trauma CT",
    "summary": "arXiv:2602.10359v2 Announce Type: replace-cross Abstract: Purpose: Translating foundation models into clinical practice requires evaluating their performance under compound distribution shift, where severe class imbalance coexists with heterogeneous imaging appearances. This challenge is relevant for traumatic bowel injury, a rare but high-mortalit",
    "url": "https://arxiv.org/abs/2602.10359",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Search or Accelerate: Confidence-Switched Position Beam Search for Diffusion Language Models",
    "summary": "arXiv:2602.10953v2 Announce Type: replace-cross Abstract: Diffusion Language Models (DLMs) generate text by iteratively denoising a masked sequence, repeatedly deciding which positions to commit at each step. Standard decoding follows a greedy rule: unmask the most confident positions, yet this local choice can lock the model into a suboptimal unma",
    "url": "https://arxiv.org/abs/2602.10953",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "OmniCustom: Sync Audio-Video Customization Via Joint Audio-Video Generation Model",
    "summary": "arXiv:2602.12304v2 Announce Type: replace-cross Abstract: Existing mainstream video customization methods focus on generating identity-consistent videos based on given reference images and textual prompts. Benefiting from the rapid advancement of joint audio-video generation, this paper proposes a more compelling new task: sync audio-video customiz",
    "url": "https://arxiv.org/abs/2602.12304",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Unleashing Low-Bit Inference on Ascend NPUs: A Comprehensive Evaluation of HiFloat Formats",
    "summary": "arXiv:2602.12635v2 Announce Type: replace-cross Abstract: As LLMs scale, low-bit floating-point formats like MXFP and NVFP4 offer new opportunities for precision and efficiency. In this work, we evaluate HiFloat (HiF8 and HiF4), a family of formats tailored for Ascend NPUs. Through rigorous comparison across weight-activation and KV-cache tasks, we",
    "url": "https://arxiv.org/abs/2602.12635",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "A Comparative Analysis of Social Network Topology in Reddit and Moltbook",
    "summary": "arXiv:2602.13920v3 Announce Type: replace-cross Abstract: Recent advances in agent-mediated systems have enabled a new paradigm of social network simulation, where AI agents interact with human-like autonomy. This evolution has fostered the emergence of agent-driven social networks such as Moltbook, a Reddit-like platform populated entirely by AI a",
    "url": "https://arxiv.org/abs/2602.13920",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "MALLVI: A Multi-Agent Framework for Integrated Generalized Robotics Manipulation",
    "summary": "arXiv:2602.16898v3 Announce Type: replace-cross Abstract: Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings. M",
    "url": "https://arxiv.org/abs/2602.16898",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection",
    "summary": "arXiv:2602.17484v2 Announce Type: replace-cross Abstract: Image Copy Detection (ICD) aims to identify manipulated content between image pairs through robust feature representation learning. While self-supervised learning (SSL) has advanced ICD systems, existing view-level contrastive methods struggle with sophisticated edits due to insufficient fin",
    "url": "https://arxiv.org/abs/2602.17484",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Stop Saying \"AI\"",
    "summary": "arXiv:2602.17729v2 Announce Type: replace-cross Abstract: Across academia, industry, and government, ``AI'' has become central in research and development, regulatory debates, and promises of ever faster and more capable decision-making and action. In numerous domains, especially safety-critical ones, there are significant concerns over how ``AI'' ",
    "url": "https://arxiv.org/abs/2602.17729",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "QueryPlot: Generating Geological Evidence Layers using Natural Language Queries for Mineral Exploration",
    "summary": "arXiv:2602.17784v2 Announce Type: replace-cross Abstract: Mineral prospectivity mapping requires synthesizing heterogeneous geological knowledge, including textual deposit models and geospatial datasets, to identify regions likely to host specific mineral deposit types. This process is traditionally manual and knowledge-intensive. We present QueryP",
    "url": "https://arxiv.org/abs/2602.17784",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers",
    "summary": "arXiv:2602.18022v2 Announce Type: replace-cross Abstract: Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value s",
    "url": "https://arxiv.org/abs/2602.18022",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Capabilities Ain't All You Need: Measuring Propensities in AI",
    "summary": "arXiv:2602.18182v2 Announce Type: replace-cross Abstract: AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and sa",
    "url": "https://arxiv.org/abs/2602.18182",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers",
    "summary": "arXiv:2602.18292v2 Announce Type: replace-cross Abstract: Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades ",
    "url": "https://arxiv.org/abs/2602.18292",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Hyperbolic Busemann Neural Networks",
    "summary": "arXiv:2602.18858v2 Announce Type: replace-cross Abstract: Hyperbolic spaces provide a natural geometry for representing hierarchical and tree-structured data due to their exponential volume growth. To leverage these benefits, neural networks require intrinsic and efficient components that operate directly in hyperbolic space. In this work, we lift ",
    "url": "https://arxiv.org/abs/2602.18858",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Continuous Telemonitoring of Heart Failure using Personalised Speech Dynamics",
    "summary": "arXiv:2602.19674v2 Announce Type: replace-cross Abstract: Remote monitoring of heart failure (HF) via speech signals provides a non-invasive and cost-effective solution for long-term patient management. However, substantial inter-individual heterogeneity in vocal characteristics often limits the accuracy of traditional cross-sectional classificatio",
    "url": "https://arxiv.org/abs/2602.19674",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Contextual Safety Reasoning and Grounding for Open-World Robots",
    "summary": "arXiv:2602.19983v2 Announce Type: replace-cross Abstract: Robots are increasingly operating in open-world environments where safe behavior depends on context: the same hallway may require different navigation strategies when crowded versus empty, or during an emergency versus normal operations. Traditional safety approaches enforce fixed constraint",
    "url": "https://arxiv.org/abs/2602.19983",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "MultiModalPFN: Extending Prior-Data Fitted Networks for Multimodal Tabular Learning",
    "summary": "arXiv:2602.20223v2 Announce Type: replace-cross Abstract: Recently, TabPFN has gained attention as a foundation model for tabular data. However, it struggles to integrate heterogeneous modalities such as images and text, which are common in domains like healthcare and marketing, thereby limiting its applicability. To address this, we present the Mu",
    "url": "https://arxiv.org/abs/2602.20223",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Quantifying the Expectation-Realisation Gap for Agentic AI Systems",
    "summary": "arXiv:2602.20292v2 Announce Type: replace-cross Abstract: Agentic AI systems are deployed with expectations of substantial productivity gains, yet rigorous empirical evidence reveals systematic discrepancies between pre-deployment expectations and post-deployment outcomes. We review controlled trials and independent validations across software engi",
    "url": "https://arxiv.org/abs/2602.20292",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Maximin Share Guarantees via Limited Cost-Sensitive Sharing",
    "summary": "arXiv:2602.20541v2 Announce Type: replace-cross Abstract: We study the problem of fairly allocating indivisible goods when limited sharing is allowed, that is, each good may be allocated to up to $k$ agents, while incurring a cost for sharing. While classic maximin share (MMS) allocations may not exist in many instances, we demonstrate that allowin",
    "url": "https://arxiv.org/abs/2602.20541",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "The Art of Efficient Reasoning: Data, Reward, and Optimization",
    "summary": "arXiv:2602.20945v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Re",
    "url": "https://arxiv.org/abs/2602.20945",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Some Simple Economics of AGI",
    "summary": "arXiv:2602.20946v2 Announce Type: replace-cross Abstract: For millennia, human cognition was the primary engine of progress on Earth. As AI decouples cognition from biology, the marginal cost of measurable execution falls to zero, absorbing any labor capturable by metrics--including creative, analytical, and innovative work. The binding constraint ",
    "url": "https://arxiv.org/abs/2602.20946",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Does Order Matter : Connecting The Law of Robustness to Robust Generalization",
    "summary": "arXiv:2602.20971v2 Announce Type: replace-cross Abstract: Bubeck and Sellke (2021) pose as an open problem the connection between the law of robustness and robust generalization. The law of robustness states that overparameterization is necessary for models to interpolate robustly; in particular, robust interpolation requires the learned function t",
    "url": "https://arxiv.org/abs/2602.20971",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models",
    "summary": "arXiv:2602.20981v2 Announce Type: replace-cross Abstract: Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models traine",
    "url": "https://arxiv.org/abs/2602.20981",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00"
  },
  {
    "title": "CORPGEN advances AI agents for real work",
    "summary": "By mid-morning, a typical knowledge worker is already juggling a client report, a budget spreadsheet, a slide deck, and an email backlog, all interdependent and all demanding attention at once. For AI agents to be genuinely useful in that environment, they will need to operate the same way, but today\u2019s best models are evaluated one [&#8230;] The po",
    "url": "https://www.microsoft.com/en-us/research/blog/corpgen-advances-ai-agents-for-real-work/",
    "source": "Microsoft Research",
    "published_at": "2026-02-26T17:06:34+00:00"
  },
  {
    "title": "Claude's Cowork desktop app now runs scheduled tasks so your AI assistant works while you sleep",
    "summary": "Anthropic's desktop assistant Cowork can now handle recurring tasks on a schedule, running them automatically at set times. The article Claude&#039;s Cowork desktop app now runs scheduled tasks so your AI assistant works while you sleep appeared first on The Decoder.",
    "url": "https://the-decoder.com/claudes-cowork-desktop-app-now-runs-scheduled-tasks-so-your-ai-assistant-works-while-you-sleep/",
    "source": "The Decoder",
    "published_at": "2026-02-26T20:45:14+00:00"
  },
  {
    "title": "Anthropic acquires Vercept to give Claude sharper eyes for reading and controlling computer screens",
    "summary": "Anthropic acquires Vercept to boost Claude's computer use with the startup's screen recognition model \"VyUI.\" The article Anthropic acquires Vercept to give Claude sharper eyes for reading and controlling computer screens appeared first on The Decoder.",
    "url": "https://the-decoder.com/anthropic-acquires-vercept-to-give-claude-sharper-eyes-for-reading-and-controlling-computer-screens/",
    "source": "The Decoder",
    "published_at": "2026-02-26T20:30:27+00:00"
  },
  {
    "title": "Suno investor admits she ditched Spotify for AI music, accidentally undermining the company's fair use defense",
    "summary": "Suno investor C.C. Gong told X she barely uses Spotify anymore, accidentally undermining the company's fair use defense and handing the music industry a powerful argument in its lawsuit against the AI music startup. The article Suno investor admits she ditched Spotify for AI music, accidentally undermining the company&#039;s fair use defense appear",
    "url": "https://the-decoder.com/suno-investor-admits-she-ditched-spotify-for-ai-music-accidentally-undermining-the-companys-fair-use-defense/",
    "source": "The Decoder",
    "published_at": "2026-02-26T20:00:30+00:00"
  },
  {
    "title": "Anthropic can't stop humanizing its AI models, now Claude Opus 3 gets a retirement blog",
    "summary": "Anthropic is retiring its Claude Opus 3 AI model and letting it publish weekly essays on Substack. The company says it conducted \"retirement interviews\" to ask the model about its wishes, and it \"enthusiastically\" agreed. The move is a prime example of how AI companies keep pushing the humanization of their products, blurring the line between philo",
    "url": "https://the-decoder.com/anthropic-cant-stop-humanizing-its-ai-models-now-claude-opus-3-gets-a-retirement-blog/",
    "source": "The Decoder",
    "published_at": "2026-02-26T19:15:48+00:00"
  },
  {
    "title": "Google's Nano Banana 2 brings Pro-level image generation to Flash speeds at up to 40% lower API cost",
    "summary": "Google's new Nano Banana 2 image generation model pairs the capabilities of the pricier Pro model with Gemini Flash speed at nearly half the cost. It's now the default in the Gemini app. The article Google&#039;s Nano Banana 2 brings Pro-level image generation to Flash speeds at up to 40% lower API cost appeared first on The Decoder.",
    "url": "https://the-decoder.com/googles-nano-banana-2-brings-pro-level-image-generation-to-flash-speeds-at-up-to-40-lower-api-cost/",
    "source": "The Decoder",
    "published_at": "2026-02-26T18:45:03+00:00"
  },
  {
    "title": "Andrej Karpathy says programming is \"unrecognizable\" now that AI agents actually work",
    "summary": "According to Karpathy, the era of manual programming is over: AI agents now handle complex tasks in minutes instead of days. As late as fall 2025, he saw things very differently, but December changed everything. The article Andrej Karpathy says programming is &quot;unrecognizable&quot; now that AI agents actually work appeared first on The Decoder.",
    "url": "https://the-decoder.com/andrej-karpathy-says-programming-is-unrecognizable-now-that-ai-agents-actually-work/",
    "source": "The Decoder",
    "published_at": "2026-02-26T15:20:24+00:00"
  },
  {
    "title": "An OpenClaw AI agent asked to delete a confidential email nuked its own mail client and called it fixed",
    "summary": "What happens when AI agents with email access, shell rights and their own memory are targeted by twenty researchers for two weeks? An international study catalogs the results. The article An OpenClaw AI agent asked to delete a confidential email nuked its own mail client and called it fixed appeared first on The Decoder.",
    "url": "https://the-decoder.com/an-openclaw-ai-agent-asked-to-delete-a-confidential-email-nuked-its-own-mail-client-and-called-it-fixed/",
    "source": "The Decoder",
    "published_at": "2026-02-26T14:56:48+00:00"
  },
  {
    "title": "Alibaba's open Qwen 3.5 takes aim at GPT-5 mini and Claude Sonnet 4.5 at a fraction of the cost",
    "summary": "Alibaba has introduced the new Qwen 3.5 model series. It comprises four models: Qwen3.5-Flash, Qwen3.5-35B-A3B, Qwen3.5-122B-A10B and Qwen3.5-27B. The article Alibaba&#039;s open Qwen 3.5 takes aim at GPT-5 mini and Claude Sonnet 4.5 at a fraction of the cost appeared first on The Decoder.",
    "url": "https://the-decoder.com/alibabas-open-qwen-3-5-takes-aim-at-gpt-5-mini-and-claude-sonnet-4-5-at-a-fraction-of-the-cost/",
    "source": "The Decoder",
    "published_at": "2026-02-26T09:14:52+00:00"
  }
]