[
  {
    "title": "Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning",
    "summary": "arXiv:2602.06107v1 Announce Type: new Abstract: Reinforcement learning (RL) for large language models (LLMs) remains expensive, particularly because the rollout is expensive. Decoupling rollout generation from policy optimization (e.g., leveraging a more efficient model to rollout) could enable substantial efficiency gains, yet doing so introduces ",
    "url": "https://arxiv.org/abs/2602.06107",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Large Language Model Reasoning Failures",
    "summary": "arXiv:2602.06176v1 Announce Type: new Abstract: Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortco",
    "url": "https://arxiv.org/abs/2602.06176",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Do It for HER: First-Order Temporal Logic Reward Specification in Reinforcement Learning (Extended Version)",
    "summary": "arXiv:2602.06227v1 Announce Type: new Abstract: In this work, we propose a novel framework for the logical specification of non-Markovian rewards in Markov Decision Processes (MDPs) with large state spaces. Our approach leverages Linear Temporal Logic Modulo Theories over finite traces (LTLfMT), a more expressive extension of classical temporal log",
    "url": "https://arxiv.org/abs/2602.06227",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Do LLMs Act Like Rational Agents? Measuring Belief Coherence in Probabilistic Decision Making",
    "summary": "arXiv:2602.06286v1 Announce Type: new Abstract: Large language models (LLMs) are increasingly deployed as agents in high-stakes domains where optimal actions depend on both uncertainty about the world and consideration of utilities of different outcomes, yet their decision logic remains difficult to interpret. We study whether LLMs are rational uti",
    "url": "https://arxiv.org/abs/2602.06286",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Exposing Weaknesses of Large Reasoning Models through Graph Algorithm Problems",
    "summary": "arXiv:2602.06319v1 Announce Type: new Abstract: Large Reasoning Models (LRMs) have advanced rapidly; however, existing benchmarks in mathematics, code, and common-sense reasoning remain limited. They lack long-context evaluation, offer insufficient challenge, and provide answers that are difficult to verify programmatically. We introduce GrAlgoBenc",
    "url": "https://arxiv.org/abs/2602.06319",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Trifuse: Enhancing Attention-Based GUI Grounding via Multimodal Fusion",
    "summary": "arXiv:2602.06351v1 Announce Type: new Abstract: GUI grounding maps natural language instructions to the correct interface elements, serving as the perception foundation for GUI agents. Existing approaches predominantly rely on fine-tuning multimodal large language models (MLLMs) using large-scale GUI datasets to predict target element coordinates, ",
    "url": "https://arxiv.org/abs/2602.06351",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Difficulty-Estimated Policy Optimization",
    "summary": "arXiv:2602.06375v1 Announce Type: new Abstract: Recent advancements in Large Reasoning Models (LRMs), exemplified by DeepSeek-R1, have underscored the potential of scaling inference-time compute through Group Relative Policy Optimization (GRPO). However, GRPO frequently suffers from gradient signal attenuation when encountering problems that are ei",
    "url": "https://arxiv.org/abs/2602.06375",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Unlocking Noisy Real-World Corpora for Foundation Model Pre-Training via Quality-Aware Tokenization",
    "summary": "arXiv:2602.06394v1 Announce Type: new Abstract: Current tokenization methods process sequential data without accounting for signal quality, limiting their effectiveness on noisy real-world corpora. We present QA-Token (Quality-Aware Tokenization), which incorporates data reliability directly into vocabulary construction. We make three key contribut",
    "url": "https://arxiv.org/abs/2602.06394",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Intrinsic Stability Limits of Autoregressive Reasoning: Structural Consequences for Long-Horizon Execution",
    "summary": "arXiv:2602.06413v1 Announce Type: new Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities, yet their performance often deteriorates sharply in long-horizon tasks, exhibiting systematic breakdown beyond certain scales. Conventional explanations primarily attribute this phenomenon to task complexity, such as combinato",
    "url": "https://arxiv.org/abs/2602.06413",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents",
    "summary": "arXiv:2602.06485v1 Announce Type: new Abstract: While Large Language Model (LLM)-based agents have shown remarkable potential for solving complex tasks, existing systems remain heavily reliant on large-scale models, leaving the capabilities of edge-scale models largely underexplored. In this paper, we present the first systematic study on training ",
    "url": "https://arxiv.org/abs/2602.06485",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "JADE: Expert-Grounded Dynamic Evaluation for Open-Ended Professional Tasks",
    "summary": "arXiv:2602.06486v1 Announce Type: new Abstract: Evaluating agentic AI on open-ended professional tasks faces a fundamental dilemma between rigor and flexibility. Static rubrics provide rigorous, reproducible assessment but fail to accommodate diverse valid response strategies, while LLM-as-a-judge approaches adapt to individual responses yet suffer",
    "url": "https://arxiv.org/abs/2602.06486",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Progress Constraints for Reinforcement Learning in Behavior Trees",
    "summary": "arXiv:2602.06525v1 Announce Type: new Abstract: Behavior Trees (BTs) provide a structured and reactive framework for decision-making, commonly used to switch between sub-controllers based on environmental conditions. Reinforcement Learning (RL), on the other hand, can learn near-optimal controllers but sometimes struggles with sparse rewards, safe ",
    "url": "https://arxiv.org/abs/2602.06525",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "HyPER: Bridging Exploration and Exploitation for Scalable LLM Reasoning with Hypothesis Path Expansion and Reduction",
    "summary": "arXiv:2602.06527v1 Announce Type: new Abstract: Scaling test-time compute with multi-path chain-of-thought improves reasoning accuracy, but its effectiveness depends critically on the exploration-exploitation trade-off. Existing approaches address this trade-off in rigid ways: tree-structured search hard-codes exploration through brittle expansion ",
    "url": "https://arxiv.org/abs/2602.06527",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "LogicSkills: A Structured Benchmark for Formal Reasoning in Large Language Models",
    "summary": "arXiv:2602.06533v1 Announce Type: new Abstract: Large language models have demonstrated notable performance across various logical reasoning benchmarks. However, it remains unclear which core logical skills they truly master. To address this, we introduce LogicSkills, a unified benchmark designed to isolate three fundamental skills in formal reason",
    "url": "https://arxiv.org/abs/2602.06533",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research",
    "summary": "arXiv:2602.06540v1 Announce Type: new Abstract: Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the in",
    "url": "https://arxiv.org/abs/2602.06540",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees",
    "summary": "arXiv:2602.06554v1 Announce Type: new Abstract: Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and",
    "url": "https://arxiv.org/abs/2602.06554",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Same Answer, Different Representations: Hidden instability in VLMs",
    "summary": "arXiv:2602.06652v1 Announce Type: new Abstract: The robustness of Vision Language Models (VLMs) is commonly assessed through output-level invariance, implicitly assuming that stable predictions reflect stable multimodal processing. In this work, we argue that this assumption is insufficient. We introduce a representation-aware and frequency-aware e",
    "url": "https://arxiv.org/abs/2602.06652",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Autoregressive Models for Knowledge Graph Generation",
    "summary": "arXiv:2602.06707v1 Announce Type: new Abstract: Knowledge Graph (KG) generation requires models to learn complex semantic dependencies between triples while maintaining domain validity constraints. Unlike link prediction, which scores triples independently, generative models must capture interdependencies across entire subgraphs to produce semantic",
    "url": "https://arxiv.org/abs/2602.06707",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Semantically Labelled Automata for Multi-Task Reinforcement Learning with LTL Instructions",
    "summary": "arXiv:2602.06746v1 Announce Type: new Abstract: We study multi-task reinforcement learning (RL), a setting in which an agent learns a single, universal policy capable of generalising to arbitrary, possibly unseen tasks. We consider tasks specified as linear temporal logic (LTL) formulae, which are commonly used in formal methods to specify properti",
    "url": "https://arxiv.org/abs/2602.06746",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Towards Understanding What State Space Models Learn About Code",
    "summary": "arXiv:2602.06774v1 Announce Type: new Abstract: State Space Models (SSMs) have emerged as an efficient alternative to the transformer architecture. Recent studies show that SSMs can match or surpass Transformers on code understanding tasks, such as code retrieval, when trained under similar conditions. However, their internal mechanisms remain a bl",
    "url": "https://arxiv.org/abs/2602.06774",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Wild Guesses and Mild Guesses in Active Concept Learning",
    "summary": "arXiv:2602.06818v1 Announce Type: new Abstract: Human concept learning is typically active: learners choose which instances to query or test in order to reduce uncertainty about an underlying rule or category. Active concept learning must balance informativeness of queries against the stability of the learner that generates and scores hypotheses. W",
    "url": "https://arxiv.org/abs/2602.06818",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training",
    "summary": "arXiv:2602.06820v1 Announce Type: new Abstract: Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration. However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. ",
    "url": "https://arxiv.org/abs/2602.06820",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "POP: Online Structural Pruning Enables Efficient Inference of Large Foundation Models",
    "summary": "arXiv:2602.06822v1 Announce Type: new Abstract: Large foundation models (LFMs) achieve strong performance through scaling, yet current structural pruning methods derive fixed pruning decisions during inference, overlooking sparsity patterns that emerge in the autoregressive token generation. In this paper, we propose POP (Partition-guided Online Pr",
    "url": "https://arxiv.org/abs/2602.06822",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "LLM Active Alignment: A Nash Equilibrium Perspective",
    "summary": "arXiv:2602.06836v1 Announce Type: new Abstract: We develop a game-theoretic framework for predicting and steering the behavior of populations of large language models (LLMs) through Nash equilibrium (NE) analysis. To avoid the intractability of equilibrium computation in open-ended text spaces, we model each agent's action as a mixture over human s",
    "url": "https://arxiv.org/abs/2602.06836",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "An Adaptive Differentially Private Federated Learning Framework with Bi-level Optimization",
    "summary": "arXiv:2602.06838v1 Announce Type: new Abstract: Federated learning enables collaborative model training across distributed clients while preserving data privacy. However, in practical deployments, device heterogeneity, non-independent, and identically distributed (Non-IID) data often lead to highly unstable and biased gradient updates. When differe",
    "url": "https://arxiv.org/abs/2602.06838",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "From Features to Actions: Explainability in Traditional and Agentic AI Systems",
    "summary": "arXiv:2602.06841v1 Announce Type: new Abstract: Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfol",
    "url": "https://arxiv.org/abs/2602.06841",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents",
    "summary": "arXiv:2602.06855v1 Announce Type: new Abstract: LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mat",
    "url": "https://arxiv.org/abs/2602.06855",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Agentic Uncertainty Reveals Agentic Overconfidence",
    "summary": "arXiv:2602.06948v1 Announce Type: new Abstract: Can AI agents predict whether they will succeed at a task? We study agentic uncertainty by eliciting success probability estimates before, during, and after task execution. All results exhibit agentic overconfidence: some agents that succeed only 22% of the time predict 77% success. Counterintuitively",
    "url": "https://arxiv.org/abs/2602.06948",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "EUGens: Efficient, Unified, and General Dense Layers",
    "summary": "arXiv:2410.09771v2 Announce Type: cross Abstract: Efficient neural networks are essential for scaling machine learning models to real-time applications and resource-constrained environments. Fully-connected feedforward layers (FFLs) introduce computation and parameter count bottlenecks within neural network architectures. To address this challenge,",
    "url": "https://arxiv.org/abs/2410.09771",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Git for Sketches: An Intelligent Tracking System for Capturing Design Evolution",
    "summary": "arXiv:2602.06047v1 Announce Type: cross Abstract: During product conceptualization, capturing the non-linear history and cognitive intent is crucial. Traditional sketching tools often lose this context. We introduce DIMES (Design Idea Management and Evolution capture System), a web-based environment featuring sGIT (SketchGit), a custom visual versi",
    "url": "https://arxiv.org/abs/2602.06047",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Recontextualizing Famous Quotes for Brand Slogan Generation",
    "summary": "arXiv:2602.06049v1 Announce Type: cross Abstract: Slogans are concise and memorable catchphrases that play a crucial role in advertising by conveying brand identity and shaping public perception. However, advertising fatigue reduces the effectiveness of repeated slogans, creating a growing demand for novel, creative, and insightful slogan generatio",
    "url": "https://arxiv.org/abs/2602.06049",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Rethinking Memory Mechanisms of Foundation Agents in the Second Half",
    "summary": "arXiv:2602.06052v1 Announce Type: cross Abstract: The research of artificial intelligence is undergoing a paradigm shift from prioritizing model innovations over benchmark scores towards emphasizing problem definition and rigorous real-world evaluation. As the field enters the \"second half,\" the central challenge becomes real utility in long-horizo",
    "url": "https://arxiv.org/abs/2602.06052",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Analyzing Diffusion and Autoregressive Vision Language Models in Multimodal Embedding Space",
    "summary": "arXiv:2602.06056v1 Announce Type: cross Abstract: Embedding models are a fundamental component of modern AI systems such as semantic search and retrieval-augmented generation. Recent advances in large foundation models have substantially accelerated the development of embedding models, including those based on Large Language Models (LLMs), Vision L",
    "url": "https://arxiv.org/abs/2602.06056",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "iScheduler: Reinforcement Learning-Driven Continual Optimization for Large-Scale Resource Investment Problems",
    "summary": "arXiv:2602.06064v1 Announce Type: cross Abstract: Scheduling precedence-constrained tasks under shared renewable resources is central to modern computing platforms. The Resource Investment Problem (RIP) models this setting by minimizing the cost of provisioned renewable resources under precedence and timing constraints. Exact mixed-integer programm",
    "url": "https://arxiv.org/abs/2602.06064",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "HQP: Sensitivity-Aware Hybrid Quantization and Pruning for Ultra-Low-Latency Edge AI Inference",
    "summary": "arXiv:2602.06069v1 Announce Type: cross Abstract: The escalating demand for high-fidelity, real-time inference in distributed edge-cloud environments necessitates aggressive model optimization to counteract severe latency and energy constraints. This paper introduces the Hybrid Quantization and Pruning (HQP) framework, a novel, integrated methodolo",
    "url": "https://arxiv.org/abs/2602.06069",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Allocate Marginal Reviews to Borderline Papers Using LLM Comparative Ranking",
    "summary": "arXiv:2602.06078v1 Announce Type: cross Abstract: This paper argues that large ML conferences should allocate marginal review capacity primarily to papers near the acceptance boundary, rather than spreading extra reviews via random or affinity-driven heuristics. We propose using LLM-based comparative ranking (via pairwise comparisons and a Bradley-",
    "url": "https://arxiv.org/abs/2602.06078",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Communication Enhances LLMs' Stability in Strategic Thinking",
    "summary": "arXiv:2602.06081v1 Announce Type: cross Abstract: Large Language Models (LLMs) often exhibit pronounced context-dependent variability that undermines predictable multi-agent behavior in tasks requiring strategic thinking. Focusing on models that range from 7 to 9 billion parameters in size engaged in a ten-round repeated Prisoner's Dilemma, we eval",
    "url": "https://arxiv.org/abs/2602.06081",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Transformer-Based Reinforcement Learning for Autonomous Orbital Collision Avoidance in Partially Observable Environments",
    "summary": "arXiv:2602.06088v1 Announce Type: cross Abstract: We introduce a Transformer-based Reinforcement Learning framework for autonomous orbital collision avoidance that explicitly models the effects of partial observability and imperfect monitoring in space operations. The framework combines a configurable encounter simulator, a distance-dependent obser",
    "url": "https://arxiv.org/abs/2602.06088",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "SVRepair: Structured Visual Reasoning for Automated Program Repair",
    "summary": "arXiv:2602.06090v1 Announce Type: cross Abstract: Large language models (LLMs) have recently shown strong potential for Automated Program Repair (APR), yet most existing approaches remain unimodal and fail to leverage the rich diagnostic signals contained in visual artifacts such as screenshots and control-flow graphs. In practice, many bug reports",
    "url": "https://arxiv.org/abs/2602.06090",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "NanoNet: Parameter-Efficient Learning with Label-Scarce Supervision for Lightweight Text Mining Model",
    "summary": "arXiv:2602.06093v1 Announce Type: cross Abstract: The lightweight semi-supervised learning (LSL) strategy provides an effective approach of conserving labeled samples and minimizing model inference costs. Prior research has effectively applied knowledge transfer learning and co-training regularization from large to small models in LSL. However, suc",
    "url": "https://arxiv.org/abs/2602.06093",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Coding Agents with Environment Interaction: A Theoretical Perspective",
    "summary": "arXiv:2602.06098v1 Announce Type: cross Abstract: Coding agents are increasingly utilized in test-driven software development, yet the theoretical mechanisms behind their environment-interaction strategies remain underexplored. We provide a probabilistic framework for two dominant paradigms: code selection after generation using the execution envir",
    "url": "https://arxiv.org/abs/2602.06098",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Urban Spatio-Temporal Foundation Models for Climate-Resilient Housing: Scaling Diffusion Transformers for Disaster Risk Prediction",
    "summary": "arXiv:2602.06129v1 Announce Type: cross Abstract: Climate hazards increasingly disrupt urban transportation and emergency-response operations by damaging housing stock, degrading infrastructure, and reducing network accessibility. This paper presents Skjold-DiT, a diffusion-transformer framework that integrates heterogeneous spatio-temporal urban d",
    "url": "https://arxiv.org/abs/2602.06129",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Self-Improving World Modelling with Latent Actions",
    "summary": "arXiv:2602.06130v1 Announce Type: cross Abstract: Internal modelling of the world -- predicting transitions between previous states $X$ and next states $Y$ under actions $Z$ -- is essential to reasoning and planning for LLMs and VLMs. Learning such models typically requires costly action-labelled trajectories. We propose SWIRL, a self-improvement f",
    "url": "https://arxiv.org/abs/2602.06130",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Hear You in Silence: Designing for Active Listening in Human Interaction with Conversational Agents Using Context-Aware Pacing",
    "summary": "arXiv:2602.06134v1 Announce Type: cross Abstract: In human conversation, empathic dialogue requires nuanced temporal cues indicating whether the conversational partner is paying attention. This type of \"active listening\" is overlooked in the design of Conversational Agents (CAs), which use the same pacing for one conversation. To model the temporal",
    "url": "https://arxiv.org/abs/2602.06134",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Protean Compiler: An Agile Framework to Drive Fine-grain Phase Ordering",
    "summary": "arXiv:2602.06142v1 Announce Type: cross Abstract: The phase ordering problem has been a long-standing challenge since the late 1970s, yet it remains an open problem due to having a vast optimization space and an unbounded nature, making it an open-ended problem without a finite solution, one can limit the scope by reducing the number and the length",
    "url": "https://arxiv.org/abs/2602.06142",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding",
    "summary": "arXiv:2602.06161v1 Announce Type: cross Abstract: Parallel diffusion decoding can accelerate diffusion language model inference by unmasking multiple tokens per step, but aggressive parallelism often harms quality. Revocable decoding mitigates this by rechecking earlier tokens, yet we observe that existing verification schemes frequently trigger fl",
    "url": "https://arxiv.org/abs/2602.06161",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Optimal rates for density and mode estimation with expand-and-sparsify representations",
    "summary": "arXiv:2602.06175v1 Announce Type: cross Abstract: Expand-and-sparsify representations are a class of theoretical models that capture sparse representation phenomena observed in the sensory systems of many animals. At a high level, these representations map an input $x \\in \\mathbb{R}^d$ to a much higher dimension $m \\gg d$ via random linear projecti",
    "url": "https://arxiv.org/abs/2602.06175",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Generics in science communication: Misaligned interpretations across laypeople, scientists, and large language models",
    "summary": "arXiv:2602.06190v1 Announce Type: cross Abstract: Scientists often use generics, that is, unquantified statements about whole categories of people or phenomena, when communicating research findings (e.g., \"statins reduce cardiovascular events\"). Large language models (LLMs), such as ChatGPT, frequently adopt the same style when summarizing scientif",
    "url": "https://arxiv.org/abs/2602.06190",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Personagram: Bridging Personas and Product Design for Creative Ideation with Multimodal LLMs",
    "summary": "arXiv:2602.06197v1 Announce Type: cross Abstract: Product designers often begin their design process with handcrafted personas. While personas are intended to ground design decisions in consumer preferences, they often fall short in practice by remaining abstract, expensive to produce, and difficult to translate into actionable design features. As ",
    "url": "https://arxiv.org/abs/2602.06197",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "AnyThermal: Towards Learning Universal Representations for Thermal Perception",
    "summary": "arXiv:2602.06203v1 Announce Type: cross Abstract: We present AnyThermal, a thermal backbone that captures robust task-agnostic thermal features suitable for a variety of tasks such as cross-modal place recognition, thermal segmentation, and monocular depth estimation using thermal images. Existing thermal backbones that follow task-specific trainin",
    "url": "https://arxiv.org/abs/2602.06203",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Learning Rate Scaling across LoRA Ranks and Transfer to Full Finetuning",
    "summary": "arXiv:2602.06204v1 Announce Type: cross Abstract: Low-Rank Adaptation (LoRA) is a standard tool for parameter-efficient finetuning of large models. While it induces a small memory footprint, its training dynamics can be surprisingly complex as they depend on several hyperparameters such as initialization, adapter rank, and learning rate. In particu",
    "url": "https://arxiv.org/abs/2602.06204",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Multi-Way Representation Alignment",
    "summary": "arXiv:2602.06205v1 Announce Type: cross Abstract: The Platonic Representation Hypothesis suggests that independently trained neural networks converge to increasingly similar latent spaces. However, current strategies for mapping these representations are inherently pairwise, scaling quadratically with the number of models and failing to yield a con",
    "url": "https://arxiv.org/abs/2602.06205",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Emergent Low-Rank Training Dynamics in MLPs with Smooth Activations",
    "summary": "arXiv:2602.06208v1 Announce Type: cross Abstract: Recent empirical evidence has demonstrated that the training dynamics of large-scale deep neural networks occur within low-dimensional subspaces. While this has inspired new research into low-rank training, compression, and adaptation, theoretical justification for these dynamics in nonlinear networ",
    "url": "https://arxiv.org/abs/2602.06208",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Addressing the Waypoint-Action Gap in End-to-End Autonomous Driving via Vehicle Motion Models",
    "summary": "arXiv:2602.06214v1 Announce Type: cross Abstract: End-to-End Autonomous Driving (E2E-AD) systems are typically grouped by the nature of their outputs: (i) waypoint-based models that predict a future trajectory, and (ii) action-based models that directly output throttle, steer and brake. Most recent benchmark protocols and training pipelines are way",
    "url": "https://arxiv.org/abs/2602.06214",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Coupled Local and Global World Models for Efficient First Order RL",
    "summary": "arXiv:2602.06219v1 Announce Type: cross Abstract: World models offer a promising avenue for more faithfully capturing complex dynamics, including contacts and non-rigidity, as well as complex sensory information, such as visual perception, in situations where standard simulators struggle. However, these models are computationally complex to evaluat",
    "url": "https://arxiv.org/abs/2602.06219",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "SR4-Fit: An Interpretable and Informative Classification Algorithm Applied to Prediction of U.S. House of Representatives Elections",
    "summary": "arXiv:2602.06229v1 Announce Type: cross Abstract: The growth of machine learning demands interpretable models for critical applications, yet most high-performing models are ``black-box'' systems that obscure input-output relationships, while traditional rule-based algorithms like RuleFit suffer from a lack of predictive power and instability despit",
    "url": "https://arxiv.org/abs/2602.06229",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "RuleSmith: Multi-Agent LLMs for Automated Game Balancing",
    "summary": "arXiv:2602.06232v1 Announce Type: cross Abstract: Game balancing is a longstanding challenge requiring repeated playtesting, expert intuition, and extensive manual tuning. We introduce RuleSmith, the first framework that achieves automated game balancing by leveraging the reasoning capabilities of multi-agent LLMs. It couples a game engine, multi-a",
    "url": "https://arxiv.org/abs/2602.06232",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "ATEX-CF: Attack-Informed Counterfactual Explanations for Graph Neural Networks",
    "summary": "arXiv:2602.06240v1 Announce Type: cross Abstract: Counterfactual explanations offer an intuitive way to interpret graph neural networks (GNNs) by identifying minimal changes that alter a model's prediction, thereby answering \"what must differ for a different outcome?\". In this work, we propose a novel framework, ATEX-CF that unifies adversarial att",
    "url": "https://arxiv.org/abs/2602.06240",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "REBEL: Hidden Knowledge Recovery via Evolutionary-Based Evaluation Loop",
    "summary": "arXiv:2602.06248v1 Announce Type: cross Abstract: Machine unlearning for LLMs aims to remove sensitive or copyrighted data from trained models. However, the true efficacy of current unlearning methods remains uncertain. Standard evaluation metrics rely on benign queries that often mistake superficial information suppression for genuine knowledge re",
    "url": "https://arxiv.org/abs/2602.06248",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "ASMa: Asymmetric Spatio-temporal Masking for Skeleton Action Representation Learning",
    "summary": "arXiv:2602.06251v1 Announce Type: cross Abstract: Self-supervised learning (SSL) has shown remarkable success in skeleton-based action recognition by leveraging data augmentations to learn meaningful representations. However, existing SSL methods rely on data augmentations that predominantly focus on masking high-motion frames and high-degree joint",
    "url": "https://arxiv.org/abs/2602.06251",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Steering Safely or Off a Cliff? Rethinking Specificity and Robustness in Inference-Time Interventions",
    "summary": "arXiv:2602.06256v1 Announce Type: cross Abstract: Model steering, which involves intervening on hidden representations at inference time, has emerged as a lightweight alternative to finetuning for precisely controlling large language models. While steering efficacy has been widely studied, evaluations of whether interventions alter only the intende",
    "url": "https://arxiv.org/abs/2602.06256",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "GRP-Obliteration: Unaligning LLMs With a Single Unlabeled Prompt",
    "summary": "arXiv:2602.06258v1 Announce Type: cross Abstract: Safety alignment is only as robust as its weakest failure mode. Despite extensive work on safety post-training, it has been shown that models can be readily unaligned through post-deployment fine-tuning. However, these methods often require extensive data curation and degrade model utility. In this ",
    "url": "https://arxiv.org/abs/2602.06258",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Can One-sided Arguments Lead to Response Change in Large Language Models?",
    "summary": "arXiv:2602.06260v1 Announce Type: cross Abstract: Polemic questions need more than one viewpoint to express a balanced answer. Large Language Models (LLMs) can provide a balanced answer, but also take a single aligned viewpoint or refuse to answer. In this paper, we study if such initial responses can be steered to a specific viewpoint in a simple ",
    "url": "https://arxiv.org/abs/2602.06260",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Toward generative machine learning for boosting ensembles of climate simulations",
    "summary": "arXiv:2602.06287v1 Announce Type: cross Abstract: Accurately quantifying uncertainty in predictions and projections arising from irreducible internal climate variability is critical for informed decision making. Such uncertainty is typically assessed using ensembles produced with physics based climate models. However, computational constraints impo",
    "url": "https://arxiv.org/abs/2602.06287",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Accelerating Vision Transformers on Brain Processing Unit",
    "summary": "arXiv:2602.06300v1 Announce Type: cross Abstract: With the advancement of deep learning technologies, specialized neural processing hardware such as Brain Processing Units (BPUs) have emerged as dedicated platforms for CNN acceleration, offering optimized INT8 computation capabilities for convolutional operations. Meanwhile, Vision Transformer (ViT",
    "url": "https://arxiv.org/abs/2602.06300",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "The Condensate Theorem: Transformers are O(n), Not $O(n^2)$",
    "summary": "arXiv:2602.06317v1 Announce Type: cross Abstract: We present the Condensate Theorem: attention sparsity is a learned topological property, not an architectural constraint. Through empirical analysis of trained language models, we find that attention mass concentrates on a distinct topological manifold -- and this manifold can be identified dynamica",
    "url": "https://arxiv.org/abs/2602.06317",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Can Post-Training Transform LLMs into Causal Reasoners?",
    "summary": "arXiv:2602.06337v1 Announce Type: cross Abstract: Causal inference is essential for decision-making but remains challenging for non-experts. While large language models (LLMs) show promise in this domain, their precise causal estimation capabilities are still limited, and the impact of post-training on these abilities is insufficiently explored. Th",
    "url": "https://arxiv.org/abs/2602.06337",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Action Hallucination in Generative Visual-Language-Action Models",
    "summary": "arXiv:2602.06339v1 Announce Type: cross Abstract: Robot Foundation Models such as Vision-Language-Action models are rapidly reshaping how robot policies are trained and deployed, replacing hand-designed planners with end-to-end generative action models. While these systems demonstrate impressive generalization, it remains unclear whether they funda",
    "url": "https://arxiv.org/abs/2602.06339",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Zero-Trust Runtime Verification for Agentic Payment Protocols: Mitigating Replay and Context-Binding Failures in AP2",
    "summary": "arXiv:2602.06345v1 Announce Type: cross Abstract: The deployment of autonomous AI agents capable of executing commercial transactions has motivated the adoption of mandate-based payment authorization protocols, including the Universal Commerce Protocol (UCP) and the Agent Payments Protocol (AP2). These protocols replace interactive, session-based a",
    "url": "https://arxiv.org/abs/2602.06345",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Di3PO -- Diptych Diffusion DPO for Targeted Improvements in Image",
    "summary": "arXiv:2602.06355v1 Announce Type: cross Abstract: Existing methods for preference tuning of text-to-image (T2I) diffusion models often rely on computationally expensive generation steps to create positive and negative pairs of images. These approaches frequently yield training pairs that either lack meaningful differences, are expensive to sample a",
    "url": "https://arxiv.org/abs/2602.06355",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "SHINE: A Scalable In-Context Hypernetwork for Mapping Context to LoRA in a Single Pass",
    "summary": "arXiv:2602.06358v1 Announce Type: cross Abstract: We propose SHINE (Scalable Hyper In-context NEtwork), a scalable hypernetwork that can map diverse meaningful contexts into high-quality LoRA adapters for large language models (LLM). By reusing the frozen LLM's own parameters in an in-context hypernetwork design and introducing architectural innova",
    "url": "https://arxiv.org/abs/2602.06358",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation",
    "summary": "arXiv:2602.06359v1 Announce Type: cross Abstract: Fine-tuning large language models (LLMs) for specialized domains often necessitates a trade-off between acquiring domain expertise and retaining general reasoning capabilities, a phenomenon known as catastrophic forgetting. Existing remedies face a dichotomy: gradient surgery methods offer geometric",
    "url": "https://arxiv.org/abs/2602.06359",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Revisiting Salient Object Detection from an Observer-Centric Perspective",
    "summary": "arXiv:2602.06369v1 Announce Type: cross Abstract: Salient object detection is inherently a subjective problem, as observers with different priors may perceive different objects as salient. However, existing methods predominantly formulate it as an objective prediction task with a single groundtruth segmentation map for each image, which renders the",
    "url": "https://arxiv.org/abs/2602.06369",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Generating High-quality Privacy-preserving Synthetic Data",
    "summary": "arXiv:2602.06390v1 Announce Type: cross Abstract: Synthetic tabular data enables sharing and analysis of sensitive records, but its practical deployment requires balancing distributional fidelity, downstream utility, and privacy protection. We study a simple, model agnostic post processing framework that can be applied on top of any synthetic data ",
    "url": "https://arxiv.org/abs/2602.06390",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Empirical Analysis of Adversarial Robustness and Explainability Drift in Cybersecurity Classifiers",
    "summary": "arXiv:2602.06395v1 Announce Type: cross Abstract: Machine learning (ML) models are increasingly deployed in cybersecurity applications such as phishing detection and network intrusion prevention. However, these models remain vulnerable to adversarial perturbations small, deliberate input modifications that can degrade detection accuracy and comprom",
    "url": "https://arxiv.org/abs/2602.06395",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "ARIS-RSMA Enhanced ISAC System: Joint Rate Splitting and Beamforming Design",
    "summary": "arXiv:2602.06399v1 Announce Type: cross Abstract: This letter proposes an active reconfigurable intelligent surface (ARIS) assisted rate-splitting multiple access (RSMA) integrated sensing and communication (ISAC) system to overcome the fairness bottleneck in multi-target sensing under obstructed line-of-sight environments. Beamforming at the trans",
    "url": "https://arxiv.org/abs/2602.06399",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "TFusionOcc: Student's t-Distribution Based Object-Centric Multi-Sensor Fusion Framework for 3D Occupancy Prediction",
    "summary": "arXiv:2602.06400v1 Announce Type: cross Abstract: 3D semantic occupancy prediction enables autonomous vehicles (AVs) to perceive fine-grained geometric and semantic structure of their surroundings from onboard sensors, which is essential for safe decision-making and navigation. Recent models for 3D semantic occupancy prediction have successfully ad",
    "url": "https://arxiv.org/abs/2602.06400",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Investigating the structure of emotions by analyzing similarity and association of emotion words",
    "summary": "arXiv:2602.06430v1 Announce Type: cross Abstract: In the field of natural language processing, some studies have attempted sentiment analysis on text by handling emotions as explanatory or response variables. One of the most popular emotion models used in this context is the wheel of emotion proposed by Plutchik. This model schematizes human emotio",
    "url": "https://arxiv.org/abs/2602.06430",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "A methodology for analyzing financial needs hierarchy from social discussions using LLM",
    "summary": "arXiv:2602.06431v1 Announce Type: cross Abstract: This study examines the hierarchical structure of financial needs as articulated in social media discourse, employing generative AI techniques to analyze large-scale textual data. While human needs encompass a broad spectrum from fundamental survival to psychological fulfillment financial needs are ",
    "url": "https://arxiv.org/abs/2602.06431",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking",
    "summary": "arXiv:2602.06440v1 Announce Type: cross Abstract: Large Language Models (LLMs) have become integral to many domains, making their safety a critical priority. Prior jailbreaking research has explored diverse approaches, including prompt optimization, automated red teaming, obfuscation, and reinforcement learning (RL) based methods. However, most exi",
    "url": "https://arxiv.org/abs/2602.06440",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "TrajAD: Trajectory Anomaly Detection for Trustworthy LLM Agents",
    "summary": "arXiv:2602.06443v1 Announce Type: cross Abstract: We address the problem of runtime trajectory anomaly detection, a critical capability for enabling trustworthy LLM agents. Current safety measures predominantly focus on static input/output filtering. However, we argue that ensuring LLM agents reliability requires auditing the intermediate execution",
    "url": "https://arxiv.org/abs/2602.06443",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "CORE: Comprehensive Ontological Relation Evaluation for Large Language Models",
    "summary": "arXiv:2602.06446v1 Announce Type: cross Abstract: Large Language Models (LLMs) perform well on many reasoning benchmarks, yet existing evaluations rarely assess their ability to distinguish between meaningful semantic relations and genuine unrelatedness. We introduce CORE (Comprehensive Ontological Relation Evaluation), a dataset of 225K multiple-c",
    "url": "https://arxiv.org/abs/2602.06446",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Principle-Evolvable Scientific Discovery via Uncertainty Minimization",
    "summary": "arXiv:2602.06448v1 Announce Type: cross Abstract: Large Language Model (LLM)-based scientific agents have accelerated scientific discovery, yet they often suffer from significant inefficiencies due to adherence to fixed initial priors. Existing approaches predominantly operate within a static hypothesis space, which restricts the discovery of novel",
    "url": "https://arxiv.org/abs/2602.06448",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Improve Large Language Model Systems with User Logs",
    "summary": "arXiv:2602.06470v1 Announce Type: cross Abstract: Scaling training data and model parameters has long driven progress in large language models (LLMs), but this paradigm is increasingly constrained by the scarcity of high-quality data and diminishing returns from rising computational costs. As a result, recent work is increasing the focus on continu",
    "url": "https://arxiv.org/abs/2602.06470",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Revisiting the Shape Convention of Transformer Language Models",
    "summary": "arXiv:2602.06471v1 Announce Type: cross Abstract: Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP, allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by rec",
    "url": "https://arxiv.org/abs/2602.06471",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Prism: Spectral Parameter Sharing for Multi-Agent Reinforcement Learning",
    "summary": "arXiv:2602.06476v1 Announce Type: cross Abstract: Parameter sharing is a key strategy in multi-agent reinforcement learning (MARL) for improving scalability, yet conventional fully shared architectures often collapse into homogeneous behaviors. Recent methods introduce diversity through clustering, pruning, or masking, but typically compromise reso",
    "url": "https://arxiv.org/abs/2602.06476",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Efficient-LVSM: Faster, Cheaper, and Better Large View Synthesis Model via Decoupled Co-Refinement Attention",
    "summary": "arXiv:2602.06478v1 Announce Type: cross Abstract: Feedforward models for novel view synthesis (NVS) have recently advanced by transformer-based methods like LVSM, using attention among all input and target views. In this work, we argue that its full self-attention design is suboptimal, suffering from quadratic complexity with respect to the number ",
    "url": "https://arxiv.org/abs/2602.06478",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Completing Missing Annotation: Multi-Agent Debate for Accurate and Scalable Relevant Assessment for IR Benchmarks",
    "summary": "arXiv:2602.06526v1 Announce Type: cross Abstract: Information retrieval (IR) evaluation remains challenging due to incomplete IR benchmark datasets that contain unlabeled relevant chunks. While LLMs and LLM-human hybrid strategies reduce costly human effort, they remain prone to LLM overconfidence and ineffective AI-to-human escalation. To address ",
    "url": "https://arxiv.org/abs/2602.06526",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "MTQE.en-he: Machine Translation Quality Estimation for English-Hebrew",
    "summary": "arXiv:2602.06546v1 Announce Type: cross Abstract: We release MTQE.en-he: to our knowledge, the first publicly available English-Hebrew benchmark for Machine Translation Quality Estimation. MTQE.en-he contains 959 English segments from WMT24++, each paired with a machine translation into Hebrew, and Direct Assessment scores of the translation qualit",
    "url": "https://arxiv.org/abs/2602.06546",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Malicious Agent Skills in the Wild: A Large-Scale Security Empirical Study",
    "summary": "arXiv:2602.06547v1 Announce Type: cross Abstract: Third-party agent skills extend LLM-based agents with instruction files and executable code that run on users' machines. Skills execute with user privileges and are distributed through community registries with minimal vetting, but no ground-truth dataset exists to characterize the resulting threats",
    "url": "https://arxiv.org/abs/2602.06547",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Dynamics-Aligned Shared Hypernetworks for Zero-Shot Actuator Inversion",
    "summary": "arXiv:2602.06550v1 Announce Type: cross Abstract: Zero-shot generalization in contextual reinforcement learning remains a core challenge, particularly when the context is latent and must be inferred from data. A canonical failure mode is actuator inversion, where identical actions produce opposite physical effects under a latent binary context. We ",
    "url": "https://arxiv.org/abs/2602.06550",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "LIBERO-X: Robustness Litmus for Vision-Language-Action Models",
    "summary": "arXiv:2602.06556v1 Announce Type: cross Abstract: Reliable benchmarking is critical for advancing Vision-Language-Action (VLA) models, as it reveals their generalization, robustness, and alignment of perception with language-driven manipulation tasks. However, existing benchmarks often provide limited or misleading assessments due to insufficient e",
    "url": "https://arxiv.org/abs/2602.06556",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Which Graph Shift Operator? A Spectral Answer to an Empirical Question",
    "summary": "arXiv:2602.06557v1 Announce Type: cross Abstract: Graph Neural Networks (GNNs) have established themselves as the leading models for learning on graph-structured data, generally categorized into spatial and spectral approaches. Central to these architectures is the Graph Shift Operator (GSO), a matrix representation of the graph structure used to f",
    "url": "https://arxiv.org/abs/2602.06557",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs",
    "summary": "arXiv:2602.06566v1 Announce Type: cross Abstract: Despite recent successes, test-time scaling - i.e., dynamically expanding the token budget during inference as needed - remains brittle for vision-language models (VLMs): unstructured chains-of-thought about images entangle perception and reasoning, leading to long, disorganized contexts where small",
    "url": "https://arxiv.org/abs/2602.06566",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Transformer-based Parameter Fitting of Models derived from Bloch-McConnell Equations for CEST MRI Analysis",
    "summary": "arXiv:2602.06574v1 Announce Type: cross Abstract: Chemical exchange saturation transfer (CEST) MRI is a non-invasive imaging modality for detecting metabolites. It offers higher resolution and sensitivity compared to conventional magnetic resonance spectroscopy (MRS). However, quantification of CEST data is challenging because the measured signal r",
    "url": "https://arxiv.org/abs/2602.06574",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Perturbing the Phase: Analyzing Adversarial Robustness of Complex-Valued Neural Networks",
    "summary": "arXiv:2602.06577v1 Announce Type: cross Abstract: Complex-valued neural networks (CVNNs) are rising in popularity for all kinds of applications. To safely use CVNNs in practice, analyzing their robustness against outliers is crucial. One well known technique to understand the behavior of deep neural networks is to investigate their behavior under a",
    "url": "https://arxiv.org/abs/2602.06577",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Exploring Sparsity and Smoothness of Arbitrary $\\ell_p$ Norms in Adversarial Attacks",
    "summary": "arXiv:2602.06578v1 Announce Type: cross Abstract: Adversarial attacks against deep neural networks are commonly constructed under $\\ell_p$ norm constraints, most often using $p=1$, $p=2$ or $p=\\infty$, and potentially regularized for specific demands such as sparsity or smoothness. These choices are typically made without a systematic investigation",
    "url": "https://arxiv.org/abs/2602.06578",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Target noise: A pre-training based neural network initialization for efficient high resolution learning",
    "summary": "arXiv:2602.06585v1 Announce Type: cross Abstract: Weight initialization plays a crucial role in the optimization behavior and convergence efficiency of neural networks. Most existing initialization methods, such as Xavier and Kaiming initializations, rely on random sampling and do not exploit information from the optimization process itself. We pro",
    "url": "https://arxiv.org/abs/2602.06585",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "ProtoQuant: Quantization of Prototypical Parts For General and Fine-Grained Image Classification",
    "summary": "arXiv:2602.06592v1 Announce Type: cross Abstract: Prototypical parts-based models offer a \"this looks like that\" paradigm for intrinsic interpretability, yet they typically struggle with ImageNet-scale generalization and often require computationally expensive backbone finetuning. Furthermore, existing methods frequently suffer from \"prototype drif",
    "url": "https://arxiv.org/abs/2602.06592",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "AgentStepper: Interactive Debugging of Software Development Agents",
    "summary": "arXiv:2602.06593v1 Announce Type: cross Abstract: Software development agents powered by large language models (LLMs) have shown great promise in automating tasks like environment setup, issue solving, and program repair. Unfortunately, understanding and debugging such agents remain challenging due to their complex and dynamic nature. Developers mu",
    "url": "https://arxiv.org/abs/2602.06593",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Personality as Relational Infrastructure: User Perceptions of Personality-Trait-Infused LLM Messaging",
    "summary": "arXiv:2602.06596v1 Announce Type: cross Abstract: Digital behaviour change systems increasingly rely on repeated, system-initiated messages to support users in everyday contexts. LLMs enable these messages to be personalised consistently across interactions, yet it remains unclear whether such personalisation improves individual messages or instead",
    "url": "https://arxiv.org/abs/2602.06596",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Sample-Efficient Policy Space Response Oracles with Joint Experience Best Response",
    "summary": "arXiv:2602.06599v1 Announce Type: cross Abstract: Multi-agent reinforcement learning (MARL) offers a scalable alternative to exact game-theoretic analysis but suffers from non-stationarity and the need to maintain diverse populations of strategies that capture non-transitive interactions. Policy Space Response Oracles (PSRO) address these issues by",
    "url": "https://arxiv.org/abs/2602.06599",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Scaling Speech Tokenizers with Diffusion Autoencoders",
    "summary": "arXiv:2602.06602v1 Announce Type: cross Abstract: Speech tokenizers are foundational to speech language models, yet existing approaches face two major challenges: (1) balancing trade-offs between encoding semantics for understanding and acoustics for reconstruction, and (2) achieving low bit rates and low token rates. We propose Speech Diffusion To",
    "url": "https://arxiv.org/abs/2602.06602",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "The challenge of generating and evolving real-life like synthetic test data without accessing real-world raw data -- a Systematic Review",
    "summary": "arXiv:2602.06609v1 Announce Type: cross Abstract: Background: High-level system testing of applications that use data from e-Government services as input requires test data that is real-life-like but where the privacy of personal information is guaranteed. Applications with such strong requirement include information exchange between countries, med",
    "url": "https://arxiv.org/abs/2602.06609",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "DAVE: Distribution-aware Attribution via ViT Gradient Decomposition",
    "summary": "arXiv:2602.06613v1 Announce Type: cross Abstract: Vision Transformers (ViTs) have become a dominant architecture in computer vision, yet producing stable and high-resolution attribution maps for these models remains challenging. Architectural components such as patch embeddings and attention routing often introduce structured artifacts in pixel-lev",
    "url": "https://arxiv.org/abs/2602.06613",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Trust Regions Sell, But Who's Buying? Overlap Geometry as an Alternative Trust Region for Policy Optimization",
    "summary": "arXiv:2602.06627v1 Announce Type: cross Abstract: Standard trust-region methods constrain policy updates via Kullback-Leibler (KL) divergence. However, KL controls only an average divergence and does not directly prevent rare, large likelihood-ratio excursions that destabilize training--precisely the failure mode that motivates heuristics such as P",
    "url": "https://arxiv.org/abs/2602.06627",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Temperature Scaling Attack Disrupting Model Confidence in Federated Learning",
    "summary": "arXiv:2602.06638v1 Announce Type: cross Abstract: Predictive confidence serves as a foundational control signal in mission-critical systems, directly governing risk-aware logic such as escalation, abstention, and conservative fallback. While prior federated learning attacks predominantly target accuracy or implant backdoors, we identify confidence ",
    "url": "https://arxiv.org/abs/2602.06638",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations",
    "summary": "arXiv:2602.06643v1 Announce Type: cross Abstract: Current approaches for humanoid whole-body manipulation, primarily relying on teleoperation or visual sim-to-real reinforcement learning, are hindered by hardware logistics and complex reward engineering. Consequently, demonstrated autonomous skills remain limited and are typically restricted to con",
    "url": "https://arxiv.org/abs/2602.06643",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "RAPID: Reconfigurable, Adaptive Platform for Iterative Design",
    "summary": "arXiv:2602.06653v1 Announce Type: cross Abstract: Developing robotic manipulation policies is iterative and hypothesis-driven: researchers test tactile sensing, gripper geometries, and sensor placements through real-world data collection and training. Yet even minor end-effector changes often require mechanical refitting and system re-integration, ",
    "url": "https://arxiv.org/abs/2602.06653",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Multimodal Generative Retrieval Model with Staged Pretraining for Food Delivery on Meituan",
    "summary": "arXiv:2602.06654v1 Announce Type: cross Abstract: Multimodal retrieval models are becoming increasingly important in scenarios such as food delivery, where rich multimodal features can meet diverse user needs and enable precise retrieval. Mainstream approaches typically employ a dual-tower architecture between queries and items, and perform joint o",
    "url": "https://arxiv.org/abs/2602.06654",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Not All Layers Need Tuning: Selective Layer Restoration Recovers Diversity",
    "summary": "arXiv:2602.06665v1 Announce Type: cross Abstract: Post-training improves instruction-following and helpfulness of large language models (LLMs) but often reduces generation diversity, which leads to repetitive outputs in open-ended settings, a phenomenon known as mode collapse. Motivated by evidence that LLM layers play distinct functional roles, we",
    "url": "https://arxiv.org/abs/2602.06665",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data",
    "summary": "arXiv:2602.06669v1 Announce Type: cross Abstract: Large Language Models (LLMs) often show reduced performance, cultural alignment, and safety robustness in non-English languages, partly because English dominates both pre-training data and human preference alignment datasets. Training methods like Reinforcement Learning from Human Feedback (RLHF) an",
    "url": "https://arxiv.org/abs/2602.06669",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "SaDiT: Efficient Protein Backbone Design via Latent Structural Tokenization and Diffusion Transformers",
    "summary": "arXiv:2602.06706v1 Announce Type: cross Abstract: Generative models for de novo protein backbone design have achieved remarkable success in creating novel protein structures. However, these diffusion-based approaches remain computationally intensive and slower than desired for large-scale structural exploration. While recent efforts like Proteina h",
    "url": "https://arxiv.org/abs/2602.06706",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare",
    "summary": "arXiv:2602.06717v1 Announce Type: cross Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller group",
    "url": "https://arxiv.org/abs/2602.06717",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "GhostCite: A Large-Scale Analysis of Citation Validity in the Age of Large Language Models",
    "summary": "arXiv:2602.06718v1 Announce Type: cross Abstract: Citations provide the basis for trusting scientific claims; when they are invalid or fabricated, this trust collapses. With the advent of Large Language Models (LLMs), this risk has intensified: LLMs are increasingly used for academic writing, yet their tendency to fabricate citations (``ghost citat",
    "url": "https://arxiv.org/abs/2602.06718",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Pairwise is Not Enough: Hypergraph Neural Networks for Multi-Agent Pathfinding",
    "summary": "arXiv:2602.06733v1 Announce Type: cross Abstract: Multi-Agent Path Finding (MAPF) is a representative multi-agent coordination problem, where multiple agents are required to navigate to their respective goals without collisions. Solving MAPF optimally is known to be NP-hard, leading to the adoption of learning-based approaches to alleviate the onli",
    "url": "https://arxiv.org/abs/2602.06733",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Optimal Abstractions for Verifying Properties of Kolmogorov-Arnold Networks (KANs)",
    "summary": "arXiv:2602.06737v1 Announce Type: cross Abstract: We present a novel approach for verifying properties of Kolmogorov-Arnold Networks (KANs), a class of neural networks characterized by nonlinear, univariate activation functions typically implemented as piecewise polynomial splines or Gaussian processes. Our method creates mathematical ``abstraction",
    "url": "https://arxiv.org/abs/2602.06737",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Gold Exploration using Representations from a Multispectral Autoencoder",
    "summary": "arXiv:2602.06748v1 Announce Type: cross Abstract: Satellite imagery is employed for large-scale prospectivity mapping due to the high cost and typically limited availability of on-site mineral exploration data. In this work, we present a proof-of-concept framework that leverages generative representations learned from multispectral Sentinel-2 image",
    "url": "https://arxiv.org/abs/2602.06748",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "A Unified Framework for LLM Watermarks",
    "summary": "arXiv:2602.06754v1 Announce Type: cross Abstract: LLM watermarks allow tracing AI-generated texts by inserting a detectable signal into their generated content. Recent works have proposed a wide range of watermarking algorithms, each with distinct designs, usually built using a bottom-up approach. Crucially, there is no general and principled formu",
    "url": "https://arxiv.org/abs/2602.06754",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "AEGIS: Adversarial Target-Guided Retention-Data-Free Robust Concept Erasure from Diffusion Models",
    "summary": "arXiv:2602.06771v1 Announce Type: cross Abstract: Concept erasure helps stop diffusion models (DMs) from generating harmful content; but current methods face robustness retention trade off. Robustness means the model fine-tuned by concept erasure methods resists reactivation of erased concepts, even under semantically related prompts. Retention mea",
    "url": "https://arxiv.org/abs/2602.06771",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Next-generation cyberattack detection with large language models: anomaly analysis across heterogeneous logs",
    "summary": "arXiv:2602.06777v1 Announce Type: cross Abstract: This project explores large language models (LLMs) for anomaly detection across heterogeneous log sources. Traditional intrusion detection systems suffer from high false positive rates, semantic blindness, and data scarcity, as logs are inherently sensitive, making clean datasets rare. We address th",
    "url": "https://arxiv.org/abs/2602.06777",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling",
    "summary": "arXiv:2602.06795v1 Announce Type: cross Abstract: An impediment to using Large Language Models (LLMs) for reasoning output verification is that LLMs struggle to reliably identify errors in thinking traces, particularly in long outputs, domains requiring expert knowledge, and problems without verifiable rewards. We propose a data-driven approach to ",
    "url": "https://arxiv.org/abs/2602.06795",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "On the Identifiability of Steering Vectors in Large Language Models",
    "summary": "arXiv:2602.06801v1 Announce Type: cross Abstract: Activation steering methods, such as persona vectors, are widely used to control large language model behavior and increasingly interpreted as revealing meaningful internal representations. This interpretation implicitly assumes steering directions are identifiable and uniquely recoverable from inpu",
    "url": "https://arxiv.org/abs/2602.06801",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "SuReNav: Superpixel Graph-based Constraint Relaxation for Navigation in Over-constrained Environments",
    "summary": "arXiv:2602.06807v1 Announce Type: cross Abstract: We address the over-constrained planning problem in semi-static environments. The planning objective is to find a best-effort solution that avoids all hard constraint regions while minimally traversing the least risky areas. Conventional methods often rely on pre-defined area costs, limiting general",
    "url": "https://arxiv.org/abs/2602.06807",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Bridging 6G IoT and AI: LLM-Based Efficient Approach for Physical Layer's Optimization Tasks",
    "summary": "arXiv:2602.06819v1 Announce Type: cross Abstract: This paper investigates the role of large language models (LLMs) in sixth-generation (6G) Internet of Things (IoT) networks and proposes a prompt-engineering-based real-time feedback and verification (PE-RTFV) framework that perform physical-layer's optimization tasks through an iteratively process.",
    "url": "https://arxiv.org/abs/2602.06819",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "AI-Generated Music Detection in Broadcast Monitoring",
    "summary": "arXiv:2602.06823v1 Announce Type: cross Abstract: AI music generators have advanced to the point where their outputs are often indistinguishable from human compositions. While detection methods have emerged, they are typically designed and validated in music streaming contexts with clean, full-length tracks. Broadcast audio, however, poses a differ",
    "url": "https://arxiv.org/abs/2602.06823",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models",
    "summary": "arXiv:2602.06825v1 Announce Type: cross Abstract: Reinforcement learning from human feedback (RLHF) shows promise for aligning diffusion and flow models, yet policy optimization methods such as GRPO suffer from inefficient and static sampling strategies. These methods treat all prompts and denoising steps uniformly, ignoring substantial variations ",
    "url": "https://arxiv.org/abs/2602.06825",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "The Representational Geometry of Number",
    "summary": "arXiv:2602.06843v1 Announce Type: cross Abstract: A central question in cognitive science is whether conceptual representations converge onto a shared manifold to support generalization, or diverge into orthogonal subspaces to minimize task interference. While prior work has discovered evidence for both, a mechanistic account of how these propertie",
    "url": "https://arxiv.org/abs/2602.06843",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping",
    "summary": "arXiv:2602.06850v1 Announce Type: cross Abstract: While modern text-to-image models excel at prompt-based generation, they often lack the fine-grained control necessary for specific user requirements like spatial layouts or subject appearances. Multi-condition control addresses this, yet its integration into Diffusion Transformers (DiTs) is bottlen",
    "url": "https://arxiv.org/abs/2602.06850",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "The Quantum Sieve Tracer: A Hybrid Framework for Layer-Wise Activation Tracing in Large Language Models",
    "summary": "arXiv:2602.06852v1 Announce Type: cross Abstract: Mechanistic interpretability aims to reverse-engineer the internal computations of Large Language Models (LLMs), yet separating sparse semantic signals from high-dimensional polysemantic noise remains a significant challenge. This paper introduces the Quantum Sieve Tracer, a hybrid quantum-classical",
    "url": "https://arxiv.org/abs/2602.06852",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Zero-shot Generalizable Graph Anomaly Detection with Mixture of Riemannian Experts",
    "summary": "arXiv:2602.06859v1 Announce Type: cross Abstract: Graph Anomaly Detection (GAD) aims to identify irregular patterns in graph data, and recent works have explored zero-shot generalist GAD to enable generalization to unseen graph datasets. However, existing zero-shot GAD methods largely ignore intrinsic geometric differences across diverse anomaly pa",
    "url": "https://arxiv.org/abs/2602.06859",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code",
    "summary": "arXiv:2602.06875v1 Announce Type: cross Abstract: Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, wi",
    "url": "https://arxiv.org/abs/2602.06875",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "NanoFLUX: Distillation-Driven Compression of Large Text-to-Image Generation Models for Mobile Devices",
    "summary": "arXiv:2602.06879v1 Announce Type: cross Abstract: While large-scale text-to-image diffusion models continue to improve in visual quality, their increasing scale has widened the gap between state-of-the-art models and on-device solutions. To address this gap, we introduce NanoFLUX, a 2.4B text-to-image flow-matching model distilled from 17B FLUX.1-S",
    "url": "https://arxiv.org/abs/2602.06879",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Supercharging Simulation-Based Inference for Bayesian Optimal Experimental Design",
    "summary": "arXiv:2602.06900v1 Announce Type: cross Abstract: Bayesian optimal experimental design (BOED) seeks to maximize the expected information gain (EIG) of experiments. This requires a likelihood estimate, which in many settings is intractable. Simulation-based inference (SBI) provides powerful tools for this regime. However, existing work explicitly co",
    "url": "https://arxiv.org/abs/2602.06900",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "TamperBench: Systematically Stress-Testing LLM Safety Under Fine-Tuning and Tampering",
    "summary": "arXiv:2602.06911v1 Announce Type: cross Abstract: As increasingly capable open-weight large language models (LLMs) are deployed, improving their tamper resistance against unsafe modifications, whether accidental or intentional, becomes critical to minimize risks. However, there is no standard approach to evaluate tamper resistance. Varied data sets",
    "url": "https://arxiv.org/abs/2602.06911",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "PANC: Prior-Aware Normalized Cut for Object Segmentation",
    "summary": "arXiv:2602.06912v1 Announce Type: cross Abstract: Fully unsupervised segmentation pipelines naively seek the most salient object, should this be present. As a result, most of the methods reported in the literature deliver non-deterministic partitions that are sensitive to initialization, seed order, and threshold heuristics. We propose PANC, a weak",
    "url": "https://arxiv.org/abs/2602.06912",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Halluverse-M^3: A multitask multilingual benchmark for hallucination in LLMs",
    "summary": "arXiv:2602.06920v1 Announce Type: cross Abstract: Hallucinations in large language models remain a persistent challenge, particularly in multilingual and generative settings where factual consistency is difficult to maintain. While recent models show strong performance on English-centric benchmarks, their behavior across languages, tasks, and hallu",
    "url": "https://arxiv.org/abs/2602.06920",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "From Kepler to Newton: Inductive Biases Guide Learned World Models in Transformers",
    "summary": "arXiv:2602.06923v1 Announce Type: cross Abstract: Can general-purpose AI architectures go beyond prediction to discover the physical laws governing the universe? True intelligence relies on \"world models\" -- causal abstractions that allow an agent to not only predict future states but understand the underlying governing dynamics. While previous \"AI",
    "url": "https://arxiv.org/abs/2602.06923",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Implementing Grassroots Logic Programs with Multiagent Transition Systems and AI",
    "summary": "arXiv:2602.06934v1 Announce Type: cross Abstract: Grassroots Logic Programs (GLP) is a concurrent logic programming language with variables partitioned into paired \\emph{readers} and \\emph{writers}, conjuring both linear logic and futures/promises: an assignment is produced at most once via the sole occurrence of a writer (promise) and consumed at ",
    "url": "https://arxiv.org/abs/2602.06934",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Cochain Perspectives on Temporal-Difference Signals for Learning Beyond Markov Dynamics",
    "summary": "arXiv:2602.06939v1 Announce Type: cross Abstract: Non-Markovian dynamics are commonly found in real-world environments due to long-range dependencies, partial observability, and memory effects. The Bellman equation that is the central pillar of Reinforcement learning (RL) becomes only approximately valid under Non-Markovian. Existing work often foc",
    "url": "https://arxiv.org/abs/2602.06939",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Endogenous Resistance to Activation Steering in Language Models",
    "summary": "arXiv:2602.06941v1 Announce Type: cross Abstract: Large language models can resist task-misaligned activation steering during inference, sometimes recovering mid-generation to produce improved responses even when steering remains active. We term this Endogenous Steering Resistance (ESR). Using sparse autoencoder (SAE) latents to steer model activat",
    "url": "https://arxiv.org/abs/2602.06941",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay",
    "summary": "arXiv:2602.06942v1 Announce Type: cross Abstract: Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typi",
    "url": "https://arxiv.org/abs/2602.06942",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos",
    "summary": "arXiv:2602.06949v1 Announce Type: cross Abstract: Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels.",
    "url": "https://arxiv.org/abs/2602.06949",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning",
    "summary": "arXiv:2602.06960v1 Announce Type: cross Abstract: Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing interme",
    "url": "https://arxiv.org/abs/2602.06960",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Learning a Generative Meta-Model of LLM Activations",
    "summary": "arXiv:2602.06964v1 Announce Type: cross Abstract: Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity. We explore this ",
    "url": "https://arxiv.org/abs/2602.06964",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "A computational framework for human values",
    "summary": "arXiv:2305.02748v2 Announce Type: replace Abstract: In the diverse array of work investigating the nature of human values from psychology, philosophy and social sciences, there is a clear consensus that values guide behaviour. More recently, a recognition that values provide a means to engineer ethical AI has emerged. Indeed, Stuart Russell propose",
    "url": "https://arxiv.org/abs/2305.02748",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "OpenDeception: Learning Deception and Trust in Human-AI Interaction via Multi-Agent Simulation",
    "summary": "arXiv:2504.13707v3 Announce Type: replace Abstract: As large language models (LLMs) are increasingly deployed as interactive agents, open-ended human-AI interactions can involve deceptive behaviors with serious real-world consequences, yet existing evaluations remain largely scenario-specific and model-centric. We introduce OpenDeception, a lightwe",
    "url": "https://arxiv.org/abs/2504.13707",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Skill Discovery for Software Scripting Automation via Offline Simulations with LLMs",
    "summary": "arXiv:2504.20406v2 Announce Type: replace Abstract: Scripting interfaces enable users to automate tasks and customize software workflows, but creating scripts traditionally requires programming expertise and familiarity with specific APIs, posing barriers for many users. While Large Language Models (LLMs) can generate code from natural language que",
    "url": "https://arxiv.org/abs/2504.20406",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT Distillation",
    "summary": "arXiv:2505.18759v2 Announce Type: replace Abstract: Data-centric distillation, including data augmentation, selection, and mixing, offers a promising path to creating smaller, more efficient student Large Language Models (LLMs) that retain strong reasoning abilities. However, there still lacks a comprehensive benchmark to systematically assess the ",
    "url": "https://arxiv.org/abs/2505.18759",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Scalable In-Context Q-Learning",
    "summary": "arXiv:2506.01299v3 Announce Type: replace Abstract: Recent advancements in language models have demonstrated remarkable in-context learning abilities, prompting the exploration of in-context reinforcement learning (ICRL) to extend the promise to decision domains. Due to involving more complex dynamics and temporal correlations, existing ICRL approa",
    "url": "https://arxiv.org/abs/2506.01299",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Conversational Intent-Driven GraphRAG: Enhancing Multi-Turn Dialogue Systems through Adaptive Dual-Retrieval of Flow Patterns and Context Semantics",
    "summary": "arXiv:2506.19385v3 Announce Type: replace Abstract: We present CID-GraphRAG (Conversational Intent-Driven Graph Retrieval Augmented Generation), a novel framework that addresses the limitations of existing dialogue systems in maintaining both contextual coherence and goal-oriented progression in multi-turn customer service conversations. Unlike tra",
    "url": "https://arxiv.org/abs/2506.19385",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "GATSim: Urban Mobility Simulation with Generative Agents",
    "summary": "arXiv:2506.23306v3 Announce Type: replace Abstract: Traditional agent-based urban mobility simulations often rely on rigid rulebased systems that struggle to capture the complexity, adaptability, and behavioral diversity inherent in human travel decision making. Inspired by recent advancements in large language models and AI agent technologies, we ",
    "url": "https://arxiv.org/abs/2506.23306",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Emergent Cognitive Convergence via Implementation: Structured Cognitive Loop Reflecting Four Theories of Mind",
    "summary": "arXiv:2507.16184v4 Announce Type: replace Abstract: We report a structural convergence among four influential theories of mind: Kahneman dual-system theory, Friston predictive processing, Minsky society of mind, and Clark extended mind, emerging unintentionally within a practical AI architecture known as Agentic Flow. Designed to address limitation",
    "url": "https://arxiv.org/abs/2507.16184",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Scaling Multi-Agent Epistemic Planning through GNN-Derived Heuristics",
    "summary": "arXiv:2508.12840v4 Announce Type: replace Abstract: Multi-agent Epistemic Planning (MEP) is an autonomous planning framework for reasoning about both the physical world and the beliefs of agents, with applications in domains where information flow and awareness among agents are critical. The richness of MEP requires states to be represented as Krip",
    "url": "https://arxiv.org/abs/2508.12840",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "AI sustains higher strategic tension than humans in chess",
    "summary": "arXiv:2508.13213v2 Announce Type: replace Abstract: Strategic decision-making requires balancing immediate opportunities against long-term objectives: a tension fundamental to competitive environments. We investigate this trade-off in chess by analyzing the dynamics of human and AI gameplay through a network-based metric that quantifies piece-to-pi",
    "url": "https://arxiv.org/abs/2508.13213",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Deceive, Detect, and Disclose: Large Language Models Play Mini-Mafia",
    "summary": "arXiv:2509.23023v2 Announce Type: replace Abstract: Mafia is a social deduction game where informed mafia compete against uninformed townsfolk. Its asymmetry of information and reliance on theory-of-mind reasoning mirror real-world multi-agent scenarios, making it a useful testbed for evaluating the social intelligence of large language models (LLM",
    "url": "https://arxiv.org/abs/2509.23023",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning",
    "summary": "arXiv:2510.04284v2 Announce Type: replace Abstract: The professionalism of a human doctor in outpatient service depends on two core abilities: the ability to make accurate medical decisions and the medical consultation skill to conduct strategic, empathetic patient inquiry. Existing Large Language Models (LLMs) have achieved remarkable accuracy on ",
    "url": "https://arxiv.org/abs/2510.04284",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Personalized Learning Path Planning with Goal-Driven Learner State Modeling",
    "summary": "arXiv:2510.13215v2 Announce Type: replace Abstract: Personalized Learning Path Planning (PLPP) aims to design adaptive learning paths that align with individual goals. While large language models (LLMs) show potential in personalizing learning experiences, existing approaches often lack mechanisms for goal-aligned planning. We introduce Pxplore, a ",
    "url": "https://arxiv.org/abs/2510.13215",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control",
    "summary": "arXiv:2510.14388v2 Announce Type: replace Abstract: Building agents that autonomously operate mobile devices has attracted increasing attention. While Vision-Language Models (VLMs) show promise, most existing approaches rely on direct state-to-action mappings, which lack structured reasoning and planning, and thus generalize poorly to novel tasks o",
    "url": "https://arxiv.org/abs/2510.14388",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "String Seed of Thought: Prompting LLMs for Distribution-Faithful and Diverse Generation",
    "summary": "arXiv:2510.21150v3 Announce Type: replace Abstract: We introduce String Seed of Thought (SSoT), a novel prompting method for LLMs that improves Probabilistic Instruction Following (PIF). We define PIF as a task requiring an LLM to select its answer from a predefined set of options, each associated with a specific probability, such that the empirica",
    "url": "https://arxiv.org/abs/2510.21150",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Human-AI Co-Embodied Intelligence for Scientific Experimentation and Manufacturing",
    "summary": "arXiv:2511.02071v2 Announce Type: replace Abstract: Scientific experimentation and manufacturing rely on prolonged protocol development and complex, multi-step implementation, which require continuous human expertise for precise execution and decision-making, limiting interpretability and scalability. Here, we introduce human-artificial intelligenc",
    "url": "https://arxiv.org/abs/2511.02071",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
    "summary": "arXiv:2511.08585v4 Announce Type: replace Abstract: The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual",
    "url": "https://arxiv.org/abs/2511.08585",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Bridging Symbolic Control and Neural Reasoning in LLM Agents: Structured Cognitive Loop with a Governance Layer",
    "summary": "arXiv:2511.17673v4 Announce Type: replace Abstract: Large language model agents suffer from fundamental architectural problems: entangled reasoning and execution, memory volatility, and uncontrolled action sequences. We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retr",
    "url": "https://arxiv.org/abs/2511.17673",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Leveraging Spreading Activation for Improved Document Retrieval in Knowledge-Graph-Based RAG Systems",
    "summary": "arXiv:2512.15922v2 Announce Type: replace Abstract: Despite initial successes and a variety of architectures, retrieval-augmented generation systems still struggle to reliably retrieve and connect the multi-step evidence required for complicated reasoning tasks. Most of the standard RAG frameworks regard all retrieved information as equally reliabl",
    "url": "https://arxiv.org/abs/2512.15922",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "UniRel: Relation-Centric Knowledge Graph Question Answering with RL-Tuned LLM Reasoning",
    "summary": "arXiv:2512.17043v2 Announce Type: replace Abstract: Knowledge Graph Question Answering (KGQA) has largely focused on entity-centric queries that return a single answer entity. However, many real-world questions are inherently relational, aiming to understand how entities are associated rather than which entity satisfies a query. In this work, we in",
    "url": "https://arxiv.org/abs/2512.17043",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms",
    "summary": "arXiv:2601.08052v2 Announce Type: replace Abstract: Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clea",
    "url": "https://arxiv.org/abs/2601.08052",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "MirrorBench: A Benchmark to Evaluate Conversational User-Proxy Agents for Human-Likeness",
    "summary": "arXiv:2601.08118v2 Announce Type: replace Abstract: Large language models (LLMs) are increasingly used as human simulators, both for evaluating conversational systems and for generating fine-tuning data. However, naive \"act-as-a-user\" prompting often yields verbose, unrealistic utterances, motivating principled evaluation of *user proxy agents*. We",
    "url": "https://arxiv.org/abs/2601.08118",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks",
    "summary": "arXiv:2601.18226v2 Announce Type: replace Abstract: Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To ad",
    "url": "https://arxiv.org/abs/2601.18226",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Think-Augmented Function Calling: Improving LLM Parameter Accuracy Through Embedded Reasoning",
    "summary": "arXiv:2601.18282v2 Announce Type: replace Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in function calling for autonomous agents, yet current mechanisms lack explicit reasoning transparency during parameter generation, particularly for complex functions with interdependent parameters. While existing approaches li",
    "url": "https://arxiv.org/abs/2601.18282",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "FadeMem: Biologically-Inspired Forgetting for Efficient Agent Memory",
    "summary": "arXiv:2601.18642v2 Announce Type: replace Abstract: Large language models deployed as autonomous agents face critical memory limitations, lacking selective forgetting mechanisms that lead to either catastrophic forgetting at context boundaries or information overload within them. While human memory naturally balances retention and forgetting throug",
    "url": "https://arxiv.org/abs/2601.18642",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Balancing Sustainability And Performance: The Role Of Small-Scale LLMs In Agentic Artificial Intelligence Systems",
    "summary": "arXiv:2601.19311v2 Announce Type: replace Abstract: As large language models become integral to agentic artificial intelligence systems, their energy demands during inference may pose significant sustainability challenges. This study investigates whether deploying smaller-scale language models can reduce energy consumption without compromising resp",
    "url": "https://arxiv.org/abs/2601.19311",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "How does information access affect LLM monitors' ability to detect sabotage?",
    "summary": "arXiv:2601.21112v2 Announce Type: replace Abstract: Frontier language model agents can exhibit misaligned behaviors, including deception, exploiting reward hacks, and pursuing hidden objectives. To control potentially misaligned agents, we can use LLMs themselves to monitor for misbehavior. In this paper, we study how information access affects LLM",
    "url": "https://arxiv.org/abs/2601.21112",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Neuro-symbolic AI for Predictive Maintenance (PdM) -- review and recommendations",
    "summary": "arXiv:2602.00731v2 Announce Type: replace Abstract: In this document we perform a systematic review of the State-of-the-art in Predictive Maintenance (PdM) over the last five years in industrial settings such as commercial buildings, pharmaceutical facilities, or semi-conductor manufacturing. In general, data-driven methods such as those based on d",
    "url": "https://arxiv.org/abs/2602.00731",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety",
    "summary": "arXiv:2602.01539v2 Announce Type: replace Abstract: Ensuring robust safety alignment is crucial for Large Language Models (LLMs), yet existing defenses often lag behind evolving adversarial attacks due to their \\textbf{reliance on static, pre-collected data distributions}. In this paper, we introduce \\textbf{MAGIC}, a novel multi-turn multi-agent r",
    "url": "https://arxiv.org/abs/2602.01539",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Methods and Open Problems in Differentiable Social Choice: Learning Mechanisms, Decisions, and Alignment",
    "summary": "arXiv:2602.03003v2 Announce Type: replace Abstract: Social choice is no longer a peripheral concern of political theory or economics-it has become a foundational component of modern machine learning systems. From auctions and resource allocation to federated learning, participatory governance, and the alignment of large language models, machine lea",
    "url": "https://arxiv.org/abs/2602.03003",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Steering LLMs via Scalable Interactive Oversight",
    "summary": "arXiv:2602.04210v2 Announce Type: replace Abstract: As Large Language Models increasingly automate complex, long-horizon tasks such as \\emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent,",
    "url": "https://arxiv.org/abs/2602.04210",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Are AI Capabilities Increasing Exponentially? A Competing Hypothesis",
    "summary": "arXiv:2602.04836v2 Announce Type: replace Abstract: Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation & Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the ",
    "url": "https://arxiv.org/abs/2602.04836",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search",
    "summary": "arXiv:2602.05014v2 Announce Type: replace Abstract: With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search framework",
    "url": "https://arxiv.org/abs/2602.05014",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "VERA-MH: Reliability and Validity of an Open-Source AI Safety Evaluation in Mental Health",
    "summary": "arXiv:2602.05088v2 Announce Type: replace Abstract: Millions now use generative AI chatbots for psychological support. Despite the promise related to availability and scale, the single most pressing question in AI for mental health is whether these tools are safe. The Validation of Ethical and Responsible AI in Mental Health (VERA-MH) evaluation wa",
    "url": "https://arxiv.org/abs/2602.05088",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "AgentXRay: White-Boxing Agentic Systems via Workflow Reconstruction",
    "summary": "arXiv:2602.05353v2 Announce Type: replace Abstract: Large Language Models have shown strong capabilities in complex problem solving, yet many agentic systems remain difficult to interpret and control due to opaque internal workflows. While some frameworks offer explicit architectures for collaboration, many deployed agentic systems operate as black",
    "url": "https://arxiv.org/abs/2602.05353",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "THOR: Inductive Link Prediction over Hyper-Relational Knowledge Graphs",
    "summary": "arXiv:2602.05424v2 Announce Type: replace Abstract: Knowledge graphs (KGs) have become a key ingredient supporting a variety of applications. Beyond the traditional triplet representation of facts where a relation connects two entities, modern KGs observe an increasing number of hyper-relational facts, where an arbitrary number of qualifiers associ",
    "url": "https://arxiv.org/abs/2602.05424",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Bayesian Matrix Decomposition and Applications",
    "summary": "arXiv:2302.11337v4 Announce Type: replace-cross Abstract: The sole aim of this book is to give a self-contained introduction to concepts and mathematical tools in Bayesian matrix decomposition in order to seamlessly introduce matrix decomposition techniques and their applications in subsequent sections. However, we clearly realize our inability to ",
    "url": "https://arxiv.org/abs/2302.11337",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "EEG-MACS: Manifold Attention and Confidence Stratification for EEG-based Cross-Center Brain Disease Diagnosis under Unreliable Annotations",
    "summary": "arXiv:2405.00734v3 Announce Type: replace-cross Abstract: Cross-center data heterogeneity and annotation unreliability significantly challenge the intelligent diagnosis of diseases using brain signals. A notable example is the EEG-based diagnosis of neurodegenerative diseases, which features subtler abnormal neural dynamics typically observed in sm",
    "url": "https://arxiv.org/abs/2405.00734",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Hyperbolic Fine-Tuning for Large Language Models",
    "summary": "arXiv:2410.04010v2 Announce Type: replace-cross Abstract: Large language models (LLMs) have demonstrated remarkable performance across various tasks. However, it remains an open question whether the default Euclidean space is the most suitable choice for LLMs. In this study, we investigate the geometric characteristics of LLMs, focusing specificall",
    "url": "https://arxiv.org/abs/2410.04010",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "ExpressivityBench: Can LLMs Communicate Implicitly?",
    "summary": "arXiv:2411.08010v2 Announce Type: replace-cross Abstract: Human communication is often implicit, conveying tone, identity, and intent beyond literal meanings. While large language models have achieved strong performance on explicit tasks such as summarization and reasoning, their capacity for expressivity, or implicit communication, remains underex",
    "url": "https://arxiv.org/abs/2411.08010",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Adventures in Demand Analysis Using AI",
    "summary": "arXiv:2501.00382v3 Announce Type: replace-cross Abstract: This paper advances empirical demand analysis by integrating multimodal product representations derived from artificial intelligence (AI). Using a detailed dataset of toy cars on textit{Amazon.com}, we combine text descriptions, images, and tabular covariates to represent each product using ",
    "url": "https://arxiv.org/abs/2501.00382",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Adaptive AI-based Decentralized Resource Management in the Cloud-Edge Continuum",
    "summary": "arXiv:2501.15802v2 Announce Type: replace-cross Abstract: In the Cloud-Edge Continuum, dynamic infrastructure change and variable workloads complicate efficient resource management. Centralized methods can struggle to adapt, whilst purely decentralized policies lack global oversight. This paper proposes a hybrid framework using Graph Neural Network",
    "url": "https://arxiv.org/abs/2501.15802",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "STAR: Stepwise Task Augmentation with Relation Learning for Aspect Sentiment Quad Prediction",
    "summary": "arXiv:2501.16093v2 Announce Type: replace-cross Abstract: Aspect-based sentiment analysis (ABSA) aims to identify four sentiment elements, including aspect term, aspect category, opinion term, and sentiment polarity. These elements construct a complete picture of sentiments. The most challenging task, aspect sentiment quad prediction (ASQP), requir",
    "url": "https://arxiv.org/abs/2501.16093",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Learning Metal Microstructural Heterogeneity through Spatial Mapping of Diffraction Latent Space Features",
    "summary": "arXiv:2501.18064v2 Announce Type: replace-cross Abstract: To leverage advancements in machine learning for metallic materials design and property prediction, it is crucial to develop a data-reduced representation of metal microstructures that surpasses the limitations of current physics-based discrete microstructure descriptors. This need is partic",
    "url": "https://arxiv.org/abs/2501.18064",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Probing Perceptual Constancy in Large Vision-Language Models",
    "summary": "arXiv:2502.10273v3 Announce Type: replace-cross Abstract: Perceptual constancy is the ability to maintain stable perceptions of objects despite changes in sensory input, such as variations in distance, angle, or lighting. This ability is crucial for visual understanding in a dynamic world. Here, we explored such ability in current Vision Language M",
    "url": "https://arxiv.org/abs/2502.10273",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Deception at Scale: Deceptive Designs in 1K LLM-Generated Ecommerce Components",
    "summary": "arXiv:2502.13499v2 Announce Type: replace-cross Abstract: Recent work has shown that front-end code generated by Large Language Models (LLMs) can embed deceptive designs. To assess the magnitude of this problem, identify the factors that influence deceptive design production, and test strategies for reducing deceptive designs, we carried out two st",
    "url": "https://arxiv.org/abs/2502.13499",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Tokenizing Single-Channel EEG with Time-Frequency Motif Learning",
    "summary": "arXiv:2502.16060v4 Announce Type: replace-cross Abstract: Foundation models are reshaping EEG analysis, yet an important problem of EEG tokenization remains a challenge. This paper presents TFM-Tokenizer, a novel tokenization framework that learns a vocabulary of time-frequency motifs from single-channel EEG signals and encodes them into discrete t",
    "url": "https://arxiv.org/abs/2502.16060",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Efficient LLM Moderation with Multi-Layer Latent Prototypes",
    "summary": "arXiv:2502.16174v3 Announce Type: replace-cross Abstract: Although modern LLMs are aligned with human values during post-training, robust moderation remains essential to prevent harmful outputs at deployment time. Existing approaches suffer from performance-efficiency trade-offs and are difficult to customize to user-specific requirements. Motivate",
    "url": "https://arxiv.org/abs/2502.16174",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
    "summary": "arXiv:2502.19614v3 Announce Type: replace-cross Abstract: Peer review is a critical process for ensuring the integrity of published scientific research. Confidence in this process is predicated on the assumption that experts in the relevant domain give careful consideration to the merits of manuscripts which are submitted for publication. With the ",
    "url": "https://arxiv.org/abs/2502.19614",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "A Taxonomy for Evaluating Generalist Robot Manipulation Policies",
    "summary": "arXiv:2503.01238v3 Announce Type: replace-cross Abstract: Machine learning for robot manipulation promises to unlock generalization to novel tasks and environments. But how should we measure the progress of these policies towards generalization? Evaluating and quantifying generalization is the Wild West of modern robotics, with each work proposing ",
    "url": "https://arxiv.org/abs/2503.01238",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "PromptPex: Automatic Test Generation for Language Model Prompts",
    "summary": "arXiv:2503.05070v2 Announce Type: replace-cross Abstract: Large language models (LLMs) are being used in many applications and prompts for these models are integrated into software applications as code-like artifacts. These prompts behave much like traditional software in that they take inputs, generate outputs, and perform some specific function. ",
    "url": "https://arxiv.org/abs/2503.05070",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "PiFlow: Principle-Aware Scientific Discovery with Multi-Agent Collaboration",
    "summary": "arXiv:2505.15047v3 Announce Type: replace-cross Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate remarkable potential for scientific discovery. Existing approaches, however, often automate scientific discovery using predefined workflows that lack rationality constraints. This often leads to aimless hypothesizing and ",
    "url": "https://arxiv.org/abs/2505.15047",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Artificial Intelligence for Direct Prediction of Molecular Dynamics Across Chemical Space",
    "summary": "arXiv:2505.16301v3 Announce Type: replace-cross Abstract: Molecular dynamics (MD) is a powerful tool for exploring the behavior of atomistic systems, but its reliance on sequential numerical integration limits simulation efficiency. We present a novel neural network architecture, MDtrajNet, and a pre-trained foundational model, MDtrajNet-1, that di",
    "url": "https://arxiv.org/abs/2505.16301",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "STFlow: Data-Coupled Flow Matching for Geometric Trajectory Simulation",
    "summary": "arXiv:2505.18647v2 Announce Type: replace-cross Abstract: Simulating trajectories of dynamical systems is a fundamental problem in a wide range of fields such as molecular dynamics, biochemistry, and pedestrian dynamics. Machine learning has become an invaluable tool for scaling physics-based simulators and developing models directly from experimen",
    "url": "https://arxiv.org/abs/2505.18647",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Rethinking the effects of data contamination in Code Intelligence",
    "summary": "arXiv:2506.02791v3 Announce Type: replace-cross Abstract: In recent years, code intelligence has gained increasing importance in the field of automated software engineering. Meanwhile, the widespread adoption of Pretrained Language Models (PLMs) and Large Language Models (LLMs) has raised concerns regarding data contamination and its potential impa",
    "url": "https://arxiv.org/abs/2506.02791",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Accelerating Diffusion Planners in Offline RL via Reward-Aware Consistency Trajectory Distillation",
    "summary": "arXiv:2506.07822v3 Announce Type: replace-cross Abstract: Although diffusion models have achieved strong results in decision-making tasks, their slow inference speed remains a key limitation. While consistency models offer a potential solution, existing applications to decision-making either struggle with suboptimal demonstrations under behavior cl",
    "url": "https://arxiv.org/abs/2506.07822",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Physics vs Distributions: Pareto Optimal Flow Matching with Physics Constraints",
    "summary": "arXiv:2506.08604v3 Announce Type: replace-cross Abstract: Physics-constrained generative modeling aims to produce high-dimensional samples that are both physically consistent and distributionally accurate, a task that remains challenging due to often conflicting optimization objectives. Recent advances in flow matching and diffusion models have ena",
    "url": "https://arxiv.org/abs/2506.08604",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "HSG-12M: A Large-Scale Benchmark of Spatial Multigraphs from the Energy Spectra of Non-Hermitian Crystals",
    "summary": "arXiv:2506.08618v2 Announce Type: replace-cross Abstract: AI is transforming scientific research by revealing new ways to understand complex physical systems, but its impact remains constrained by the lack of large, high-quality domain-specific datasets. A rich, largely untapped resource lies in non-Hermitian quantum physics, where the energy spect",
    "url": "https://arxiv.org/abs/2506.08618",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "code_transformed: The Influence of Large Language Models on Code",
    "summary": "arXiv:2506.12014v2 Announce Type: replace-cross Abstract: Coding remains one of the most fundamental modes of interaction between humans and machines. With the rapid advancement of Large Language Models (LLMs), code generation capabilities have begun to significantly reshape programming practices. This development prompts a central question: Have L",
    "url": "https://arxiv.org/abs/2506.12014",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models",
    "summary": "arXiv:2506.14625v3 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have shown impressive moral reasoning abilities. Yet they often diverge when confronted with complex, multi-factor moral dilemmas. To address these discrepancies, we propose a framework that synthesizes multiple LLMs' moral judgments into a collectively formulate",
    "url": "https://arxiv.org/abs/2506.14625",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks",
    "summary": "arXiv:2507.02735v3 Announce Type: replace-cross Abstract: Prompt injection attacks, where untrusted data contains an injected prompt to manipulate the system, have been listed as the top security threat to LLM-integrated applications. Model-level prompt injection defenses have shown strong effectiveness, but the strongest defenses are proprietary. ",
    "url": "https://arxiv.org/abs/2507.02735",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Echo State Transformer: Attention Over Finite Memories",
    "summary": "arXiv:2507.02917v3 Announce Type: replace-cross Abstract: While Large Language Models and their underlying Transformer architecture are remarkably efficient, they do not reflect how our brain processes and learns a diversity of cognitive tasks such as language, nor how it leverages working memory. Furthermore, Transformers encounters a computationa",
    "url": "https://arxiv.org/abs/2507.02917",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Efficient Perplexity Bound and Ratio Matching in Discrete Diffusion Language Models",
    "summary": "arXiv:2507.04341v2 Announce Type: replace-cross Abstract: While continuous diffusion models excel in modeling continuous distributions, their application to categorical data has been less effective. Recent work has shown that ratio-matching through score-entropy within a continuous-time discrete Markov chain (CTMC) framework serves as a competitive",
    "url": "https://arxiv.org/abs/2507.04341",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Constella: Supporting Storywriters' Interconnected Character Creation through LLM-based Multi-Agents",
    "summary": "arXiv:2507.05820v2 Announce Type: replace-cross Abstract: Creating a cast of characters by attending to their relational dynamics is a critical aspect of most long-form storywriting. However, our formative study (N=14) reveals that writers struggle to envision new characters that could influence existing ones, balance similarities and differences a",
    "url": "https://arxiv.org/abs/2507.05820",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation",
    "summary": "arXiv:2508.01309v2 Announce Type: replace-cross Abstract: The scarcity and high cost of high-quality domain-specific question-answering (QA) datasets limit supervised fine-tuning of large language models (LLMs). We introduce $\\textbf{D-SCoRE}$, a training-free framework that leverages LLMs and prompt engineering to automatically generate diverse, r",
    "url": "https://arxiv.org/abs/2508.01309",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Reparameterization Proximal Policy Optimization",
    "summary": "arXiv:2508.06214v3 Announce Type: replace-cross Abstract: By leveraging differentiable dynamics, Reparameterization Policy Gradient (RPG) achieves high sample efficiency. However, current approaches are hindered by two critical limitations: the under-utilization of computationally expensive dynamics Jacobians and inherent training instability. Whil",
    "url": "https://arxiv.org/abs/2508.06214",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Multimodal Iterative RAG for Knowledge-Intensive Visual Question Answering",
    "summary": "arXiv:2509.00798v5 Announce Type: replace-cross Abstract: Knowledge-intensive visual question answering (VQA) requires external knowledge beyond image content, demanding precise visual grounding and coherent integration of visual and textual information. Although multimodal retrieval-augmented generation has achieved notable advances by incorporati",
    "url": "https://arxiv.org/abs/2509.00798",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Calibration and Transformation-Free Weight-Only LLMs Quantization via Dynamic Grouping",
    "summary": "arXiv:2509.03054v4 Announce Type: replace-cross Abstract: Large Language Models (LLMs) deliver strong performance but are difficult to deploy under tight memory and compute constraints. Low-bit post-training quantization (PTQ) is a promising direction; however, it typically relies on calibration data, auxiliary transformations, and GPU tools. To ad",
    "url": "https://arxiv.org/abs/2509.03054",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "FS-DFM: Fast and Accurate Long Text Generation with Few-Step Diffusion Language Models",
    "summary": "arXiv:2509.20624v4 Announce Type: replace-cross Abstract: Autoregressive language models (ARMs) deliver strong likelihoods, but are inherently serial: they generate one token per forward pass, which limits throughput and inflates latency for long sequences. Diffusion Language Models (DLMs) parallelize across positions and thus appear promising for ",
    "url": "https://arxiv.org/abs/2509.20624",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Knowledgeable Language Models as Black-Box Optimizers for Personalized Medicine",
    "summary": "arXiv:2509.20975v2 Announce Type: replace-cross Abstract: The goal of personalized medicine is to discover a treatment regimen that optimizes a patient's clinical outcome based on their personal genetic and environmental factors. However, candidate treatments cannot be arbitrarily administered to the patient to assess their efficacy; we often inste",
    "url": "https://arxiv.org/abs/2509.20975",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "A Data-driven Typology of Vision Models from Integrated Representational Metrics",
    "summary": "arXiv:2509.21628v3 Announce Type: replace-cross Abstract: Large vision models differ widely in architecture and training paradigm, yet we lack principled methods to determine which aspects of their representations are shared across families and which reflect distinctive computational strategies. We leverage a suite of representational similarity me",
    "url": "https://arxiv.org/abs/2509.21628",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Let LLMs Speak Embedding Languages: Generative Text Embeddings via Iterative Contrastive Refinement",
    "summary": "arXiv:2509.24291v2 Announce Type: replace-cross Abstract: Existing large language model (LLM)-based embeddings typically adopt an encoder-only paradigm, treating LLMs as static feature extractors and overlooking their core generative strengths. We introduce GIRCSE (Generative Iterative Refinement for Contrastive Sentence Embeddings), a novel framew",
    "url": "https://arxiv.org/abs/2509.24291",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Feature Identification via the Empirical NTK",
    "summary": "arXiv:2510.00468v3 Announce Type: replace-cross Abstract: We provide evidence that eigenanalysis of the empirical neural tangent kernel (eNTK) can surface the features used by trained neural networks. Across three standard toy models for mechanistic interpretability, Toy Models of Superposition (TMS), a 1-layer MLP trained on modular addition and a",
    "url": "https://arxiv.org/abs/2510.00468",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "HyPlan: Hybrid Learning-Assisted Planning Under Uncertainty for Safe Autonomous Driving",
    "summary": "arXiv:2510.07210v2 Announce Type: replace-cross Abstract: We present a novel hybrid learning-assisted planning method, named HyPlan, for solving the collision-free navigation problem for self-driving cars in partially observable traffic environments. HyPlan combines methods for multi-agent behavior prediction, deep reinforcement learning with proxi",
    "url": "https://arxiv.org/abs/2510.07210",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Learning to Guarantee Type Correctness in Code Generation through Type-Guided Program Synthesis",
    "summary": "arXiv:2510.10216v2 Announce Type: replace-cross Abstract: Language models have shown remarkable proficiency in code generation; nevertheless, ensuring type correctness remains a challenge. Although traditional methods, such as constrained decoding, alleviate this problem by externally rejecting untypable code, the model itself does not effectively ",
    "url": "https://arxiv.org/abs/2510.10216",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "A Free Lunch in LLM Compression: Revisiting Retraining after Pruning",
    "summary": "arXiv:2510.14444v2 Announce Type: replace-cross Abstract: Post-training pruning substantially reduces inference costs but often causes severe quality degradation without adapting the remaining weights. For LLMs, such retraining is commonly considered impractical due to large computational costs, motivating increasingly sophisticated pruning criteri",
    "url": "https://arxiv.org/abs/2510.14444",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark",
    "summary": "arXiv:2510.19585v3 Announce Type: replace-cross Abstract: This paper presents a novel task of extracting low-resourced and noisy Latin fragments from mixed-language historical documents with varied layouts. We benchmark and evaluate the performance of large foundation models against a multimodal dataset of 724 annotated pages. The results demonstra",
    "url": "https://arxiv.org/abs/2510.19585",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Key and Value Weights Are Probably All You Need: On the Necessity of the Query, Key, Value weight Triplet in Encoder-Only and Decoder-Only Transformers",
    "summary": "arXiv:2510.23912v5 Announce Type: replace-cross Abstract: We theoretically investigate whether the Query, Key, Value weight triplet can be reduced in encoder-only and decoder-only transformers. Under mild assumptions, we prove that Query weights are redundant and can be replaced with the identity matrix, reducing attention parameters by $25\\%$. Thi",
    "url": "https://arxiv.org/abs/2510.23912",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "FeNN-DMA: A RISC-V SoC for SNN acceleration",
    "summary": "arXiv:2511.00732v2 Announce Type: replace-cross Abstract: Spiking Neural Networks (SNNs) are a promising, energy-efficient alternative to standard Artificial Neural Networks (ANNs) and are particularly well-suited to spatio-temporal tasks such as keyword spotting and video classification. However, SNNs have a much lower arithmetic intensity than AN",
    "url": "https://arxiv.org/abs/2511.00732",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "An item is worth one token in Multimodal Large Language Models-based Sequential Recommendation",
    "summary": "arXiv:2511.05885v3 Announce Type: replace-cross Abstract: Sequential recommendations (SR) predict users' future interactions based on their historical behavior. The rise of Large Language Models (LLMs) has brought powerful generative and reasoning capabilities, significantly enhancing SR performance, while Multimodal LLMs (MLLMs) further extend thi",
    "url": "https://arxiv.org/abs/2511.05885",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Structural Enforcement of Statistical Rigor in AI-Driven Discovery: A Functional Architecture",
    "summary": "arXiv:2511.06701v2 Announce Type: replace-cross Abstract: AI-Scientist systems that use large language models to automate research risk generating spurious discoveries through uncontrolled multiple testing. We present a functional architecture that enforces statistical rigor at two levels: a Haskell embedded DSL (the Research monad) that makes it i",
    "url": "https://arxiv.org/abs/2511.06701",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "GraphToxin: Reconstructing Full Unlearned Graphs from Graph Unlearning",
    "summary": "arXiv:2511.10936v2 Announce Type: replace-cross Abstract: Graph unlearning has emerged as a promising solution to comply with \"the right to be forgotten\" regulations by enabling the removal of sensitive information upon request. However, this solution is not foolproof. The involvement of multiple parties creates new attack surfaces, and residual tr",
    "url": "https://arxiv.org/abs/2511.10936",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Explainable Transformer-Based Email Phishing Classification with Adversarial Robustness",
    "summary": "arXiv:2511.12085v2 Announce Type: replace-cross Abstract: Phishing and related cyber threats are becoming more varied and technologically advanced. Among these, email-based phishing remains the most dominant and persistent threat. These attacks exploit human vulnerabilities to disseminate malware or gain unauthorized access to sensitive information",
    "url": "https://arxiv.org/abs/2511.12085",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Dynamic Expert Quantization for Scalable Mixture-of-Experts Inference",
    "summary": "arXiv:2511.15015v3 Announce Type: replace-cross Abstract: Mixture-of-Experts (MoE) has become a practical architecture for scaling LLM capacity while keeping per-token compute modest, but deploying MoE models on a single, memory-limited GPU remains difficult because expert weights dominate the HBM footprint. Existing expert offloading and prefetchi",
    "url": "https://arxiv.org/abs/2511.15015",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "SeSE: Black-Box Uncertainty Quantification for Large Language Models Based on Structural Information Theory",
    "summary": "arXiv:2511.16275v3 Announce Type: replace-cross Abstract: Reliable uncertainty quantification (UQ) is essential for deploying large language models (LLMs) in safety-critical scenarios, as it enables them to abstain from responding when uncertain, thereby avoiding hallucinations, i.e., plausible yet factually incorrect responses. However, while sema",
    "url": "https://arxiv.org/abs/2511.16275",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Pascal-Weighted Genetic Algorithms: A Binomially-Structured Recombination Framework",
    "summary": "arXiv:2512.01249v2 Announce Type: replace-cross Abstract: This paper introduces a new family of multi-parent recombination operators for Genetic Algorithms (GAs), based on normalized Pascal (binomial) coefficients. Unlike classical two-parent crossover operators, Pascal-Weighted Recombination (PWR) forms offsprings as structured convex combination ",
    "url": "https://arxiv.org/abs/2512.01249",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Adaptive Regime-Switching Forecasts with Distribution-Free Uncertainty: Deep Switching State-Space Models Meet Conformal Prediction",
    "summary": "arXiv:2512.03298v2 Announce Type: replace-cross Abstract: Regime transitions routinely break stationarity in time series, making calibrated uncertainty as important as point accuracy. We study distribution-free uncertainty for regime-switching forecasting by coupling Deep Switching State Space Models with Adaptive Conformal Inference (ACI) and its ",
    "url": "https://arxiv.org/abs/2512.03298",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "ReflexFlow: Rethinking Learning Objective for Exposure Bias Alleviation in Flow Matching",
    "summary": "arXiv:2512.04904v2 Announce Type: replace-cross Abstract: Despite tremendous recent progress, Flow Matching methods still suffer from exposure bias due to discrepancies in training and inference. This paper investigates the root causes of exposure bias in Flow Matching, including: (1) the model lacks generalization to biased inputs during training,",
    "url": "https://arxiv.org/abs/2512.04904",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning",
    "summary": "arXiv:2512.21446v2 Announce Type: replace-cross Abstract: Masked diffusion language models (MDLMs) offer the potential for parallel token generation, but most open-source MDLMs decode fewer than 5 tokens per model forward pass even with sophisticated sampling strategies, limiting their parallel generation potential. Existing acceleration methods ei",
    "url": "https://arxiv.org/abs/2512.21446",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Synergizing Kolmogorov-Arnold Networks with Dynamic Adaptive Weighting for High-Frequency and Multi-Scale PDE Solutions",
    "summary": "arXiv:2512.22283v3 Announce Type: replace-cross Abstract: PINNs enhance scientific computing by incorporating physical laws into neural network structures, leading to significant advancements in scientific computing. However, PINNs struggle with multi-scale and high-frequency problems due to pathological gradient flow and spectral bias, which sever",
    "url": "https://arxiv.org/abs/2512.22283",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "DECEPTICON: How Dark Patterns Manipulate Web Agents",
    "summary": "arXiv:2512.22894v2 Announce Type: replace-cross Abstract: Deceptive UI designs, widely instantiated across the web and commonly known as dark patterns, manipulate users into performing actions misaligned with their goals. In this paper, we show that dark patterns are highly effective in steering agent trajectories, posing a significant risk to agen",
    "url": "https://arxiv.org/abs/2512.22894",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Trust Region Masking for Long-Horizon LLM Reinforcement Learning",
    "summary": "arXiv:2512.23075v2 Announce Type: replace-cross Abstract: Policy gradient methods for Large Language Models optimize a policy $\\pi_\\theta$ via a surrogate objective computed from samples of a rollout policy $\\pi_{\\text{roll}}$. However, modern LLM-RL pipelines suffer from unavoidable implementation divergences, such as backend discrepancies, Mixtur",
    "url": "https://arxiv.org/abs/2512.23075",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Dynamic Vocabulary Pruning: Stable LLM-RL by Taming the Tail",
    "summary": "arXiv:2512.23087v2 Announce Type: replace-cross Abstract: Reinforcement Learning (RL) for Large Language Models (LLMs) faces a fundamental tension: the numerical divergence between high-throughput inference engines and numerically precise training engines. Although these systems share the same parameters, they produce slightly different probability",
    "url": "https://arxiv.org/abs/2512.23087",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process",
    "summary": "arXiv:2512.23213v2 Announce Type: replace-cross Abstract: We propose LLM-PeerReview, an unsupervised LLM Ensemble method that selects the most ideal response from multiple LLM-generated candidates for each query, harnessing the collective wisdom of multiple models with diverse strengths. LLM-PeerReview is built on a novel, peer-review-inspired fram",
    "url": "https://arxiv.org/abs/2512.23213",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments",
    "summary": "arXiv:2512.24985v3 Announce Type: replace-cross Abstract: Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents. Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-li",
    "url": "https://arxiv.org/abs/2512.24985",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "An Example for Domain Adaptation Using CycleGAN",
    "summary": "arXiv:2601.08776v3 Announce Type: replace-cross Abstract: Cycle-Consistent Adversarial Network (CycleGAN) is very promising in domain adaptation. In this report, an example in medical domain will be explained. We present struecture of a CycleGAN model for unpaired image-to-image translation from microscopy to pseudo H\\&amp;E stained histopathology ",
    "url": "https://arxiv.org/abs/2601.08776",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "A.X K1 Technical Report",
    "summary": "arXiv:2601.09200v4 Announce Type: replace-cross Abstract: We introduce A.X K1, a 519B-parameter Mixture-of-Experts (MoE) language model trained from scratch. Our design leverages scaling laws to optimize training configurations and vocabulary size under fixed computational budgets. A.X K1 is pre-trained on a corpus of approximately 10T tokens, cura",
    "url": "https://arxiv.org/abs/2601.09200",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Layer-adaptive Expert Pruning for Pre-Training of Mixture-of-Experts Large Language Models",
    "summary": "arXiv:2601.14327v2 Announce Type: replace-cross Abstract: Although Mixture-of-Experts (MoE) Large Language Models (LLMs) deliver superior accuracy with a reduced number of active parameters, their pre-training represents a significant computationally bottleneck due to underutilized experts and limited training efficiency. This work introduces a Lay",
    "url": "https://arxiv.org/abs/2601.14327",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "LLM-Enhanced Reinforcement Learning for Long-Term User Satisfaction in Interactive Recommendation",
    "summary": "arXiv:2601.19585v2 Announce Type: replace-cross Abstract: Interactive recommender systems can dynamically adapt to user feedback, but often suffer from content homogeneity and filter bubble effects due to overfitting short-term user preferences. While recent efforts aim to improve content diversity, they predominantly operate in static or one-shot ",
    "url": "https://arxiv.org/abs/2601.19585",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Towards Agentic Intelligence for Materials Science",
    "summary": "arXiv:2602.00169v2 Announce Type: replace-cross Abstract: The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This s",
    "url": "https://arxiv.org/abs/2602.00169",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "In-Run Data Shapley for Adam Optimizer",
    "summary": "arXiv:2602.00329v2 Announce Type: replace-cross Abstract: Reliable data attribution is essential for mitigating bias and reducing computational waste in modern machine learning, with the Shapley value serving as the theoretical gold standard. While recent \"In-Run\" methods bypass the prohibitive cost of retraining by estimating contributions dynamic",
    "url": "https://arxiv.org/abs/2602.00329",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "CIPHER: Cryptographic Insecurity Profiling via Hybrid Evaluation of Responses",
    "summary": "arXiv:2602.01438v2 Announce Type: replace-cross Abstract: Large language models (LLMs) are increasingly used to assist developers with code, yet their implementations of cryptographic functionality often contain exploitable flaws. Minor design choices (e.g., static initialization vectors or missing authentication) can silently invalidate security g",
    "url": "https://arxiv.org/abs/2602.01438",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Cardinality-Preserving Structured Sparse Graph Transformers for Molecular Property Prediction",
    "summary": "arXiv:2602.02201v2 Announce Type: replace-cross Abstract: Drug discovery motivates efficient molecular property prediction under limited labeled data. Chemical space is vast, often estimated at approximately 10^60 drug-like molecules, while only thousands of drugs have been approved. As a result, self-supervised pretraining on large unlabeled molec",
    "url": "https://arxiv.org/abs/2602.02201",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "OmniCode: A Benchmark for Evaluating Software Engineering Agents",
    "summary": "arXiv:2602.02262v2 Announce Type: replace-cross Abstract: LLM-powered coding agents are redefining how real-world software is developed. To drive the research towards better coding agents, we require challenging benchmarks that can rigorously evaluate the ability of such agents to perform various software engineering tasks. However, popular coding ",
    "url": "https://arxiv.org/abs/2602.02262",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "D$^2$Quant: Accurate Low-bit Post-Training Weight Quantization for LLMs",
    "summary": "arXiv:2602.02546v2 Announce Type: replace-cross Abstract: Large language models (LLMs) deliver strong performance, but their high compute and memory costs make deployment difficult in resource-constrained scenarios. Weight-only post-training quantization (PTQ) is appealing, as it reduces memory usage and enables practical speedup without low-bit op",
    "url": "https://arxiv.org/abs/2602.02546",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Testing Storage-System Correctness: Challenges, Fuzzing Limitations, and AI-Augmented Opportunities",
    "summary": "arXiv:2602.02614v2 Announce Type: replace-cross Abstract: Storage systems are fundamental to modern computing infrastructures, yet ensuring their correctness remains challenging in practice. Despite decades of research on system testing, many storage-system failures (including durability, ordering, recovery, and consistency violations) remain diffi",
    "url": "https://arxiv.org/abs/2602.02614",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs",
    "summary": "arXiv:2602.03048v3 Announce Type: replace-cross Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning. However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptiv",
    "url": "https://arxiv.org/abs/2602.03048",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures",
    "summary": "arXiv:2602.03604v2 Announce Type: replace-cross Abstract: We present EB-JEPA, an open-source library for learning representations and world models using Joint-Embedding Predictive Architectures (JEPAs). JEPAs learn to predict in representation space rather than pixel space, avoiding the pitfalls of generative modeling while capturing semantically m",
    "url": "https://arxiv.org/abs/2602.03604",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Benchmarking Automatic Speech Recognition for Indian Languages in Agricultural Contexts",
    "summary": "arXiv:2602.03868v2 Announce Type: replace-cross Abstract: The digitization of agricultural advisory services in India requires robust Automatic Speech Recognition (ASR) systems capable of accurately transcribing domain-specific terminology in multiple Indian languages. This paper presents a benchmarking framework for evaluating ASR performance in a",
    "url": "https://arxiv.org/abs/2602.03868",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "PromptSplit: Revealing Prompt-Level Disagreement in Generative Models",
    "summary": "arXiv:2602.04009v2 Announce Type: replace-cross Abstract: Prompt-guided generative AI models have rapidly expanded across vision and language domains, producing realistic and diverse outputs from textual inputs. The growing variety of such models, trained with different data and architectures, calls for principled methods to identify which types of",
    "url": "https://arxiv.org/abs/2602.04009",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Performative Learning Theory",
    "summary": "arXiv:2602.04402v2 Announce Type: replace-cross Abstract: Performative predictions influence the very outcomes they aim to forecast. We study performative predictions that affect a sample (e.g., only existing users of an app) and/or the whole population (e.g., all potential app users). This raises the question of how well models generalize under pe",
    "url": "https://arxiv.org/abs/2602.04402",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "DRMOT: A Dataset and Framework for RGBD Referring Multi-Object Tracking",
    "summary": "arXiv:2602.04692v2 Announce Type: replace-cross Abstract: Referring Multi-Object Tracking (RMOT) aims to track specific targets based on language descriptions and is vital for interactive AI systems such as robotics and autonomous driving. However, existing RMOT models rely solely on 2D RGB data, making it challenging to accurately detect and assoc",
    "url": "https://arxiv.org/abs/2602.04692",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Near-Optimal Dynamic Matching via Coarsening with Application to Heart Transplantation",
    "summary": "arXiv:2602.04989v2 Announce Type: replace-cross Abstract: Online matching has been a mainstay in domains such as Internet advertising and organ allocation, but practical algorithms often lack strong theoretical guarantees. We take an important step toward addressing this by developing new online matching algorithms based on a coarsening approach. A",
    "url": "https://arxiv.org/abs/2602.04989",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders",
    "summary": "arXiv:2602.05027v2 Announce Type: replace-cross Abstract: Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical u",
    "url": "https://arxiv.org/abs/2602.05027",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning",
    "summary": "arXiv:2602.05183v2 Announce Type: replace-cross Abstract: Large language models (LLMs) are increasingly trained in complex Reinforcement Learning, multi-agent environments, making it difficult to understand how behavior changes over training. Sparse Autoencoders (SAEs) have recently shown to be useful for data-centric interpretability. In this work",
    "url": "https://arxiv.org/abs/2602.05183",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "FlashBlock: Attention Caching for Efficient Long-Context Block Diffusion",
    "summary": "arXiv:2602.05305v2 Announce Type: replace-cross Abstract: Generating long-form content, such as minute-long videos and extended texts, is increasingly important for modern generative models. Block diffusion improves inference efficiency via KV caching and block-wise causal inference and has been widely adopted in diffusion language models and video",
    "url": "https://arxiv.org/abs/2602.05305",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening",
    "summary": "arXiv:2602.05386v2 Announce Type: replace-cross Abstract: As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at ",
    "url": "https://arxiv.org/abs/2602.05386",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "DisCa: Accelerating Video Diffusion Transformers with Distillation-Compatible Learnable Feature Caching",
    "summary": "arXiv:2602.05449v2 Announce Type: replace-cross Abstract: While diffusion models have achieved great success in the field of video generation, this progress is accompanied by a rapidly escalating computational burden. Among the existing acceleration methods, Feature Caching is popular due to its training-free property and considerable speedup perfo",
    "url": "https://arxiv.org/abs/2602.05449",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Exploring AI-Augmented Sensemaking of Patient-Generated Health Data: A Mixed-Method Study with Healthcare Professionals in Cardiac Risk Reduction",
    "summary": "arXiv:2602.05687v2 Announce Type: replace-cross Abstract: Individuals are increasingly generating substantial personal health and lifestyle data, e.g. through wearables and smartphones. While such data could transform preventative care, its integration into clinical practice is hindered by its scale, heterogeneity and the time pressure and data lit",
    "url": "https://arxiv.org/abs/2602.05687",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "CSRv2: Unlocking Ultra-Sparse Embeddings",
    "summary": "arXiv:2602.05735v2 Announce Type: replace-cross Abstract: In the era of large foundation models, the quality of embeddings has become a central determinant of downstream task performance and overall system capability. Yet widely used dense embeddings are often extremely high-dimensional, incurring substantial costs in storage, memory, and inference",
    "url": "https://arxiv.org/abs/2602.05735",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "TimelyFreeze: Adaptive Parameter Freezing Mechanism for Pipeline Parallelism",
    "summary": "arXiv:2602.05754v2 Announce Type: replace-cross Abstract: Pipeline parallelism enables training models that exceed single-device memory, but practical throughput remains limited by pipeline bubbles. Although parameter freezing can improve training throughput by adaptively skipping backward computation, existing methods often over-freeze parameters,",
    "url": "https://arxiv.org/abs/2602.05754",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  },
  {
    "title": "Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations",
    "summary": "arXiv:2602.05885v2 Announce Type: replace-cross Abstract: High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In ",
    "url": "https://arxiv.org/abs/2602.05885",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00"
  }
]