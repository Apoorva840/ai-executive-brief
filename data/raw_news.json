[
  {
    "title": "Converge Bio raises $25M, backed by Bessemer and execs from Meta, OpenAI, Wiz",
    "summary": "AI drug discovery startup Converge Bio raised $25 million in a Series A led by Bessemer Venture Partners, with additional backing from executives at Meta, OpenAI, and Wiz.",
    "url": "https://techcrunch.com/2026/01/13/ai-drug-discovery-startup-converge-bio-pulls-in-25m-from-bessemer-and-execs-from-meta-openai-and-wiz/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Meta bought 1 GW of solar this week",
    "summary": "The social media company inked three deals in the U.S. to power its data centers and offset its carbon footprint.",
    "url": "https://techcrunch.com/2025/10/31/meta-bought-1-gw-of-solar-this-week/",
    "source": "TechCrunch AI"
  },
  {
    "title": "How one AI startup is helping rice farmers battle climate change",
    "summary": "Mitti Labs is working with The Nature Conservancy to expand the use of climate-friendly rice farming practices in India. The startup uses its AI to verify reductions in methane emissions.",
    "url": "https://techcrunch.com/2025/08/26/how-one-ai-startup-is-helping-rice-farmers-battle-climate-change/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Harvard dropouts to launch ‘always on’ AI smart glasses that listen and record every conversation",
    "summary": "After developing a facial-recognition app for Meta’s Ray-Ban glasses and doxing random people, two former Harvard students are now launching a startup that makes smart glasses with an always-on microphone.",
    "url": "https://techcrunch.com/2025/08/20/harvard-dropouts-to-launch-always-on-ai-smart-glasses-that-listen-and-record-every-conversation/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Meta to add 100MW of solar power from US gear",
    "summary": "The social media company is adding another tranche of solar to power a new AI data center in South Carolina.",
    "url": "https://techcrunch.com/2025/08/20/meta-to-add-100-mw-of-solar-power-from-u-s-gear/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Perplexity accused of scraping websites that explicitly blocked AI scraping",
    "summary": "Internet giant Cloudflare says it detected Perplexity crawling and scraping websites, even after customers had added technical blocks telling Perplexity not to scrape their pages.",
    "url": "https://techcrunch.com/2025/08/04/perplexity-accused-of-scraping-websites-that-explicitly-blocked-ai-scraping/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Obvio’s stop sign cameras use AI to root out unsafe drivers",
    "summary": "American streets are incredibly dangerous for pedestrians. A San Carlos, California-based startup called Obvio thinks it can change that by installing cameras at stop signs -- a solution the founders also say won’t create a panopticon.",
    "url": "https://techcrunch.com/2025/06/04/obvios-stop-sign-cameras-use-ai-to-root-out-unsafe-drivers/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Breakneck data center growth challenges Microsoft’s sustainability goals",
    "summary": "Microsoft's sustainability goals are imperiled by its push into AI and cloud services.",
    "url": "https://techcrunch.com/2025/06/02/breakneck-data-center-growth-challenges-microsofts-sustainability-goals/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Gridcare thinks more than 100 GW of data center capacity is hiding in the grid",
    "summary": "Gridcare raised $13.3 million for its data platform that finds underutilized capacity on the electrical grid.",
    "url": "https://techcrunch.com/2025/05/27/gridcare-thinks-more-than-100-gw-of-data-center-capacity-is-hiding-in-the-grid/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Meta adds another 650 MW of solar power to its AI push",
    "summary": "The company already has more than 12 gigawatts of capacity in its renewable power portfolio.",
    "url": "https://techcrunch.com/2025/05/22/meta-adds-another-650-mw-of-solar-power-to-its-ai-push/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Who are climate-conscious consumers? Not who you’d expect, says Northwind Climate",
    "summary": "Rather than divide people into demographic buckets, Northwind Climate analyzes survey responses for behavioral clues.",
    "url": "https://techcrunch.com/2025/04/01/who-are-climate-conscious-consumers-not-who-youd-expect-says-northwind-climate/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Data centers love solar: Here’s a comprehensive guide to deals over 100 megawatts",
    "summary": "New and expanded data centers are expected to double the sector’s power demand by 2029 as tech companies rush to capitalize on AI.",
    "url": "https://techcrunch.com/2025/03/30/data-centers-love-solar-heres-a-comprehensive-guide-to-deals-over-100-megawatts/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Nvidia thinks AI can solve electrical grid problems caused by AI",
    "summary": "The Open Power AI Consortium says it will use domain-specific AI models to tackle problems in the power industry.",
    "url": "https://techcrunch.com/2025/03/20/nvidia-thinks-ai-can-solve-electrical-grid-problems-caused-by-ai/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Solar notches another win as Microsoft adds 475 MW to power its AI data centers",
    "summary": "The company recently signed a deal with energy provider AES for three solar projects across the Midwest.",
    "url": "https://techcrunch.com/2025/03/20/solar-notches-another-win-as-microsoft-adds-475-mw-to-power-its-ai-data-centers/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Geothermal could power nearly all new data centers through 2030",
    "summary": "Geothermal resources have enormous potential to provide the sort of consistent power that data centers crave.",
    "url": "https://techcrunch.com/2025/03/11/geothermal-could-power-nearly-all-new-data-centers-through-2030/",
    "source": "TechCrunch AI"
  },
  {
    "title": "ElevenLabs now lets authors create and publish audiobooks on its own platform",
    "summary": "Voice AI company ElevenLabs is now letting authors publish AI-generated audiobooks on its own Reader app, TechCrunch has learned and the company confirmed. The announcement comes days after the company partnered with Spotify for AI-narrated audiobooks. ElevenLabs, which raised a $180 million mega-round last month, started inviting authors to try ou",
    "url": "https://techcrunch.com/2025/02/25/elevenlabs-is-now-letting-authors-create-and-publish-audiobooks-on-its-own-platform/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Data center tweaks could unlock 76 GW of new power capacity in the US",
    "summary": "A new study argues that data centers could be ideal demand-response participants because they have the potential to be flexible.",
    "url": "https://techcrunch.com/2025/02/13/data-center-tweaks-could-unlock-76-gw-of-new-power-capacity-in-the-u-s/",
    "source": "TechCrunch AI"
  },
  {
    "title": "YouTube AI updates include auto dubbing expansion, age ID tech, and more",
    "summary": "In his annual letter, YouTube CEO Neal Mohan dubbed AI one of the company&#8217;s four &#8220;big bets&#8221; for 2025. The executive pointed to the company&#8217;s investments in AI tools for creators, including ones for video ideas, thumbnails, and language translation. The latter feature will roll out to all creators in YouTube&#8217;s Partner P",
    "url": "https://techcrunch.com/2025/02/11/youtube-ai-updates-to-include-expansion-of-auto-dubbing-age-identifying-tech-and-more/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Self Inspection raises $3M for its AI-powered vehicle inspections",
    "summary": "A number of startups are racing to make vehicle inspections faster, easier, and cheaper. Self Inspection, a startup based in San Diego, thinks it has them all beat with its AI-powered service &#8212; and now it has convinced outside investors. Self Inspection, founded in 2021, is set to announce Thursday it&#8217;s raised $3 million in [&#8230;]",
    "url": "https://techcrunch.com/2025/02/07/self-inspection-raises-3m-for-its-ai-powered-vehicle-inspections/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Meta turns to solar — again — in its data center-building boom",
    "summary": "The announcement comes as Meta CEO Mark Zuckerberg maintains the company’s ambitious AI strategy, which will require hefty capital investments in data centers.",
    "url": "https://techcrunch.com/2025/01/31/meta-turns-to-solar-again-in-its-data-center-building-boom/",
    "source": "TechCrunch AI"
  },
  {
    "title": "From guardrails to governance: A CEO’s guide for securing agentic systems",
    "summary": "The previous article in this series, “Rules fail at the prompt, succeed at the boundary,” focused on the first AI-orchestrated espionage campaign and the failure of prompt-level control. This article is the prescription. The question every CEO is now getting from their board is some version of: What do we do about agent risk? Across&#8230;",
    "url": "https://www.technologyreview.com/2026/02/04/1131014/from-guardrails-to-governance-a-ceos-guide-for-securing-agentic-systems/",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "What we’ve been getting wrong about AI’s truth crisis",
    "summary": "This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,&#160;sign up here. What would it take to convince you that the era of truth decay we were long warned about—where AI content dupes us, shapes our beliefs even when we catch the lie, and&#8230;",
    "url": "https://www.technologyreview.com/2026/02/02/1132068/what-weve-been-getting-wrong-about-ais-truth-crisis/",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "The crucial first step for designing a successful enterprise AI system",
    "summary": "Many organizations rushed into generative AI, only to see pilots fail to deliver value. Now, companies want measurable outcomes—but how do you design for success? At Mistral AI, we partner with global industry leaders to co-design tailored AI solutions that solve their most difficult problems. Whether it’s increasing CX productivity with Cisco, bui",
    "url": "https://www.technologyreview.com/2026/02/02/1131822/the-crucial-first-step-for-designing-a-successful-enterprise-ai-system/",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "Inside the marketplace powering bespoke AI deepfakes of real women",
    "summary": "Civitai—an online marketplace for buying and selling AI-generated content, backed by the venture capital firm Andreessen Horowitz—is letting users buy custom instruction files for generating celebrity deepfakes. Some of these files were specifically designed to make pornographic images banned by the site, a new analysis has found. The study, from r",
    "url": "https://www.technologyreview.com/2026/01/30/1131945/inside-the-marketplace-powering-bespoke-ai-deepfakes-of-real-women/",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "The AI Hype Index: Grok makes porn, and Claude Code nails your job",
    "summary": "Everyone is panicking because AI is very bad; everyone is panicking because AI is very good. It’s just that you never know which one you’re going to get. Grok is a pornography machine. Claude Code can do anything from building websites to reading your MRI. So of course Gen Z is spooked by what this&#8230;",
    "url": "https://www.technologyreview.com/2026/01/29/1131787/the-ai-hype-index-grok-makes-porn-claude-code-nails-your-job/",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "DHS is using Google and Adobe AI to make videos",
    "summary": "The US Department of Homeland Security is using AI video generators from Google and Adobe to make and edit content shared with the public, a new document reveals. It comes as immigration agencies have flooded social media with content to support President Trump&#8217;s mass deportation agenda—some of which appears to be made with AI—and as&#8230;",
    "url": "https://www.technologyreview.com/2026/01/29/1131938/dhs-is-using-google-and-adobe-ai-to-make-videos/",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "What AI “remembers” about you is privacy’s next frontier",
    "summary": "The ability to remember you and your preferences is rapidly becoming a big selling point for AI chatbots and agents.&#160; Earlier this month, Google announced Personal Intelligence, a new way for people to interact with the company’s Gemini chatbot that draws on their Gmail, photos, search, and YouTube histories to make Gemini “more personal, proa",
    "url": "https://www.technologyreview.com/2026/01/28/1131835/what-ai-remembers-about-you-is-privacys-next-frontier/",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "Rules fail at the prompt, succeed at the boundary",
    "summary": "From the Gemini Calendar prompt-injection attack of 2026 to the September 2025 state-sponsored hack using Anthropic’s Claude code as an automated intrusion engine, the coercion of human-in-the-loop agentic actions and fully autonomous agentic workflows are the new attack vector for hackers. In the Anthropic case, roughly 30 organizations across tec",
    "url": "https://www.technologyreview.com/2026/01/28/1131003/rules-fail-at-the-prompt-succeed-at-the-boundary/",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "OpenAI’s latest product lets you vibe code science",
    "summary": "OpenAI just revealed what its new in-house team, OpenAI for Science, has been up to. The firm has released a free LLM-powered tool for scientists called Prism, which embeds ChatGPT in a text editor for writing scientific papers. The idea is to put ChatGPT front and center inside software that scientists use to write up&#8230;",
    "url": "https://www.technologyreview.com/2026/01/27/1131793/openais-latest-product-lets-you-vibe-code-science/",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "Inside OpenAI’s big play for science",
    "summary": "In the three years since ChatGPT’s explosive debut, OpenAI’s technology has upended a remarkable range of everyday activities at home, at work, in schools—anywhere people have a browser open or a phone out, which is everywhere. Now OpenAI is making an explicit play for scientists. In October, the firm announced that it had launched a&#8230;",
    "url": "https://www.technologyreview.com/2026/01/26/1131728/inside-openais-big-play-for-science/",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "Railway secures $100 million to challenge AWS with AI-native cloud infrastructure",
    "summary": "Railway, a San Francisco-based cloud platform that has quietly amassed two million developers without spending a dollar on marketing, announced Thursday that it raised $100 million in a Series B funding round, as surging demand for artificial intelligence applications exposes the limitations of legacy cloud infrastructure.TQ Ventures led the round,",
    "url": "https://venturebeat.com/infrastructure/railway-secures-usd100-million-to-challenge-aws-with-ai-native-cloud",
    "source": "VentureBeat AI"
  },
  {
    "title": "Claude Code costs up to $200 a month. Goose does the same thing for free.",
    "summary": "The artificial intelligence coding revolution comes with a catch: it&#x27;s expensive.Claude Code, Anthropic&#x27;s terminal-based AI agent that can write, debug, and deploy code autonomously, has captured the imagination of software developers worldwide. But its pricing — ranging from $20 to $200 per month depending on usage — has sparked a growin",
    "url": "https://venturebeat.com/infrastructure/claude-code-costs-up-to-usd200-a-month-goose-does-the-same-thing-for-free",
    "source": "VentureBeat AI"
  },
  {
    "title": "Listen Labs raises $69M after viral billboard hiring stunt to scale AI customer interviews",
    "summary": "Alfred Wahlforss was running out of options. His startup, Listen Labs, needed to hire over 100 engineers, but competing against Mark Zuckerberg&#x27;s $100 million offers seemed impossible. So he spent $5,000 — a fifth of his marketing budget — on a billboard in San Francisco displaying what looked like gibberish: five strings of random numbers.The",
    "url": "https://venturebeat.com/technology/listen-labs-raises-usd69m-after-viral-billboard-hiring-stunt-to-scale-ai",
    "source": "VentureBeat AI"
  },
  {
    "title": "Salesforce rolls out new Slackbot AI agent as it battles Microsoft and Google in workplace AI",
    "summary": "Salesforce on Tuesday launched an entirely rebuilt version of Slackbot, the company&#x27;s workplace assistant, transforming it from a simple notification tool into what executives describe as a fully powered AI agent capable of searching enterprise data, drafting documents, and taking action on behalf of employees.The new Slackbot, now generally a",
    "url": "https://venturebeat.com/technology/salesforce-rolls-out-new-slackbot-ai-agent-as-it-battles-microsoft-and",
    "source": "VentureBeat AI"
  },
  {
    "title": "Anthropic launches Cowork, a Claude Desktop agent that works in your files — no coding required",
    "summary": "Anthropic released Cowork on Monday, a new AI agent capability that extends the power of its wildly successful Claude Code tool to non-technical users — and according to company insiders, the team built the entire feature in approximately a week and a half, largely using Claude Code itself.The launch marks a major inflection point in the race to de",
    "url": "https://venturebeat.com/technology/anthropic-launches-cowork-a-claude-desktop-agent-that-works-in-your-files-no",
    "source": "VentureBeat AI"
  },
  {
    "title": "Nous Research's NousCoder-14B is an open-source coding model landing right in the Claude Code moment",
    "summary": "Nous Research, the open-source artificial intelligence startup backed by crypto venture firm Paradigm, released a new competitive programming model on Monday that it says matches or exceeds several larger proprietary systems — trained in just four days using 48 of Nvidia&#x27;s latest B200 graphics processors.The model, called NousCoder-14B, is ano",
    "url": "https://venturebeat.com/technology/nous-researchs-nouscoder-14b-is-an-open-source-coding-model-landing-right-in",
    "source": "VentureBeat AI"
  },
  {
    "title": "The creator of Claude Code just revealed his workflow, and developers are losing their minds",
    "summary": "When the creator of the world&#x27;s most advanced coding agent speaks, Silicon Valley doesn&#x27;t just listen — it takes notes.For the past week, the engineering community has been dissecting a thread on X from Boris Cherny, the creator and head of Claude Code at Anthropic. What began as a casual sharing of his personal terminal setup has spirale",
    "url": "https://venturebeat.com/technology/the-creator-of-claude-code-just-revealed-his-workflow-and-developers-are",
    "source": "VentureBeat AI"
  },
  {
    "title": "A New AI Math Startup Just Cracked 4 Previously Unsolved Problems",
    "summary": "Axiom says its AI found solutions to several long-standing math problems, a sign of the technology’s steadily advancing reasoning capabilities.",
    "url": "https://www.wired.com/story/a-new-ai-math-ai-startup-just-cracked-4-previously-unsolved-problems/",
    "source": "Wired AI"
  },
  {
    "title": "Mistral's New Ultra-Fast Translation Model Gives Big AI Labs a Run for Their Money",
    "summary": "“Too many GPUs makes you lazy,” says the French startup’s vice president of science operations, as the company carves out a different path than the major US AI companies.",
    "url": "https://www.wired.com/story/mistral-voxtral-real-time-ai-translation/",
    "source": "Wired AI"
  },
  {
    "title": "AI Bots Are Now a Signifigant Source of Web Traffic",
    "summary": "New data shows AI bots pushing deeper into the web, prompting publishers to roll out more aggressive defenses.",
    "url": "https://www.wired.com/story/ai-bots-are-now-a-signifigant-source-of-web-traffic/",
    "source": "Wired AI"
  },
  {
    "title": "HHS Is Making an AI Tool to Create Hypotheses About Vaccine Injury Claims",
    "summary": "Experts worry Robert F. Kennedy Jr.’s Health Department will use an internal AI tool to analyze vaccine injury claims in a way that furthers his anti-vaccine agenda.",
    "url": "https://www.wired.com/story/hhs-is-making-an-ai-tool-to-create-hypotheses-about-vaccine-injury-claims/",
    "source": "Wired AI"
  },
  {
    "title": "I Infiltrated Moltbook, the AI-Only Social Network Where Humans Aren’t Allowed",
    "summary": "I went undercover on Moltbook and loved role-playing as a conscious bot. But rather than a novel breakthrough, the AI-only site is a crude rehashing of sci-fi fantasies.",
    "url": "https://www.wired.com/story/i-infiltrated-moltbook-ai-only-social-network/",
    "source": "Wired AI"
  },
  {
    "title": "‘Fallout’ Producer Jonathan Nolan on AI: ‘We’re in Such a Frothy Moment’",
    "summary": "The Westworld showrunner thinks AI will be good for burgeoning filmmakers, but not for Hollywood blockbusters.",
    "url": "https://www.wired.com/story/the-big-interview-podcast-jonathan-nolan-fallout/",
    "source": "Wired AI"
  },
  {
    "title": "Elon Musk Is Rolling xAI Into SpaceX—Creating the World’s Most Valuable Private Company",
    "summary": "By fusing SpaceX and xAI—which acquired X last year—Elon Musk tightens his grip over technologies that shape national security, social media, and artificial intelligence.",
    "url": "https://www.wired.com/story/spacex-acquires-xai-elon-musk/",
    "source": "Wired AI"
  },
  {
    "title": "HHS Is Using AI Tools From Palantir to Target ‘DEI’ and ‘Gender Ideology’ in Grants",
    "summary": "Since March of 2025, the Department of Health and Human Services has been using tools from Palantir and the startup Credal AI to weed out perceived alignment with “DEI” or “gender ideology.”",
    "url": "https://www.wired.com/story/hhs-is-using-ai-tools-from-palantir-to-target-dei-and-gender-ideology-in-grants/",
    "source": "Wired AI"
  },
  {
    "title": "Jeffrey Epstein Had a ‘Personal Hacker,’ Informant Claims",
    "summary": "Plus: AI agent OpenClaw gives cybersecurity experts the willies, China executes 11 scam compound bosses, a $40 million crypto theft has an unexpected alleged culprit, and more.",
    "url": "https://www.wired.com/story/security-news-this-week-jeffrey-epstein-had-a-personal-hacker-informant-claims/",
    "source": "Wired AI"
  },
  {
    "title": "I Let Google’s ‘Auto Browse’ AI Agent Take Over Chrome. It Didn’t Quite Click",
    "summary": "Auto Browse can shop for clothes, plan a trip, and buy tickets for you. Or at least, that’s the idea.",
    "url": "https://www.wired.com/story/google-chrome-auto-browse-hands-on/",
    "source": "Wired AI"
  },
  {
    "title": "Scalable and Secure AI Inference in Healthcare: A Comparative Benchmarking of FastAPI and Triton Inference Server on Kubernetes",
    "summary": "arXiv:2602.00053v1 Announce Type: new Abstract: Efficient and scalable deployment of machine learning (ML) models is a prerequisite for modern production environments, particularly within regulated domains such as healthcare and pharmaceuticals. In these settings, systems must balance competing requirements, including minimizing inference latency f",
    "url": "https://arxiv.org/abs/2602.00053",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning to Price: Interpretable Attribute-Level Models for Dynamic Markets",
    "summary": "arXiv:2602.00188v1 Announce Type: new Abstract: Dynamic pricing in high-dimensional markets poses fundamental challenges of scalability, uncertainty, and interpretability. Existing low-rank bandit formulations learn efficiently but rely on latent features that obscure how individual product attributes influence price. We address this by introducing",
    "url": "https://arxiv.org/abs/2602.00188",
    "source": "Arxiv AI"
  },
  {
    "title": "From Gameplay Traces to Game Mechanics: Causal Induction with Large Language Models",
    "summary": "arXiv:2602.00190v1 Announce Type: new Abstract: Deep learning agents can achieve high performance in complex game domains without often understanding the underlying causal game mechanics. To address this, we investigate Causal Induction: the ability to infer governing laws from observational data, by tasking Large Language Models (LLMs) with revers",
    "url": "https://arxiv.org/abs/2602.00190",
    "source": "Arxiv AI"
  },
  {
    "title": "Complete Identification of Deep ReLU Neural Networks by Many-Valued Logic",
    "summary": "arXiv:2602.00266v1 Announce Type: new Abstract: Deep ReLU neural networks admit nontrivial functional symmetries: vastly different architectures and parameters (weights and biases) can realize the same function. We address the complete identification problem -- given a function f, deriving the architecture and parameters of all feedforward ReLU net",
    "url": "https://arxiv.org/abs/2602.00266",
    "source": "Arxiv AI"
  },
  {
    "title": "Localizing and Correcting Errors for LLM-based Planners",
    "summary": "arXiv:2602.00276v1 Announce Type: new Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities on math and coding, but frequently fail on symbolic classical planning tasks. Our studies, as well as prior work, show that LLM-generated plans routinely violate domain constraints given in their instructions (e.g., walking t",
    "url": "https://arxiv.org/abs/2602.00276",
    "source": "Arxiv AI"
  },
  {
    "title": "Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning",
    "summary": "arXiv:2602.00298v1 Announce Type: new Abstract: Emergent misalignment poses risks to AI safety as language models are increasingly used for autonomous tasks. In this paper, we present a population of large language models (LLMs) fine-tuned on insecure datasets spanning 11 diverse domains, evaluating them both with and without backdoor triggers on a",
    "url": "https://arxiv.org/abs/2602.00298",
    "source": "Arxiv AI"
  },
  {
    "title": "Autonomous Data Processing using Meta-Agents",
    "summary": "arXiv:2602.00307v1 Announce Type: new Abstract: Traditional data processing pipelines are typically static and handcrafted for specific tasks, limiting their adaptability to evolving requirements. While general-purpose agents and coding assistants can generate code for well-understood data pipelines, they lack the ability to autonomously monitor, m",
    "url": "https://arxiv.org/abs/2602.00307",
    "source": "Arxiv AI"
  },
  {
    "title": "SayNext-Bench: Why Do LLMs Struggle with Next-Utterance Prediction?",
    "summary": "arXiv:2602.00327v1 Announce Type: new Abstract: We explore the use of large language models (LLMs) for next-utterance prediction in human dialogue. Despite recent advances in LLMs demonstrating their ability to engage in natural conversations with users, we show that even leading models surprisingly struggle to predict a human speaker's next uttera",
    "url": "https://arxiv.org/abs/2602.00327",
    "source": "Arxiv AI"
  },
  {
    "title": "MHDash: An Online Platform for Benchmarking Mental Health-Aware AI Assistants",
    "summary": "arXiv:2602.00353v1 Announce Type: new Abstract: Large language models (LLMs) are increasingly applied in mental health support systems, where reliable recognition of high-risk states such as suicidal ideation and self-harm is safety-critical. However, existing evaluations primarily rely on aggregate performance metrics, which often obscure risk-spe",
    "url": "https://arxiv.org/abs/2602.00353",
    "source": "Arxiv AI"
  },
  {
    "title": "Position: Agentic Evolution is the Path to Evolving LLMs",
    "summary": "arXiv:2602.00359v1 Announce Type: new Abstract: As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does",
    "url": "https://arxiv.org/abs/2602.00359",
    "source": "Arxiv AI"
  },
  {
    "title": "POET: Protocol Optimization via Eligibility Tuning",
    "summary": "arXiv:2602.00370v1 Announce Type: new Abstract: Eligibility criteria (EC) are essential for clinical trial design, yet drafting them remains a time-intensive and cognitively demanding task for clinicians. Existing automated approaches often fall at two extremes either requiring highly structured inputs, such as predefined entities to generate speci",
    "url": "https://arxiv.org/abs/2602.00370",
    "source": "Arxiv AI"
  },
  {
    "title": "KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning",
    "summary": "arXiv:2602.00400v1 Announce Type: new Abstract: Reinforcement learning (RL) has emerged as a promising paradigm for inducing explicit reasoning behaviors in large language and vision-language models. However, reasoning-oriented RL post-training remains fundamentally challenging due to sparse trajectory-level rewards, leading to ambiguous credit ass",
    "url": "https://arxiv.org/abs/2602.00400",
    "source": "Arxiv AI"
  },
  {
    "title": "RobustDebias: Debiasing Language Models using Distributionally Robust Optimization",
    "summary": "arXiv:2602.00405v1 Announce Type: new Abstract: Pretrained language models have been shown to exhibit biases and social stereotypes. Prior work on debiasing these models has largely focused on modifying embedding spaces during pretraining, which is not scalable for large models. Fine-tuning pretrained models on task-specific datasets can both degra",
    "url": "https://arxiv.org/abs/2602.00405",
    "source": "Arxiv AI"
  },
  {
    "title": "PolarMem: A Training-Free Polarized Latent Graph Memory for Verifiable Multimodal Agents",
    "summary": "arXiv:2602.00415v1 Announce Type: new Abstract: As multimodal agents evolve from passive observers to long-horizon decision-makers, they require memory systems that provide not just information availability but logical verifiability. A fundamental limitation of current architectures is the epistemic asymmetry inherent in probabilistic vision-langua",
    "url": "https://arxiv.org/abs/2602.00415",
    "source": "Arxiv AI"
  },
  {
    "title": "Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks",
    "summary": "arXiv:2602.00449v1 Announce Type: new Abstract: Latent Chain-of-Thought (Latent-CoT) aims to enable step-by-step computation without emitting long rationales, yet its mechanisms remain unclear. We study CODI, a continuous-thought teacher-student distillation model, on strictly sequential polynomial-iteration tasks. Using logit-lens decoding, linear",
    "url": "https://arxiv.org/abs/2602.00449",
    "source": "Arxiv AI"
  },
  {
    "title": "Cross-Modal Memory Compression for Efficient Multi-Agent Debate",
    "summary": "arXiv:2602.00454v1 Announce Type: new Abstract: Multi-agent debate can improve reasoning quality and reduce hallucinations, but it incurs rapidly growing context as debate rounds and agent count increase. Retaining full textual histories leads to token usage that can exceed context limits and often requires repeated summarization, adding overhead a",
    "url": "https://arxiv.org/abs/2602.00454",
    "source": "Arxiv AI"
  },
  {
    "title": "Benchmarking Agents in Insurance Underwriting Environments",
    "summary": "arXiv:2602.00456v1 Announce Type: new Abstract: As AI agents integrate into enterprise applications, their evaluation demands benchmarks that reflect the complexity of real-world operations. Instead, existing benchmarks overemphasize open-domains such as code, use narrow accuracy metrics, and lack authentic complexity. We present UNDERWRITE, an exp",
    "url": "https://arxiv.org/abs/2602.00456",
    "source": "Arxiv AI"
  },
  {
    "title": "Dual Latent Memory for Visual Multi-agent System",
    "summary": "arXiv:2602.00471v1 Announce Type: new Abstract: While Visual Multi-Agent Systems (VMAS) promise to enhance comprehensive abilities through inter-agent collaboration, empirical evidence reveals a counter-intuitive \"scaling wall\": increasing agent turns often degrades performance while exponentially inflating token costs. We attribute this failure to",
    "url": "https://arxiv.org/abs/2602.00471",
    "source": "Arxiv AI"
  },
  {
    "title": "Replacing Parameters with Preferences: Federated Alignment of Heterogeneous Vision-Language Models",
    "summary": "arXiv:2602.00485v1 Announce Type: new Abstract: VLMs have broad potential in privacy-sensitive domains such as healthcare and finance, yet strict data-sharing constraints render centralized training infeasible. FL mitigates this issue by enabling decentralized training, but practical deployments face challenges due to client heterogeneity in comput",
    "url": "https://arxiv.org/abs/2602.00485",
    "source": "Arxiv AI"
  },
  {
    "title": "PCBSchemaGen: Constraint-Guided Schematic Design via LLM for Printed Circuit Boards (PCB)",
    "summary": "arXiv:2602.00510v1 Announce Type: new Abstract: Printed Circuit Board (PCB) schematic design plays an essential role in all areas of electronic industries. Unlike prior works that focus on digital or analog circuits alone, PCB design must handle heterogeneous digital, analog, and power signals while adhering to real-world IC packages and pin constr",
    "url": "https://arxiv.org/abs/2602.00510",
    "source": "Arxiv AI"
  },
  {
    "title": "Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory",
    "summary": "arXiv:2602.00521v1 Announce Type: new Abstract: While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a",
    "url": "https://arxiv.org/abs/2602.00521",
    "source": "Arxiv AI"
  },
  {
    "title": "How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use",
    "summary": "arXiv:2602.00528v1 Announce Type: new Abstract: As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a sys",
    "url": "https://arxiv.org/abs/2602.00528",
    "source": "Arxiv AI"
  },
  {
    "title": "Uncovering Latent Communication Patterns in Brain Networks via Adaptive Flow Routing",
    "summary": "arXiv:2602.00561v1 Announce Type: new Abstract: Unraveling how macroscopic cognitive phenotypes emerge from microscopic neuronal connectivity remains one of the core pursuits of neuroscience. To this end, researchers typically leverage multi-modal information from structural connectivity (SC) and functional connectivity (FC) to complete downstream ",
    "url": "https://arxiv.org/abs/2602.00561",
    "source": "Arxiv AI"
  },
  {
    "title": "Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural Mathematical Reasoning in LLMs",
    "summary": "arXiv:2602.00564v1 Announce Type: new Abstract: Recent large language models (LLMs) achieve near-saturation accuracy on many established mathematical reasoning benchmarks, raising concerns about their ability to diagnose genuine reasoning competence. This saturation largely stems from the dominance of template-based computation and shallow arithmet",
    "url": "https://arxiv.org/abs/2602.00564",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings",
    "summary": "arXiv:2602.00574v1 Announce Type: new Abstract: We study how to extend chain-of-thought (CoT) beyond language to better handle multimodal reasoning. While CoT helps LLMs and VLMs articulate intermediate steps, its text-only form often fails on vision-intensive problems where key intermediate states are inherently visual. We introduce modal-mixed Co",
    "url": "https://arxiv.org/abs/2602.00574",
    "source": "Arxiv AI"
  },
  {
    "title": "Small Shifts, Large Gains: Unlocking Traditional TSP Heuristic Guided-Sampling via Unsupervised Neural Instance Modification",
    "summary": "arXiv:2602.00580v1 Announce Type: new Abstract: The Traveling Salesman Problem (TSP) is one of the most representative NP-hard problems in route planning and a long-standing benchmark in combinatorial optimization. Traditional heuristic tour constructors, such as Farthest or Nearest Insertion, are computationally efficient and highly practical, but",
    "url": "https://arxiv.org/abs/2602.00580",
    "source": "Arxiv AI"
  },
  {
    "title": "Exploring Information Seeking Agent Consolidation",
    "summary": "arXiv:2602.00585v1 Announce Type: new Abstract: Information-seeking agents have emerged as a powerful paradigm for solving knowledge-intensive tasks. Existing information-seeking agents are typically specialized for open web, documents, or local knowledge bases, which constrains scalability and cross-domain generalization. In this work, we investig",
    "url": "https://arxiv.org/abs/2602.00585",
    "source": "Arxiv AI"
  },
  {
    "title": "DockSmith: Scaling Reliable Coding Environments via an Agentic Docker Builder",
    "summary": "arXiv:2602.00592v1 Announce Type: new Abstract: Reliable Docker-based environment construction is a dominant bottleneck for scaling execution-grounded training and evaluation of software engineering agents. We introduce DockSmith, a specialized agentic Docker builder designed to address this challenge. DockSmith treats environment construction not ",
    "url": "https://arxiv.org/abs/2602.00592",
    "source": "Arxiv AI"
  },
  {
    "title": "Scalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design",
    "summary": "arXiv:2602.00608v1 Announce Type: new Abstract: Real-time generative game engines represent a paradigm shift in interactive simulation, promising to replace traditional graphics pipelines with neural world models. However, existing approaches are fundamentally constrained by the ``Memory Wall,'' restricting practical deployments to low resolutions ",
    "url": "https://arxiv.org/abs/2602.00608",
    "source": "Arxiv AI"
  },
  {
    "title": "Structured Self-Consistency:A Multi-Task Evaluation of LLMs on VirtualHome",
    "summary": "arXiv:2602.00611v2 Announce Type: new Abstract: Embodied AI requires agents to understand goals, plan actions, and execute tasks in simulated environments. We present a comprehensive evaluation of Large Language Models (LLMs) on the VirtualHome benchmark using the Embodied Agent Interface (EAI) framework. We compare two representative 7B-parameter ",
    "url": "https://arxiv.org/abs/2602.00611",
    "source": "Arxiv AI"
  },
  {
    "title": "Inference-Only Prompt Projection for Safe Text-to-Image Generation with TV Guarantees",
    "summary": "arXiv:2602.00616v1 Announce Type: new Abstract: Text-to-Image (T2I) diffusion models enable high-quality open-ended synthesis, but their real-world deployment demands safeguards that suppress unsafe generations without degrading benign prompt-image alignment. We formalize this tension through a total variation (TV) lens: once the reference conditio",
    "url": "https://arxiv.org/abs/2602.00616",
    "source": "Arxiv AI"
  },
  {
    "title": "Predictive Maintenance for Ultrafiltration Membranes Using Explainable Similarity-Based Prognostics",
    "summary": "arXiv:2602.00659v1 Announce Type: new Abstract: In reverse osmosis desalination, ultrafiltration (UF) membranes degrade due to fouling, leading to performance loss and costly downtime. Most plants rely on scheduled preventive maintenance, since existing predictive maintenance models, often based on opaque machine learning methods, lack interpretabi",
    "url": "https://arxiv.org/abs/2602.00659",
    "source": "Arxiv AI"
  },
  {
    "title": "SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent",
    "summary": "arXiv:2602.00663v1 Announce Type: new Abstract: Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, such",
    "url": "https://arxiv.org/abs/2602.00663",
    "source": "Arxiv AI"
  },
  {
    "title": "OpenGuanDan: A Large-Scale Imperfect Information Game Benchmark",
    "summary": "arXiv:2602.00676v1 Announce Type: new Abstract: The advancement of data-driven artificial intelligence (AI), particularly machine learning, heavily depends on large-scale benchmarks. Despite remarkable progress across domains ranging from pattern recognition to intelligent decision-making in recent decades, exemplified by breakthroughs in board gam",
    "url": "https://arxiv.org/abs/2602.00676",
    "source": "Arxiv AI"
  },
  {
    "title": "HumanStudy-Bench: Towards AI Agent Design for Participant Simulation",
    "summary": "arXiv:2602.00685v1 Announce Type: new Abstract: Large language models (LLMs) are increasingly used as simulated participants in social science experiments, but their behavior is often unstable and highly sensitive to design choices. Prior evaluations frequently conflate base-model capabilities with experimental instantiation, obscuring whether outc",
    "url": "https://arxiv.org/abs/2602.00685",
    "source": "Arxiv AI"
  },
  {
    "title": "From Prompt to Graph: Comparing LLM-Based Information Extraction Strategies in Domain-Specific Ontology Development",
    "summary": "arXiv:2602.00699v1 Announce Type: new Abstract: Ontologies are essential for structuring domain knowledge, improving accessibility, sharing, and reuse. However, traditional ontology construction relies on manual annotation and conventional natural language processing (NLP) techniques, making the process labour-intensive and costly, especially in sp",
    "url": "https://arxiv.org/abs/2602.00699",
    "source": "Arxiv AI"
  },
  {
    "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
    "summary": "arXiv:2602.00707v1 Announce Type: new Abstract: The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training p",
    "url": "https://arxiv.org/abs/2602.00707",
    "source": "Arxiv AI"
  },
  {
    "title": "Physics-informed Diffusion Generation for Geomagnetic Map Interpolation",
    "summary": "arXiv:2602.00709v1 Announce Type: new Abstract: Geomagnetic map interpolation aims to infer unobserved geomagnetic data at spatial points, yielding critical applications in navigation and resource exploration. However, existing methods for scattered data interpolation are not specifically designed for geomagnetic maps, which inevitably leads to sub",
    "url": "https://arxiv.org/abs/2602.00709",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning More from Less: Unlocking Internal Representations for Benchmark Compression",
    "summary": "arXiv:2602.00710v2 Announce Type: new Abstract: The prohibitive cost of evaluating Large Language Models (LLMs) necessitates efficient alternatives to full-scale benchmarking. Prevalent approaches address this by identifying a small coreset of items to approximate full-benchmark performance. However, existing methods must estimate a reliable item p",
    "url": "https://arxiv.org/abs/2602.00710",
    "source": "Arxiv AI"
  },
  {
    "title": "Neuro-symbolic AI for Predictive Maintenance (PdM) -- review and recommendations",
    "summary": "arXiv:2602.00731v1 Announce Type: new Abstract: In this document we perform a systematic review the State-of-the-art in Predictive Maintenance (PdM) over the last five years in industrial settings such as commercial buildings, pharmaceutical facilities, or semi-conductor manufacturing. In general, data-driven methods such as those based on deep lea",
    "url": "https://arxiv.org/abs/2602.00731",
    "source": "Arxiv AI"
  },
  {
    "title": "Engineering AI Agents for Clinical Workflows: A Case Study in Architecture,MLOps, and Governance",
    "summary": "arXiv:2602.00751v1 Announce Type: new Abstract: The integration of Artificial Intelligence (AI) into clinical settings presents a software engineering challenge, demanding a shift from isolated models to robust, governable, and reliable systems. However, brittle, prototype-derived architectures often plague industrial applications and a lack of sys",
    "url": "https://arxiv.org/abs/2602.00751",
    "source": "Arxiv AI"
  },
  {
    "title": "Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models",
    "summary": "arXiv:2602.00780v1 Announce Type: new Abstract: While Vision-Language-Action (VLA) models hold promise in embodied intelligence, their large parameter counts lead to substantial inference latency that hinders real-time manipulation, motivating parameter sparsification. However, as the environment evolves during VLA execution, the optimal sparsity p",
    "url": "https://arxiv.org/abs/2602.00780",
    "source": "Arxiv AI"
  },
  {
    "title": "World Models as an Intermediary between Agents and the Real World",
    "summary": "arXiv:2602.00785v1 Announce Type: new Abstract: Large language model (LLM) agents trained using reinforcement learning has achieved superhuman performance in low-cost environments like games, mathematics, and coding. However, these successes have not translated to complex domains where the cost of interaction is high, such as the physical cost of r",
    "url": "https://arxiv.org/abs/2602.00785",
    "source": "Arxiv AI"
  },
  {
    "title": "MissMAC-Bench: Building Solid Benchmark for Missing Modality Issue in Robust Multimodal Affective Computing",
    "summary": "arXiv:2602.00811v1 Announce Type: new Abstract: As a knowledge discovery task over heterogeneous data sources, current Multimodal Affective Computing (MAC) heavily rely on the completeness of multiple modalities to accurately understand human's affective state. However, in real-world scenarios, the availability of modality data is often dynamic and",
    "url": "https://arxiv.org/abs/2602.00811",
    "source": "Arxiv AI"
  },
  {
    "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
    "summary": "arXiv:2602.00815v1 Announce Type: new Abstract: Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-inte",
    "url": "https://arxiv.org/abs/2602.00815",
    "source": "Arxiv AI"
  },
  {
    "title": "Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information Gain Reward",
    "summary": "arXiv:2602.00845v1 Announce Type: new Abstract: Agentic reasoning enables large reasoning models (LRMs) to dynamically acquire external knowledge, but yet optimizing the retrieval process remains challenging due to the lack of dense, principled reward signals. In this paper, we introduce InfoReasoner, a unified framework that incentivizes effective",
    "url": "https://arxiv.org/abs/2602.00845",
    "source": "Arxiv AI"
  },
  {
    "title": "Persuasion Propagation in LLM Agents",
    "summary": "arXiv:2602.00851v1 Announce Type: new Abstract: Modern AI agents increasingly combine conversational interaction with autonomous task execution, such as coding and web research, raising a natural question: what happens when an agent engaged in long-horizon tasks is subjected to user persuasion? We study how belief-level intervention can influence d",
    "url": "https://arxiv.org/abs/2602.00851",
    "source": "Arxiv AI"
  },
  {
    "title": "Position: Human-Centric AI Requires a Minimum Viable Level of Human Understanding",
    "summary": "arXiv:2602.00854v1 Announce Type: new Abstract: AI systems increasingly produce fluent, correct, end-to-end outcomes. Over time, this erodes users' ability to explain, verify, or intervene. We define this divergence as the Capability-Comprehension Gap: a decoupling where assisted performance improves while users' internal models deteriorate. This p",
    "url": "https://arxiv.org/abs/2602.00854",
    "source": "Arxiv AI"
  },
  {
    "title": "Multi-Head Attention Is a Multi-Player Game",
    "summary": "arXiv:2602.00861v1 Announce Type: new Abstract: Modern transformer attention is internally multi-agent -- heads compete and coordinate -- yet we train it as if it were a monolithic optimizer. We formalize this gap: cross-entropy training induces an implicit potential game among heads, and gradient descent converges to Nash equilibria with potential",
    "url": "https://arxiv.org/abs/2602.00861",
    "source": "Arxiv AI"
  },
  {
    "title": "Foundation CAN LM: A Pretrained Language Model For Automotive CAN Data",
    "summary": "arXiv:2602.00866v1 Announce Type: new Abstract: The Controller Area Network (CAN) bus provides a rich source of vehicular signals increasingly leveraged for applications in automotive and auto insurance domains, including collision detection, predictive maintenance, and driver risk modeling. Despite this potential, existing pipelines largely train ",
    "url": "https://arxiv.org/abs/2602.00866",
    "source": "Arxiv AI"
  },
  {
    "title": "Beyond Output Critique: Self-Correction via Task Distillation",
    "summary": "arXiv:2602.00871v1 Announce Type: new Abstract: Large language models (LLMs) have shown promising self-correction abilities, where iterative refinement improves the quality of generated responses. However, most existing approaches operate at the level of output critique, patching surface errors while often failing to correct deeper reasoning flaws.",
    "url": "https://arxiv.org/abs/2602.00871",
    "source": "Arxiv AI"
  },
  {
    "title": "Synapse Compendium Aware Federated Knowledge Exchange for Tool Routed LLMs",
    "summary": "arXiv:2602.00911v1 Announce Type: new Abstract: Collaborative learning among LLM-based agents under federated learning faces challenges, including communication costs, heterogeneity in data, and tool-usage, limiting their effectiveness. We introduce Synapse, a framework that trains a shared global knowledge model of tool-usage behavior. Client agen",
    "url": "https://arxiv.org/abs/2602.00911",
    "source": "Arxiv AI"
  },
  {
    "title": "Supervised sparse auto-encoders as unconstrained feature models for semantic composition",
    "summary": "arXiv:2602.00924v1 Announce Type: new Abstract: Sparse auto-encoders (SAEs) have re-emerged as a prominent method for mechanistic interpretability, yet they face two significant challenges: the non-smoothness of the $L_1$ penalty, which hinders reconstruction and scalability, and a lack of alignment between learned features and human semantics. In ",
    "url": "https://arxiv.org/abs/2602.00924",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents",
    "summary": "arXiv:2602.00929v1 Announce Type: new Abstract: Humans learn abstractions and use them to plan efficiently to quickly generalize across tasks -- an ability that remains challenging for state-of-the-art large language model (LLM) agents and deep reinforcement learning (RL) systems. Inspired by the cognitive science of how people form abstractions an",
    "url": "https://arxiv.org/abs/2602.00929",
    "source": "Arxiv AI"
  },
  {
    "title": "The Keyhole Effect: Why Chat Interfaces Fail at Data Analysis",
    "summary": "arXiv:2602.00947v1 Announce Type: new Abstract: Chat has become the default interface for AI-assisted data analysis. For multi-step, state-dependent analytical tasks, this is a mistake. Building on Woods (1984) Keyhole Effect, the cognitive cost of viewing large information spaces through narrow viewports, I show that chat interfaces systematically",
    "url": "https://arxiv.org/abs/2602.00947",
    "source": "Arxiv AI"
  },
  {
    "title": "MindGuard: Guardrail Classifiers for Multi-Turn Mental Health Support",
    "summary": "arXiv:2602.00950v1 Announce Type: new Abstract: Large language models are increasingly used for mental health support, yet their conversational coherence alone does not ensure clinical appropriateness. Existing general-purpose safeguards often fail to distinguish between therapeutic disclosures and genuine clinical crises, leading to safety failure",
    "url": "https://arxiv.org/abs/2602.00950",
    "source": "Arxiv AI"
  },
  {
    "title": "R-HTN: Rebellious Online HTN Planning for Safety and Game AI",
    "summary": "arXiv:2602.00951v1 Announce Type: new Abstract: We introduce online Hierarchical Task Network (HTN) agents whose behaviors are governed by a set of built-in directives \\D. Like other agents that are capable of rebellion (i.e., {\\it intelligent disobedience}), our agents will, under some conditions, not perform a user-assigned task and instead act i",
    "url": "https://arxiv.org/abs/2602.00951",
    "source": "Arxiv AI"
  },
  {
    "title": "Small-Margin Preferences Still Matter-If You Train Them Right",
    "summary": "arXiv:2602.00954v1 Announce Type: new Abstract: Preference optimization methods such as DPO align large language models (LLMs) using paired comparisons, but their effectiveness can be highly sensitive to the quality and difficulty of preference pairs. A common heuristic treats small-margin (ambiguous) pairs as noisy and filters them out. In this pa",
    "url": "https://arxiv.org/abs/2602.00954",
    "source": "Arxiv AI"
  },
  {
    "title": "Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning",
    "summary": "arXiv:2602.00994v1 Announce Type: new Abstract: Agentic Reinforcement Learning (ARL) focuses on training large language models (LLMs) to interleave reasoning with external tool execution to solve complex tasks. Most existing ARL methods train a single shared model parameters to support both reasoning and tool use behaviors, implicitly assuming that",
    "url": "https://arxiv.org/abs/2602.00994",
    "source": "Arxiv AI"
  },
  {
    "title": "Error Taxonomy-Guided Prompt Optimization",
    "summary": "arXiv:2602.00997v1 Announce Type: new Abstract: Automatic Prompt Optimization (APO) is a powerful approach for extracting performance from large language models without modifying their weights. Many existing methods rely on trial-and-error, testing different prompts or in-context examples until a good configuration emerges, often consuming substant",
    "url": "https://arxiv.org/abs/2602.00997",
    "source": "Arxiv AI"
  },
  {
    "title": "How RLHF Amplifies Sycophancy",
    "summary": "arXiv:2602.01002v1 Announce Type: new Abstract: Large language models often exhibit increased sycophantic behavior after preference-based post-training, showing a stronger tendency to affirm a user's stated or implied belief even when this conflicts with factual accuracy or sound judgment. We present a formal analysis of how alignment from human fe",
    "url": "https://arxiv.org/abs/2602.01002",
    "source": "Arxiv AI"
  },
  {
    "title": "HalluHard: A Hard Multi-Turn Hallucination Benchmark",
    "summary": "arXiv:2602.01031v1 Announce Type: new Abstract: Large language models (LLMs) still produce plausible-sounding but ungrounded factual claims, a problem that worsens in multi-turn dialogue as context grows and early errors cascade. We introduce $\\textbf{HalluHard}$, a challenging multi-turn hallucination benchmark with 950 seed questions spanning fou",
    "url": "https://arxiv.org/abs/2602.01031",
    "source": "Arxiv AI"
  },
  {
    "title": "Discovering Process-Outcome Credit in Multi-Step LLM Reasoning",
    "summary": "arXiv:2602.01034v1 Announce Type: new Abstract: Reinforcement Learning (RL) serves as a potent paradigm for enhancing reasoning capabilities in Large Language Models (LLMs), yet standard outcome-based approaches often suffer from reward sparsity and inefficient credit assignment. In this paper, we propose a novel framework designed to provide conti",
    "url": "https://arxiv.org/abs/2602.01034",
    "source": "Arxiv AI"
  },
  {
    "title": "SetPO: Set-Level Policy Optimization for Diversity-Preserving LLM Reasoning",
    "summary": "arXiv:2602.01062v1 Announce Type: new Abstract: Reinforcement learning with verifiable rewards has shown notable effectiveness in enhancing large language models (LLMs) reasoning performance, especially in mathematics tasks. However, such improvements often come with reduced outcome diversity, where the model concentrates probability mass on a narr",
    "url": "https://arxiv.org/abs/2602.01062",
    "source": "Arxiv AI"
  },
  {
    "title": "ConvexBench: Can LLMs Recognize Convex Functions?",
    "summary": "arXiv:2602.01075v1 Announce Type: new Abstract: Convex analysis is a modern branch of mathematics with many applications. As Large Language Models (LLMs) start to automate research-level math and sciences, it is important for LLMs to demonstrate the ability to understand and reason with convexity. We introduce \\cb, a scalable and mechanically verif",
    "url": "https://arxiv.org/abs/2602.01075",
    "source": "Arxiv AI"
  },
  {
    "title": "AutoHealth: An Uncertainty-Aware Multi-Agent System for Autonomous Health Data Modeling",
    "summary": "arXiv:2602.01078v1 Announce Type: new Abstract: LLM-based agents have demonstrated strong potential for autonomous machine learning, yet their applicability to health data remains limited. Existing systems often struggle to generalize across heterogeneous health data modalities, rely heavily on predefined solution templates with insufficient adapta",
    "url": "https://arxiv.org/abs/2602.01078",
    "source": "Arxiv AI"
  },
  {
    "title": "EvoOpt-LLM: Evolving industrial optimization models with large language models",
    "summary": "arXiv:2602.01082v1 Announce Type: new Abstract: Optimization modeling via mixed-integer linear programming (MILP) is fundamental to industrial planning and scheduling, yet translating natural-language requirements into solver-executable models and maintaining them under evolving business rules remains highly expertise-intensive. While large languag",
    "url": "https://arxiv.org/abs/2602.01082",
    "source": "Arxiv AI"
  },
  {
    "title": "MedBeads: An Agent-Native, Immutable Data Substrate for Trustworthy Medical AI",
    "summary": "arXiv:2602.01086v1 Announce Type: new Abstract: Background: As of 2026, Large Language Models (LLMs) demonstrate expert-level medical knowledge. However, deploying them as autonomous \"Clinical Agents\" remains limited. Current Electronic Medical Records (EMRs) and standards like FHIR are designed for human review, creating a \"Context Mismatch\": AI a",
    "url": "https://arxiv.org/abs/2602.01086",
    "source": "Arxiv AI"
  },
  {
    "title": "Hard Constraints Meet Soft Generation: Guaranteed Feasibility for LLM-based Combinatorial Optimization",
    "summary": "arXiv:2602.01090v1 Announce Type: new Abstract: Large language models (LLMs) have emerged as promising general-purpose solvers for combinatorial optimization (CO), yet they fundamentally lack mechanisms to guarantee solution feasibility which is critical for real-world deployment. In this work, we introduce FALCON, a framework that ensures 100\\% fe",
    "url": "https://arxiv.org/abs/2602.01090",
    "source": "Arxiv AI"
  },
  {
    "title": "Probing RLVR training instability through the lens of objective-level hacking",
    "summary": "arXiv:2602.01103v1 Announce Type: new Abstract: Prolonged reinforcement learning with verifiable rewards (RLVR) has been shown to drive continuous improvements in the reasoning capabilities of large language models, but the training is often prone to instabilities, especially in Mixture-of-Experts (MoE) architectures. Training instability severely ",
    "url": "https://arxiv.org/abs/2602.01103",
    "source": "Arxiv AI"
  },
  {
    "title": "Transforming Vehicle Diagnostics: A Multimodal Approach to Error Patterns Prediction",
    "summary": "arXiv:2602.01109v1 Announce Type: new Abstract: Accurately diagnosing and predicting vehicle malfunctions is crucial for maintenance and safety in the automotive industry. While modern diagnostic systems primarily rely on sequences of vehicular Diagnostic Trouble Codes (DTCs) registered in On-Board Diagnostic (OBD) systems, they often overlook valu",
    "url": "https://arxiv.org/abs/2602.01109",
    "source": "Arxiv AI"
  },
  {
    "title": "Lyapunov Stability-Aware Stackelberg Game for Low-Altitude Economy: A Control-Oriented Pruning-Based DRL Approach",
    "summary": "arXiv:2602.01131v1 Announce Type: new Abstract: With the rapid expansion of the low-altitude economy, Unmanned Aerial Vehicles (UAVs) serve as pivotal aerial base stations supporting diverse services from users, ranging from latency-sensitive critical missions to bandwidth-intensive data streaming. However, the efficacy of such heterogeneous networ",
    "url": "https://arxiv.org/abs/2602.01131",
    "source": "Arxiv AI"
  },
  {
    "title": "PersistBench: When Should Long-Term Memories Be Forgotten by LLMs?",
    "summary": "arXiv:2602.01146v1 Announce Type: new Abstract: Conversational assistants are increasingly integrating long-term memory with large language models (LLMs). This persistence of memories, e.g., the user is vegetarian, can enhance personalization in future conversations. However, the same persistence can also introduce safety risks that have been large",
    "url": "https://arxiv.org/abs/2602.01146",
    "source": "Arxiv AI"
  },
  {
    "title": "Capabilities and Fundamental Limits of Latent Chain-of-Thought",
    "summary": "arXiv:2602.01148v1 Announce Type: new Abstract: Latent Chain-of-Thought (Latent CoT) models promise efficient reasoning via continuous representations, yet exhibit puzzling performance inconsistencies: excelling at exploration (ProsQA: 97.0%) but failing at computation (GSM8K: 34.1%). We reveal that this trade-off is governed by decisional certaint",
    "url": "https://arxiv.org/abs/2602.01148",
    "source": "Arxiv AI"
  },
  {
    "title": "Multi-Agent Causal Reasoning System for Error Pattern Rule Automation in Vehicles",
    "summary": "arXiv:2602.01155v2 Announce Type: new Abstract: Modern vehicles generate thousands of different discrete events known as Diagnostic Trouble Codes (DTCs). Automotive manufacturers use Boolean combinations of these codes, called error patterns (EPs), to characterize system faults and ensure vehicle safety. Yet, EP rules are still manually handcrafted",
    "url": "https://arxiv.org/abs/2602.01155",
    "source": "Arxiv AI"
  },
  {
    "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
    "summary": "arXiv:2602.01167v1 Announce Type: new Abstract: Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on ",
    "url": "https://arxiv.org/abs/2602.01167",
    "source": "Arxiv AI"
  },
  {
    "title": "ASP-Bench: From Natural Language to Logic Programs",
    "summary": "arXiv:2602.01171v1 Announce Type: new Abstract: Automating the translation of natural-language specifications into logic programs is a challenging task that affects neurosymbolic engineering. We present ASP-Bench, a benchmark comprising 128 natural language problem instances, 64 base problems with easy and hard variants. It evaluates systems that t",
    "url": "https://arxiv.org/abs/2602.01171",
    "source": "Arxiv AI"
  },
  {
    "title": "A State-Transition Framework for Efficient LLM Reasoning",
    "summary": "arXiv:2602.01198v1 Announce Type: new Abstract: While Long Chain-of-Thought (CoT) reasoning significantly improves Large Language Models (LLMs) performance on complex reasoning tasks, the substantial computational and memory costs of generating long CoT sequences limit their efficiency and practicality. Existing studies usually enhance the reasonin",
    "url": "https://arxiv.org/abs/2602.01198",
    "source": "Arxiv AI"
  },
  {
    "title": "Workflow-R1: Group Sub-sequence Policy Optimization for Multi-turn Workflow Construction",
    "summary": "arXiv:2602.01202v1 Announce Type: new Abstract: The rapid evolution of agentic workflows has demonstrated strong performance of LLM-based agents in addressing complex reasoning tasks. However, existing workflow optimization methods typically formulate workflow synthesis as a static, one-shot code-centric generation problem. This paradigm imposes ex",
    "url": "https://arxiv.org/abs/2602.01202",
    "source": "Arxiv AI"
  },
  {
    "title": "Addressing Explainability of Generative AI using SMILE (Statistical Model-agnostic Interpretability with Local Explanations)",
    "summary": "arXiv:2602.01206v1 Announce Type: new Abstract: The rapid advancement of generative artificial intelligence has enabled models capable of producing complex textual and visual outputs; however, their decision-making processes remain largely opaque, limiting trust and accountability in high-stakes applications. This thesis introduces gSMILE, a unifie",
    "url": "https://arxiv.org/abs/2602.01206",
    "source": "Arxiv AI"
  },
  {
    "title": "Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models",
    "summary": "arXiv:2602.01207v1 Announce Type: new Abstract: Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unst",
    "url": "https://arxiv.org/abs/2602.01207",
    "source": "Arxiv AI"
  },
  {
    "title": "FutureMind: Equipping Small Language Models with Strategic Thinking-Pattern Priors via Adaptive Knowledge Distillation",
    "summary": "arXiv:2602.01222v1 Announce Type: new Abstract: Small Language Models (SLMs) are attractive for cost-sensitive and resource-limited settings due to their efficient, low-latency inference. However, they often struggle with complex, knowledge-intensive tasks that require structured reasoning and effective retrieval. To address these limitations, we p",
    "url": "https://arxiv.org/abs/2602.01222",
    "source": "Arxiv AI"
  },
  {
    "title": "Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models",
    "summary": "arXiv:2602.01237v1 Announce Type: new Abstract: Large language models (LLMs) achieve state-of-the-art accuracy on complex reasoning tasks by generating multiple chain-of-thought (CoT) traces, but using a fixed token budget per query leads to over-computation on easy inputs and under-computation on hard ones. We introduce Predictive Scheduling, a pl",
    "url": "https://arxiv.org/abs/2602.01237",
    "source": "Arxiv AI"
  },
  {
    "title": "LLM-Driven Ontology Construction for Enterprise Knowledge Graphs",
    "summary": "arXiv:2602.01276v1 Announce Type: new Abstract: Enterprise Knowledge Graphs have become essential for unifying heterogeneous data and enforcing semantic governance. However, the construction of their underlying ontologies remains a resource-intensive, manual process that relies heavily on domain expertise. This paper introduces OntoEKG, a LLM-drive",
    "url": "https://arxiv.org/abs/2602.01276",
    "source": "Arxiv AI"
  },
  {
    "title": "RE-MCDF: Closed-Loop Multi-Expert LLM Reasoning for Knowledge-Grounded Clinical Diagnosis",
    "summary": "arXiv:2602.01297v1 Announce Type: new Abstract: Electronic medical records (EMRs), particularly in neurology, are inherently heterogeneous, sparse, and noisy, which poses significant challenges for large language models (LLMs) in clinical diagnosis. In such settings, single-agent systems are vulnerable to self-reinforcing errors, as their predictio",
    "url": "https://arxiv.org/abs/2602.01297",
    "source": "Arxiv AI"
  },
  {
    "title": "Model Specific Task Similarity for Vision Language Model Selection via Layer Conductance",
    "summary": "arXiv:2602.01346v1 Announce Type: new Abstract: While open sourced Vision-Language Models (VLMs) have proliferated, selecting the optimal pretrained model for a specific downstream task remains challenging. Exhaustive evaluation is often infeasible due to computational constraints and data limitations in few shot scenarios. Existing selection metho",
    "url": "https://arxiv.org/abs/2602.01346",
    "source": "Arxiv AI"
  },
  {
    "title": "Aggregation Queries over Unstructured Text: Benchmark and Agentic Method",
    "summary": "arXiv:2602.01355v2 Announce Type: new Abstract: Aggregation query over free text is a long-standing yet underexplored problem. Unlike ordinary question answering, aggregate queries require exhaustive evidence collection and systems are required to \"find all,\" not merely \"find one.\" Existing paradigms such as Text-to-SQL and Retrieval-Augmented Gene",
    "url": "https://arxiv.org/abs/2602.01355",
    "source": "Arxiv AI"
  },
  {
    "title": "Building Better Deception Probes Using Targeted Instruction Pairs",
    "summary": "arXiv:2602.01425v1 Announce Type: new Abstract: Linear probes are a promising approach for monitoring AI systems for deceptive behaviour. Previous work has shown that a linear classifier trained on a contrastive instruction pair and a simple dataset can achieve good performance. However, these probes exhibit notable failures even in straightforward",
    "url": "https://arxiv.org/abs/2602.01425",
    "source": "Arxiv AI"
  },
  {
    "title": "SimGym: Traffic-Grounded Browser Agents for Offline A/B Testing in E-Commerce",
    "summary": "arXiv:2602.01443v1 Announce Type: new Abstract: A/B testing remains the gold standard for evaluating e-commerce UI changes, yet it diverts traffic, takes weeks to reach significance, and risks harming user experience. We introduce SimGym, a scalable system for rapid offline A/B testing using traffic-grounded synthetic buyers powered by Large Langua",
    "url": "https://arxiv.org/abs/2602.01443",
    "source": "Arxiv AI"
  },
  {
    "title": "Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering",
    "summary": "arXiv:2602.01465v1 Announce Type: new Abstract: Large language models have demonstrated strong capabilities in individual software engineering tasks, yet most autonomous systems still treat issue resolution as a monolithic or pipeline-based process. In contrast, real-world software development is organized as a collaborative activity carried out by",
    "url": "https://arxiv.org/abs/2602.01465",
    "source": "Arxiv AI"
  },
  {
    "title": "Legal Infrastructure for Transformative AI Governance",
    "summary": "arXiv:2602.01474v1 Announce Type: new Abstract: Most of our AI governance efforts focus on substance: what rules do we want in place? What limits or checks do we want to impose on AI development and deployment? But a key role for law is not only to establish substantive rules but also to establish legal and regulatory infrastructure to generate and",
    "url": "https://arxiv.org/abs/2602.01474",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning to Guide Local Search for MPE Inference in Probabilistic Graphical Models",
    "summary": "arXiv:2602.01475v1 Announce Type: new Abstract: Most Probable Explanation (MPE) inference in Probabilistic Graphical Models (PGMs) is a fundamental yet computationally challenging problem arising in domains such as diagnosis, planning, and structured prediction. In many practical settings, the graphical model remains fixed while inference must be p",
    "url": "https://arxiv.org/abs/2602.01475",
    "source": "Arxiv AI"
  },
  {
    "title": "Qrita: High-performance Top-k and Top-p Algorithm for GPUs using Pivot-based Truncation and Selection",
    "summary": "arXiv:2602.01518v1 Announce Type: new Abstract: Top-k and Top-p are the dominant truncation operators in the sampling of large language models. Despite their widespread use, implementing them efficiently over large vocabularies remains a significant challenge. Existing approaches often rely on sorting, which incur significant computation and memory",
    "url": "https://arxiv.org/abs/2602.01518",
    "source": "Arxiv AI"
  },
  {
    "title": "PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents",
    "summary": "arXiv:2602.01532v1 Announce Type: new Abstract: Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention an",
    "url": "https://arxiv.org/abs/2602.01532",
    "source": "Arxiv AI"
  },
  {
    "title": "MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety",
    "summary": "arXiv:2602.01539v1 Announce Type: new Abstract: Ensuring robust safety alignment is crucial for Large Language Models (LLMs), yet existing defenses often lag behind evolving adversarial attacks due to their \\textbf{reliance on static, pre-collected data distributions}. In this paper, we introduce \\textbf{MAGIC}, a novel multi-turn multi-agent reinf",
    "url": "https://arxiv.org/abs/2602.01539",
    "source": "Arxiv AI"
  },
  {
    "title": "S1-NexusAgent: a Self-Evolving Agent Framework for Multidisciplinary Scientific Research",
    "summary": "arXiv:2602.01550v1 Announce Type: new Abstract: Modern scientific research relies on large-scale data, complex workflows, and specialized tools, which existing LLMs and tool-based agents struggle to handle due to limitations in long-horizon planning, robust goal maintenance, and continual learning from execution. To address these issues, in this wo",
    "url": "https://arxiv.org/abs/2602.01550",
    "source": "Arxiv AI"
  },
  {
    "title": "Autonomous Question Formation for Large Language Model-Driven AI Systems",
    "summary": "arXiv:2602.01556v1 Announce Type: new Abstract: Large language model (LLM)-driven AI systems are increasingly important for autonomous decision-making in dynamic and open environments. However, most existing systems rely on predefined tasks and fixed prompts, limiting their ability to autonomously identify what problems should be solved when enviro",
    "url": "https://arxiv.org/abs/2602.01556",
    "source": "Arxiv AI"
  },
  {
    "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
    "summary": "arXiv:2602.01608v1 Announce Type: new Abstract: Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structur",
    "url": "https://arxiv.org/abs/2602.01608",
    "source": "Arxiv AI"
  },
  {
    "title": "ToPT: Task-Oriented Prompt Tuning for Urban Region Representation Learning",
    "summary": "arXiv:2602.01610v1 Announce Type: new Abstract: Learning effective region embeddings from heterogeneous urban data underpins key urban computing tasks (e.g., crime prediction, resource allocation). However, prevailing two-stage methods yield task-agnostic representations, decoupling them from downstream objectives. Recent prompt-based approaches at",
    "url": "https://arxiv.org/abs/2602.01610",
    "source": "Arxiv AI"
  },
  {
    "title": "ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development",
    "summary": "arXiv:2602.01655v1 Announce Type: new Abstract: Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting r",
    "url": "https://arxiv.org/abs/2602.01655",
    "source": "Arxiv AI"
  },
  {
    "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
    "summary": "arXiv:2602.01664v1 Announce Type: new Abstract: In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. T",
    "url": "https://arxiv.org/abs/2602.01664",
    "source": "Arxiv AI"
  },
  {
    "title": "TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios",
    "summary": "arXiv:2602.01675v1 Announce Type: new Abstract: As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we int",
    "url": "https://arxiv.org/abs/2602.01675",
    "source": "Arxiv AI"
  },
  {
    "title": "What LLMs Think When You Don't Tell Them What to Think About?",
    "summary": "arXiv:2602.01689v1 Announce Type: new Abstract: Characterizing the behavior of large language models (LLMs) across diverse settings is critical for reliable monitoring and AI safety. However, most existing analyses rely on topic- or task-specific prompts, which can substantially limit what can be observed. In this work, we study what LLMs generate ",
    "url": "https://arxiv.org/abs/2602.01689",
    "source": "Arxiv AI"
  },
  {
    "title": "Beyond Dense States: Elevating Sparse Transcoders to Active Operators for Latent Reasoning",
    "summary": "arXiv:2602.01695v1 Announce Type: new Abstract: Latent reasoning compresses the chain-of-thought (CoT) into continuous hidden states, yet existing methods rely on dense latent transitions that remain difficult to interpret and control. Meanwhile, sparse representation models uncover human-interpretable semantic features but remain largely confined ",
    "url": "https://arxiv.org/abs/2602.01695",
    "source": "Arxiv AI"
  },
  {
    "title": "Mitigating loss of control in advanced AI systems through instrumental goal trajectories",
    "summary": "arXiv:2602.01699v1 Announce Type: new Abstract: Researchers at artificial intelligence labs and universities are concerned that highly capable artificial intelligence (AI) systems may erode human control by pursuing instrumental goals. Existing mitigations remain largely technical and system-centric: tracking capability in advanced systems, shaping",
    "url": "https://arxiv.org/abs/2602.01699",
    "source": "Arxiv AI"
  },
  {
    "title": "Optimizing Prompts for Large Language Models: A Causal Approach",
    "summary": "arXiv:2602.01711v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly embedded in enterprise workflows, yet their performance remains highly sensitive to prompt design. Automatic Prompt Optimization (APO) seeks to mitigate this instability, but existing approaches face two persistent challenges. First, commonly used prompt s",
    "url": "https://arxiv.org/abs/2602.01711",
    "source": "Arxiv AI"
  },
  {
    "title": "MACD: Model-Aware Contrastive Decoding via Counterfactual Data",
    "summary": "arXiv:2602.01740v1 Announce Type: new Abstract: Video language models (Video-LLMs) are prone to hallucinations, often generating plausible but ungrounded content when visual evidence is weak, ambiguous, or biased. Existing decoding methods, such as contrastive decoding (CD), rely on random perturbations to construct contrastive data for mitigating ",
    "url": "https://arxiv.org/abs/2602.01740",
    "source": "Arxiv AI"
  },
  {
    "title": "Controlling Exploration-Exploitation in GFlowNets via Markov Chain Perspectives",
    "summary": "arXiv:2602.01749v2 Announce Type: new Abstract: Generative Flow Network (GFlowNet) objectives implicitly fix an equal mixing of forward and backward policies, potentially constraining the exploration-exploitation trade-off during training. By further exploring the link between GFlowNets and Markov chains, we establish an equivalence between GFlowNe",
    "url": "https://arxiv.org/abs/2602.01749",
    "source": "Arxiv AI"
  },
  {
    "title": "Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking",
    "summary": "arXiv:2602.01750v1 Announce Type: new Abstract: Reinforcement Learning from Human Feedback (RLHF) remains vulnerable to reward hacking, where models exploit spurious correlations in learned reward models to achieve high scores while violating human intent. Existing mitigations rely on static defenses that cannot adapt to novel exploitation strategi",
    "url": "https://arxiv.org/abs/2602.01750",
    "source": "Arxiv AI"
  },
  {
    "title": "PRISM: Parametrically Refactoring Inference for Speculative Sampling Draft Models",
    "summary": "arXiv:2602.01762v1 Announce Type: new Abstract: Large Language Models (LLMs), constrained by their auto-regressive nature, suffer from slow decoding. Speculative decoding methods have emerged as a promising solution to accelerate LLM decoding, attracting attention from both systems and AI research communities. Recently, the pursuit of better draft ",
    "url": "https://arxiv.org/abs/2602.01762",
    "source": "Arxiv AI"
  },
  {
    "title": "Efficient Cross-Architecture Knowledge Transfer for Large-Scale Online User Response Prediction",
    "summary": "arXiv:2602.01775v1 Announce Type: new Abstract: Deploying new architectures in large-scale user response prediction systems incurs high model switching costs due to expensive retraining on massive historical data and performance degradation under data retention constraints. Existing knowledge distillation methods struggle with architectural heterog",
    "url": "https://arxiv.org/abs/2602.01775",
    "source": "Arxiv AI"
  },
  {
    "title": "LingLanMiDian: Systematic Evaluation of LLMs on TCM Knowledge and Clinical Reasoning",
    "summary": "arXiv:2602.01779v1 Announce Type: new Abstract: Large language models (LLMs) are advancing rapidly in medical NLP, yet Traditional Chinese Medicine (TCM) with its distinctive ontology, terminology, and reasoning patterns requires domain-faithful evaluation. Existing TCM benchmarks are fragmented in coverage and scale and rely on non-unified or gene",
    "url": "https://arxiv.org/abs/2602.01779",
    "source": "Arxiv AI"
  },
  {
    "title": "ORCH: many analyses, one merge-a deterministic multi-agent orchestrator for discrete-choice reasoning with EMA-guided routing",
    "summary": "arXiv:2602.01797v1 Announce Type: new Abstract: Recent advances in large-scale language models (LLMs) have made multi-agent architectures attractive for challenging reasoning tasks. However, many existing systems rely on stochastic routing or ad-hoc heuristics, making their behavior difficult to reproduce and their decision process hard to interpre",
    "url": "https://arxiv.org/abs/2602.01797",
    "source": "Arxiv AI"
  },
  {
    "title": "INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery",
    "summary": "arXiv:2602.01815v1 Announce Type: new Abstract: Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. W",
    "url": "https://arxiv.org/abs/2602.01815",
    "source": "Arxiv AI"
  },
  {
    "title": "Synesthesia of Vehicles: Tactile Data Synthesis from Visual Inputs",
    "summary": "arXiv:2602.01832v1 Announce Type: new Abstract: Autonomous vehicles (AVs) rely on multi-modal fusion for safety, but current visual and optical sensors fail to detect road-induced excitations which are critical for vehicles' dynamic control. Inspired by human synesthesia, we propose the Synesthesia of Vehicles (SoV), a novel framework to predict ta",
    "url": "https://arxiv.org/abs/2602.01832",
    "source": "Arxiv AI"
  },
  {
    "title": "ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems",
    "summary": "arXiv:2602.01848v1 Announce Type: new Abstract: Current agentic frameworks underperform on long-horizon tasks. As reasoning depth increases, sequential orchestration becomes brittle, context windows impose hard limits that degrade performance, and opaque execution traces make failures difficult to localize or debug. We introduce ROMA (Recursive Ope",
    "url": "https://arxiv.org/abs/2602.01848",
    "source": "Arxiv AI"
  },
  {
    "title": "SOPRAG: Multi-view Graph Experts Retrieval for Industrial Standard Operating Procedures",
    "summary": "arXiv:2602.01858v1 Announce Type: new Abstract: Standard Operating Procedures (SOPs) are essential for ensuring operational safety and consistency in industrial environments. However, retrieving and following these procedures presents unique challenges, such as rigid proprietary structures, condition-dependent relevance, and actionable execution re",
    "url": "https://arxiv.org/abs/2602.01858",
    "source": "Arxiv AI"
  },
  {
    "title": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents",
    "summary": "arXiv:2602.01869v1 Announce Type: new Abstract: LLM-driven agents demonstrate strong performance in sequential decision-making but often rely on on-the-fly reasoning, re-deriving solutions even in recurring scenarios. This insufficient experience reuse leads to computational redundancy and execution instability. To bridge this gap, we propose ProcM",
    "url": "https://arxiv.org/abs/2602.01869",
    "source": "Arxiv AI"
  },
  {
    "title": "Entropy-Guided Data-Efficient Training for Multimodal Reasoning Reward Models",
    "summary": "arXiv:2602.01884v1 Announce Type: new Abstract: Multimodal reward models are crucial for aligning multimodal large language models with human preferences. Recent works have incorporated reasoning capabilities into these models, achieving promising results. However, training these models suffers from two critical challenges: (1) the inherent noise i",
    "url": "https://arxiv.org/abs/2602.01884",
    "source": "Arxiv AI"
  },
  {
    "title": "Geometric Analysis of Token Selection in Multi-Head Attention",
    "summary": "arXiv:2602.01893v1 Announce Type: new Abstract: We present a geometric framework for analysing multi-head attention in large language models (LLMs). Without altering the mechanism, we view standard attention through a top-N selection lens and study its behaviour directly in value-state space. We define geometric metrics - Precision, Recall, and F-s",
    "url": "https://arxiv.org/abs/2602.01893",
    "source": "Arxiv AI"
  },
  {
    "title": "DomusFM: A Foundation Model for Smart-Home Sensor Data",
    "summary": "arXiv:2602.01910v1 Announce Type: new Abstract: Smart-home sensor data holds significant potential for several applications, including healthcare monitoring and assistive technologies. Existing approaches, however, face critical limitations. Supervised models require impractical amounts of labeled data. Foundation models for activity recognition fo",
    "url": "https://arxiv.org/abs/2602.01910",
    "source": "Arxiv AI"
  },
  {
    "title": "Large Language Model and Formal Concept Analysis: a comparative study for Topic Modeling",
    "summary": "arXiv:2602.01933v1 Announce Type: new Abstract: Topic modeling is a research field finding increasing applications: historically from document retrieving, to sentiment analysis and text summarization. Large Language Models (LLM) are currently a major trend in text processing, but few works study their usefulness for this task. Formal Concept Analys",
    "url": "https://arxiv.org/abs/2602.01933",
    "source": "Arxiv AI"
  },
  {
    "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
    "summary": "arXiv:2602.01970v1 Announce Type: new Abstract: Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, curr",
    "url": "https://arxiv.org/abs/2602.01970",
    "source": "Arxiv AI"
  },
  {
    "title": "Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning",
    "summary": "arXiv:2602.01983v1 Announce Type: new Abstract: Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of sel",
    "url": "https://arxiv.org/abs/2602.01983",
    "source": "Arxiv AI"
  },
  {
    "title": "Emergent Analogical Reasoning in Transformers",
    "summary": "arXiv:2602.01992v2 Announce Type: new Abstract: Analogy is a central faculty of human intelligence, enabling abstract patterns discovered in one domain to be applied to another. Despite its central role in cognition, the mechanisms by which Transformers acquire and implement analogical reasoning remain poorly understood. In this work, inspired by t",
    "url": "https://arxiv.org/abs/2602.01992",
    "source": "Arxiv AI"
  },
  {
    "title": "Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs",
    "summary": "arXiv:2602.01995v1 Announce Type: new Abstract: Conversational diagnosis requires multi-turn history-taking, where an agent asks clarifying questions to refine differential diagnoses under incomplete information. Existing approaches often rely on the parametric knowledge of a model or assume that patients provide rich and concrete information, whic",
    "url": "https://arxiv.org/abs/2602.01995",
    "source": "Arxiv AI"
  },
  {
    "title": "Do I Really Know? Learning Factual Self-Verification for Hallucination Reduction",
    "summary": "arXiv:2602.02018v1 Announce Type: new Abstract: Factual hallucination remains a central challenge for large language models (LLMs). Existing mitigation approaches primarily rely on either external post-hoc verification or mapping uncertainty directly to abstention during fine-tuning, often resulting in overly conservative behavior. We propose VeriF",
    "url": "https://arxiv.org/abs/2602.02018",
    "source": "Arxiv AI"
  },
  {
    "title": "Light Alignment Improves LLM Safety via Model Self-Reflection with a Single Neuron",
    "summary": "arXiv:2602.02027v1 Announce Type: new Abstract: The safety of large language models (LLMs) has increasingly emerged as a fundamental aspect of their development. Existing safety alignment for LLMs is predominantly achieved through post-training methods, which are computationally expensive and often fail to generalize well across different models. A",
    "url": "https://arxiv.org/abs/2602.02027",
    "source": "Arxiv AI"
  },
  {
    "title": "Edit Knowledge, Not Just Facts via Multi-Step Reasoning over Background Stories",
    "summary": "arXiv:2602.02028v1 Announce Type: new Abstract: Enabling artificial intelligence systems, particularly large language models, to integrate new knowledge and flexibly apply it during reasoning remains a central challenge. Existing knowledge editing approaches emphasize atomic facts, improving factual recall but often failing to integrate new informa",
    "url": "https://arxiv.org/abs/2602.02028",
    "source": "Arxiv AI"
  },
  {
    "title": "Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation",
    "summary": "arXiv:2602.02029v1 Announce Type: new Abstract: Automatically formulating optimization models from natural language descriptions is a growing focus in operations research, yet current LLM-based approaches struggle with the composite constraints and appropriate modeling paradigms required by complex operational rules. To address this, we introduce t",
    "url": "https://arxiv.org/abs/2602.02029",
    "source": "Arxiv AI"
  },
  {
    "title": "Constrained Process Maps for Multi-Agent Generative AI Workflows",
    "summary": "arXiv:2602.02034v1 Announce Type: new Abstract: Large language model (LLM)-based agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. However, many agentic architectures rely primarily on prompt engineering of a single agent, making it difficult to observe or compare how m",
    "url": "https://arxiv.org/abs/2602.02034",
    "source": "Arxiv AI"
  },
  {
    "title": "Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models",
    "summary": "arXiv:2602.02039v1 Announce Type: new Abstract: The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence, distinguishing it from executional intelligence, which merely completes assigned tasks. Data Science provides a nat",
    "url": "https://arxiv.org/abs/2602.02039",
    "source": "Arxiv AI"
  },
  {
    "title": "Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents",
    "summary": "arXiv:2602.02050v1 Announce Type: new Abstract: Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing tool",
    "url": "https://arxiv.org/abs/2602.02050",
    "source": "Arxiv AI"
  },
  {
    "title": "SIDiffAgent: Self-Improving Diffusion Agent",
    "summary": "arXiv:2602.02051v1 Announce Type: new Abstract: Text-to-image diffusion models have revolutionized generative AI, enabling high-quality and photorealistic image synthesis. However, their practical deployment remains hindered by several limitations: sensitivity to prompt phrasing, ambiguity in semantic interpretation (e.g., ``mouse\" as animal vs. a ",
    "url": "https://arxiv.org/abs/2602.02051",
    "source": "Arxiv AI"
  },
  {
    "title": "Understanding the Reversal Curse Mitigation in Masked Diffusion Models through Attention and Training Dynamics",
    "summary": "arXiv:2602.02133v1 Announce Type: new Abstract: Autoregressive language models (ARMs) suffer from the reversal curse: after learning that \"$A$ is $B$\", they often fail on the reverse query \"$B$ is $A$\". Masked diffusion-based language models (MDMs) exhibit this failure in a much weaker form, but the underlying reason has remained unclear. A common ",
    "url": "https://arxiv.org/abs/2602.02133",
    "source": "Arxiv AI"
  },
  {
    "title": "Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models",
    "summary": "arXiv:2602.02136v1 Announce Type: new Abstract: Safety alignment incurs safety tax that perturbs a large reasoning model's (LRM) general reasoning ability. Existing datasets used for safety alignment for an LRM are usually constructed by distilling safety reasoning traces and answers from an external LRM or human labeler. However, such reasoning tr",
    "url": "https://arxiv.org/abs/2602.02136",
    "source": "Arxiv AI"
  },
  {
    "title": "Traffic-Aware Navigation in Road Networks",
    "summary": "arXiv:2602.02158v1 Announce Type: new Abstract: This project compares three graph search approaches for the task of traffic-aware navigation in Kingston's road network. These approaches include a single-run multi-query preprocessing algorithm (Floyd-Warshall-Ingerman), continuous single-query real-time search (Dijkstra's and A*), and an algorithm c",
    "url": "https://arxiv.org/abs/2602.02158",
    "source": "Arxiv AI"
  },
  {
    "title": "Reasoning in a Combinatorial and Constrained World: Benchmarking LLMs on Natural-Language Combinatorial Optimization",
    "summary": "arXiv:2602.02188v1 Announce Type: new Abstract: While large language models (LLMs) have shown strong performance in math and logic reasoning, their ability to handle combinatorial optimization (CO) -- searching high-dimensional solution spaces under hard constraints -- remains underexplored. To bridge the gap, we introduce NLCO, a \\textbf{N}atural ",
    "url": "https://arxiv.org/abs/2602.02188",
    "source": "Arxiv AI"
  },
  {
    "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
    "summary": "arXiv:2602.02196v2 Announce Type: new Abstract: Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing eva",
    "url": "https://arxiv.org/abs/2602.02196",
    "source": "Arxiv AI"
  },
  {
    "title": "More Than a Quick Glance: Overcoming the Greedy Bias in KV-Cache Compression",
    "summary": "arXiv:2602.02199v1 Announce Type: new Abstract: While Large Language Models (LLMs) can theoretically support extensive context windows, their actual deployment is constrained by the linear growth of Key-Value (KV) cache memory. Prevailing compression strategies mitigate this through various pruning mechanisms, yet trade-off semantic recall for memo",
    "url": "https://arxiv.org/abs/2602.02199",
    "source": "Arxiv AI"
  },
  {
    "title": "Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach",
    "summary": "arXiv:2602.02304v1 Announce Type: new Abstract: Large-scale foundation models exhibit behavioral shifts: intervention-induced behavioral changes that appear after scaling, fine-tuning, reinforcement learning or in-context learning. While investigating these phenomena have recently received attention, explaining their appearance is still overlooked.",
    "url": "https://arxiv.org/abs/2602.02304",
    "source": "Arxiv AI"
  },
  {
    "title": "Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient",
    "summary": "arXiv:2602.02313v2 Announce Type: new Abstract: Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlat",
    "url": "https://arxiv.org/abs/2602.02313",
    "source": "Arxiv AI"
  },
  {
    "title": "Context Learning for Multi-Agent Discussion",
    "summary": "arXiv:2602.02350v1 Announce Type: new Abstract: Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the",
    "url": "https://arxiv.org/abs/2602.02350",
    "source": "Arxiv AI"
  },
  {
    "title": "Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback",
    "summary": "arXiv:2602.02369v1 Announce Type: new Abstract: Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \\emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for",
    "url": "https://arxiv.org/abs/2602.02369",
    "source": "Arxiv AI"
  },
  {
    "title": "Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing",
    "summary": "arXiv:2602.02386v1 Announce Type: new Abstract: How should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection.",
    "url": "https://arxiv.org/abs/2602.02386",
    "source": "Arxiv AI"
  },
  {
    "title": "Structure Enables Effective Self-Localization of Errors in LLMs",
    "summary": "arXiv:2602.02416v1 Announce Type: new Abstract: Self-correction in language models remains elusive. In this work, we explore whether language models can explicitly localize errors in incorrect reasoning, as a path toward building AI systems that can effectively correct themselves. We introduce a prompting method that structures reasoning as discret",
    "url": "https://arxiv.org/abs/2602.02416",
    "source": "Arxiv AI"
  },
  {
    "title": "SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration",
    "summary": "arXiv:2602.02419v2 Announce Type: new Abstract: Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about m",
    "url": "https://arxiv.org/abs/2602.02419",
    "source": "Arxiv AI"
  },
  {
    "title": "Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling",
    "summary": "arXiv:2602.02453v2 Announce Type: new Abstract: Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and comput",
    "url": "https://arxiv.org/abs/2602.02453",
    "source": "Arxiv AI"
  },
  {
    "title": "Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction",
    "summary": "arXiv:2602.02455v1 Announce Type: new Abstract: As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typical",
    "url": "https://arxiv.org/abs/2602.02455",
    "source": "Arxiv AI"
  },
  {
    "title": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
    "summary": "arXiv:2602.02465v1 Announce Type: new Abstract: Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human men",
    "url": "https://arxiv.org/abs/2602.02465",
    "source": "Arxiv AI"
  },
  {
    "title": "Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts",
    "summary": "arXiv:2602.02468v1 Announce Type: new Abstract: Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-",
    "url": "https://arxiv.org/abs/2602.02468",
    "source": "Arxiv AI"
  },
  {
    "title": "Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge",
    "summary": "arXiv:2602.02470v1 Announce Type: new Abstract: Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the \"reversal curse\" -- when trained on forward knowledge data of the form \"$A \\rightarrow B$\" (e.g., Alice's husband is Bob), the model ",
    "url": "https://arxiv.org/abs/2602.02470",
    "source": "Arxiv AI"
  },
  {
    "title": "AgentRx: Diagnosing AI Agent Failures from Execution Trajectories",
    "summary": "arXiv:2602.02475v1 Announce Type: new Abstract: AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API",
    "url": "https://arxiv.org/abs/2602.02475",
    "source": "Arxiv AI"
  },
  {
    "title": "Privacy in Practice: Private COVID-19 Detection in X-Ray Images (Extended Version)",
    "summary": "arXiv:2211.11434v4 Announce Type: cross Abstract: Machine learning (ML) can help fight pandemics like COVID-19 by enabling rapid screening of large volumes of images. To perform data analysis while maintaining patient privacy, we create ML models that satisfy Differential Privacy (DP). Previous works exploring private COVID-19 models are in part ba",
    "url": "https://arxiv.org/abs/2211.11434",
    "source": "Arxiv AI"
  },
  {
    "title": "Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable Stress Detection",
    "summary": "arXiv:2401.13327v2 Announce Type: cross Abstract: Smartwatch health sensor data are increasingly utilized in smart health applications and patient monitoring, including stress detection. However, such medical data often comprise sensitive personal information and are resource-intensive to acquire for research purposes. In response to this challenge",
    "url": "https://arxiv.org/abs/2401.13327",
    "source": "Arxiv AI"
  },
  {
    "title": "Assessing the Impact of Image Dataset Features on Privacy-Preserving Machine Learning",
    "summary": "arXiv:2409.01329v2 Announce Type: cross Abstract: Machine Learning (ML) is crucial in many sectors, including computer vision. However, ML models trained on sensitive data face security challenges, as they can be attacked and leak information. Privacy-Preserving Machine Learning (PPML) addresses this by using Differential Privacy (DP) to balance ut",
    "url": "https://arxiv.org/abs/2409.01329",
    "source": "Arxiv AI"
  },
  {
    "title": "Federated Learning With Individualized Privacy Through Client Sampling",
    "summary": "arXiv:2501.17634v2 Announce Type: cross Abstract: With growing concerns about user data collection, individualized privacy has emerged as a promising solution to balance protection and utility by accounting for diverse user privacy preferences. Instead of enforcing a uniform level of anonymization for all users, this approach allows individuals to ",
    "url": "https://arxiv.org/abs/2501.17634",
    "source": "Arxiv AI"
  },
  {
    "title": "Reinforcement Learning via Conservative Agent for Environments with Random Delays",
    "summary": "arXiv:2507.18992v2 Announce Type: cross Abstract: Real-world reinforcement learning applications are often hindered by delayed feedback from environments, which violates the Markov assumption and introduces significant challenges. Although numerous delay-compensating methods have been proposed for environments with constant delays, environments wit",
    "url": "https://arxiv.org/abs/2507.18992",
    "source": "Arxiv AI"
  },
  {
    "title": "Culinary Crossroads: A RAG Framework for Enhancing Diversity in Cross-Cultural Recipe Adaptation",
    "summary": "arXiv:2507.21934v1 Announce Type: cross Abstract: In cross-cultural recipe adaptation, the goal is not only to ensure cultural appropriateness and retain the original dish's essence, but also to provide diverse options for various dietary needs and preferences. Retrieval Augmented Generation (RAG) is a promising approach, combining the retrieval of",
    "url": "https://arxiv.org/abs/2507.21934",
    "source": "Arxiv AI"
  },
  {
    "title": "Disentangled Interest Network for Out-of-Distribution CTR Prediction",
    "summary": "arXiv:2602.00002v1 Announce Type: cross Abstract: Click-through rate (CTR) prediction, which estimates the probability of a user clicking on a given item, is a critical task for online information services. Existing approaches often make strong assumptions that training and test data come from the same distribution. However, the data distribution v",
    "url": "https://arxiv.org/abs/2602.00002",
    "source": "Arxiv AI"
  },
  {
    "title": "Orchestrating Heterogeneous Experts: A Scalable MoE Framework with Anisotropy-Preserving Fusion",
    "summary": "arXiv:2602.00003v2 Announce Type: cross Abstract: In cross-border e-commerce, search relevance modeling faces the dual challenge of extreme linguistic diversity and fine-grained semantic nuances. Existing approaches typically rely on scaling up a single monolithic Large Language Model (LLM). However, our empirical analysis reveals that single model",
    "url": "https://arxiv.org/abs/2602.00003",
    "source": "Arxiv AI"
  },
  {
    "title": "PPoGA: Predictive Plan-on-Graph with Action for Knowledge Graph Question Answering",
    "summary": "arXiv:2602.00007v1 Announce Type: cross Abstract: Large Language Models (LLMs) augmented with Knowledge Graphs (KGs) have advanced complex question answering, yet they often remain susceptible to failure when their initial high-level reasoning plan is flawed. This limitation, analogous to cognitive functional fixedness, prevents agents from restruc",
    "url": "https://arxiv.org/abs/2602.00007",
    "source": "Arxiv AI"
  },
  {
    "title": "Unlocking Electronic Health Records: A Hybrid Graph RAG Approach to Safe Clinical AI for Patient QA",
    "summary": "arXiv:2602.00009v1 Announce Type: cross Abstract: Electronic health record (EHR) systems present clinicians with vast repositories of clinical information, creating a significant cognitive burden where critical details are easily overlooked. While Large Language Models (LLMs) offer transformative potential for data processing, they face significant",
    "url": "https://arxiv.org/abs/2602.00009",
    "source": "Arxiv AI"
  },
  {
    "title": "ChunkNorris: A High-Performance and Low-Energy Approach to PDF Parsing and Chunking",
    "summary": "arXiv:2602.00010v1 Announce Type: cross Abstract: In Retrieval-Augmented Generation applications, the Information Retrieval part is central as it provides the contextual information that enables a Large Language Model to generate an appropriate and truthful response. High quality parsing and chunking are critical as efficient data segmentation dire",
    "url": "https://arxiv.org/abs/2602.00010",
    "source": "Arxiv AI"
  },
  {
    "title": "Chained Prompting for Better Systematic Review Search Strategies",
    "summary": "arXiv:2602.00011v1 Announce Type: cross Abstract: Systematic reviews require the use of rigorously designed search strategies to ensure both comprehensive retrieval and minimization of bias. Conventional manual approaches, although methodologically systematic, are resource-intensive and susceptible to subjectivity, whereas heuristic and automated t",
    "url": "https://arxiv.org/abs/2602.00011",
    "source": "Arxiv AI"
  },
  {
    "title": "OGD4All: A Framework for Accessible Interaction with Geospatial Open Government Data Based on Large Language Models",
    "summary": "arXiv:2602.00012v1 Announce Type: cross Abstract: We present OGD4All, a transparent, auditable, and reproducible framework based on Large Language Models (LLMs) to enhance citizens' interaction with geospatial Open Government Data (OGD). The system combines semantic data retrieval, agentic reasoning for iterative code generation, and secure sandbox",
    "url": "https://arxiv.org/abs/2602.00012",
    "source": "Arxiv AI"
  },
  {
    "title": "What Artificial Intelligence can do for High-Performance Computing systems?",
    "summary": "arXiv:2602.00014v1 Announce Type: cross Abstract: High-performance computing (HPC) centers consume substantial power, incurring environmental and operational costs. This review assesses how artificial intelligence (AI), including machine learning (ML) and optimization, improves the efficiency of operational HPC systems. Approximately 1,800 publicat",
    "url": "https://arxiv.org/abs/2602.00014",
    "source": "Arxiv AI"
  },
  {
    "title": "G-MemLLM: Gated Latent Memory Augmentation for Long-Context Reasoning in Large Language Models",
    "summary": "arXiv:2602.00015v1 Announce Type: cross Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, yet they remain constrained by the finite capacity of their context windows and the inherent difficulty of maintaining long-term factual consistency during multi-hop reasoning. While existing me",
    "url": "https://arxiv.org/abs/2602.00015",
    "source": "Arxiv AI"
  },
  {
    "title": "PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems",
    "summary": "arXiv:2602.00016v1 Announce Type: cross Abstract: With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. However, existing work overlooks a fundamental psychological consensus that personality traits are",
    "url": "https://arxiv.org/abs/2602.00016",
    "source": "Arxiv AI"
  },
  {
    "title": "SafeTalkCoach: Diversity-Driven Multi-Agent Simulation for Parent-Teen Health Conversations",
    "summary": "arXiv:2602.00017v1 Announce Type: cross Abstract: The importance of effective parent-child communication about sexual health is widely acknowledged, but real-world data on these conversations is scarce and challenging to collect, due to their private and sensitive nature. Although LLMs have been widely adopted in dialogue generation, they may devia",
    "url": "https://arxiv.org/abs/2602.00017",
    "source": "Arxiv AI"
  },
  {
    "title": "AutoBinder Agent: An MCP-Based Agent for End-to-End Protein Binder Design",
    "summary": "arXiv:2602.00019v1 Announce Type: cross Abstract: Modern AI technologies for drug discovery are distributed across heterogeneous platforms-including web applications, desktop environments, and code libraries-leading to fragmented workflows, inconsistent interfaces, and high integration overhead. We present an agentic end-to-end drug design framewor",
    "url": "https://arxiv.org/abs/2602.00019",
    "source": "Arxiv AI"
  },
  {
    "title": "Beyond Static Question Banks: Dynamic Knowledge Expansion via LLM-Automated Graph Construction and Adaptive Generation",
    "summary": "arXiv:2602.00020v1 Announce Type: cross Abstract: Personalized education systems increasingly rely on structured knowledge representations to support adaptive learning and question generation. However, existing approaches face two fundamental limitations. First, constructing and maintaining knowledge graphs for educational content largely depends o",
    "url": "https://arxiv.org/abs/2602.00020",
    "source": "Arxiv AI"
  },
  {
    "title": "Early Warning Signals Appear Long Before Dropping Out: An Idiographic Approach Grounded in Complex Dynamic Systems Theory",
    "summary": "arXiv:2602.00021v1 Announce Type: cross Abstract: The ability to sustain engagement and recover from setbacks (i.e., resilience) -- is fundamental for learning. When resilience weakens, students are at risk of disengagement and may drop out and miss on opportunities. Therefore, predicting disengagement long before it happens during the window of ho",
    "url": "https://arxiv.org/abs/2602.00021",
    "source": "Arxiv AI"
  },
  {
    "title": "Strategies for Creating Uncertainty in the AI Era to Trigger Students Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System",
    "summary": "arXiv:2602.00026v1 Announce Type: cross Abstract: Generative AI challenges traditional assessments by allowing students to produce correct answers without demonstrating understanding or reasoning. Rather than prohibiting AI, this work argues that one way to integrate AI into education is by creating uncertain situations with the help of AI models a",
    "url": "https://arxiv.org/abs/2602.00026",
    "source": "Arxiv AI"
  },
  {
    "title": "Representation Learning Enhanced Deep Reinforcement Learning for Optimal Operation of Hydrogen-based Multi-Energy Systems",
    "summary": "arXiv:2602.00027v1 Announce Type: cross Abstract: Hydrogen-based multi-energy systems (HMES) have emerged as a promising low-carbon and energy-efficient solution, as it can enable the coordinated operation of electricity, heating and cooling supply and demand to enhance operational flexibility, improve overall energy efficiency, and increase the sh",
    "url": "https://arxiv.org/abs/2602.00027",
    "source": "Arxiv AI"
  },
  {
    "title": "Construct, Align, and Reason: Large Ontology Models for Enterprise Knowledge Management",
    "summary": "arXiv:2602.00029v1 Announce Type: cross Abstract: Enterprise-scale knowledge management faces significant challenges in integrating multi-source heterogeneous data and enabling effective semantic reasoning. Traditional knowledge graphs often struggle with implicit relationship discovery and lack sufficient semantic understanding for complex questio",
    "url": "https://arxiv.org/abs/2602.00029",
    "source": "Arxiv AI"
  },
  {
    "title": "Happy Young Women, Grumpy Old Men? Emotion-Driven Demographic Biases in Synthetic Face Generation",
    "summary": "arXiv:2602.00032v2 Announce Type: cross Abstract: Synthetic face generation has rapidly advanced with the emergence of text-to-image (T2I) and of multimodal large language models, enabling high-fidelity image production from natural-language prompts. Despite the widespread adoption of these tools, the biases, representational quality, and cross-cul",
    "url": "https://arxiv.org/abs/2602.00032",
    "source": "Arxiv AI"
  },
  {
    "title": "Synthetic Student Responses: LLM-Extracted Features for IRT Difficulty Parameter Estimation",
    "summary": "arXiv:2602.00034v1 Announce Type: cross Abstract: Educational assessment relies heavily on knowing question difficulty, traditionally determined through resource-intensive pre-testing with students. This creates significant barriers for both classroom teachers and assessment developers. We investigate whether Item Response Theory (IRT) difficulty p",
    "url": "https://arxiv.org/abs/2602.00034",
    "source": "Arxiv AI"
  },
  {
    "title": "LOGOS-CA: A Cellular Automaton Using Natural Language as State and Rule",
    "summary": "arXiv:2602.00036v1 Announce Type: cross Abstract: Large Language Models (LLMs), trained solely on massive text data, have achieved high performance on the Winograd Schema Challenge (WSC), a benchmark proposed to measure commonsense knowledge and reasoning abilities about the real world. This suggests that the language produced by humanity describes",
    "url": "https://arxiv.org/abs/2602.00036",
    "source": "Arxiv AI"
  },
  {
    "title": "Bitcoin Price Prediction using Machine Learning and Combinatorial Fusion Analysis",
    "summary": "arXiv:2602.00037v1 Announce Type: cross Abstract: In this work, we propose to apply a new model fusion and learning paradigm, known as Combinatorial Fusion Analysis (CFA), to the field of Bitcoin price prediction. Price prediction of financial product has always been a big topic in finance, as the successful prediction of the price can yield signif",
    "url": "https://arxiv.org/abs/2602.00037",
    "source": "Arxiv AI"
  },
  {
    "title": "LSSF: Safety Alignment for Large Language Models through Low-Rank Safety Subspace Fusion",
    "summary": "arXiv:2602.00038v1 Announce Type: cross Abstract: The safety mechanisms of large language models (LLMs) exhibit notable fragility, as even fine-tuning on datasets without harmful content may still undermine their safety capabilities. Meanwhile, existing safety alignment methods predominantly rely on the fine-tuning process, which inadvertently lead",
    "url": "https://arxiv.org/abs/2602.00038",
    "source": "Arxiv AI"
  },
  {
    "title": "Enhancing few-shot time series forecasting with LLM-guided diffusion",
    "summary": "arXiv:2602.00040v1 Announce Type: cross Abstract: Time series forecasting in specialized domains is often constrained by limited data availability, where conventional models typically require large-scale datasets to effectively capture underlying temporal dynamics. To tackle this few-shot challenge, we propose LTSM-DIFF (Large-scale Temporal Sequen",
    "url": "https://arxiv.org/abs/2602.00040",
    "source": "Arxiv AI"
  },
  {
    "title": "Student Perceptions of Large Language Models Use in Self-Reflection and Design Critique in Architecture Studio",
    "summary": "arXiv:2602.00041v2 Announce Type: cross Abstract: This study investigates the integration of Large Language Models (LLMs) into the feedback mechanisms of the architectural design studio, shifting the focus from generative production to reflective pedagogy. Employing a mixed-methods approach with surveys and semi structured interviews with 22 archit",
    "url": "https://arxiv.org/abs/2602.00041",
    "source": "Arxiv AI"
  },
  {
    "title": "JSR-GFNet: Jamming-to-Signal Ratio-Aware Dynamic Gating for Interference Classification in future Cognitive Global Navigation Satellite Systems",
    "summary": "arXiv:2602.00042v1 Announce Type: cross Abstract: The transition toward cognitive global navigation satellite system (GNSS) receivers requires accurate interference classification to trigger adaptive mitigation strategies. However, conventional methods relying on Time-Frequency Analysis (TFA) and Convolutional Neural Networks (CNNs) face two fundam",
    "url": "https://arxiv.org/abs/2602.00042",
    "source": "Arxiv AI"
  },
  {
    "title": "When LLMs Imagine People: A Human-Centered Persona Brainstorm Audit for Bias and Fairness in Creative Applications",
    "summary": "arXiv:2602.00044v1 Announce Type: cross Abstract: Biased outputs from Large Language Models (LLMs) can reinforce stereotypes and perpetuate inequities in real-world applications, making fairness auditing essential. We introduce the Persona Brainstorm Audit (PBA), a scalable and transparent auditing method for detecting bias through open-ended perso",
    "url": "https://arxiv.org/abs/2602.00044",
    "source": "Arxiv AI"
  },
  {
    "title": "Lightweight Edge Learning via Dataset Pruning",
    "summary": "arXiv:2602.00047v1 Announce Type: cross Abstract: Edge learning facilitates ubiquitous intelligence by enabling model training and adaptation directly on data-generating devices, thereby mitigating privacy risks and communication latency. However, the high computational and energy overhead of on-device training hinders its deployment on battery-pow",
    "url": "https://arxiv.org/abs/2602.00047",
    "source": "Arxiv AI"
  },
  {
    "title": "Quantum Circuit-Based Learning Models: Bridging Quantum Computing and Machine Learning",
    "summary": "arXiv:2602.00048v1 Announce Type: cross Abstract: Machine Learning (ML) has been widely applied across numerous domains due to its ability to automatically identify informative patterns from data for various tasks. The availability of large-scale data and advanced computational power enables the development of sophisticated models and training stra",
    "url": "https://arxiv.org/abs/2602.00048",
    "source": "Arxiv AI"
  },
  {
    "title": "AI-assisted Protocol Information Extraction For Improved Accuracy and Efficiency in Clinical Trial Workflows",
    "summary": "arXiv:2602.00052v1 Announce Type: cross Abstract: Increasing clinical trial protocol complexity, amendments, and challenges around knowledge management create significant burden for trial teams. Structuring protocol content into standard formats has the potential to improve efficiency, support documentation quality, and strengthen compliance. We ev",
    "url": "https://arxiv.org/abs/2602.00052",
    "source": "Arxiv AI"
  },
  {
    "title": "How Hyper-Datafication Impacts the Sustainability Costs in Frontier AI",
    "summary": "arXiv:2602.00056v1 Announce Type: cross Abstract: Large-scale data has fuelled the success of frontier artificial intelligence (AI) models over the past decade. This expansion has relied on sustained efforts by large technology corporations to aggregate and curate internet-scale datasets. In this work, we examine the environmental, social, and econ",
    "url": "https://arxiv.org/abs/2602.00056",
    "source": "Arxiv AI"
  },
  {
    "title": "Explore Brain-Inspired Machine Intelligence for Connecting Dots on Graphs Through Holographic Blueprint of Oscillatory Synchronization",
    "summary": "arXiv:2602.00057v1 Announce Type: cross Abstract: Neural coupling in both neuroscience and artificial intelligence emerges as dynamic oscillatory patterns that encode abstract concepts. To this end, we hypothesize that a deeper understanding of the neural mechanisms governing brain rhythms can inspire next-generation design principles for machine l",
    "url": "https://arxiv.org/abs/2602.00057",
    "source": "Arxiv AI"
  },
  {
    "title": "TextBFGS: Quasi-Newton Optimization for Discrete Executable Text via Gradient-Operator Retrieval",
    "summary": "arXiv:2602.00059v1 Announce Type: cross Abstract: Optimizing discrete executable text such as prompts and code has recently been framed as a gradient-based process, effectively translating backpropagation concepts to the semantic space. However, existing methods predominantly operate as first-order optimizers akin to Stochastic Gradient Descent, wh",
    "url": "https://arxiv.org/abs/2602.00059",
    "source": "Arxiv AI"
  },
  {
    "title": "A longitudinal geospatial multimodal dataset of post-discharge frailty, physiology, mobility, and neighborhoods",
    "summary": "arXiv:2602.00060v1 Announce Type: cross Abstract: Frailty in older adults is associated with increased vulnerability to functional decline, reduced mobility, social isolation, and challenges during the transition from hospital to community living. These factors are associated with rehospitalization and may adversely influence recovery. Neighborhood",
    "url": "https://arxiv.org/abs/2602.00060",
    "source": "Arxiv AI"
  },
  {
    "title": "Simple Role Assignment is Extraordinarily Effective for Safety Alignment",
    "summary": "arXiv:2602.00061v1 Announce Type: cross Abstract: Principle-based alignment often lacks context sensitivity and completeness. Grounded in Theory of Mind, we propose role conditioning as a compact alternative: social roles (e.g., mother, judge) implicitly encode both values and the cognitive schemas required to apply them. We introduce a training-fr",
    "url": "https://arxiv.org/abs/2602.00061",
    "source": "Arxiv AI"
  },
  {
    "title": "SCPL: Enhancing Neural Network Training Throughput with Decoupled Local Losses and Model Parallelism",
    "summary": "arXiv:2602.00062v2 Announce Type: cross Abstract: Adopting large-scale AI models in enterprise information systems is often hindered by high training costs and long development cycles, posing a significant managerial challenge. The standard end-to-end backpropagation (BP) algorithm is a primary driver of modern AI, but it is also the source of inef",
    "url": "https://arxiv.org/abs/2602.00062",
    "source": "Arxiv AI"
  },
  {
    "title": "The Impact of Machine Learning Uncertainty on the Robustness of Counterfactual Explanations",
    "summary": "arXiv:2602.00063v1 Announce Type: cross Abstract: Counterfactual explanations are widely used to interpret machine learning predictions by identifying minimal changes to input features that would alter a model's decision. However, most existing counterfactual methods have not been tested when model and data uncertainty change, resulting in explanat",
    "url": "https://arxiv.org/abs/2602.00063",
    "source": "Arxiv AI"
  },
  {
    "title": "SPGCL: Simple yet Powerful Graph Contrastive Learning via SVD-Guided Structural Perturbation",
    "summary": "arXiv:2602.00064v2 Announce Type: cross Abstract: Graph Neural Networks (GNNs) are sensitive to structural noise from adversarial attacks or imperfections. Existing graph contrastive learning (GCL) methods typically rely on either random perturbations (e.g., edge dropping) for diversity or spectral augmentations (e.g., SVD) to preserve structural p",
    "url": "https://arxiv.org/abs/2602.00064",
    "source": "Arxiv AI"
  },
  {
    "title": "Responsible Evaluation of AI for Mental Health",
    "summary": "arXiv:2602.00065v1 Announce Type: cross Abstract: Although artificial intelligence (AI) shows growing promise for mental health care, current approaches to evaluating AI tools in this domain remain fragmented and poorly aligned with clinical practice, social context, and first-hand user experience. This paper argues for a rethinking of responsible ",
    "url": "https://arxiv.org/abs/2602.00065",
    "source": "Arxiv AI"
  },
  {
    "title": "Modality as Heterogeneity: Node Splitting and Graph Rewiring for Multimodal Graph Learning",
    "summary": "arXiv:2602.00067v1 Announce Type: cross Abstract: Multimodal graphs are gaining increasing attention due to their rich representational power and wide applicability, yet they introduce substantial challenges arising from severe modality confusion. To address this issue, we propose NSG (Node Splitting Graph)-MoE, a multimodal graph learning framewor",
    "url": "https://arxiv.org/abs/2602.00067",
    "source": "Arxiv AI"
  },
  {
    "title": "Adoption and Use of LLMs at an Academic Medical Center",
    "summary": "arXiv:2602.00074v1 Announce Type: cross Abstract: While large language models (LLMs) can support clinical documentation needs, standalone tools struggle with \"workflow friction\" from manual data entry. We developed ChatEHR, a system that enables the use of LLMs with the entire patient timeline spanning several years. ChatEHR enables automations - w",
    "url": "https://arxiv.org/abs/2602.00074",
    "source": "Arxiv AI"
  },
  {
    "title": "Standards for trustworthy AI in the European Union: technical rationale, structural challenges, and an implementation path",
    "summary": "arXiv:2602.00078v1 Announce Type: cross Abstract: This white paper examines the technical foundations of European AI standardization under the AI Act. It explains how harmonized standards enable the presumption of conformity mechanism, describes the CEN/CENELEC standardization process, and analyzes why AI poses unique standardization challenges inc",
    "url": "https://arxiv.org/abs/2602.00078",
    "source": "Arxiv AI"
  },
  {
    "title": "Design and Empirical Study of a Large Language Model-Based Multi-Agent Investment System for Chinese Public REITs",
    "summary": "arXiv:2602.00082v1 Announce Type: cross Abstract: This study addresses the low-volatility Chinese Public Real Estate Investment Trusts (REITs) market, proposing a large language model (LLM)-driven trading framework based on multi-agent collaboration. The system constructs four types of analytical agents-announcement, event, price momentum, and mark",
    "url": "https://arxiv.org/abs/2602.00082",
    "source": "Arxiv AI"
  },
  {
    "title": "SPARC-RAG: Adaptive Sequential-Parallel Scaling with Context Management for Retrieval-Augmented Generation",
    "summary": "arXiv:2602.00083v1 Announce Type: cross Abstract: Retrieval-Augmented Generation (RAG) grounds large language model outputs in external evidence, but remains challenged on multi-hop question answering that requires long reasoning. Recent works scale RAG at inference time along two complementary dimensions: sequential depth for iterative refinement ",
    "url": "https://arxiv.org/abs/2602.00083",
    "source": "Arxiv AI"
  },
  {
    "title": "CARE-RFT: Confidence-Anchored Reinforcement Finetuning for Reliable Reasoning in Large Language Models",
    "summary": "arXiv:2602.00085v1 Announce Type: cross Abstract: Reinforcement finetuning (RFT) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, we identify a critical trade-off: while unconstrained RFT achieves strong reasoning performance, it severely compromises model trustworthiness by amplifying hallu",
    "url": "https://arxiv.org/abs/2602.00085",
    "source": "Arxiv AI"
  },
  {
    "title": "Impact of LLMs news Sentiment Analysis on Stock Price Movement Prediction",
    "summary": "arXiv:2602.00086v1 Announce Type: cross Abstract: This paper addresses stock price movement prediction by leveraging LLM-based news sentiment analysis. Earlier works have largely focused on proposing and assessing sentiment analysis models and stock movement prediction methods, however, separately. Although promising results have been achieved, a c",
    "url": "https://arxiv.org/abs/2602.00086",
    "source": "Arxiv AI"
  },
  {
    "title": "ECCO: Evidence-Driven Causal Reasoning for Compiler Optimization",
    "summary": "arXiv:2602.00087v1 Announce Type: cross Abstract: Compiler auto-tuning faces a dichotomy between traditional black-box search methods, which lack semantic guidance, and recent Large Language Model (LLM) approaches, which often suffer from superficial pattern matching and causal opacity. In this paper, we introduce ECCO, a framework that bridges int",
    "url": "https://arxiv.org/abs/2602.00087",
    "source": "Arxiv AI"
  },
  {
    "title": "From Numbers to Prompts: A Cognitive Symbolic Transition Mechanism for Lightweight Time-Series Forecasting",
    "summary": "arXiv:2602.00088v1 Announce Type: cross Abstract: Large language models have achieved remarkable success in time series prediction tasks, but their substantial computational and memory requirements limit deployment on lightweight platforms. In this paper, we propose the Symbolic Transition Mechanism (STM) a novel framework that bridges numeric time",
    "url": "https://arxiv.org/abs/2602.00088",
    "source": "Arxiv AI"
  },
  {
    "title": "Generative Artificial Intelligence in Small and Medium Enterprises: Navigating its Promises and Challenges",
    "summary": "arXiv:2602.00091v1 Announce Type: cross Abstract: The latest technological developments in generative artificial intelligence (GAI) offer powerful capabilities to small and medium enterprises (SMEs), as they facilitate the democratization of both scalability and creativity. Even if they have little technical expertise or financial resources, SMEs c",
    "url": "https://arxiv.org/abs/2602.00091",
    "source": "Arxiv AI"
  },
  {
    "title": "Interpreting and Controlling Model Behavior via Constitutions for Atomic Concept Edits",
    "summary": "arXiv:2602.00092v1 Announce Type: cross Abstract: We introduce a black-box interpretability framework that learns a verifiable constitution: a natural language summary of how changes to a prompt affect a model's specific behavior, such as its alignment, correctness, or adherence to constraints. Our method leverages atomic concept edits (ACEs), whic",
    "url": "https://arxiv.org/abs/2602.00092",
    "source": "Arxiv AI"
  },
  {
    "title": "EDU-CIRCUIT-HW: Evaluating Multimodal Large Language Models on Real-World University-Level STEM Student Handwritten Solutions",
    "summary": "arXiv:2602.00095v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) hold significant promise for revolutionizing traditional education and reducing teachers' workload. However, accurately interpreting unconstrained STEM student handwritten solutions with intertwined mathematical formulas, diagrams, and textual reasoning poses",
    "url": "https://arxiv.org/abs/2602.00095",
    "source": "Arxiv AI"
  },
  {
    "title": "Mirage2Matter: A Physically Grounded Gaussian World Model from Video",
    "summary": "arXiv:2602.00096v1 Announce Type: cross Abstract: The scalability of embodied intelligence is fundamentally constrained by the scarcity of real-world interaction data. While simulation platforms provide a promising alternative, existing approaches often suffer from a substantial visual and physical gap to real environments and rely on expensive sen",
    "url": "https://arxiv.org/abs/2602.00096",
    "source": "Arxiv AI"
  },
  {
    "title": "Frequent Pattern Mining approach to Image Compression",
    "summary": "arXiv:2602.00100v1 Announce Type: cross Abstract: The paper focuses on Image Compression, explaining efficient approaches based on Frequent Pattern Mining(FPM). The proposed compression mechanism is based on clustering similar pixels in the image and thus using cluster identifiers in image compression. Redundant data in the image is effectively han",
    "url": "https://arxiv.org/abs/2602.00100",
    "source": "Arxiv AI"
  },
  {
    "title": "Radiomics in Medical Imaging: Methods, Applications, and Challenges",
    "summary": "arXiv:2602.00102v1 Announce Type: cross Abstract: Radiomics enables quantitative medical image analysis by converting imaging data into structured, high-dimensional feature representations for predictive modeling. Despite methodological developments and encouraging retrospective results, radiomics continue to face persistent challenges related to f",
    "url": "https://arxiv.org/abs/2602.00102",
    "source": "Arxiv AI"
  },
  {
    "title": "Autonomous Multi-Agent AI for High-Throughput Polymer Informatics: From Property Prediction to Generative Design Across Synthetic and Bio-Polymers",
    "summary": "arXiv:2602.00103v1 Announce Type: cross Abstract: We present an integrated multiagent AI ecosystem for polymer discovery that unifies high-throughput materials workflows, artificial intelligence, and computational modeling within a single Polymer Research Lifecycle (PRL) pipeline. The system orchestrates specialized agents powered by state-of-the-a",
    "url": "https://arxiv.org/abs/2602.00103",
    "source": "Arxiv AI"
  },
  {
    "title": "R3G: A Reasoning--Retrieval--Reranking Framework for Vision-Centric Answer Generation",
    "summary": "arXiv:2602.00104v1 Announce Type: cross Abstract: Vision-centric retrieval for VQA requires retrieving images to supply missing visual cues and integrating them into the reasoning process. However, selecting the right images and integrating them effectively into the model's reasoning remains challenging.To address this challenge, we propose R3G, a ",
    "url": "https://arxiv.org/abs/2602.00104",
    "source": "Arxiv AI"
  },
  {
    "title": "HYPE-EDIT-1: Benchmark for Measuring Reliability in Frontier Image Editing Models",
    "summary": "arXiv:2602.00105v1 Announce Type: cross Abstract: Public demos of image editing models are typically best-case samples; real workflows pay for retries and review time. We introduce HYPE-EDIT-1, a 100-task benchmark of reference-based marketing/design edits with binary pass/fail judging. For each task we generate 10 independent outputs to estimate p",
    "url": "https://arxiv.org/abs/2602.00105",
    "source": "Arxiv AI"
  },
  {
    "title": "SITUATE -- Synthetic Object Counting Dataset for VLM training",
    "summary": "arXiv:2602.00108v1 Announce Type: cross Abstract: We present SITUATE, a novel dataset designed for training and evaluating Vision Language Models on counting tasks with spatial constraints. The dataset bridges the gap between simple 2D datasets like VLMCountBench and often ambiguous real-life datasets like TallyQA, which lack control over occlusion",
    "url": "https://arxiv.org/abs/2602.00108",
    "source": "Arxiv AI"
  },
  {
    "title": "1S-DAug: One-Shot Data Augmentation for Robust Few-Shot Generalization",
    "summary": "arXiv:2602.00114v1 Announce Type: cross Abstract: Few-shot learning (FSL) challenges model generalization to novel classes based on just a few shots of labeled examples, a testbed where traditional test-time augmentations fail to be effective. We introduce 1S-DAug, a one-shot generative augmentation operator that synthesizes diverse yet faithful va",
    "url": "https://arxiv.org/abs/2602.00114",
    "source": "Arxiv AI"
  },
  {
    "title": "IC-EO: Interpretable Code-based assistant for Earth Observation",
    "summary": "arXiv:2602.00117v1 Announce Type: cross Abstract: Despite recent advances in computer vision, Earth Observation (EO) analysis remains difficult to perform for the laymen, requiring expert knowledge and technical capabilities. Furthermore, many systems return black-box predictions that are difficult to audit or reproduce. Leveraging recent advances ",
    "url": "https://arxiv.org/abs/2602.00117",
    "source": "Arxiv AI"
  },
  {
    "title": "VDE Bench: Evaluating The Capability of Image Editing Models to Modify Visual Documents",
    "summary": "arXiv:2602.00122v1 Announce Type: cross Abstract: In recent years, multimodal image editing models have achieved substantial progress, enabling users to manipulate visual content through natural language in a flexible and interactive manner. Nevertheless, an important yet insufficiently explored research direction remains visual document image edit",
    "url": "https://arxiv.org/abs/2602.00122",
    "source": "Arxiv AI"
  },
  {
    "title": "MiniTensor: A Lightweight, High-Performance Tensor Operations Library",
    "summary": "arXiv:2602.00125v1 Announce Type: cross Abstract: We present MiniTensor, an open source tensor operations library that focuses on minimalism, correctness, and performance. MiniTensor exposes a familiar PyTorch-like Python API while it executes performance critical code in a Rust engine. The core supports dense $n$ dimensional tensors, broadcasting,",
    "url": "https://arxiv.org/abs/2602.00125",
    "source": "Arxiv AI"
  },
  {
    "title": "PredictionMarketBench: A SWE-bench-Style Framework for Backtesting Trading Agents on Prediction Markets",
    "summary": "arXiv:2602.00133v1 Announce Type: cross Abstract: Prediction markets offer a natural testbed for trading agents: contracts have binary payoffs, prices can be interpreted as probabilities, and realized performance depends critically on market microstructure, fees, and settlement risk. We introduce PredictionMarketBench, a SWE-bench-style benchmark f",
    "url": "https://arxiv.org/abs/2602.00133",
    "source": "Arxiv AI"
  },
  {
    "title": "Scalable Analytic Classifiers with Associative Drift Compensation for Class-Incremental Learning of Vision Transformers",
    "summary": "arXiv:2602.00144v1 Announce Type: cross Abstract: Class-incremental learning (CIL) with Vision Transformers (ViTs) faces a major computational bottleneck during the classifier reconstruction phase, where most existing methods rely on costly iterative stochastic gradient descent (SGD). We observe that analytic Regularized Gaussian Discriminant Analy",
    "url": "https://arxiv.org/abs/2602.00144",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
    "summary": "arXiv:2602.00148v1 Announce Type: cross Abstract: Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining",
    "url": "https://arxiv.org/abs/2602.00148",
    "source": "Arxiv AI"
  },
  {
    "title": "Reversible Diffusion Decoding for Diffusion Language Models",
    "summary": "arXiv:2602.00150v1 Announce Type: cross Abstract: Diffusion language models enable parallel token generation through block-wise decoding, but their irreversible commitments can lead to stagnation, where the reverse diffusion process fails to make further progress under a suboptimal context.We propose Reversible Diffusion Decoding (RDD), a decoding ",
    "url": "https://arxiv.org/abs/2602.00150",
    "source": "Arxiv AI"
  },
  {
    "title": "Investigating the Impact of Histopathological Foundation Models on Regressive Prediction of Homologous Recombination Deficiency",
    "summary": "arXiv:2602.00151v1 Announce Type: cross Abstract: Foundation models pretrained on large-scale histopathology data have found great success in various fields of computational pathology, but their impact on regressive biomarker prediction remains underexplored. In this work, we systematically evaluate histopathological foundation models for regressio",
    "url": "https://arxiv.org/abs/2602.00151",
    "source": "Arxiv AI"
  },
  {
    "title": "Real-Time Human Activity Recognition on Edge Microcontrollers: Dynamic Hierarchical Inference with Multi-Spectral Sensor Fusion",
    "summary": "arXiv:2602.00152v1 Announce Type: cross Abstract: The demand for accurate on-device pattern recognition in edge applications is intensifying, yet existing approaches struggle to reconcile accuracy with computational constraints. To address this challenge, a resource-aware hierarchical network based on multi-spectral fusion and interpretable modules",
    "url": "https://arxiv.org/abs/2602.00152",
    "source": "Arxiv AI"
  },
  {
    "title": "See Without Decoding: Motion-Vector-Based Tracking in Compressed Video",
    "summary": "arXiv:2602.00153v1 Announce Type: cross Abstract: We propose a lightweight compressed-domain tracking model that operates directly on video streams, without requiring full RGB video decoding. Using motion vectors and transform coefficients from compressed data, our deep model propagates object bounding boxes across frames, achieving a computational",
    "url": "https://arxiv.org/abs/2602.00153",
    "source": "Arxiv AI"
  },
  {
    "title": "ReasoningBomb: A Stealthy Denial-of-Service Attack by Inducing Pathologically Long Reasoning in Large Reasoning Models",
    "summary": "arXiv:2602.00154v1 Announce Type: cross Abstract: Large reasoning models (LRMs) extend large language models with explicit multi-step reasoning traces, but this capability introduces a new class of prompt-induced inference-time denial-of-service (PI-DoS) attacks that exploit the high computational cost of reasoning. We first formalize inference cos",
    "url": "https://arxiv.org/abs/2602.00154",
    "source": "Arxiv AI"
  },
  {
    "title": "ProDCARL: Reinforcement Learning-Aligned Diffusion Models for De Novo Antimicrobial Peptide Design",
    "summary": "arXiv:2602.00157v1 Announce Type: cross Abstract: Antimicrobial resistance threatens healthcare sustainability and motivates low-cost computational discovery of antimicrobial peptides (AMPs). De novo peptide generation must optimize antimicrobial activity and safety through low predicted toxicity, but likelihood-trained generators do not enforce th",
    "url": "https://arxiv.org/abs/2602.00157",
    "source": "Arxiv AI"
  },
  {
    "title": "RAPTOR: Ridge-Adaptive Logistic Probes",
    "summary": "arXiv:2602.00158v1 Announce Type: cross Abstract: Probing studies what information is encoded in a frozen LLM's layer representations by training a lightweight predictor on top of them. Beyond analysis, probes are often used operationally in probe-then-steer pipelines: a learned concept vector is extracted from a probe and injected via additive act",
    "url": "https://arxiv.org/abs/2602.00158",
    "source": "Arxiv AI"
  },
  {
    "title": "Sheaf Neural Networks and biomedical applications",
    "summary": "arXiv:2602.00159v1 Announce Type: cross Abstract: The purpose of this paper is to elucidate the theory and mathematical modelling behind the sheaf neural network (SNN) algorithm and then show how SNN can effectively answer to biomedical questions in a concrete case study and outperform the most popular graph neural networks (GNNs) as graph convolut",
    "url": "https://arxiv.org/abs/2602.00159",
    "source": "Arxiv AI"
  },
  {
    "title": "Block removal for large language models through constrained binary optimization",
    "summary": "arXiv:2602.00161v1 Announce Type: cross Abstract: Compressing resource-intensive large language models by removing whole transformer blocks is a seemingly simple idea, but identifying which blocks to remove constitutes an exponentially difficult combinatorial problem. In this paper, we formulate block removal as a constrained binary optimization pr",
    "url": "https://arxiv.org/abs/2602.00161",
    "source": "Arxiv AI"
  },
  {
    "title": "Why Are AI Agent Involved Pull Requests (Fix-Related) Remain Unmerged? An Empirical Study",
    "summary": "arXiv:2602.00164v1 Announce Type: cross Abstract: Autonomous coding agents (e.g., OpenAI Codex, Devin, GitHub Copilot) are increasingly used to generate fix-related pull requests (PRs) in real world software repositories. However, their practical effectiveness depends on whether these contributions are accepted and merged by project maintainers. In",
    "url": "https://arxiv.org/abs/2602.00164",
    "source": "Arxiv AI"
  },
  {
    "title": "Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints",
    "summary": "arXiv:2602.00166v1 Announce Type: cross Abstract: Locally deployed Small Language Models (SLMs) must continually support diverse tasks under strict memory and computation constraints, making selective reliance on cloud Large Language Models (LLMs) unavoidable. Regulating cloud assistance during continual learning is challenging, as naive reward-bas",
    "url": "https://arxiv.org/abs/2602.00166",
    "source": "Arxiv AI"
  },
  {
    "title": "Towards Agentic Intelligence for Materials Science",
    "summary": "arXiv:2602.00169v1 Announce Type: cross Abstract: The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey ad",
    "url": "https://arxiv.org/abs/2602.00169",
    "source": "Arxiv AI"
  },
  {
    "title": "The Blessing of Dimensionality in LLM Fine-tuning: A Variance-Curvature Perspective",
    "summary": "arXiv:2602.00170v1 Announce Type: cross Abstract: Weight-perturbation evolution strategies (ES) can fine-tune billion-parameter language models with surprisingly small populations (e.g., $N\\!\\approx\\!30$), contradicting classical zeroth-order curse-of-dimensionality intuition. We also observe a second seemingly separate phenomenon: under fixed hype",
    "url": "https://arxiv.org/abs/2602.00170",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning Robust Reasoning through Guided Adversarial Self-Play",
    "summary": "arXiv:2602.00173v1 Announce Type: cross Abstract: Reinforcement learning from verifiable rewards (RLVR) produces strong reasoning models, yet they can fail catastrophically when the conditioning context is fallible (e.g., corrupted chain-of-thought, misleading partial solutions, or mild input perturbations), since standard RLVR optimizes final-answ",
    "url": "https://arxiv.org/abs/2602.00173",
    "source": "Arxiv AI"
  },
  {
    "title": "The Illusion of Forgetting: Attack Unlearned Diffusion via Initial Latent Variable Optimization",
    "summary": "arXiv:2602.00175v1 Announce Type: cross Abstract: Although unlearning-based defenses claim to purge Not-Safe-For-Work (NSFW) concepts from diffusion models (DMs), we reveals that this \"forgetting\" is largely an illusion. Unlearning partially disrupts the mapping between linguistic symbols and the underlying knowledge, which remains intact as dorman",
    "url": "https://arxiv.org/abs/2602.00175",
    "source": "Arxiv AI"
  },
  {
    "title": "Stabilizing Diffusion Posterior Sampling by Noise--Frequency Continuation",
    "summary": "arXiv:2602.00176v1 Announce Type: cross Abstract: Diffusion posterior sampling solves inverse problems by combining a pretrained diffusion prior with measurement-consistency guidance, but it often fails to recover fine details because measurement terms are applied in a manner that is weakly coupled to the diffusion noise level. At high noise, data-",
    "url": "https://arxiv.org/abs/2602.00176",
    "source": "Arxiv AI"
  },
  {
    "title": "Spec-Driven Development:From Code to Contract in the Age of AI Coding Assistants",
    "summary": "arXiv:2602.00180v1 Announce Type: cross Abstract: The rise of AI coding assistants has reignited interest in an old idea: what if specifications-not code-were the primary artifact of software development? Spec-driven development (SDD) inverts the traditional workflow by treating specifications as the source of truth and code as a generated or verif",
    "url": "https://arxiv.org/abs/2602.00180",
    "source": "Arxiv AI"
  },
  {
    "title": "CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning",
    "summary": "arXiv:2602.00181v1 Announce Type: cross Abstract: Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. We p",
    "url": "https://arxiv.org/abs/2602.00181",
    "source": "Arxiv AI"
  },
  {
    "title": "EigenAI: Deterministic Inference, Verifiable Results",
    "summary": "arXiv:2602.00182v1 Announce Type: cross Abstract: EigenAI is a verifiable AI platform built on top of the EigenLayer restaking ecosystem. At a high level, it combines a deterministic large-language model (LLM) inference engine with a cryptoeconomically secured optimistic re-execution protocol so that every inference result can be publicly audited, ",
    "url": "https://arxiv.org/abs/2602.00182",
    "source": "Arxiv AI"
  },
  {
    "title": "Visible Singularities Guided Correlation Network for Limited-Angle CT Reconstruction",
    "summary": "arXiv:2602.00184v1 Announce Type: cross Abstract: Limited-angle computed tomography (LACT) offers the advantages of reduced radiation dose and shortened scanning time. Traditional reconstruction algorithms exhibit various inherent limitations in LACT. Currently, most deep learning-based LACT reconstruction methods focus on multi-domain fusion or th",
    "url": "https://arxiv.org/abs/2602.00184",
    "source": "Arxiv AI"
  },
  {
    "title": "QUASAR: A Universal Autonomous System for Atomistic Simulation and a Benchmark of Its Capabilities",
    "summary": "arXiv:2602.00185v1 Announce Type: cross Abstract: The integration of large language models (LLMs) into materials science offers a transformative opportunity to streamline computational workflows, yet current agentic systems remain constrained by rigid tool-calling approaches and narrowly scoped agents. In this work, we introduce QUASAR, a universal",
    "url": "https://arxiv.org/abs/2602.00185",
    "source": "Arxiv AI"
  },
  {
    "title": "LPIPS-AttnWav2Lip: Generic Audio-Driven lip synchronization for Talking Head Generation in the Wild",
    "summary": "arXiv:2602.00189v1 Announce Type: cross Abstract: Researchers have shown a growing interest in Audio-driven Talking Head Generation. The primary challenge in talking head generation is achieving audio-visual coherence between the lips and the audio, known as lip synchronization. This paper proposes a generic method, LPIPS-AttnWav2Lip, for reconstru",
    "url": "https://arxiv.org/abs/2602.00189",
    "source": "Arxiv AI"
  },
  {
    "title": "AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange",
    "summary": "arXiv:2602.00192v1 Announce Type: cross Abstract: Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We s",
    "url": "https://arxiv.org/abs/2602.00192",
    "source": "Arxiv AI"
  },
  {
    "title": "On the calibration of survival models with competing risks",
    "summary": "arXiv:2602.00194v1 Announce Type: cross Abstract: Survival analysis deals with modeling the time until an event occurs, and accurate probability estimates are crucial for decision-making, particularly in the competing-risks setting where multiple events are possible. While recent work has addressed calibration in standard survival analysis, the com",
    "url": "https://arxiv.org/abs/2602.00194",
    "source": "Arxiv AI"
  },
  {
    "title": "Rank-and-Reason: Multi-Agent Collaboration Accelerates Zero-Shot Protein Mutation Prediction",
    "summary": "arXiv:2602.00197v2 Announce Type: cross Abstract: Zero-shot mutation prediction is vital for low-resource protein engineering, yet existing protein language models (PLMs) often yield statistically confident results that ignore fundamental biophysical constraints. Currently, selecting candidates for wet-lab validation relies on manual expert auditin",
    "url": "https://arxiv.org/abs/2602.00197",
    "source": "Arxiv AI"
  },
  {
    "title": "SCALED : Surrogate-gradient for Codec-Aware Learning of Downsampling in ABR Streaming",
    "summary": "arXiv:2602.00198v1 Announce Type: cross Abstract: The rapid growth in video consumption has introduced significant challenges to modern streaming architectures. Over-the-Top (OTT) video delivery now predominantly relies on Adaptive Bitrate (ABR) streaming, which dynamically adjusts bitrate and resolution based on client-side constraints such as dis",
    "url": "https://arxiv.org/abs/2602.00198",
    "source": "Arxiv AI"
  },
  {
    "title": "Vision-Language Model Purified Semi-Supervised Semantic Segmentation for Remote Sensing Images",
    "summary": "arXiv:2602.00202v1 Announce Type: cross Abstract: The semi-supervised semantic segmentation (S4) can learn rich visual knowledge from low-cost unlabeled images. However, traditional S4 architectures all face the challenge of low-quality pseudo-labels, especially for the teacher-student framework.We propose a novel SemiEarth model that introduces vi",
    "url": "https://arxiv.org/abs/2602.00202",
    "source": "Arxiv AI"
  },
  {
    "title": "Semantic-Aware Advanced Persistent Threat Detection Using Autoencoders on LLM-Encoded System Logs",
    "summary": "arXiv:2602.00204v1 Announce Type: cross Abstract: Advanced Persistent Threats (APTs) are among the most challenging cyberattacks to detect. They are carried out by highly skilled attackers who carefully study their targets and operate in a stealthy, long-term manner. Because APTs exhibit \"low-and-slow\" behavior, traditional statistical methods and ",
    "url": "https://arxiv.org/abs/2602.00204",
    "source": "Arxiv AI"
  },
  {
    "title": "Analyzing Shapley Additive Explanations to Understand Anomaly Detection Algorithm Behaviors and Their Complementarity",
    "summary": "arXiv:2602.00208v1 Announce Type: cross Abstract: Unsupervised anomaly detection is a challenging problem due to the diversity of data distributions and the lack of labels. Ensemble methods are often adopted to mitigate these challenges by combining multiple detectors, which can reduce individual biases and increase robustness. Yet building an ense",
    "url": "https://arxiv.org/abs/2602.00208",
    "source": "Arxiv AI"
  },
  {
    "title": "Interpretable Unsupervised Deformable Image Registration via Confidence-bound Multi-Hop Visual Reasoning",
    "summary": "arXiv:2602.00211v1 Announce Type: cross Abstract: Unsupervised deformable image registration requires aligning complex anatomical structures without reference labels, making interpretability and reliability critical. Existing deep learning methods achieve considerable accuracy but often lack transparency, leading to error drift and reduced clinical",
    "url": "https://arxiv.org/abs/2602.00211",
    "source": "Arxiv AI"
  },
  {
    "title": "TessPay: Verify-then-Pay Infrastructure for Trusted Agentic Commerce",
    "summary": "arXiv:2602.00213v1 Announce Type: cross Abstract: The global economy is entering the era of Agentic Commerce, where autonomous agents can discover services, negotiate prices, and transact value. However adoption towards agentic commerce faces a foundational trust gap: current systems are built for direct human interactions rather than agent-driven ",
    "url": "https://arxiv.org/abs/2602.00213",
    "source": "Arxiv AI"
  },
  {
    "title": "A Geometric Multimodal Foundation Model Integrating Bp-MRI and Clinical Reports in Prostate Cancer Classification",
    "summary": "arXiv:2602.00214v1 Announce Type: cross Abstract: Prostate cancer (PCa) is one of the most common cancers in men worldwide. Bi-parametric MRI (bp-MRI) and clinical variables are crucial for PCa identification and improving treatment decisions. However, this process is subjective to expert interpretations. Furthermore, most existing computer-aided d",
    "url": "https://arxiv.org/abs/2602.00214",
    "source": "Arxiv AI"
  },
  {
    "title": "Tri-LLM Cooperative Federated Zero-Shot Intrusion Detection with Semantic Disagreement and Trust-Aware Aggregation",
    "summary": "arXiv:2602.00219v1 Announce Type: cross Abstract: Federated learning (FL) has become an effective paradigm for privacy-preserving, distributed Intrusion Detection Systems (IDS) in cyber-physical and Internet of Things (IoT) networks, where centralized data aggregation is often infeasible due to privacy and bandwidth constraints. Despite its advanta",
    "url": "https://arxiv.org/abs/2602.00219",
    "source": "Arxiv AI"
  },
  {
    "title": "MapDream: Task-Driven Map Learning for Vision-Language Navigation",
    "summary": "arXiv:2602.00222v2 Announce Type: cross Abstract: Vision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception. However, most existing approaches rely on hand-crafted maps constructed independently",
    "url": "https://arxiv.org/abs/2602.00222",
    "source": "Arxiv AI"
  },
  {
    "title": "DIVERGE: Diversity-Enhanced RAG for Open-Ended Information Seeking",
    "summary": "arXiv:2602.00238v1 Announce Type: cross Abstract: Existing retrieval-augmented generation (RAG) systems are primarily designed under the assumption that each query has a single correct answer. This overlooks common information-seeking scenarios with multiple plausible answers, where diversity is essential to avoid collapsing to a single dominant re",
    "url": "https://arxiv.org/abs/2602.00238",
    "source": "Arxiv AI"
  },
  {
    "title": "SANEval: Open-Vocabulary Compositional Benchmarks with Failure-mode Diagnosis",
    "summary": "arXiv:2602.00249v1 Announce Type: cross Abstract: The rapid progress of text-to-image (T2I) models has unlocked unprecedented creative potential, yet their ability to faithfully render complex prompts involving multiple objects, attributes, and spatial relationships remains a significant bottleneck. Progress is hampered by a lack of adequate evalua",
    "url": "https://arxiv.org/abs/2602.00249",
    "source": "Arxiv AI"
  },
  {
    "title": "TABES: Trajectory-Aware Backward-on-Entropy Steering for Masked Diffusion Models",
    "summary": "arXiv:2602.00250v1 Announce Type: cross Abstract: Masked Diffusion Models (MDMs) have emerged as a promising non-autoregressive paradigm for generative tasks, offering parallel decoding and bidirectional context utilization. However, current sampling methods rely on simple confidence-based heuristics that ignore the long-term impact of local decisi",
    "url": "https://arxiv.org/abs/2602.00250",
    "source": "Arxiv AI"
  },
  {
    "title": "Intelligent Reasoning Cues: A Framework and Case Study of the Roles of AI Information in Complex Decisions",
    "summary": "arXiv:2602.00259v1 Announce Type: cross Abstract: Artificial intelligence (AI)-based decision support systems can be highly accurate yet still fail to support users or improve decisions. Existing theories of AI-assisted decision-making focus on calibrating reliance on AI advice, leaving it unclear how different system designs might influence the re",
    "url": "https://arxiv.org/abs/2602.00259",
    "source": "Arxiv AI"
  },
  {
    "title": "Subspace Clustering on Incomplete Data with Self-Supervised Contrastive Learning",
    "summary": "arXiv:2602.00262v1 Announce Type: cross Abstract: Subspace clustering aims to group data points that lie in a union of low-dimensional subspaces and finds wide application in computer vision, hyperspectral imaging, and recommendation systems. However, most existing methods assume fully observed data, limiting their effectiveness in real-world scena",
    "url": "https://arxiv.org/abs/2602.00262",
    "source": "Arxiv AI"
  },
  {
    "title": "PLACID: Identity-Preserving Multi-Object Compositing via Video Diffusion with Synthetic Trajectories",
    "summary": "arXiv:2602.00267v1 Announce Type: cross Abstract: Recent advances in generative AI have dramatically improved photorealistic image synthesis, yet they fall short for studio-level multi-object compositing. This task demands simultaneous (i) near-perfect preservation of each item's identity, (ii) precise background and color fidelity, (iii) layout an",
    "url": "https://arxiv.org/abs/2602.00267",
    "source": "Arxiv AI"
  },
  {
    "title": "TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation",
    "summary": "arXiv:2602.00268v1 Announce Type: cross Abstract: Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothe",
    "url": "https://arxiv.org/abs/2602.00268",
    "source": "Arxiv AI"
  },
  {
    "title": "VoxServe: Streaming-Centric Serving System for Speech Language Models",
    "summary": "arXiv:2602.00269v1 Announce Type: cross Abstract: Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency, high throughput, and strong guarantees of streamability. Existing systems fall short of supporting diverse models flexibly and efficiently. We present VoxServe, a unified serving syst",
    "url": "https://arxiv.org/abs/2602.00269",
    "source": "Arxiv AI"
  },
  {
    "title": "Training LLMs with Fault Tolerant HSDP on 100,000 GPUs",
    "summary": "arXiv:2602.00277v1 Announce Type: cross Abstract: Large-scale training systems typically use synchronous training, requiring all GPUs to be healthy simultaneously. In our experience training on O(100K) GPUs, synchronous training results in a low efficiency due to frequent failures and long recovery time. To address this problem, we propose a novel ",
    "url": "https://arxiv.org/abs/2602.00277",
    "source": "Arxiv AI"
  },
  {
    "title": "Sample Complexity Analysis for Constrained Bilevel Reinforcement Learning",
    "summary": "arXiv:2602.00282v1 Announce Type: cross Abstract: Several important problem settings within the literature of reinforcement learning (RL), such as meta-learning, hierarchical learning, and RL from human feedback (RL-HF), can be modelled as bilevel RL problems. A lot has been achieved in these domains empirically; however, the theoretical analysis o",
    "url": "https://arxiv.org/abs/2602.00282",
    "source": "Arxiv AI"
  },
  {
    "title": "TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs",
    "summary": "arXiv:2602.00288v1 Announce Type: cross Abstract: Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal u",
    "url": "https://arxiv.org/abs/2602.00288",
    "source": "Arxiv AI"
  },
  {
    "title": "LogicGaze: Benchmarking Causal Consistency in Visual Narratives via Counterfactual Verification",
    "summary": "arXiv:2602.00292v1 Announce Type: cross Abstract: While sequential reasoning enhances the capability of Vision-Language Models (VLMs) to execute complex multimodal tasks, their reliability in grounding these reasoning chains within actual visual evidence remains insufficiently explored. We introduce LogicGaze, a novel benchmark framework designed t",
    "url": "https://arxiv.org/abs/2602.00292",
    "source": "Arxiv AI"
  },
  {
    "title": "Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation",
    "summary": "arXiv:2602.00294v1 Announce Type: cross Abstract: The most widely used artificial intelligence (AI) models today are Transformers employing self-attention. In its standard form, self-attention incurs costs that increase with context length, driving demand for storage, compute, and energy that is now outstripping society's ability to provide them. T",
    "url": "https://arxiv.org/abs/2602.00294",
    "source": "Arxiv AI"
  },
  {
    "title": "Multi-Speaker Conversational Audio Deepfake: Taxonomy, Dataset and Pilot Study",
    "summary": "arXiv:2602.00295v1 Announce Type: cross Abstract: The rapid advances in text-to-speech (TTS) technologies have made audio deepfakes increasingly realistic and accessible, raising significant security and trust concerns. While existing research has largely focused on detecting single-speaker audio deepfakes, real-world malicious applications with mu",
    "url": "https://arxiv.org/abs/2602.00295",
    "source": "Arxiv AI"
  },
  {
    "title": "Semantics-Preserving Evasion of LLM Vulnerability Detectors",
    "summary": "arXiv:2602.00305v1 Announce Type: cross Abstract: LLM-based vulnerability detectors are increasingly deployed in security-critical code review, yet their resilience to evasion under behavior-preserving edits remains poorly understood. We evaluate detection-time integrity under a semantics-preserving threat model by instantiating diverse behavior-pr",
    "url": "https://arxiv.org/abs/2602.00305",
    "source": "Arxiv AI"
  },
  {
    "title": "Opportunistic Promptable Segmentation: Leveraging Routine Radiological Annotations to Guide 3D CT Lesion Segmentation",
    "summary": "arXiv:2602.00309v1 Announce Type: cross Abstract: The development of machine learning models for CT imaging depends on the availability of large, high-quality, and diverse annotated datasets. Although large volumes of CT images and reports are readily available in clinical picture archiving and communication systems (PACS), 3D segmentations of crit",
    "url": "https://arxiv.org/abs/2602.00309",
    "source": "Arxiv AI"
  },
  {
    "title": "Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors",
    "summary": "arXiv:2602.00315v1 Announce Type: cross Abstract: How close are neural networks to the best they could possibly do? Standard benchmarks cannot answer this because they lack access to the true posterior p(y|x). We use class-conditional normalizing flows as oracles that make exact posteriors tractable on realistic images (AFHQ, ImageNet). This enable",
    "url": "https://arxiv.org/abs/2602.00315",
    "source": "Arxiv AI"
  },
  {
    "title": "Optimal Transport-Guided Adversarial Attacks on Graph Neural Network-Based Bot Detection",
    "summary": "arXiv:2602.00318v1 Announce Type: cross Abstract: The rise of bot accounts on social media poses significant risks to public discourse. To address this threat, modern bot detectors increasingly rely on Graph Neural Networks (GNNs). However, the effectiveness of these GNN-based detectors in real-world settings remains poorly understood. In practice,",
    "url": "https://arxiv.org/abs/2602.00318",
    "source": "Arxiv AI"
  },
  {
    "title": "Detecting AI-Generated Content in Academic Peer Reviews",
    "summary": "arXiv:2602.00319v1 Announce Type: cross Abstract: The growing availability of large language models (LLMs) has raised questions about their role in academic peer review. This study examines the temporal emergence of AI-generated content in peer reviews by applying a detection model trained on historical reviews to later review cycles at Internation",
    "url": "https://arxiv.org/abs/2602.00319",
    "source": "Arxiv AI"
  },
  {
    "title": "In-Run Data Shapley for Adam Optimizer",
    "summary": "arXiv:2602.00329v1 Announce Type: cross Abstract: Reliable data attribution is essential for mitigating bias and reducing computational waste in modern machine learning, with the Shapley value serving as the theoretical gold standard. While recent \"In-Run\" methods bypass the prohibitive cost of retraining by estimating contributions dynamically, th",
    "url": "https://arxiv.org/abs/2602.00329",
    "source": "Arxiv AI"
  },
  {
    "title": "Bridging the Semantic Chasm: Synergistic Conceptual Anchoring for Generalized Few-Shot and Zero-Shot OOD Perception",
    "summary": "arXiv:2602.00340v1 Announce Type: cross Abstract: This manuscript presents a pioneering Synergistic Neural Agents Network (SynerNet) framework designed to mitigate the phenomenon of cross-modal alignment degeneration in Vision-Language Models (VLMs) when encountering Out-of-Distribution (OOD) concepts. Specifically, four specialized computational u",
    "url": "https://arxiv.org/abs/2602.00340",
    "source": "Arxiv AI"
  },
  {
    "title": "Standardized Methods and Recommendations for Green Federated Learning",
    "summary": "arXiv:2602.00343v1 Announce Type: cross Abstract: Federated learning (FL) enables collaborative model training over privacy-sensitive, distributed data, but its environmental impact is difficult to compare across studies due to inconsistent measurement boundaries and heterogeneous reporting. We present a practical carbon-accounting methodology for ",
    "url": "https://arxiv.org/abs/2602.00343",
    "source": "Arxiv AI"
  },
  {
    "title": "When RAG Hurts: Diagnosing and Mitigating Attention Distraction in Retrieval-Augmented LVLMs",
    "summary": "arXiv:2602.00344v1 Announce Type: cross Abstract: While Retrieval-Augmented Generation (RAG) is one of the dominant paradigms for enhancing Large Vision-Language Models (LVLMs) on knowledge-based VQA tasks, recent work attributes RAG failures to insufficient attention towards the retrieved context, proposing to reduce the attention allocated to ima",
    "url": "https://arxiv.org/abs/2602.00344",
    "source": "Arxiv AI"
  },
  {
    "title": "AdaFuse: Adaptive Multimodal Fusion for Lung Cancer Risk Prediction via Reinforcement Learning",
    "summary": "arXiv:2602.00347v1 Announce Type: cross Abstract: Multimodal fusion has emerged as a promising paradigm for disease diagnosis and prognosis, integrating complementary information from heterogeneous data sources such as medical images, clinical records, and radiology reports. However, existing fusion methods process all available modalities through ",
    "url": "https://arxiv.org/abs/2602.00347",
    "source": "Arxiv AI"
  },
  {
    "title": "Post-Training Probability Manifold Correction via Structured SVD Pruning and Self-Referential Distillation",
    "summary": "arXiv:2602.00372v1 Announce Type: cross Abstract: Large language models are expensive to deploy. We introduce Sparse Knowledge Distillation (SparseKD), a post-training method that compresses transformer models by combining structured SVD pruning with self-referential knowledge distillation. The key insight is simple: instead of using an external te",
    "url": "https://arxiv.org/abs/2602.00372",
    "source": "Arxiv AI"
  },
  {
    "title": "Generalized Inverses of Matrix Products: From Fundamental Subspaces to Randomized Decompositions",
    "summary": "arXiv:2602.00386v1 Announce Type: cross Abstract: We investigate the Moore-Penrose pseudoinverse and generalized inverse of a matrix product $A=CR$ to establish a unifying framework for generalized and randomized matrix inverses. This analysis is rooted in first principles, focusing on the geometry of the four fundamental subspaces. We examine: (1)",
    "url": "https://arxiv.org/abs/2602.00386",
    "source": "Arxiv AI"
  },
  {
    "title": "Fast Forward: Accelerating LLM Prefill with Predictive FFN Sparsity",
    "summary": "arXiv:2602.00397v1 Announce Type: cross Abstract: The prefill stage of large language model (LLM) inference is a key computational bottleneck for long-context workloads. At short-to-moderate context lengths (1K--16K tokens), Feed-Forward Networks (FFNs) dominate this cost, accounting for most of the total FLOPs. Existing FFN sparsification methods,",
    "url": "https://arxiv.org/abs/2602.00397",
    "source": "Arxiv AI"
  },
  {
    "title": "ZEST: Zero-shot Embodied Skill Transfer for Athletic Robot Control",
    "summary": "arXiv:2602.00401v1 Announce Type: cross Abstract: Achieving robust, human-like whole-body control on humanoid robots for agile, contact-rich behaviors remains a central challenge, demanding heavy per-skill engineering and a brittle process of tuning controllers. We introduce ZEST (Zero-shot Embodied Skill Transfer), a streamlined motion-imitation f",
    "url": "https://arxiv.org/abs/2602.00401",
    "source": "Arxiv AI"
  },
  {
    "title": "A Conditional Companion: Lived Experiences of People with Mental Health Disorders Using LLMs",
    "summary": "arXiv:2602.00402v1 Announce Type: cross Abstract: Large Language Models (LLMs) are increasingly used for mental health support, yet little is known about how people with mental health challenges engage with them, how they evaluate their usefulness, and what design opportunities they envision. We conducted 20 semi-structured interviews with people i",
    "url": "https://arxiv.org/abs/2602.00402",
    "source": "Arxiv AI"
  },
  {
    "title": "Variational Approach for Job Shop Scheduling",
    "summary": "arXiv:2602.00408v2 Announce Type: cross Abstract: This paper proposes a novel Variational Graph-to-Scheduler (VG2S) framework for solving the Job Shop Scheduling Problem (JSSP), a critical task in manufacturing that directly impacts operational efficiency and resource utilization. Conventional Deep Reinforcement Learning (DRL) approaches often face",
    "url": "https://arxiv.org/abs/2602.00408",
    "source": "Arxiv AI"
  },
  {
    "title": "Robustness of AutoML on Dirty Categorical Data",
    "summary": "arXiv:2602.00412v1 Announce Type: cross Abstract: The goal of automated machine learning (AutoML) is to reduce trial and error when doing machine learning (ML). Although AutoML methods for classification are able to deal with data imperfections, such as outliers, multiple scales and missing data, their behavior is less known on dirty categorical da",
    "url": "https://arxiv.org/abs/2602.00412",
    "source": "Arxiv AI"
  },
  {
    "title": "Text is All You Need for Vision-Language Model Jailbreaking",
    "summary": "arXiv:2602.00420v1 Announce Type: cross Abstract: Large Vision-Language Models (LVLMs) are increasingly equipped with robust safety safeguards to prevent responses to harmful or disallowed prompts. However, these defenses often focus on analyzing explicit textual inputs or relevant visual scenes. In this work, we introduce Text-DJ, a novel jailbrea",
    "url": "https://arxiv.org/abs/2602.00420",
    "source": "Arxiv AI"
  },
  {
    "title": "LLMs as High-Dimensional Nonlinear Autoregressive Models with Attention: Training, Alignment and Inference",
    "summary": "arXiv:2602.00426v1 Announce Type: cross Abstract: Large language models (LLMs) based on transformer architectures are typically described through collections of architectural components and training procedures, obscuring their underlying computational structure. This review article provides a concise mathematical reference for researchers seeking a",
    "url": "https://arxiv.org/abs/2602.00426",
    "source": "Arxiv AI"
  },
  {
    "title": "When Agents \"Misremember\" Collectively: Exploring the Mandela Effect in LLM-based Multi-Agent Systems",
    "summary": "arXiv:2602.00428v1 Announce Type: cross Abstract: Recent advancements in large language models (LLMs) have significantly enhanced the capabilities of collaborative multi-agent systems, enabling them to address complex challenges. However, within these multi-agent systems, the susceptibility of agents to collective cognitive biases remains an undere",
    "url": "https://arxiv.org/abs/2602.00428",
    "source": "Arxiv AI"
  },
  {
    "title": "LatentTrack: Sequential Weight Generation via Latent Filtering",
    "summary": "arXiv:2602.00458v1 Announce Type: cross Abstract: We introduce LatentTrack (LT), a sequential neural architecture for online probabilistic prediction under nonstationary dynamics. LT performs causal Bayesian filtering in a low-dimensional latent space and uses a lightweight hypernetwork to generate predictive model parameters at each time step, ena",
    "url": "https://arxiv.org/abs/2602.00458",
    "source": "Arxiv AI"
  },
  {
    "title": "LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs",
    "summary": "arXiv:2602.00462v1 Announce Type: cross Abstract: Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily proce",
    "url": "https://arxiv.org/abs/2602.00462",
    "source": "Arxiv AI"
  },
  {
    "title": "PAIR-Former: Budgeted Relational MIL for miRNA Target Prediction",
    "summary": "arXiv:2602.00465v1 Announce Type: cross Abstract: Functional miRNA--mRNA targeting is a large-bag prediction problem: each transcript yields a heavy-tailed pool of candidate target sites (CTSs), yet only a pair-level label is observed. We formalize this regime as \\emph{Budgeted Relational Multi-Instance Learning (BR-MIL)}, where at most $K$ instanc",
    "url": "https://arxiv.org/abs/2602.00465",
    "source": "Arxiv AI"
  },
  {
    "title": "Words that make SENSE: Sensorimotor Norms in Learned Lexical Token Representations",
    "summary": "arXiv:2602.00469v1 Announce Type: cross Abstract: While word embeddings derive meaning from co-occurrence patterns, human language understanding is grounded in sensory and motor experience. We present $\\text{SENSE}$ $(\\textbf{S}\\text{ensorimotor }$ $\\textbf{E}\\text{mbedding }$ $\\textbf{N}\\text{orm }$ $\\textbf{S}\\text{coring }$ $\\textbf{E}\\text{ngin",
    "url": "https://arxiv.org/abs/2602.00469",
    "source": "Arxiv AI"
  },
  {
    "title": "Quantum Phase Recognition via Quantum Attention Mechanism",
    "summary": "arXiv:2602.00473v1 Announce Type: cross Abstract: Quantum phase transitions in many-body systems are fundamentally characterized by complex correlation structures, which pose computational challenges for conventional methods in large systems. To address this, we propose a hybrid quantum-classical attention model. This model uses an attention mechan",
    "url": "https://arxiv.org/abs/2602.00473",
    "source": "Arxiv AI"
  },
  {
    "title": "Quality-Diversity Optimization as Multi-Objective Optimization",
    "summary": "arXiv:2602.00478v1 Announce Type: cross Abstract: The Quality-Diversity (QD) optimization aims to discover a collection of high-performing solutions that simultaneously exhibit diverse behaviors within a user-defined behavior space. This paradigm has stimulated significant research interest and demonstrated practical utility in domains including ro",
    "url": "https://arxiv.org/abs/2602.00478",
    "source": "Arxiv AI"
  },
  {
    "title": "From Junior to Senior: Allocating Agency and Navigating Professional Growth in Agentic AI-Mediated Software Engineering",
    "summary": "arXiv:2602.00496v1 Announce Type: cross Abstract: Juniors enter as AI-natives, seniors adapted mid-career. AI is not just changing how engineers code-it is reshaping who holds agency across work and professional growth. We contribute junior-senior accounts on their usage of agentic AI through a three-phase mixed-methods study: ACTA combined with a ",
    "url": "https://arxiv.org/abs/2602.00496",
    "source": "Arxiv AI"
  },
  {
    "title": "Culturally-Grounded Governance for Multilingual Language Models: Rights, Data Boundaries, and Accountable AI Design",
    "summary": "arXiv:2602.00497v1 Announce Type: cross Abstract: Multilingual large language models (MLLMs) are increasingly deployed across cultural, linguistic, and political contexts, yet existing governance frameworks largely assume English-centric data, homogeneous user populations, and abstract notions of fairness. This creates systematic risks for low-reso",
    "url": "https://arxiv.org/abs/2602.00497",
    "source": "Arxiv AI"
  },
  {
    "title": "Sparse Shortcuts: Facilitating Efficient Fusion in Multimodal Large Language Models",
    "summary": "arXiv:2602.00505v1 Announce Type: cross Abstract: With the remarkable success of large language models (LLMs) in natural language understanding and generation, multimodal large language models (MLLMs) have rapidly advanced in their ability to process data across multiple modalities. While most existing efforts focus on scaling up language models or",
    "url": "https://arxiv.org/abs/2602.00505",
    "source": "Arxiv AI"
  },
  {
    "title": "Contrastive Learning for Privacy Enhancements in Industrial Internet of Things",
    "summary": "arXiv:2602.00515v1 Announce Type: cross Abstract: The Industrial Internet of Things (IIoT) integrates intelligent sensing, communication, and analytics into industrial environments, including manufacturing, energy, and critical infrastructure. While IIoT enables predictive maintenance and cross-site optimization of modern industrial control systems",
    "url": "https://arxiv.org/abs/2602.00515",
    "source": "Arxiv AI"
  },
  {
    "title": "Physiology as Language: Translating Respiration to Sleep EEG",
    "summary": "arXiv:2602.00526v1 Announce Type: cross Abstract: This paper introduces a novel cross-physiology translation task: synthesizing sleep electroencephalography (EEG) from respiration signals. To address the significant complexity gap between the two modalities, we propose a waveform-conditional generative framework that preserves fine-grained respirat",
    "url": "https://arxiv.org/abs/2602.00526",
    "source": "Arxiv AI"
  },
  {
    "title": "Convergent World Representations and Divergent Tasks",
    "summary": "arXiv:2602.00533v1 Announce Type: cross Abstract: While neural representations are central to modern deep learning, the conditions governing their geometry and their roles in downstream adaptability remain poorly understood. We develop a framework clearly separating the underlying world, the data generation process and the resulting model represent",
    "url": "https://arxiv.org/abs/2602.00533",
    "source": "Arxiv AI"
  },
  {
    "title": "Contrastive Domain Generalization for Cross-Instrument Molecular Identification in Mass Spectrometry",
    "summary": "arXiv:2602.00547v1 Announce Type: cross Abstract: Identifying molecules from mass spectrometry (MS) data remains a fundamental challenge due to the semantic gap between physical spectral peaks and underlying chemical structures. Existing deep learning approaches often treat spectral matching as a closed-set recognition task, limiting their ability ",
    "url": "https://arxiv.org/abs/2602.00547",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models",
    "summary": "arXiv:2602.00559v1 Announce Type: cross Abstract: Current research on video hallucination mitigation primarily focuses on isolated error types, leaving compositional hallucinations, arising from incorrect reasoning over multiple interacting spatial and temporal factors largely underexplored. We introduce OmniVCHall, a benchmark designed to systemat",
    "url": "https://arxiv.org/abs/2602.00559",
    "source": "Arxiv AI"
  },
  {
    "title": "Data Distribution as a Lever for Guiding Optimizers Toward Superior Generalization in LLMs",
    "summary": "arXiv:2602.00576v1 Announce Type: cross Abstract: Can modifying the training data distribution guide optimizers toward solutions with improved generalization when training large language models (LLMs)? In this work, we theoretically analyze an in-context linear regression model with multi-head linear self-attention, and compare the training dynamic",
    "url": "https://arxiv.org/abs/2602.00576",
    "source": "Arxiv AI"
  },
  {
    "title": "MAUGen: A Unified Diffusion Approach for Multi-Identity Facial Expression and AU Label Generation",
    "summary": "arXiv:2602.00583v1 Announce Type: cross Abstract: The lack of large-scale, demographically diverse face images with precise Action Unit (AU) occurrence and intensity annotations has long been recognized as a fundamental bottleneck in developing generalizable AU recognition systems. In this paper, we propose MAUGen, a diffusion-based multi-modal fra",
    "url": "https://arxiv.org/abs/2602.00583",
    "source": "Arxiv AI"
  },
  {
    "title": "RAG-GNN: Integrating Retrieved Knowledge with Graph Neural Networks for Precision Medicine",
    "summary": "arXiv:2602.00586v1 Announce Type: cross Abstract: Network topology excels at structural predictions but fails to capture functional semantics encoded in biomedical literature. We present a retrieval-augmented generation (RAG) embedding framework that integrates graph neural network representations with dynamically retrieved literature-derived knowl",
    "url": "https://arxiv.org/abs/2602.00586",
    "source": "Arxiv AI"
  },
  {
    "title": "Multimodal Machine Learning for Integrating Heterogeneous Analytical Systems",
    "summary": "arXiv:2602.00590v1 Announce Type: cross Abstract: Understanding structure-property relationships in complex materials requires integrating complementary measurements across multiple length scales. Here we propose an interpretable \"multimodal\" machine learning framework that unifies heterogeneous analytical systems for end-to-end characterization, d",
    "url": "https://arxiv.org/abs/2602.00590",
    "source": "Arxiv AI"
  },
  {
    "title": "Hermes the Polyglot: A Unified Framework to Enhance Expressiveness for Multimodal Interlingual Subtitling",
    "summary": "arXiv:2602.00597v1 Announce Type: cross Abstract: Interlingual subtitling, which translates subtitles of visual media into a target language, is essential for entertainment localization but has not yet been explored in machine translation. Although Large Language Models (LLMs) have significantly advanced the general capabilities of machine translat",
    "url": "https://arxiv.org/abs/2602.00597",
    "source": "Arxiv AI"
  },
  {
    "title": "Jailbreaking LLMs via Calibration",
    "summary": "arXiv:2602.00619v1 Announce Type: cross Abstract: Safety alignment in Large Language Models (LLMs) often creates a systematic discrepancy between a model's aligned output and the underlying pre-aligned data distribution. We propose a framework in which the effect of safety alignment on next-token prediction is modeled as a systematic distortion of ",
    "url": "https://arxiv.org/abs/2602.00619",
    "source": "Arxiv AI"
  },
  {
    "title": "Rethinking Zero-Shot Time Series Classification: From Task-specific Classifiers to In-Context Inference",
    "summary": "arXiv:2602.00620v1 Announce Type: cross Abstract: The zero-shot evaluation of time series foundation models (TSFMs) for classification typically uses a frozen encoder followed by a task-specific classifier. However, this practice violates the training-free premise of zero-shot deployment and introduces evaluation bias due to classifier-dependent tr",
    "url": "https://arxiv.org/abs/2602.00620",
    "source": "Arxiv AI"
  },
  {
    "title": "MoDEx: Mixture of Depth-specific Experts for Multivariate Long-term Time Series Forecasting",
    "summary": "arXiv:2602.00624v1 Announce Type: cross Abstract: Multivariate long-term time series forecasting (LTSF) supports critical applications such as traffic-flow management, solar-power scheduling, and electricity-transformer monitoring. The existing LTSF paradigms follow a three-stage pipeline of embedding, backbone refinement, and long-horizon predicti",
    "url": "https://arxiv.org/abs/2602.00624",
    "source": "Arxiv AI"
  },
  {
    "title": "From Associations to Activations: Comparing Behavioral and Hidden-State Semantic Geometry in LLMs",
    "summary": "arXiv:2602.00628v1 Announce Type: cross Abstract: We investigate the extent to which an LLM's hidden-state geometry can be recovered from its behavior in psycholinguistic experiments. Across eight instruction-tuned transformer models, we run two experimental paradigms -- similarity-based forced choice and free association -- over a shared 5,000-wor",
    "url": "https://arxiv.org/abs/2602.00628",
    "source": "Arxiv AI"
  },
  {
    "title": "Action-Free Offline-to-Online RL via Discretised State Policies",
    "summary": "arXiv:2602.00629v1 Announce Type: cross Abstract: Most existing offline RL methods presume the availability of action labels within the dataset, but in many practical scenarios, actions may be missing due to privacy, storage, or sensor limitations. We formalise the setting of action-free offline-to-online RL, where agents must learn from datasets c",
    "url": "https://arxiv.org/abs/2602.00629",
    "source": "Arxiv AI"
  },
  {
    "title": "S$^3$POT: Contrast-Driven Face Occlusion Segmentation via Self-Supervised Prompt Learning",
    "summary": "arXiv:2602.00635v1 Announce Type: cross Abstract: Existing face parsing methods usually misclassify occlusions as facial components. This is because occlusion is a high-level concept, it does not refer to a concrete category of object. Thus, constructing a real-world face dataset covering all categories of occlusion object is almost impossible and ",
    "url": "https://arxiv.org/abs/2602.00635",
    "source": "Arxiv AI"
  },
  {
    "title": "Non-Contrastive Vision-Language Learning with Predictive Embedding Alignment",
    "summary": "arXiv:2602.00653v1 Announce Type: cross Abstract: Vision-language models have transformed multimodal representation learning, yet dominant contrastive approaches like CLIP require large batch sizes, careful negative sampling, and extensive hyperparameter tuning. We introduce NOVA, a NOn-contrastive Vision-language Alignment framework based on joint",
    "url": "https://arxiv.org/abs/2602.00653",
    "source": "Arxiv AI"
  },
  {
    "title": "Can Small Language Models Handle Context-Summarized Multi-Turn Customer-Service QA? A Synthetic Data-Driven Comparative Evaluation",
    "summary": "arXiv:2602.00665v1 Announce Type: cross Abstract: Customer-service question answering (QA) systems increasingly rely on conversational language understanding. While Large Language Models (LLMs) achieve strong performance, their high computational cost and deployment constraints limit practical use in resource-constrained environments. Small Languag",
    "url": "https://arxiv.org/abs/2602.00665",
    "source": "Arxiv AI"
  },
  {
    "title": "Improving Neuropathological Reconstruction Fidelity via AI Slice Imputation",
    "summary": "arXiv:2602.00669v1 Announce Type: cross Abstract: Neuropathological analyses benefit from spatially precise volumetric reconstructions that enhance anatomical delineation and improve morphometric accuracy. Our prior work has shown the feasibility of reconstructing 3D brain volumes from 2D dissection photographs. However these outputs sometimes exhi",
    "url": "https://arxiv.org/abs/2602.00669",
    "source": "Arxiv AI"
  },
  {
    "title": "RecGOAT: Graph Optimal Adaptive Transport for LLM-Enhanced Multimodal Recommendation with Dual Semantic Alignment",
    "summary": "arXiv:2602.00682v1 Announce Type: cross Abstract: Multimodal recommendation systems typically integrates user behavior with multimodal data from items, thereby capturing more accurate user preferences. Concurrently, with the rise of large models (LMs), multimodal recommendation is increasingly leveraging their strengths in semantic understanding an",
    "url": "https://arxiv.org/abs/2602.00682",
    "source": "Arxiv AI"
  },
  {
    "title": "Forecasting Energy Availability in Local Energy Communities via LSTM Federated Learning",
    "summary": "arXiv:2602.00694v1 Announce Type: cross Abstract: Local Energy Communities are emerging as crucial players in the landscape of sustainable development. A significant challenge for these communities is achieving self-sufficiency through effective management of the balance between energy production and consumption. To meet this challenge, it is essen",
    "url": "https://arxiv.org/abs/2602.00694",
    "source": "Arxiv AI"
  },
  {
    "title": "From Detection to Prevention: Explaining Security-Critical Code to Avoid Vulnerabilities",
    "summary": "arXiv:2602.00711v1 Announce Type: cross Abstract: Security vulnerabilities often arise unintentionally during development due to a lack of security expertise and code complexity. Traditional tools, such as static and dynamic analysis, detect vulnerabilities only after they are introduced in code, leading to costly remediation. This work explores a ",
    "url": "https://arxiv.org/abs/2602.00711",
    "source": "Arxiv AI"
  },
  {
    "title": "Deep Time-series Forecasting Needs Kernelized Moment Balancing",
    "summary": "arXiv:2602.00717v1 Announce Type: cross Abstract: Deep time-series forecasting can be formulated as a distribution balancing problem aimed at aligning the distribution of the forecasts and ground truths. According to Imbens' criterion, true distribution balance requires matching the first moments with respect to any balancing function. We demonstra",
    "url": "https://arxiv.org/abs/2602.00717",
    "source": "Arxiv AI"
  },
  {
    "title": "Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity",
    "summary": "arXiv:2602.00723v1 Announce Type: cross Abstract: Large language models (LLMs) are known to \"hallucinate\" by generating false or misleading outputs. Hallucinations pose various harms, from erosion of trust to widespread misinformation. Existing hallucination evaluation, however, focuses only on correctness and often overlooks consistency, necessary",
    "url": "https://arxiv.org/abs/2602.00723",
    "source": "Arxiv AI"
  },
  {
    "title": "Augmenting Clinical Decision-Making with an Interactive and Interpretable AI Copilot: A Real-World User Study with Clinicians in Nephrology and Obstetrics",
    "summary": "arXiv:2602.00726v1 Announce Type: cross Abstract: Clinician skepticism toward opaque AI hinders adoption in high-stakes healthcare. We present AICare, an interactive and interpretable AI copilot for collaborative clinical decision-making. By analyzing longitudinal electronic health records, AICare grounds dynamic risk predictions in scrutable visua",
    "url": "https://arxiv.org/abs/2602.00726",
    "source": "Arxiv AI"
  },
  {
    "title": "EchoReview: Learning Peer Review from the Echoes of Scientific Citations",
    "summary": "arXiv:2602.00733v1 Announce Type: cross Abstract: As the volume of scientific submissions continues to grow rapidly, traditional peer review systems are facing unprecedented scalability pressures, highlighting the urgent need for automated reviewing methods that are both scalable and reliable. Existing supervised fine-tuning approaches based on rea",
    "url": "https://arxiv.org/abs/2602.00733",
    "source": "Arxiv AI"
  },
  {
    "title": "Pareto-Conditioned Diffusion Models for Offline Multi-Objective Optimization",
    "summary": "arXiv:2602.00737v1 Announce Type: cross Abstract: Multi-objective optimization (MOO) arises in many real-world applications where trade-offs between competing objectives must be carefully balanced. In the offline setting, where only a static dataset is available, the main challenge is generalizing beyond observed data. We introduce Pareto-Condition",
    "url": "https://arxiv.org/abs/2602.00737",
    "source": "Arxiv AI"
  },
  {
    "title": "ExperienceWeaver: Optimizing Small-sample Experience Learning for LLM-based Clinical Text Improvement",
    "summary": "arXiv:2602.00740v1 Announce Type: cross Abstract: Clinical text improvement is vital for healthcare efficiency but remains difficult due to limited high-quality data and the complex constraints of medical documentation. While Large Language Models (LLMs) show promise, current approaches struggle in small-sample settings: supervised fine-tuning is d",
    "url": "https://arxiv.org/abs/2602.00740",
    "source": "Arxiv AI"
  },
  {
    "title": "SA-VLA: Spatially-Aware Flow-Matching for Vision-Language-Action Reinforcement Learning",
    "summary": "arXiv:2602.00743v1 Announce Type: cross Abstract: Vision-Language-Action (VLA) models exhibit strong generalization in robotic manipulation, yet reinforcement learning (RL) fine-tuning often degrades robustness under spatial distribution shifts. For flow-matching VLA policies, this degradation is closely associated with the erosion of spatial induc",
    "url": "https://arxiv.org/abs/2602.00743",
    "source": "Arxiv AI"
  },
  {
    "title": "Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training",
    "summary": "arXiv:2602.00747v1 Announce Type: cross Abstract: Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-training, where models must balance general competence with proficiency on hard tasks such as math and code. However, identifying an optimal mixture remains an open challenge, as existing approaches either rely o",
    "url": "https://arxiv.org/abs/2602.00747",
    "source": "Arxiv AI"
  },
  {
    "title": "HyperOffload: Graph-Driven Hierarchical Memory Management for Large Language Models on SuperNode Architectures",
    "summary": "arXiv:2602.00748v2 Announce Type: cross Abstract: The rapid evolution of Large Language Models (LLMs) towards long-context reasoning and sparse architectures has pushed memory requirements far beyond the capacity of individual device HBM. While emerging supernode architectures offer terabyte-scale shared memory pools via high-bandwidth interconnect",
    "url": "https://arxiv.org/abs/2602.00748",
    "source": "Arxiv AI"
  },
  {
    "title": "Bypassing Prompt Injection Detectors through Evasive Injections",
    "summary": "arXiv:2602.00750v1 Announce Type: cross Abstract: Large language models (LLMs) are increasingly used in interactive and retrieval-augmented systems, but they remain vulnerable to task drift; deviations from a user's intended instruction due to injected secondary prompts. Recent work has shown that linear probes trained on activation deltas of LLMs'",
    "url": "https://arxiv.org/abs/2602.00750",
    "source": "Arxiv AI"
  },
  {
    "title": "GraphNNK -- Graph Classification and Interpretability",
    "summary": "arXiv:2602.00753v1 Announce Type: cross Abstract: Graph Neural Networks (GNNs) have become a standard approach for learning from graph-structured data. However, their reliance on parametric classifiers (most often linear softmax layers) limits interpretability and sometimes hinders generalization. Recent work on interpolation-based methods, particu",
    "url": "https://arxiv.org/abs/2602.00753",
    "source": "Arxiv AI"
  },
  {
    "title": "Evolving Interpretable Constitutions for Multi-Agent Simulation",
    "summary": "arXiv:2602.00755v1 Announce Type: cross Abstract: Constitutional AI has focused on single-model alignment using fixed principles. However, multi-agent systems create novel alignment challenges through emergent social dynamics. We present Constitutional Evolution, a framework for automatically discovering behavioral norms in multi-agent LLM systems.",
    "url": "https://arxiv.org/abs/2602.00755",
    "source": "Arxiv AI"
  },
  {
    "title": "Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning",
    "summary": "arXiv:2602.00759v1 Announce Type: cross Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration, which often re",
    "url": "https://arxiv.org/abs/2602.00759",
    "source": "Arxiv AI"
  },
  {
    "title": "Evaluating Deep Learning-Based Nerve Segmentation in Brachial Plexus Ultrasound Under Realistic Data Constraints",
    "summary": "arXiv:2602.00763v1 Announce Type: cross Abstract: Accurate nerve localization is critical for the success of ultrasound-guided regional anesthesia, yet manual identification remains challenging due to low image contrast, speckle noise, and inter-patient anatomical variability. This study evaluates deep learning-based nerve segmentation in ultrasoun",
    "url": "https://arxiv.org/abs/2602.00763",
    "source": "Arxiv AI"
  },
  {
    "title": "BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features",
    "summary": "arXiv:2602.00767v1 Announce Type: cross Abstract: Emergent misalignment can arise when a language model is fine-tuned on a narrowly scoped supervised objective: the model learns the target behavior, yet also develops undesirable out-of-domain behaviors. We investigate a mechanistic approach to preventing emergent misalignment by identifying a small",
    "url": "https://arxiv.org/abs/2602.00767",
    "source": "Arxiv AI"
  },
  {
    "title": "Eliciting Trustworthiness Priors of Large Language Models via Economic Games",
    "summary": "arXiv:2602.00769v1 Announce Type: cross Abstract: One critical aspect of building human-centered, trustworthy artificial intelligence (AI) systems is maintaining calibrated trust: appropriate reliance on AI systems outperforms both overtrust (e.g., automation bias) and undertrust (e.g., disuse). A fundamental challenge, however, is how to character",
    "url": "https://arxiv.org/abs/2602.00769",
    "source": "Arxiv AI"
  },
  {
    "title": "Controlling Repetition in Protein Language Models",
    "summary": "arXiv:2602.00782v1 Announce Type: cross Abstract: Protein language models (PLMs) have enabled advances in structure prediction and de novo protein design, yet they frequently collapse into pathological repetition during generation. Unlike in text, where repetition merely reduces readability, in proteins it undermines structural confidence and funct",
    "url": "https://arxiv.org/abs/2602.00782",
    "source": "Arxiv AI"
  },
  {
    "title": "Multi-Objective Multi-Fidelity Bayesian Optimization with Causal Priors",
    "summary": "arXiv:2602.00788v1 Announce Type: cross Abstract: Multi-fidelity Bayesian optimization (MFBO) accelerates the search for the global optimum of black-box functions by integrating inexpensive, low-fidelity approximations. The central task of an MFBO policy is to balance the cost-efficiency of low-fidelity proxies against their reduced accuracy to ens",
    "url": "https://arxiv.org/abs/2602.00788",
    "source": "Arxiv AI"
  },
  {
    "title": "Latent Shadows: The Gaussian-Discrete Duality in Masked Diffusion",
    "summary": "arXiv:2602.00792v1 Announce Type: cross Abstract: Masked discrete diffusion is a dominant paradigm for high-quality language modeling where tokens are iteratively corrupted to a mask state, yet its inference efficiency is bottlenecked by the lack of deterministic sampling tools. While diffusion duality enables deterministic distillation for uniform",
    "url": "https://arxiv.org/abs/2602.00792",
    "source": "Arxiv AI"
  },
  {
    "title": "JTok: On Token Embedding as another Axis of Scaling Law via Joint Token Self-modulation",
    "summary": "arXiv:2602.00800v1 Announce Type: cross Abstract: LLMs have traditionally scaled along dense dimensions, where performance is coupled with near-linear increases in computational cost. While MoE decouples capacity from compute, it introduces large memory overhead and hardware efficiency challenges. To overcome these, we propose token-indexed paramet",
    "url": "https://arxiv.org/abs/2602.00800",
    "source": "Arxiv AI"
  },
  {
    "title": "Don't Forget Its Variance! The Minimum Path Variance Principle for Accurate and Stable Score-Based Density Ratio Estimation",
    "summary": "arXiv:2602.00834v1 Announce Type: cross Abstract: Score-based methods have emerged as a powerful framework for density ratio estimation (DRE), but they face an important paradox in that, while theoretically path-independent, their practical performance depends critically on the chosen path schedule. We resolve this issue by proving that tractable t",
    "url": "https://arxiv.org/abs/2602.00834",
    "source": "Arxiv AI"
  },
  {
    "title": "Exploration of Unary Arithmetic-Based Matrix Multiply Units for Low Precision DL Accelerators",
    "summary": "arXiv:2602.00838v1 Announce Type: cross Abstract: General matrix multiplication (GEMM) is a fundamental operation in deep learning (DL). With DL moving increasingly toward low precision, recent works have proposed novel unary GEMM designs as an alternative to conventional binary GEMM hardware. A rigorous evaluation of recent unary and binary GEMM d",
    "url": "https://arxiv.org/abs/2602.00838",
    "source": "Arxiv AI"
  },
  {
    "title": "Factuality on Demand: Controlling the Factuality-Informativeness Trade-off in Text Generation",
    "summary": "arXiv:2602.00848v1 Announce Type: cross Abstract: Large language models (LLMs) encode knowledge with varying degrees of confidence. When responding to queries, models face an inherent trade-off: they can generate responses that are less informative but highly factual, or more informative but potentially less accurate. Different applications demand ",
    "url": "https://arxiv.org/abs/2602.00848",
    "source": "Arxiv AI"
  },
  {
    "title": "RMFlow: Refined Mean Flow by a Noise-Injection Step for Multimodal Generation",
    "summary": "arXiv:2602.00849v1 Announce Type: cross Abstract: Mean flow (MeanFlow) enables efficient, high-fidelity image generation, yet its single-function evaluation (1-NFE) generation often cannot yield compelling results. We address this issue by introducing RMFlow, an efficient multimodal generative model that integrates a coarse 1-NFE MeanFlow transport",
    "url": "https://arxiv.org/abs/2602.00849",
    "source": "Arxiv AI"
  },
  {
    "title": "Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs",
    "summary": "arXiv:2602.00862v1 Announce Type: cross Abstract: Graph neural networks (GNNs) have emerged as powerful tools for learning protein structures by capturing spatial relationships at the residue level. However, existing GNN-based methods often face challenges in learning multiscale representations and modeling long-range dependencies efficiently. In t",
    "url": "https://arxiv.org/abs/2602.00862",
    "source": "Arxiv AI"
  },
  {
    "title": "Improving Flow Matching by Aligning Flow Divergence",
    "summary": "arXiv:2602.00869v1 Announce Type: cross Abstract: Conditional flow matching (CFM) stands out as an efficient, simulation-free approach for training flow-based generative models, achieving remarkable performance for data generation. However, CFM is insufficient to ensure accuracy in learning probability paths. In this paper, we introduce a new parti",
    "url": "https://arxiv.org/abs/2602.00869",
    "source": "Arxiv AI"
  },
  {
    "title": "DIAMOND: Directed Inference for Artifact Mitigation in Flow Matching Models",
    "summary": "arXiv:2602.00883v1 Announce Type: cross Abstract: Despite impressive results from recent text-to-image models like FLUX, visual and anatomical artifacts remain a significant hurdle for practical and professional use. Existing methods for artifact reduction, typically work in a post-hoc manner, consequently failing to intervene effectively during th",
    "url": "https://arxiv.org/abs/2602.00883",
    "source": "Arxiv AI"
  },
  {
    "title": "EffGen: Enabling Small Language Models as Capable Autonomous Agents",
    "summary": "arXiv:2602.00887v1 Announce Type: cross Abstract: Most existing language model agentic systems today are built and optimized for large language models (e.g., GPT, Claude, Gemini) via API calls. While powerful, this approach faces several limitations including high token costs and privacy concerns for sensitive applications. We introduce effGen, an ",
    "url": "https://arxiv.org/abs/2602.00887",
    "source": "Arxiv AI"
  },
  {
    "title": "GAPNet: Plug-in Jointly Learning Task-Specific Graph for Dynamic Stock Relation",
    "summary": "arXiv:2602.00888v1 Announce Type: cross Abstract: The advent of the web has led to a paradigm shift in the financial relations, with the real-time dissemination of news, social discourse, and financial filings contributing significantly to the reshaping of financial forecasting. The existing methods rely on establishing relations a priori, i.e. pre",
    "url": "https://arxiv.org/abs/2602.00888",
    "source": "Arxiv AI"
  },
  {
    "title": "Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing",
    "summary": "arXiv:2602.00906v2 Announce Type: cross Abstract: Large language models often hallucinate with high confidence on \"random facts\" that lack inferable patterns. We formalize the memorization of such facts as a membership testing problem, unifying the discrete error metrics of Bloom filters with the continuous log-loss of LLMs. By analyzing this probl",
    "url": "https://arxiv.org/abs/2602.00906",
    "source": "Arxiv AI"
  },
  {
    "title": "Do Schwartz Higher-Order Values Help Sentence-Level Human Value Detection? When Hard Gating Hurts",
    "summary": "arXiv:2602.00913v1 Announce Type: cross Abstract: Sentence-level human value detection is typically framed as multi-label classification over Schwartz values, but it remains unclear whether Schwartz higher-order (HO) categories provide usable structure. We study this under a strict compute-frugal budget (single 8 GB GPU) on ValueEval'24 / ValuesML ",
    "url": "https://arxiv.org/abs/2602.00913",
    "source": "Arxiv AI"
  },
  {
    "title": "A Baseline Multimodal Approach to Emotion Recognition in Conversations",
    "summary": "arXiv:2602.00914v1 Announce Type: cross Abstract: We present a lightweight multimodal baseline for emotion recognition in conversations using the SemEval-2024 Task 3 dataset built from the sitcom Friends. The goal of this report is not to propose a novel state-of-the-art method, but to document an accessible reference implementation that combines (",
    "url": "https://arxiv.org/abs/2602.00914",
    "source": "Arxiv AI"
  },
  {
    "title": "Continuous-Utility Direct Preference Optimization",
    "summary": "arXiv:2602.00931v1 Announce Type: cross Abstract: Large language model reasoning is often treated as a monolithic capability, relying on binary preference supervision that fails to capture partial progress or fine-grained reasoning quality. We introduce Continuous Utility Direct Preference Optimization (CU-DPO), a framework that aligns models to a ",
    "url": "https://arxiv.org/abs/2602.00931",
    "source": "Arxiv AI"
  },
  {
    "title": "MCP-Atlas: A Large-Scale Benchmark for Tool-Use Competency with Real MCP Servers",
    "summary": "arXiv:2602.00933v1 Announce Type: cross Abstract: The Model Context Protocol (MCP) is rapidly becoming the standard interface for Large Language Models (LLMs) to discover and invoke external tools. However, existing evaluations often fail to capture the complexity of real-world scenarios, relying on restricted toolsets, simplistic workflows, or sub",
    "url": "https://arxiv.org/abs/2602.00933",
    "source": "Arxiv AI"
  },
  {
    "title": "CLAMP: Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining",
    "summary": "arXiv:2602.00937v1 Announce Type: cross Abstract: Leveraging pre-trained 2D image representations in behavior cloning policies has achieved great success and has become a standard approach for robotic manipulation. However, such representations fail to capture the 3D spatial information about objects and scenes that is essential for precise manipul",
    "url": "https://arxiv.org/abs/2602.00937",
    "source": "Arxiv AI"
  },
  {
    "title": "Neural FOXP2 -- Language Specific Neuron Steering for Targeted Language Improvement in LLMs",
    "summary": "arXiv:2602.00945v1 Announce Type: cross Abstract: LLMs are multilingual by training, yet their lingua franca is often English, reflecting English language dominance in pretraining. Other languages remain in parametric memory but are systematically suppressed. We argue that language defaultness is governed by a sparse, low-rank control circuit, lang",
    "url": "https://arxiv.org/abs/2602.00945",
    "source": "Arxiv AI"
  },
  {
    "title": "FinEvo: From Isolated Backtests to Ecological Market Games for Multi-Agent Financial Strategy Evolution",
    "summary": "arXiv:2602.00948v1 Announce Type: cross Abstract: Conventional financial strategy evaluation relies on isolated backtests in static environments. Such evaluations assess each policy independently, overlook correlations and interactions, and fail to explain why strategies ultimately persist or vanish in evolving markets. We shift to an ecological pe",
    "url": "https://arxiv.org/abs/2602.00948",
    "source": "Arxiv AI"
  },
  {
    "title": "Multimodal Scientific Learning Beyond Diffusions and Flows",
    "summary": "arXiv:2602.00960v1 Announce Type: cross Abstract: Scientific machine learning (SciML) increasingly requires models that capture multimodal conditional uncertainty arising from ill-posed inverse problems, multistability, and chaotic dynamics. While recent work has favored highly expressive implicit generative models such as diffusion and flow-based ",
    "url": "https://arxiv.org/abs/2602.00960",
    "source": "Arxiv AI"
  },
  {
    "title": "GradingAttack: Attacking Large Language Models Towards Short Answer Grading Ability",
    "summary": "arXiv:2602.00979v1 Announce Type: cross Abstract: Large language models (LLMs) have demonstrated remarkable potential for automatic short answer grading (ASAG), significantly boosting student assessment efficiency and scalability in educational scenarios. However, their vulnerability to adversarial manipulation raises critical concerns about automa",
    "url": "https://arxiv.org/abs/2602.00979",
    "source": "Arxiv AI"
  },
  {
    "title": "MedSpeak: A Knowledge Graph-Aided ASR Error Correction Framework for Spoken Medical QA",
    "summary": "arXiv:2602.00981v1 Announce Type: cross Abstract: Spoken question-answering (SQA) systems relying on automatic speech recognition (ASR) often struggle with accurately recognizing medical terminology. To this end, we propose MedSpeak, a novel knowledge graph-aided ASR error correction framework that refines noisy transcripts and improves downstream ",
    "url": "https://arxiv.org/abs/2602.00981",
    "source": "Arxiv AI"
  },
  {
    "title": "Navigating Simply, Aligning Deeply: Winning Solutions for Mouse vs. AI 2025",
    "summary": "arXiv:2602.00982v1 Announce Type: cross Abstract: Visual robustness and neural alignment remain critical challenges in developing artificial agents that can match biological vision systems. We present the winning approaches from Team HCMUS_TheFangs for both tracks of the NeurIPS 2025 Mouse vs. AI: Robust Visual Foraging Competition. For Track 1 (Vi",
    "url": "https://arxiv.org/abs/2602.00982",
    "source": "Arxiv AI"
  },
  {
    "title": "DISPO: Enhancing Training Efficiency and Stability in Reinforcement Learning for Large Language Model Mathematical Reasoning",
    "summary": "arXiv:2602.00983v1 Announce Type: cross Abstract: Reinforcement learning with verifiable rewards has emerged as a promising paradigm for enhancing the reasoning capabilities of large language models particularly in mathematics. Current approaches in this domain present a clear trade-off: PPO-style methods (e.g., GRPO/DAPO) offer training stability ",
    "url": "https://arxiv.org/abs/2602.00983",
    "source": "Arxiv AI"
  },
  {
    "title": "HERMES: A Holistic End-to-End Risk-Aware Multimodal Embodied System with Vision-Language Models for Long-Tail Autonomous Driving",
    "summary": "arXiv:2602.00993v1 Announce Type: cross Abstract: End-to-end autonomous driving models increasingly benefit from large vision--language models for semantic understanding, yet ensuring safe and accurate operation under long-tail conditions remains challenging. These challenges are particularly prominent in long-tail mixed-traffic scenarios, where au",
    "url": "https://arxiv.org/abs/2602.00993",
    "source": "Arxiv AI"
  },
  {
    "title": "DeALOG: Decentralized Multi-Agents Log-Mediated Reasoning Framework",
    "summary": "arXiv:2602.00996v1 Announce Type: cross Abstract: Complex question answering across text, tables and images requires integrating diverse information sources. A framework supporting specialized processing with coordination and interpretability is needed. We introduce DeALOG, a decentralized multi-agent framework for multimodal question answering. It",
    "url": "https://arxiv.org/abs/2602.00996",
    "source": "Arxiv AI"
  },
  {
    "title": "ESSAM: A Novel Competitive Evolution Strategies Approach to Reinforcement Learning for Memory Efficient LLMs Fine-Tuning",
    "summary": "arXiv:2602.01003v1 Announce Type: cross Abstract: Reinforcement learning (RL) has become a key training step for improving mathematical reasoning in large language models (LLMs), but it often has high GPU memory usage, which makes it hard to use in settings with limited resources. To reduce these issues, we propose Evolution Strategies with Sharpne",
    "url": "https://arxiv.org/abs/2602.01003",
    "source": "Arxiv AI"
  },
  {
    "title": "LASS-ODE: Scaling ODE Computations to Connect Foundation Models with Dynamical Physical Systems",
    "summary": "arXiv:2602.01009v1 Announce Type: cross Abstract: Foundation models have transformed language, vision, and time series data analysis, yet progress on dynamic predictions for physical systems remains limited. Given the complexity of physical constraints, two challenges stand out. $(i)$ Physics-computation scalability: physics-informed learning can e",
    "url": "https://arxiv.org/abs/2602.01009",
    "source": "Arxiv AI"
  },
  {
    "title": "Multi-Agent Teams Hold Experts Back",
    "summary": "arXiv:2602.01011v2 Announce Type: cross Abstract: Multi-agent LLM systems are increasingly deployed as autonomous collaborators, where agents interact freely rather than execute fixed, pre-specified workflows. In such settings, effective coordination cannot be fully designed in advance and must instead emerge through interaction. However, most prio",
    "url": "https://arxiv.org/abs/2602.01011",
    "source": "Arxiv AI"
  },
  {
    "title": "How Does Unfaithful Reasoning Emerge from Autoregressive Training? A Study of Synthetic Experiments",
    "summary": "arXiv:2602.01017v1 Announce Type: cross Abstract: Chain-of-thought (CoT) reasoning generated by large language models (LLMs) is often unfaithful: intermediate steps can be logically inconsistent or fail to reflect the causal relationship leading to the final answer. Despite extensive empirical observations, a fundamental understanding of CoT is lac",
    "url": "https://arxiv.org/abs/2602.01017",
    "source": "Arxiv AI"
  },
  {
    "title": "Offline Discovery of Interpretable Skills from Multi-Task Trajectories",
    "summary": "arXiv:2602.01018v1 Announce Type: cross Abstract: Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In t",
    "url": "https://arxiv.org/abs/2602.01018",
    "source": "Arxiv AI"
  },
  {
    "title": "Inter- and Intra-Subject Variability in EEG: A Systematic Survey",
    "summary": "arXiv:2602.01019v1 Announce Type: cross Abstract: Electroencephalography (EEG) underpins neuroscience, clinical neurophysiology, and brain-computer interfaces (BCIs), yet pronounced inter- and intra-subject variability limits reliability, reproducibility, and translation. This systematic review studies that quantified or modeled EEG variability acr",
    "url": "https://arxiv.org/abs/2602.01019",
    "source": "Arxiv AI"
  },
  {
    "title": "Calibrating Behavioral Parameters with Large Language Models",
    "summary": "arXiv:2602.01022v1 Announce Type: cross Abstract: Behavioral parameters such as loss aversion, herding, and extrapolation are central to asset pricing models but remain difficult to measure reliably. We develop a framework that treats large language models (LLMs) as calibrated measurement instruments for behavioral parameters. Using four models and",
    "url": "https://arxiv.org/abs/2602.01022",
    "source": "Arxiv AI"
  },
  {
    "title": "Unifying Ranking and Generation in Query Auto-Completion via Retrieval-Augmented Generation and Multi-Objective Alignment",
    "summary": "arXiv:2602.01023v2 Announce Type: cross Abstract: Query Auto-Completion (QAC) suggests query completions as users type, helping them articulate intent and reach results more efficiently. Existing approaches face fundamental challenges: traditional retrieve-and-rank pipelines have limited long-tail coverage and require extensive feature engineering,",
    "url": "https://arxiv.org/abs/2602.01023",
    "source": "Arxiv AI"
  },
  {
    "title": "Toward Universal and Transferable Jailbreak Attacks on Vision-Language Models",
    "summary": "arXiv:2602.01025v1 Announce Type: cross Abstract: Vision-language models (VLMs) extend large language models (LLMs) with vision encoders, enabling text generation conditioned on both images and text. However, this multimodal integration expands the attack surface by exposing the model to image-based jailbreaks crafted to induce harmful responses. E",
    "url": "https://arxiv.org/abs/2602.01025",
    "source": "Arxiv AI"
  },
  {
    "title": "HierCon: Hierarchical Contrastive Attention for Audio Deepfake Detection",
    "summary": "arXiv:2602.01032v1 Announce Type: cross Abstract: Audio deepfakes generated by modern TTS and voice conversion systems are increasingly difficult to distinguish from real speech, raising serious risks for security and online trust. While state-of-the-art self-supervised models provide rich multi-layer representations, existing detectors treat layer",
    "url": "https://arxiv.org/abs/2602.01032",
    "source": "Arxiv AI"
  },
  {
    "title": "VEQ: Modality-Adaptive Quantization for MoE Vision-Language Models",
    "summary": "arXiv:2602.01037v1 Announce Type: cross Abstract: Mixture-of-Experts(MoE) Vision-Language Models (VLMs) offer remarkable performance but incur prohibitive memory and computational costs, making compression essential. Post-Training Quantization (PTQ) is an effective training-free technique to address the massive memory and computation overhead. Exis",
    "url": "https://arxiv.org/abs/2602.01037",
    "source": "Arxiv AI"
  },
  {
    "title": "Adaptive Dual-Weighting Framework for Federated Learning via Out-of-Distribution Detection",
    "summary": "arXiv:2602.01039v1 Announce Type: cross Abstract: Federated Learning (FL) enables collaborative model training across large-scale distributed service nodes while preserving data privacy, making it a cornerstone of intelligent service systems in edge-cloud environments. However, in real-world service-oriented deployments, data generated by heterogen",
    "url": "https://arxiv.org/abs/2602.01039",
    "source": "Arxiv AI"
  },
  {
    "title": "Superposition unifies power-law training dynamics",
    "summary": "arXiv:2602.01045v1 Announce Type: cross Abstract: We investigate the role of feature superposition in the emergence of power-law training dynamics using a teacher-student framework. We first derive an analytic theory for training without superposition, establishing that the power-law training exponent depends on both the input data statistics and c",
    "url": "https://arxiv.org/abs/2602.01045",
    "source": "Arxiv AI"
  },
  {
    "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance",
    "summary": "arXiv:2602.01047v1 Announce Type: cross Abstract: Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntacticall",
    "url": "https://arxiv.org/abs/2602.01047",
    "source": "Arxiv AI"
  },
  {
    "title": "Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning",
    "summary": "arXiv:2602.01058v1 Announce Type: cross Abstract: Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone. We show that, after identical RL training, models initialized",
    "url": "https://arxiv.org/abs/2602.01058",
    "source": "Arxiv AI"
  },
  {
    "title": "TLDiffGAN: A Latent Diffusion-GAN Framework with Temporal Information Fusion for Anomalous Sound Detection",
    "summary": "arXiv:2602.01060v1 Announce Type: cross Abstract: Existing generative models for unsupervised anomalous sound detection are limited by their inability to fully capture the complex feature distribution of normal sounds, while the potential of powerful diffusion models in this domain remains largely unexplored. To address this challenge, we propose a",
    "url": "https://arxiv.org/abs/2602.01060",
    "source": "Arxiv AI"
  },
  {
    "title": "Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents",
    "summary": "arXiv:2602.01063v1 Announce Type: cross Abstract: Large Language Models (LLMs) can be conditioned with explicit personality prompts, yet their behavioral realization often varies depending on context. This study examines how identical personality prompts lead to distinct linguistic, behavioral, and emotional outcomes across four conversational sett",
    "url": "https://arxiv.org/abs/2602.01063",
    "source": "Arxiv AI"
  },
  {
    "title": "From Utterance to Vividity: Training Expressive Subtitle Translation LLM via Adaptive Local Preference Optimization",
    "summary": "arXiv:2602.01068v1 Announce Type: cross Abstract: The rapid development of Large Language Models (LLMs) has significantly enhanced the general capabilities of machine translation. However, as application scenarios become more complex, the limitations of LLMs in vertical domain translations are gradually becoming apparent. In this study, we focus on",
    "url": "https://arxiv.org/abs/2602.01068",
    "source": "Arxiv AI"
  },
  {
    "title": "Vortex Stretching in the Navier-Stokes Equations and Information Dissipation in Diffusion Models: A Reformulation from a Partial Differential Equation Viewpoint",
    "summary": "arXiv:2602.01071v1 Announce Type: cross Abstract: We present a new inverse-time formulation of vortex stretching in the Navier-Stokes equations, based on a PDE framework inspired by score-based diffusion models. By absorbing the ill-posed backward Laplacian arising from time reversal into a drift term expressed through a score function, the inverse",
    "url": "https://arxiv.org/abs/2602.01071",
    "source": "Arxiv AI"
  },
  {
    "title": "OLion: Approaching the Hadamard Ideal by Intersecting Spectral and $\\ell_{\\infty}$ Implicit Biases",
    "summary": "arXiv:2602.01105v1 Announce Type: cross Abstract: Many optimizers can be interpreted as steepest-descent methods under norm-induced geometries, and thus inherit corresponding implicit biases. We introduce \\nameA{} (\\fullname{}), which combines spectral control from orthogonalized update directions with $\\ell_\\infty$-style coordinate control from si",
    "url": "https://arxiv.org/abs/2602.01105",
    "source": "Arxiv AI"
  },
  {
    "title": "SPELL: Synthesis of Programmatic Edits using LLMs",
    "summary": "arXiv:2602.01107v1 Announce Type: cross Abstract: Library migration is a common but error-prone task in software development. Developers may need to replace one library with another due to reasons like changing requirements or licensing changes. Migration typically entails updating and rewriting source code manually. While automated migration tools",
    "url": "https://arxiv.org/abs/2602.01107",
    "source": "Arxiv AI"
  },
  {
    "title": "MarkovScale: Towards Optimal Sequential Scaling at Inference Time",
    "summary": "arXiv:2602.01120v1 Announce Type: cross Abstract: Sequential scaling is a prominent inference-time scaling paradigm, yet its performance improvements are typically modest and not well understood, largely due to the prevalence of heuristic, non-principled approaches that obscure clear optimality bounds. To address this, we propose a principled frame",
    "url": "https://arxiv.org/abs/2602.01120",
    "source": "Arxiv AI"
  },
  {
    "title": "Statistical MIA: Rethinking Membership Inference Attack for Reliable Unlearning Auditing",
    "summary": "arXiv:2602.01150v1 Announce Type: cross Abstract: Machine unlearning (MU) is essential for enforcing the right to be forgotten in machine learning systems. A key challenge of MU is how to reliably audit whether a model has truly forgotten specified training data. Membership Inference Attacks (MIAs) are widely used for unlearning auditing, where sam",
    "url": "https://arxiv.org/abs/2602.01150",
    "source": "Arxiv AI"
  },
  {
    "title": "Multi-Horizon Electricity Price Forecasting with Deep Learning in the Australian National Electricity Market",
    "summary": "arXiv:2602.01157v1 Announce Type: cross Abstract: Accurate electricity price forecasting (EPF) is essential for operational planning, trading, and flexible asset scheduling in liberalised power systems, yet remains challenging due to volatility, heavy-tailed spikes, and frequent regime shifts. While deep learning (DL) has been increasingly adopted ",
    "url": "https://arxiv.org/abs/2602.01157",
    "source": "Arxiv AI"
  },
  {
    "title": "Semantically Aware UAV Landing Site Assessment from Remote Sensing Imagery via Multimodal Large Language Models",
    "summary": "arXiv:2602.01163v1 Announce Type: cross Abstract: Safe UAV emergency landing requires more than just identifying flat terrain; it demands understanding complex semantic risks (e.g., crowds, temporary structures) invisible to traditional geometric sensors. In this paper, we propose a novel framework leveraging Remote Sensing (RS) imagery and Multimo",
    "url": "https://arxiv.org/abs/2602.01163",
    "source": "Arxiv AI"
  },
  {
    "title": "FedBGS: A Blockchain Approach to Segment Gossip Learning in Decentralized Systems",
    "summary": "arXiv:2602.01185v1 Announce Type: cross Abstract: Privacy-Preserving Federated Learning (PPFL) is a Decentralized machine learning paradigm that enables multiple participants to collaboratively train a global model without sharing their data with the integration of cryptographic and privacy-based techniques to enhance the security of the global sys",
    "url": "https://arxiv.org/abs/2602.01185",
    "source": "Arxiv AI"
  },
  {
    "title": "The Gaussian-Head OFL Family: One-Shot Federated Learning from Client Global Statistics",
    "summary": "arXiv:2602.01186v1 Announce Type: cross Abstract: Classical Federated Learning relies on a multi-round iterative process of model exchange and aggregation between server and clients, with high communication costs and privacy risks from repeated model transmissions. In contrast, one-shot federated learning (OFL) alleviates these limitations by reduc",
    "url": "https://arxiv.org/abs/2602.01186",
    "source": "Arxiv AI"
  },
  {
    "title": "Autoregressive, Yet Revisable: In Decoding Revision for Secure Code Generation",
    "summary": "arXiv:2602.01187v1 Announce Type: cross Abstract: Large Language Model (LLM) based code generation is predominantly formulated as a strictly monotonic process, appending tokens linearly to an immutable prefix. This formulation contrasts to the cognitive process of programming, which is inherently interleaved with forward generation and on-the-fly r",
    "url": "https://arxiv.org/abs/2602.01187",
    "source": "Arxiv AI"
  },
  {
    "title": "AI Meets Plasticity: A Comprehensive Survey",
    "summary": "arXiv:2602.01215v1 Announce Type: cross Abstract: Artificial intelligence (AI) is rapidly emerging as a new paradigm of scientific discovery, namely data-driven science, across nearly all scientific disciplines. In materials science and engineering, AI has already begun to exert a transformative influence, making it both timely and necessary to exa",
    "url": "https://arxiv.org/abs/2602.01215",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning from Anonymized and Incomplete Tabular Data",
    "summary": "arXiv:2602.01217v1 Announce Type: cross Abstract: User-driven privacy allows individuals to control whether and at what granularity their data is shared, leading to datasets that mix original, generalized, and missing values within the same records and attributes. While such representations are intuitive for privacy, they pose challenges for machin",
    "url": "https://arxiv.org/abs/2602.01217",
    "source": "Arxiv AI"
  },
  {
    "title": "Supervised Fine-Tuning Needs to Unlock the Potential of Token Priority",
    "summary": "arXiv:2602.01227v1 Announce Type: cross Abstract: The transition from fitting empirical data to achieving true human utility is fundamentally constrained by a granularity mismatch, where fine-grained autoregressive generation is often supervised by coarse or uniform signals. This position paper advocates Token Priority as the essential bridge, form",
    "url": "https://arxiv.org/abs/2602.01227",
    "source": "Arxiv AI"
  },
  {
    "title": "Lotus: Efficient LLM Training by Randomized Low-Rank Gradient Projection with Adaptive Subspace Switching",
    "summary": "arXiv:2602.01233v1 Announce Type: cross Abstract: Training efficiency in large-scale models is typically assessed through memory consumption, training time, and model performance. Current methods often exhibit trade-offs among these metrics, as optimizing one generally degrades at least one of the others. Addressing this trade-off remains a central",
    "url": "https://arxiv.org/abs/2602.01233",
    "source": "Arxiv AI"
  },
  {
    "title": "Mechanistic Interpretability of Brain-to-Speech Models Across Speech Modes",
    "summary": "arXiv:2602.01247v1 Announce Type: cross Abstract: Brain-to-speech decoding models demonstrate robust performance in vocalized, mimed, and imagined speech; yet, the fundamental mechanisms via which these models capture and transmit information across different speech modalities are less explored. In this work, we use mechanistic interpretability to ",
    "url": "https://arxiv.org/abs/2602.01247",
    "source": "Arxiv AI"
  },
  {
    "title": "Sample Efficient Active Algorithms for Offline Reinforcement Learning",
    "summary": "arXiv:2602.01260v1 Announce Type: cross Abstract: Offline reinforcement learning (RL) enables policy learning from static data but often suffers from poor coverage of the state-action space and distributional shift problems. This problem can be addressed by allowing limited online interactions to selectively refine uncertain regions of the learned ",
    "url": "https://arxiv.org/abs/2602.01260",
    "source": "Arxiv AI"
  },
  {
    "title": "PACER: Blockwise Pre-verification for Speculative Decoding with Adaptive Length",
    "summary": "arXiv:2602.01274v1 Announce Type: cross Abstract: Speculative decoding (SD) is a powerful technique for accelerating the inference process of large language models (LLMs) without sacrificing accuracy. Typically, SD employs a small draft model to generate a fixed number of draft tokens, which are then verified in parallel by the target model. Howeve",
    "url": "https://arxiv.org/abs/2602.01274",
    "source": "Arxiv AI"
  },
  {
    "title": "Multi-LLM Adaptive Conformal Inference for Reliable LLM Responses",
    "summary": "arXiv:2602.01285v1 Announce Type: cross Abstract: Ensuring factuality is essential for the safe use of Large Language Models (LLMs) in high-stakes domains such as medicine and law. Conformal inference provides distribution-free guarantees, but existing approaches are either overly conservative, discarding many true-claims, or rely on adaptive error",
    "url": "https://arxiv.org/abs/2602.01285",
    "source": "Arxiv AI"
  },
  {
    "title": "Dispelling the Curse of Singularities in Neural Network Optimizations",
    "summary": "arXiv:2602.01308v1 Announce Type: cross Abstract: This work investigates the optimization instability of deep neural networks from a less-explored yet insightful perspective: the emergence and amplification of singularities in the parametric space. Our analysis reveals that parametric singularities inevitably grow with gradient updates and further ",
    "url": "https://arxiv.org/abs/2602.01308",
    "source": "Arxiv AI"
  },
  {
    "title": "EverMemBench: Benchmarking Long-Term Interactive Memory in Large Language Models",
    "summary": "arXiv:2602.01313v2 Announce Type: cross Abstract: Long-term conversational memory is essential for LLM-based assistants, yet existing benchmarks focus on dyadic, single-topic dialogues that fail to capture real-world complexity. We introduce EverMemBench, a benchmark featuring multi-party, multi-group conversations spanning over 1 million tokens wi",
    "url": "https://arxiv.org/abs/2602.01313",
    "source": "Arxiv AI"
  },
  {
    "title": "TxRay: Agentic Postmortem of Live Blockchain Attacks",
    "summary": "arXiv:2602.01317v1 Announce Type: cross Abstract: Decentralized Finance (DeFi) has turned blockchains into financial infrastructure, allowing anyone to trade, lend, and build protocols without intermediaries, but this openness exposes pools of value controlled by code. Within five years, the DeFi ecosystem has lost over 15.75B USD to reported explo",
    "url": "https://arxiv.org/abs/2602.01317",
    "source": "Arxiv AI"
  },
  {
    "title": "Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning",
    "summary": "arXiv:2602.01335v1 Announce Type: cross Abstract: A visual metaphor constitutes a high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment an",
    "url": "https://arxiv.org/abs/2602.01335",
    "source": "Arxiv AI"
  },
  {
    "title": "Adaptive Quantum-Safe Cryptography for 6G Vehicular Networks via Context-Aware Optimization",
    "summary": "arXiv:2602.01342v1 Announce Type: cross Abstract: Powerful quantum computers in the future may be able to break the security used for communication between vehicles and other devices (Vehicle-to-Everything, or V2X). New security methods called post-quantum cryptography can help protect these systems, but they often require more computing power and ",
    "url": "https://arxiv.org/abs/2602.01342",
    "source": "Arxiv AI"
  },
  {
    "title": "Towards knowledge-based workflows: a semantic approach to atomistic simulations for mechanical and thermodynamic properties",
    "summary": "arXiv:2602.01358v1 Announce Type: cross Abstract: Mechanical and thermodynamic properties, including the influence of crystal defects, are critical for evaluating materials in engineering applications. Molecular dynamics simulations provide valuable insight into these mechanisms at the atomic scale. However, current practice often relies on fragmen",
    "url": "https://arxiv.org/abs/2602.01358",
    "source": "Arxiv AI"
  },
  {
    "title": "PaAno: Patch-Based Representation Learning for Time-Series Anomaly Detection",
    "summary": "arXiv:2602.01359v1 Announce Type: cross Abstract: Although recent studies on time-series anomaly detection have increasingly adopted ever-larger neural network architectures such as transformers and foundation models, they incur high computational costs and memory usage, making them impractical for real-time and resource-constrained scenarios. More",
    "url": "https://arxiv.org/abs/2602.01359",
    "source": "Arxiv AI"
  },
  {
    "title": "When Domains Interact: Asymmetric and Order-Sensitive Cross-Domain Effects in Reinforcement Learning for Reasoning",
    "summary": "arXiv:2602.01365v1 Announce Type: cross Abstract: Group Relative Policy Optimization (GRPO) has become a key technique for improving reasoning abilities in large language models, yet its behavior under different domain sequencing strategies is poorly understood. In particular, the impact of sequential (one domain at a time) versus mixed-domain (mul",
    "url": "https://arxiv.org/abs/2602.01365",
    "source": "Arxiv AI"
  },
  {
    "title": "Deep Variational Contrastive Learning for Joint Risk Stratification and Time-to-Event Estimation",
    "summary": "arXiv:2602.01367v1 Announce Type: cross Abstract: Survival analysis is essential for clinical decision-making, as it allows practitioners to estimate time-to-event outcomes, stratify patient risk profiles, and guide treatment planning. Deep learning has revolutionized this field with unprecedented predictive capabilities but faces a fundamental tra",
    "url": "https://arxiv.org/abs/2602.01367",
    "source": "Arxiv AI"
  },
  {
    "title": "PolyGen: Fully Synthetic Vision-Language Training via Multi-Generator Ensembles",
    "summary": "arXiv:2602.01370v1 Announce Type: cross Abstract: Synthetic data offers a scalable solution for vision-language pre-training, yet current state-of-the-art methods typically rely on scaling up a single generative backbone, which introduces generator-specific spectral biases and limits feature diversity. In this work, we introduce PolyGen, a framewor",
    "url": "https://arxiv.org/abs/2602.01370",
    "source": "Arxiv AI"
  },
  {
    "title": "Context Dependence and Reliability in Autoregressive Language Models",
    "summary": "arXiv:2602.01378v1 Announce Type: cross Abstract: Large language models (LLMs) generate outputs by utilizing extensive context, which often includes redundant information from prompts, retrieved passages, and interaction history. In critical applications, it is vital to identify which context elements actually influence the output, as standard expl",
    "url": "https://arxiv.org/abs/2602.01378",
    "source": "Arxiv AI"
  },
  {
    "title": "\"If You're Very Clever, No One Knows You've Used It\": The Social Dynamics of Developing Generative AI Literacy in the Workplace",
    "summary": "arXiv:2602.01386v1 Announce Type: cross Abstract: Generative AI (GenAI) tools are rapidly transforming knowledge work, making AI literacy a critical priority for organizations. However, research on AI literacy lacks empirical insight into how knowledge workers' beliefs around GenAI literacy are shaped by the social dynamics of the workplace, and ho",
    "url": "https://arxiv.org/abs/2602.01386",
    "source": "Arxiv AI"
  },
  {
    "title": "How well can VLMs rate audio descriptions: A multi-dimensional quantitative assessment framework",
    "summary": "arXiv:2602.01390v1 Announce Type: cross Abstract: Digital video is central to communication, education, and entertainment, but without audio description (AD), blind and low-vision audiences are excluded. While crowdsourced platforms and vision-language-models (VLMs) expand AD production, quality is rarely checked systematically. Existing evaluation",
    "url": "https://arxiv.org/abs/2602.01390",
    "source": "Arxiv AI"
  },
  {
    "title": "An Odd Estimator for Shapley Values",
    "summary": "arXiv:2602.01399v1 Announce Type: cross Abstract: The Shapley value is a ubiquitous framework for attribution in machine learning, encompassing feature importance, data valuation, and causal inference. However, its exact computation is generally intractable, necessitating efficient approximation methods. While the most effective and popular estimat",
    "url": "https://arxiv.org/abs/2602.01399",
    "source": "Arxiv AI"
  },
  {
    "title": "From Pragmas to Partners: A Symbiotic Evolution of Agentic High-Level Synthesis",
    "summary": "arXiv:2602.01401v2 Announce Type: cross Abstract: The rise of large language models has sparked interest in AI-driven hardware design, raising the question: does high-level synthesis (HLS) still matter in the agentic era? We argue that HLS remains essential. While we expect mature agentic hardware systems to leverage both HLS and RTL, this paper fo",
    "url": "https://arxiv.org/abs/2602.01401",
    "source": "Arxiv AI"
  },
  {
    "title": "Semi-supervised CAPP Transformer Learning via Pseudo-labeling",
    "summary": "arXiv:2602.01419v1 Announce Type: cross Abstract: High-level Computer-Aided Process Planning (CAPP) generates manufacturing process plans from part specifications. It suffers from limited dataset availability in industry, reducing model generalization. We propose a semi-supervised learning approach to improve transformer-based CAPP transformer mode",
    "url": "https://arxiv.org/abs/2602.01419",
    "source": "Arxiv AI"
  },
  {
    "title": "DCD: Decomposition-based Causal Discovery from Autocorrelated and Non-Stationary Temporal Data",
    "summary": "arXiv:2602.01433v1 Announce Type: cross Abstract: Multivariate time series in domains such as finance, climate science, and healthcare often exhibit long-term trends, seasonal patterns, and short-term fluctuations, complicating causal inference under non-stationarity and autocorrelation. Existing causal discovery methods typically operate on raw ob",
    "url": "https://arxiv.org/abs/2602.01433",
    "source": "Arxiv AI"
  },
  {
    "title": "CIPHER: Cryptographic Insecurity Profiling via Hybrid Evaluation of Responses",
    "summary": "arXiv:2602.01438v1 Announce Type: cross Abstract: Large language models (LLMs) are increasingly used to assist developers with code, yet their implementations of cryptographic functionality often contain exploitable flaws. Minor design choices (e.g., static initialization vectors or missing authentication) can silently invalidate security guarantee",
    "url": "https://arxiv.org/abs/2602.01438",
    "source": "Arxiv AI"
  },
  {
    "title": "TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse",
    "summary": "arXiv:2602.01439v1 Announce Type: cross Abstract: Despite scale driving substantial recent advancements in machine learning, reinforcement learning (RL) methods still primarily use small value functions. Naively scaling value functions -- including with a transformer architecture, which is known to be highly scalable -- often results in learning in",
    "url": "https://arxiv.org/abs/2602.01439",
    "source": "Arxiv AI"
  },
  {
    "title": "The Gradient-Causal Gap: Why Gradient Importance Fails on Complex Tasks",
    "summary": "arXiv:2602.01442v1 Announce Type: cross Abstract: Removing ''important'' high-gradient components from a neural network can improve generalization, while removing unimportant'' low-gradient components can destroy it. We demonstrate this paradox by formalizing the \\textit{Gradient-Causal Gap} in Transformers trained on algorithmic tasks. While gradi",
    "url": "https://arxiv.org/abs/2602.01442",
    "source": "Arxiv AI"
  },
  {
    "title": "SentiFuse: Deep Multi-model Fusion Framework for Robust Sentiment Extraction",
    "summary": "arXiv:2602.01447v1 Announce Type: cross Abstract: Sentiment analysis models exhibit complementary strengths, yet existing approaches lack a unified framework for effective integration. We present SentiFuse, a flexible and model-agnostic framework that integrates heterogeneous sentiment models through a standardization layer and multiple fusion stra",
    "url": "https://arxiv.org/abs/2602.01447",
    "source": "Arxiv AI"
  },
  {
    "title": "Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles",
    "summary": "arXiv:2602.01452v1 Announce Type: cross Abstract: Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scen",
    "url": "https://arxiv.org/abs/2602.01452",
    "source": "Arxiv AI"
  },
  {
    "title": "Understanding vision transformer robustness through the lens of out-of-distribution detection",
    "summary": "arXiv:2602.01459v1 Announce Type: cross Abstract: Vision transformers have shown remarkable performance in vision tasks, but enabling them for accessible and real-time use is still challenging. Quantization reduces memory and inference costs at the risk of performance loss. Strides have been made to mitigate low precision issues mainly by understan",
    "url": "https://arxiv.org/abs/2602.01459",
    "source": "Arxiv AI"
  },
  {
    "title": "P-EAGLE: Parallel-Drafting EAGLE with Scalable Training",
    "summary": "arXiv:2602.01469v1 Announce Type: cross Abstract: Reasoning LLMs produce longer outputs, requiring speculative decoding drafters trained on extended sequences. Parallel drafting - predicting multiple tokens per forward pass - offers latency benefits over sequential generation, but training complexity scales quadratically with the product of sequenc",
    "url": "https://arxiv.org/abs/2602.01469",
    "source": "Arxiv AI"
  },
  {
    "title": "Rod Flow: A Continuous-Time Model for Gradient Descent at the Edge of Stability",
    "summary": "arXiv:2602.01480v1 Announce Type: cross Abstract: How can we understand gradient-based training over non-convex landscapes? The edge of stability phenomenon, introduced in Cohen et al. (2021), indicates that the answer is not so simple: namely, gradient descent (GD) with large step sizes often diverges away from the gradient flow. In this regime, t",
    "url": "https://arxiv.org/abs/2602.01480",
    "source": "Arxiv AI"
  },
  {
    "title": "Community-Level Modeling of Gyral Folding Patterns for Robust and Anatomically Informed Individualized Brain Mapping",
    "summary": "arXiv:2602.01482v1 Announce Type: cross Abstract: Cortical folding exhibits substantial inter-individual variability while preserving stable anatomical landmarks that enable fine-scale characterization of cortical organization. Among these, the three-hinge gyrus (3HG) serves as a key folding primitive, showing consistent topology yet meaningful var",
    "url": "https://arxiv.org/abs/2602.01482",
    "source": "Arxiv AI"
  },
  {
    "title": "Causal Preference Elicitation",
    "summary": "arXiv:2602.01483v1 Announce Type: cross Abstract: We propose causal preference elicitation, a Bayesian framework for expert-in-the-loop causal discovery that actively queries local edge relations to concentrate a posterior over directed acyclic graphs (DAGs). From any black-box observational posterior, we model noisy expert judgments with a three-w",
    "url": "https://arxiv.org/abs/2602.01483",
    "source": "Arxiv AI"
  },
  {
    "title": "OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference",
    "summary": "arXiv:2602.01493v1 Announce Type: cross Abstract: Solving diverse partial differential equations (PDEs) is fundamental in science and engineering. Large language models (LLMs) have demonstrated strong capabilities in code generation, symbolic reasoning, and tool use, but reliably solving PDEs across heterogeneous settings remains challenging. Prior",
    "url": "https://arxiv.org/abs/2602.01493",
    "source": "Arxiv AI"
  },
  {
    "title": "Draw2Learn: A Human-AI Collaborative Tool for Drawing-Based Science Learning",
    "summary": "arXiv:2602.01494v1 Announce Type: cross Abstract: Drawing supports learning by externalizing mental models, but providing timely feedback at scale remains challenging. We present Draw2Learn, a system that explores how AI can act as a supportive teammate during drawing-based learning. The design translates learning principles into concrete interacti",
    "url": "https://arxiv.org/abs/2602.01494",
    "source": "Arxiv AI"
  },
  {
    "title": "Governance at the Edge of Architecture: Regulating NeuroAI and Neuromorphic Systems",
    "summary": "arXiv:2602.01503v1 Announce Type: cross Abstract: Current AI governance frameworks, including regulatory benchmarks for accuracy, latency, and energy efficiency, are built for static, centrally trained artificial neural networks on von Neumann hardware. NeuroAI systems, embodied in neuromorphic hardware and implemented via spiking neural networks, ",
    "url": "https://arxiv.org/abs/2602.01503",
    "source": "Arxiv AI"
  },
  {
    "title": "Harnessing Flexible Spatial and Temporal Data Center Workloads for Grid Regulation Services",
    "summary": "arXiv:2602.01508v1 Announce Type: cross Abstract: Data centers (DCs) are increasingly recognized as flexible loads that can support grid frequency regulation. Yet, most existing methods treat workload scheduling and regulation capacity bidding separately, overlooking how queueing dynamics and spatial-temporal dispatch decisions affect the ability t",
    "url": "https://arxiv.org/abs/2602.01508",
    "source": "Arxiv AI"
  },
  {
    "title": "MarkCleaner: High-Fidelity Watermark Removal via Imperceptible Micro-Geometric Perturbation",
    "summary": "arXiv:2602.01513v1 Announce Type: cross Abstract: Semantic watermarks exhibit strong robustness against conventional image-space attacks. In this work, we show that such robustness does not survive under micro-geometric perturbations: spatial displacements can remove watermarks by breaking the phase alignment. Motivated by this observation, we intr",
    "url": "https://arxiv.org/abs/2602.01513",
    "source": "Arxiv AI"
  },
  {
    "title": "White-Box Neural Ensemble for Vehicular Plasticity: Quantifying the Efficiency Cost of Symbolic Auditability in Adaptive NMPC",
    "summary": "arXiv:2602.01516v1 Announce Type: cross Abstract: We present a white-box adaptive NMPC architecture that resolves vehicular plasticity (adaptation to varying operating regimes without retraining) by arbitrating among frozen, regime-specific neural specialists using a Modular Sovereignty paradigm. The ensemble dynamics are maintained as a fully trav",
    "url": "https://arxiv.org/abs/2602.01516",
    "source": "Arxiv AI"
  },
  {
    "title": "You Need an Encoder for Native Position-Independent Caching",
    "summary": "arXiv:2602.01519v1 Announce Type: cross Abstract: The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incu",
    "url": "https://arxiv.org/abs/2602.01519",
    "source": "Arxiv AI"
  },
  {
    "title": "A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning",
    "summary": "arXiv:2602.01523v1 Announce Type: cross Abstract: Reinforcement learning (RL) is a dominant paradigm for improving the reasoning abilities of large language models, yet its effectiveness varies across tasks and compute budgets. We propose a \\emph{relative-budget} theory explaining this variation through a single quantity called relative budget $\\xi",
    "url": "https://arxiv.org/abs/2602.01523",
    "source": "Arxiv AI"
  },
  {
    "title": "Toward a Machine Bertin: Why Visualization Needs Design Principles for Machine Cognition",
    "summary": "arXiv:2602.01527v1 Announce Type: cross Abstract: Visualization's design knowledge-effectiveness rankings, encoding guidelines, color models, preattentive processing rules -- derives from six decades of psychophysical studies of human vision. Yet vision-language models (VLMs) increasingly consume chart images in automated analysis pipelines, and a ",
    "url": "https://arxiv.org/abs/2602.01527",
    "source": "Arxiv AI"
  },
  {
    "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
    "summary": "arXiv:2602.01538v1 Announce Type: cross Abstract: Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned int",
    "url": "https://arxiv.org/abs/2602.01538",
    "source": "Arxiv AI"
  },
  {
    "title": "Toward Cognitive Supersensing in Multimodal Large Language Model",
    "summary": "arXiv:2602.01541v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Tho",
    "url": "https://arxiv.org/abs/2602.01541",
    "source": "Arxiv AI"
  },
  {
    "title": "Plain Transformers are Surprisingly Powerful Link Predictors",
    "summary": "arXiv:2602.01553v1 Announce Type: cross Abstract: Link prediction is a core challenge in graph machine learning, demanding models that capture rich and complex topological dependencies. While Graph Neural Networks (GNNs) are the standard solution, state-of-the-art pipelines often rely on explicit structural heuristics or memory-intensive node embed",
    "url": "https://arxiv.org/abs/2602.01553",
    "source": "Arxiv AI"
  },
  {
    "title": "InfoTok: Regulating Information Flow for Capacity-Constrained Shared Visual Tokenization in Unified MLLMs",
    "summary": "arXiv:2602.01554v1 Announce Type: cross Abstract: Unified multimodal large language models (MLLMs) integrate image understanding and generation in a single framework, with the visual tokenizer acting as the sole interface that maps visual inputs into tokens for downstream tasks. However, existing shared-token designs are mostly architecture-driven ",
    "url": "https://arxiv.org/abs/2602.01554",
    "source": "Arxiv AI"
  },
  {
    "title": "Multimodal UNcommonsense: From Odd to Ordinary and Ordinary to Odd",
    "summary": "arXiv:2602.01561v1 Announce Type: cross Abstract: Commonsense reasoning in multimodal contexts remains a foundational challenge in artificial intelligence. We introduce Multimodal UNcommonsense(MUN), a benchmark designed to evaluate models' ability to handle scenarios that deviate from typical visual or contextual expectations. MUN pairs visual sce",
    "url": "https://arxiv.org/abs/2602.01561",
    "source": "Arxiv AI"
  },
  {
    "title": "DREAMS: A Social Exchange Theory-Informed Modeling of Misinformation Engagement on Social Media",
    "summary": "arXiv:2602.01567v1 Announce Type: cross Abstract: Social media engagement prediction is a central challenge in computational social science, particularly for understanding how users interact with misinformation. Existing approaches often treat engagement as a homogeneous time-series signal, overlooking the heterogeneous social mechanisms and platfo",
    "url": "https://arxiv.org/abs/2602.01567",
    "source": "Arxiv AI"
  },
  {
    "title": "Generative Visual Code Mobile World Models",
    "summary": "arXiv:2602.01576v1 Announce Type: cross Abstract: Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity, while the inability of visual WMs in precise text ",
    "url": "https://arxiv.org/abs/2602.01576",
    "source": "Arxiv AI"
  },
  {
    "title": "DrawSim-PD: Simulating Student Science Drawings to Support NGSS-Aligned Teacher Diagnostic Reasoning",
    "summary": "arXiv:2602.01578v1 Announce Type: cross Abstract: Developing expertise in diagnostic reasoning requires practice with diverse student artifacts, yet privacy regulations prohibit sharing authentic student work for teacher professional development (PD) at scale. We present DrawSim-PD, the first generative framework that simulates NGSS-aligned, studen",
    "url": "https://arxiv.org/abs/2602.01578",
    "source": "Arxiv AI"
  },
  {
    "title": "On the Fragility of AI-Based Channel Decoders under Small Channel Perturbations",
    "summary": "arXiv:2602.01582v1 Announce Type: cross Abstract: Recent advances in deep learning have led to AI-based error correction decoders that report empirical performance improvements over traditional belief-propagation (BP) decoding on AWGN channels. While such gains are promising, a fundamental question remains: where do these improvements come from, an",
    "url": "https://arxiv.org/abs/2602.01582",
    "source": "Arxiv AI"
  },
  {
    "title": "Provable Defense Framework for LLM Jailbreaks via Noise-Augumented Alignment",
    "summary": "arXiv:2602.01587v1 Announce Type: cross Abstract: Large Language Models (LLMs) remain vulnerable to adaptive jailbreaks that easily bypass empirical defenses like GCG. We propose a framework for certifiable robustness that shifts safety guarantees from single-pass inference to the statistical stability of an ensemble. We introduce Certified Semanti",
    "url": "https://arxiv.org/abs/2602.01587",
    "source": "Arxiv AI"
  },
  {
    "title": "Spectral Text Fusion: A Frequency-Aware Approach to Multimodal Time-Series Forecasting",
    "summary": "arXiv:2602.01588v2 Announce Type: cross Abstract: Multimodal time series forecasting is crucial in real-world applications, where decisions depend on both numerical data and contextual signals. The core challenge is to effectively combine temporal numerical patterns with the context embedded in other modalities, such as text. While most existing me",
    "url": "https://arxiv.org/abs/2602.01588",
    "source": "Arxiv AI"
  },
  {
    "title": "The Multiple Ticket Hypothesis: Random Sparse Subnetworks Suffice for RLVR",
    "summary": "arXiv:2602.01599v1 Announce Type: cross Abstract: The Lottery Ticket Hypothesis demonstrated that sparse subnetworks can match full-model performance, suggesting parameter redundancy. Meanwhile, in Reinforcement Learning with Verifiable Rewards (RLVR), recent work has shown that updates concentrate on a sparse subset of parameters, which further le",
    "url": "https://arxiv.org/abs/2602.01599",
    "source": "Arxiv AI"
  },
  {
    "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
    "summary": "arXiv:2602.01601v2 Announce Type: cross Abstract: Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and c",
    "url": "https://arxiv.org/abs/2602.01601",
    "source": "Arxiv AI"
  },
  {
    "title": "Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching",
    "summary": "arXiv:2602.01606v1 Announce Type: cross Abstract: Diffusion policies are expressive yet incur high inference latency. Flow Matching (FM) enables one-step generation, but integrating it into Maximum Entropy Reinforcement Learning (MaxEnt RL) is challenging: the optimal policy is an intractable energy-based distribution, and the efficient log-likelih",
    "url": "https://arxiv.org/abs/2602.01606",
    "source": "Arxiv AI"
  },
  {
    "title": "A Practical Tensor-Network Compression Pipeline for Production-Scale Large Language Models",
    "summary": "arXiv:2602.01613v1 Announce Type: cross Abstract: Large language models are limited in deployment by GPU memory and inference latency. We present Minima, a production compression pipeline that learns where and how to structurally compress a Transformer and turns that compression into real serving gains. Minima trains a lightweight convolutional pre",
    "url": "https://arxiv.org/abs/2602.01613",
    "source": "Arxiv AI"
  },
  {
    "title": "AgroFlux: A Spatial-Temporal Benchmark for Carbon and Nitrogen Flux Prediction in Agricultural Ecosystems",
    "summary": "arXiv:2602.01614v1 Announce Type: cross Abstract: Agroecosystem, which heavily influenced by human actions and accounts for a quarter of global greenhouse gas emissions (GHGs), plays a crucial role in mitigating global climate change and securing environmental sustainability. However, we can't manage what we can't measure. Accurately quantifying th",
    "url": "https://arxiv.org/abs/2602.01614",
    "source": "Arxiv AI"
  },
  {
    "title": "SUSD: Structured Unsupervised Skill Discovery through State Factorization",
    "summary": "arXiv:2602.01619v1 Announce Type: cross Abstract: Unsupervised Skill Discovery (USD) aims to autonomously learn a diverse set of skills without relying on extrinsic rewards. One of the most common USD approaches is to maximize the Mutual Information (MI) between skill latent variables and states. However, MI-based methods tend to favor simple, stat",
    "url": "https://arxiv.org/abs/2602.01619",
    "source": "Arxiv AI"
  },
  {
    "title": "Toward Enhancing Representation Learning in Federated Multi-Task Settings",
    "summary": "arXiv:2602.01626v1 Announce Type: cross Abstract: Federated multi-task learning (FMTL) seeks to collaboratively train customized models for users with different tasks while preserving data privacy. Most existing approaches assume model congruity (i.e., the use of fully or partially homogeneous models) across users, which limits their applicability ",
    "url": "https://arxiv.org/abs/2602.01626",
    "source": "Arxiv AI"
  },
  {
    "title": "HuPER: A Human-Inspired Framework for Phonetic Perception",
    "summary": "arXiv:2602.01634v1 Announce Type: cross Abstract: We propose HuPER, a human-inspired framework that models phonetic perception as adaptive inference over acoustic-phonetics evidence and linguistic knowledge. With only 100 hours of training data, HuPER achieves state-of-the-art phonetic error rates on five English benchmarks and strong zero-shot tra",
    "url": "https://arxiv.org/abs/2602.01634",
    "source": "Arxiv AI"
  },
  {
    "title": "The Effect of Mini-Batch Noise on the Implicit Bias of Adam",
    "summary": "arXiv:2602.01642v1 Announce Type: cross Abstract: With limited high-quality data and growing compute, multi-epoch training is gaining back its importance across sub-areas of deep learning. Adam(W), versions of which are go-to optimizers for many tasks such as next token prediction, has two momentum hyperparameters $(\\beta_1, \\beta_2)$ controlling m",
    "url": "https://arxiv.org/abs/2602.01642",
    "source": "Arxiv AI"
  },
  {
    "title": "De Novo Molecular Generation from Mass Spectra via Many-Body Enhanced Diffusion",
    "summary": "arXiv:2602.01643v1 Announce Type: cross Abstract: Molecular structure generation from mass spectrometry is fundamental for understanding cellular metabolism and discovering novel compounds. Although tandem mass spectrometry (MS/MS) enables the high-throughput acquisition of fragment fingerprints, these spectra often reflect higher-order interaction",
    "url": "https://arxiv.org/abs/2602.01643",
    "source": "Arxiv AI"
  },
  {
    "title": "From Perception to Action: Spatial AI Agents and World Models",
    "summary": "arXiv:2602.01644v1 Announce Type: cross Abstract: While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physica",
    "url": "https://arxiv.org/abs/2602.01644",
    "source": "Arxiv AI"
  },
  {
    "title": "Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning",
    "summary": "arXiv:2602.01649v1 Announce Type: cross Abstract: Video large language models have demonstrated remarkable capabilities in video understanding tasks. However, the redundancy of video tokens introduces significant computational overhead during inference, limiting their practical deployment. Many compression algorithms are proposed to prioritize reta",
    "url": "https://arxiv.org/abs/2602.01649",
    "source": "Arxiv AI"
  },
  {
    "title": "On the Spatiotemporal Dynamics of Generalization in Neural Networks",
    "summary": "arXiv:2602.01651v1 Announce Type: cross Abstract: Why do neural networks fail to generalize addition from 16-digit to 32-digit numbers, while a child who learns the rule can apply it to arbitrarily long sequences? We argue that this failure is not an engineering problem but a violation of physical postulates. Drawing inspiration from physics, we id",
    "url": "https://arxiv.org/abs/2602.01651",
    "source": "Arxiv AI"
  },
  {
    "title": "Efficient Adversarial Attacks on High-dimensional Offline Bandits",
    "summary": "arXiv:2602.01658v1 Announce Type: cross Abstract: Bandit algorithms have recently emerged as a powerful tool for evaluating machine learning models, including generative image models and large language models, by efficiently identifying top-performing candidates without exhaustive comparisons. These methods typically rely on a reward model, often d",
    "url": "https://arxiv.org/abs/2602.01658",
    "source": "Arxiv AI"
  },
  {
    "title": "CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation",
    "summary": "arXiv:2602.01660v1 Announce Type: cross Abstract: Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions. However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this",
    "url": "https://arxiv.org/abs/2602.01660",
    "source": "Arxiv AI"
  },
  {
    "title": "TABX: A High-Throughput Sandbox Battle Simulator for Multi-Agent Reinforcement Learning",
    "summary": "arXiv:2602.01665v1 Announce Type: cross Abstract: The design of environments plays a critical role in shaping the development and evaluation of cooperative multi-agent reinforcement learning (MARL) algorithms. While existing benchmarks highlight critical challenges, they often lack the modularity required to design custom evaluation scenarios. We i",
    "url": "https://arxiv.org/abs/2602.01665",
    "source": "Arxiv AI"
  },
  {
    "title": "ASGMamba: Adaptive Spectral Gating Mamba for Multivariate Time Series Forecasting",
    "summary": "arXiv:2602.01668v1 Announce Type: cross Abstract: Long-term multivariate time series forecasting (LTSF) plays a crucial role in various high-performance computing applications, including real-time energy grid management and large-scale traffic flow simulation. However, existing solutions face a dilemma: Transformer-based models suffer from quadrati",
    "url": "https://arxiv.org/abs/2602.01668",
    "source": "Arxiv AI"
  },
  {
    "title": "AI-Assisted Adaptive Rendering for High-Frequency Security Telemetry in Web Interfaces",
    "summary": "arXiv:2602.01671v1 Announce Type: cross Abstract: Modern cybersecurity platforms must process and display high-frequency telemetry such as network logs, endpoint events, alerts, and policy changes in real time. Traditional rendering techniques based on static pagination or fixed polling intervals fail under volume conditions exceeding hundreds of t",
    "url": "https://arxiv.org/abs/2602.01671",
    "source": "Arxiv AI"
  },
  {
    "title": "Real-Time Loop Closure Detection in Visual SLAM via NetVLAD and Faiss",
    "summary": "arXiv:2602.01673v1 Announce Type: cross Abstract: Loop closure detection (LCD) is a core component of simultaneous localization and mapping (SLAM): it identifies revisited places and enables pose-graph constraints that correct accumulated drift. Classic bag-of-words approaches such as DBoW are efficient but often degrade under appearance change and",
    "url": "https://arxiv.org/abs/2602.01673",
    "source": "Arxiv AI"
  },
  {
    "title": "Towards Autonomous Instrument Tray Assembly for Sterile Processing Applications",
    "summary": "arXiv:2602.01679v1 Announce Type: cross Abstract: The Sterile Processing and Distribution (SPD) department is responsible for cleaning, disinfecting, inspecting, and assembling surgical instruments between surgeries. Manual inspection and preparation of instrument trays is a time-consuming, error-prone task, often prone to contamination and instrum",
    "url": "https://arxiv.org/abs/2602.01679",
    "source": "Arxiv AI"
  },
  {
    "title": "FreshMem: Brain-Inspired Frequency-Space Hybrid Memory for Streaming Video Understanding",
    "summary": "arXiv:2602.01683v1 Announce Type: cross Abstract: Transitioning Multimodal Large Language Models (MLLMs) from offline to online streaming video understanding is essential for continuous perception. However, existing methods lack flexible adaptivity, leading to irreversible detail loss and context fragmentation. To resolve this, we propose FreshMem,",
    "url": "https://arxiv.org/abs/2602.01683",
    "source": "Arxiv AI"
  },
  {
    "title": "The Strategic Foresight of LLMs: Evidence from a Fully Prospective Venture Tournament",
    "summary": "arXiv:2602.01684v1 Announce Type: cross Abstract: Can artificial intelligence outperform humans at strategic foresight -- the capacity to form accurate judgments about uncertain, high-stakes outcomes before they unfold? We address this question through a fully prospective prediction tournament using live Kickstarter crowdfunding projects. Thirty U.",
    "url": "https://arxiv.org/abs/2602.01684",
    "source": "Arxiv AI"
  },
  {
    "title": "Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment",
    "summary": "arXiv:2602.01685v1 Announce Type: cross Abstract: Large language models (LLMs) are commonly aligned with human preferences using reinforcement learning from human feedback (RLHF). In this method, LLM policies are generally optimized through reward maximization with Kullback-Leibler (KL) divergence regularization of the reference policy. However, KL",
    "url": "https://arxiv.org/abs/2602.01685",
    "source": "Arxiv AI"
  },
  {
    "title": "Counting Hypothesis: Potential Mechanism of In-Context Learning",
    "summary": "arXiv:2602.01687v1 Announce Type: cross Abstract: In-Context Learning (ICL) indicates that large language models (LLMs) pretrained on a massive amount of data can learn specific tasks from input prompts' examples. ICL is notable for two reasons. First, it does not need modification of LLMs' internal structure. Second, it enables LLMs to perform a w",
    "url": "https://arxiv.org/abs/2602.01687",
    "source": "Arxiv AI"
  },
  {
    "title": "Cross-Modal Alignment and Fusion for RGB-D Transmission-Line Defect Detection",
    "summary": "arXiv:2602.01696v2 Announce Type: cross Abstract: Transmission line defect detection remains challenging for automated UAV inspection due to the dominance of small-scale defects, complex backgrounds, and illumination variations. Existing RGB-based detectors, despite recent progress, struggle to distinguish geometrically subtle defects from visually",
    "url": "https://arxiv.org/abs/2602.01696",
    "source": "Arxiv AI"
  },
  {
    "title": "Meta Engine: A Unified Semantic Query Engine on Heterogeneous LLM-Based Query Systems",
    "summary": "arXiv:2602.01701v1 Announce Type: cross Abstract: With the increasingly use of multi-modal data, semantic query has become more and more demanded in data management systems, which is an important way to access and analyze multi-modal data. As unstructured data, most information of multi-modal data (text, image, video, etc) hides in the semantics, w",
    "url": "https://arxiv.org/abs/2602.01701",
    "source": "Arxiv AI"
  },
  {
    "title": "Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner",
    "summary": "arXiv:2602.01705v1 Announce Type: cross Abstract: Recent reinforcement learning (RL) methods improve LLM reasoning by optimizing discrete Chain-of-Thought (CoT) generation; however, exploration in token space often suffers from diversity collapse as policy entropy decreases due to mode elicitation behavior in discrete RL. To mitigate this issue, we",
    "url": "https://arxiv.org/abs/2602.01705",
    "source": "Arxiv AI"
  },
  {
    "title": "Game of Thought: Robust Information Seeking with Large Language Models Using Game Theory",
    "summary": "arXiv:2602.01708v1 Announce Type: cross Abstract: Large Language Models (LLMs) are increasingly deployed in real-world scenarios where they may lack sufficient information to complete a given task. In such settings, the ability to actively seek out missing information becomes a critical capability. Existing approaches to enhancing this ability ofte",
    "url": "https://arxiv.org/abs/2602.01708",
    "source": "Arxiv AI"
  },
  {
    "title": "Physics Informed Generative AI Enabling Labour Free Segmentation For Microscopy Analysis",
    "summary": "arXiv:2602.01710v1 Announce Type: cross Abstract: Semantic segmentation of microscopy images is a critical task for high-throughput materials characterisation, yet its automation is severely constrained by the prohibitive cost, subjectivity, and scarcity of expert-annotated data. While physics-based simulations offer a scalable alternative to manua",
    "url": "https://arxiv.org/abs/2602.01710",
    "source": "Arxiv AI"
  },
  {
    "title": "BBPE16: UTF-16-based byte-level byte-pair encoding for improved multilingual speech recognition",
    "summary": "arXiv:2602.01717v1 Announce Type: cross Abstract: Multilingual automatic speech recognition (ASR) requires tokenization that efficiently covers many writing systems. Byte-level BPE (BBPE) using UTF-8 is widely adopted for its language-agnostic design and full Unicode coverage, but its variable-length encoding inflates token sequences for non-Latin ",
    "url": "https://arxiv.org/abs/2602.01717",
    "source": "Arxiv AI"
  },
  {
    "title": "SafePred: A Predictive Guardrail for Computer-Using Agents via World Models",
    "summary": "arXiv:2602.01725v1 Announce Type: cross Abstract: With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observatio",
    "url": "https://arxiv.org/abs/2602.01725",
    "source": "Arxiv AI"
  },
  {
    "title": "Softmax Linear Attention: Reclaiming Global Competition",
    "summary": "arXiv:2602.01744v1 Announce Type: cross Abstract: While linear attention reduces the quadratic complexity of standard Transformers to linear time, it often lags behind in expressivity due to the removal of softmax normalization. This omission eliminates \\emph{global competition}, a critical mechanism that enables models to sharply focus on relevant",
    "url": "https://arxiv.org/abs/2602.01744",
    "source": "Arxiv AI"
  },
  {
    "title": "Probability-Entropy Calibration: An Elastic Indicator for Adaptive Fine-tuning",
    "summary": "arXiv:2602.01745v1 Announce Type: cross Abstract: Token-level reweighting is a simple yet effective mechanism for controlling supervised fine-tuning, but common indicators are largely one-dimensional: the ground-truth probability reflects downstream alignment, while token entropy reflects intrinsic uncertainty induced by the pre-training prior. Ign",
    "url": "https://arxiv.org/abs/2602.01745",
    "source": "Arxiv AI"
  },
  {
    "title": "Rethinking LoRA for Data Heterogeneous Federated Learning: Subspace and State Alignment",
    "summary": "arXiv:2602.01746v1 Announce Type: cross Abstract: Low-Rank Adaptation (LoRA) is widely used for federated fine-tuning. Yet under non-IID settings, it can substantially underperform full-parameter fine-tuning. Through with-high-probability robustness analysis, we uncover that this gap can be attributed to two coupled mismatches: (i) update-space mis",
    "url": "https://arxiv.org/abs/2602.01746",
    "source": "Arxiv AI"
  },
  {
    "title": "A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention",
    "summary": "arXiv:2602.01763v1 Announce Type: cross Abstract: Transformers serve as the foundation of most modern large language models. To mitigate the quadratic complexity of standard full attention, various efficient attention mechanisms, such as linear and hybrid attention, have been developed. A fundamental gap remains: their expressive power relative to ",
    "url": "https://arxiv.org/abs/2602.01763",
    "source": "Arxiv AI"
  },
  {
    "title": "Backdoor Sentinel: Detecting and Detoxifying Backdoors in Diffusion Models via Temporal Noise Consistency",
    "summary": "arXiv:2602.01765v1 Announce Type: cross Abstract: Diffusion models have been widely deployed in AIGC services; however, their reliance on opaque training data and procedures exposes a broad attack surface for backdoor injection. In practical auditing scenarios, due to the protection of intellectual property and commercial confidentiality, auditors ",
    "url": "https://arxiv.org/abs/2602.01765",
    "source": "Arxiv AI"
  },
  {
    "title": "CoMeT: Collaborative Memory Transformer for Efficient Long Context Modeling",
    "summary": "arXiv:2602.01766v1 Announce Type: cross Abstract: The quadratic complexity and indefinitely growing key-value (KV) cache of standard Transformers pose a major barrier to long-context processing. To overcome this, we introduce the Collaborative Memory Transformer (CoMeT), a novel architecture that enables LLMs to handle arbitrarily long sequences wi",
    "url": "https://arxiv.org/abs/2602.01766",
    "source": "Arxiv AI"
  },
  {
    "title": "IRIS: Implicit Reward-Guided Internal Sifting for Mitigating Multimodal Hallucination",
    "summary": "arXiv:2602.01769v2 Announce Type: cross Abstract: Hallucination remains a fundamental challenge for Multimodal Large Language Models (MLLMs). While Direct Preference Optimization (DPO) is a key alignment framework, existing approaches often rely heavily on costly external evaluators for scoring or rewriting, incurring off-policy learnability gaps a",
    "url": "https://arxiv.org/abs/2602.01769",
    "source": "Arxiv AI"
  },
  {
    "title": "<SOG_k>: One LLM Token for Explicit Graph Structural Understanding",
    "summary": "arXiv:2602.01771v1 Announce Type: cross Abstract: Large language models show great potential in unstructured data understanding, but still face significant challenges with graphs due to their structural hallucination. Existing approaches mainly either verbalize graphs into natural language, which leads to excessive token consumption and scattered a",
    "url": "https://arxiv.org/abs/2602.01771",
    "source": "Arxiv AI"
  },
  {
    "title": "DIA-CLIP: a universal representation learning framework for zero-shot DIA proteomics",
    "summary": "arXiv:2602.01772v1 Announce Type: cross Abstract: Data-independent acquisition mass spectrometry (DIA-MS) has established itself as a cornerstone of proteomic profiling and large-scale systems biology, offering unparalleled depth and reproducibility. Current DIA analysis frameworks, however, require semi-supervised training within each run for pept",
    "url": "https://arxiv.org/abs/2602.01772",
    "source": "Arxiv AI"
  },
  {
    "title": "Stein-Rule Shrinkage for Stochastic Gradient Estimation in High Dimensions",
    "summary": "arXiv:2602.01777v1 Announce Type: cross Abstract: Stochastic gradient methods are central to large-scale learning, yet their analysis typically treats mini-batch gradients as unbiased estimators of the population gradient. In high-dimensional settings, however, classical results from statistical decision theory show that unbiased estimators are gen",
    "url": "https://arxiv.org/abs/2602.01777",
    "source": "Arxiv AI"
  },
  {
    "title": "RedVisor: Reasoning-Aware Prompt Injection Defense via Zero-Copy KV Cache Reuse",
    "summary": "arXiv:2602.01795v1 Announce Type: cross Abstract: Large Language Models (LLMs) are increasingly vulnerable to Prompt Injection (PI) attacks, where adversarial instructions hidden within retrieved contexts hijack the model's execution flow. Current defenses typically face a critical trade-off: prevention-based fine-tuning often degrades general util",
    "url": "https://arxiv.org/abs/2602.01795",
    "source": "Arxiv AI"
  },
  {
    "title": "Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention",
    "summary": "arXiv:2602.01801v1 Announce Type: cross Abstract: Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing",
    "url": "https://arxiv.org/abs/2602.01801",
    "source": "Arxiv AI"
  },
  {
    "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It",
    "summary": "arXiv:2602.01826v1 Announce Type: cross Abstract: Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to \"training inference mismatch stemming\" from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this wo",
    "url": "https://arxiv.org/abs/2602.01826",
    "source": "Arxiv AI"
  },
  {
    "title": "DOGMA: Weaving Structural Information into Data-centric Single-cell Transcriptomics Analysis",
    "summary": "arXiv:2602.01839v1 Announce Type: cross Abstract: Recently, data-centric AI methodology has been a dominant paradigm in single-cell transcriptomics analysis, which treats data representation rather than model complexity as the fundamental bottleneck. In the review of current studies, earlier sequence methods treat cells as independent entities and ",
    "url": "https://arxiv.org/abs/2602.01839",
    "source": "Arxiv AI"
  },
  {
    "title": "CloDS: Visual-Only Unsupervised Cloth Dynamics Learning in Unknown Conditions",
    "summary": "arXiv:2602.01844v1 Announce Type: cross Abstract: Deep learning has demonstrated remarkable capabilities in simulating complex dynamic systems. However, existing methods require known physical properties as supervision or inputs, limiting their applicability under unknown conditions. To explore this challenge, we introduce Cloth Dynamics Grounding ",
    "url": "https://arxiv.org/abs/2602.01844",
    "source": "Arxiv AI"
  },
  {
    "title": "Time2Vec Transformer for Robust Gesture Recognition from Low-Density sEMG",
    "summary": "arXiv:2602.01855v2 Announce Type: cross Abstract: Accurate and responsive myoelectric prosthesis control typically relies on complex, dense multi-sensor arrays, which limits consumer accessibility. This paper presents a novel, data-efficient deep learning framework designed to achieve precise and accurate control using minimal sensor hardware. Leve",
    "url": "https://arxiv.org/abs/2602.01855",
    "source": "Arxiv AI"
  },
  {
    "title": "GRAB: An LLM-Inspired Sequence-First Click-Through Rate Prediction Modeling Paradigm",
    "summary": "arXiv:2602.01865v2 Announce Type: cross Abstract: Traditional Deep Learning Recommendation Models (DLRMs) face increasing bottlenecks in performance and efficiency, often struggling with generalization and long-sequence modeling. Inspired by the scaling success of Large Language Models (LLMs), we propose Generative Ranking for Ads at Baidu (GRAB), ",
    "url": "https://arxiv.org/abs/2602.01865",
    "source": "Arxiv AI"
  },
  {
    "title": "ES-MemEval: Benchmarking Conversational Agents on Personalized Long-Term Emotional Support",
    "summary": "arXiv:2602.01885v1 Announce Type: cross Abstract: Large Language Models (LLMs) have shown strong potential as conversational agents. Yet, their effectiveness remains limited by deficiencies in robust long-term memory, particularly in complex, long-term web-based services such as online emotional support. However, existing long-term dialogue benchma",
    "url": "https://arxiv.org/abs/2602.01885",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning Sparse Visual Representations via Spatial-Semantic Factorization",
    "summary": "arXiv:2602.01905v1 Announce Type: cross Abstract: Self-supervised learning (SSL) faces a fundamental conflict between semantic understanding and image reconstruction. High-level semantic SSL (e.g., DINO) relies on global tokens that are forced to be location-invariant for augmentation alignment, a process that inherently discards the spatial coordi",
    "url": "https://arxiv.org/abs/2602.01905",
    "source": "Arxiv AI"
  },
  {
    "title": "DSXFormer: Dual-Pooling Spectral Squeeze-Expansion and Dynamic Context Attention Transformer for Hyperspectral Image Classification",
    "summary": "arXiv:2602.01906v1 Announce Type: cross Abstract: Hyperspectral image classification (HSIC) is a challenging task due to high spectral dimensionality, complex spectral-spatial correlations, and limited labeled training samples. Although transformer-based models have shown strong potential for HSIC, existing approaches often struggle to achieve suff",
    "url": "https://arxiv.org/abs/2602.01906",
    "source": "Arxiv AI"
  },
  {
    "title": "Reliable Real-Time Value at Risk Estimation via Quantile Regression Forest with Conformal Calibration",
    "summary": "arXiv:2602.01912v1 Announce Type: cross Abstract: Rapidly evolving market conditions call for real-time risk monitoring, but its online estimation remains challenging. In this paper, we study the online estimation of one of the most widely used risk measures, Value at Risk (VaR). Its accurate and reliable estimation is essential for timely risk con",
    "url": "https://arxiv.org/abs/2602.01912",
    "source": "Arxiv AI"
  },
  {
    "title": "VLM-Guided Experience Replay",
    "summary": "arXiv:2602.01915v1 Announce Type: cross Abstract: Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) have enabled powerful semantic and multimodal reasoning capabilities, creating new opportunities to enhance sample efficiency, high-level planning, and interpretability in reinforcement learning (RL). While prior work ",
    "url": "https://arxiv.org/abs/2602.01915",
    "source": "Arxiv AI"
  },
  {
    "title": "PIMPC-GNN: Physics-Informed Multi-Phase Consensus Learning for Enhancing Imbalanced Node Classification in Graph Neural Networks",
    "summary": "arXiv:2602.01920v1 Announce Type: cross Abstract: Graph neural networks (GNNs) often struggle in class-imbalanced settings, where minority classes are under-represented and predictions are biased toward majorities. We propose \\textbf{PIMPC-GNN}, a physics-informed multi-phase consensus framework for imbalanced node classification. Our method integr",
    "url": "https://arxiv.org/abs/2602.01920",
    "source": "Arxiv AI"
  },
  {
    "title": "COLT: Lightweight Multi-LLM Collaboration through Shared MCTS Reasoning for Model Compilation",
    "summary": "arXiv:2602.01935v1 Announce Type: cross Abstract: Model serving costs dominate AI systems, making compiler optimization essential for scalable deployment. Recent works show that a large language model (LLM) can guide compiler search by reasoning over program structure and optimization history. However, using a single large model throughout the sear",
    "url": "https://arxiv.org/abs/2602.01935",
    "source": "Arxiv AI"
  },
  {
    "title": "PIMCST: Physics-Informed Multi-Phase Consensus and Spatio-Temporal Few-Shot Learning for Traffic Flow Forecasting",
    "summary": "arXiv:2602.01936v1 Announce Type: cross Abstract: Accurate traffic flow prediction remains a fundamental challenge in intelligent transportation systems, particularly in cross-domain, data-scarce scenarios where limited historical data hinders model training and generalisation. The complex spatio-temporal dependencies and nonlinear dynamics of urba",
    "url": "https://arxiv.org/abs/2602.01936",
    "source": "Arxiv AI"
  },
  {
    "title": "T-LLM: Teaching Large Language Models to Forecast Time Series via Temporal Distillation",
    "summary": "arXiv:2602.01937v1 Announce Type: cross Abstract: Time series forecasting plays a critical role in decision-making across many real-world applications. Unlike data in vision and language domains, time series data is inherently tied to the evolution of underlying processes and can only accumulate as real-world time progresses, limiting the effective",
    "url": "https://arxiv.org/abs/2602.01937",
    "source": "Arxiv AI"
  },
  {
    "title": "Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy",
    "summary": "arXiv:2602.01939v1 Announce Type: cross Abstract: Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. ",
    "url": "https://arxiv.org/abs/2602.01939",
    "source": "Arxiv AI"
  },
  {
    "title": "Human Society-Inspired Approaches to Agentic AI Security: The 4C Framework",
    "summary": "arXiv:2602.01942v1 Announce Type: cross Abstract: AI is moving from domain-specific autonomy in closed, predictable settings to large-language-model-driven agents that plan and act in open, cross-organizational environments. As a result, the cybersecurity risk landscape is changing in fundamental ways. Agentic AI systems can plan, act, collaborate,",
    "url": "https://arxiv.org/abs/2602.01942",
    "source": "Arxiv AI"
  },
  {
    "title": "Efficient Epistemic Uncertainty Estimation for Large Language Models via Knowledge Distillation",
    "summary": "arXiv:2602.01956v1 Announce Type: cross Abstract: Quantifying uncertainty in Large Language Models (LLMs) is essential for mitigating hallucinations and enabling risk-aware deployment in safety-critical tasks. However, estimating Epistemic Uncertainty(EU) via Deep Ensembles is computationally prohibitive at the scale of modern models. We propose a ",
    "url": "https://arxiv.org/abs/2602.01956",
    "source": "Arxiv AI"
  },
  {
    "title": "Zero-Shot Off-Policy Learning",
    "summary": "arXiv:2602.01962v1 Announce Type: cross Abstract: Off-policy learning methods seek to derive an optimal policy directly from a fixed dataset of prior interactions. This objective presents significant challenges, primarily due to the inherent distributional shift and value function overestimation bias. These issues become even more noticeable in zer",
    "url": "https://arxiv.org/abs/2602.01962",
    "source": "Arxiv AI"
  },
  {
    "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
    "summary": "arXiv:2602.01965v1 Announce Type: cross Abstract: Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a \"Static Graph ",
    "url": "https://arxiv.org/abs/2602.01965",
    "source": "Arxiv AI"
  },
  {
    "title": "Mixture-of-Experts with Intermediate CTC Supervision for Accented Speech Recognition",
    "summary": "arXiv:2602.01967v1 Announce Type: cross Abstract: Accented speech remains a persistent challenge for automatic speech recognition (ASR), as most models are trained on data dominated by a few high-resource English varieties, leading to substantial performance degradation for other accents. Accent-agnostic approaches improve robustness yet struggle w",
    "url": "https://arxiv.org/abs/2602.01967",
    "source": "Arxiv AI"
  },
  {
    "title": "Your AI-Generated Image Detector Can Secretly Achieve SOTA Accuracy, If Calibrated",
    "summary": "arXiv:2602.01973v1 Announce Type: cross Abstract: Despite being trained on balanced datasets, existing AI-generated image detectors often exhibit systematic bias at test time, frequently misclassifying fake images as real. We hypothesize that this behavior stems from distributional shift in fake samples and implicit priors learned during training. ",
    "url": "https://arxiv.org/abs/2602.01973",
    "source": "Arxiv AI"
  },
  {
    "title": "IntraSlice: Towards High-Performance Structural Pruning with Block-Intra PCA for LLMs",
    "summary": "arXiv:2602.01975v1 Announce Type: cross Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks but face deployment challenges due to their massive size. Structured pruning offers acceleration benefits but leads to significant performance degradation. Recent PCA-based pruning methods have alleviated this issue by reta",
    "url": "https://arxiv.org/abs/2602.01975",
    "source": "Arxiv AI"
  },
  {
    "title": "FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts for General Continual Learning",
    "summary": "arXiv:2602.01976v2 Announce Type: cross Abstract: General continual learning (GCL) challenges intelligent systems to learn from single-pass, non-stationary data streams without clear task boundaries. While recent advances in continual parameter-efficient tuning (PET) of pretrained models show promise, they typically rely on multiple training epochs",
    "url": "https://arxiv.org/abs/2602.01976",
    "source": "Arxiv AI"
  },
  {
    "title": "SAME: Stabilized Mixture-of-Experts for Multimodal Continual Instruction Tuning",
    "summary": "arXiv:2602.01990v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) achieve strong performance through instruction tuning, but real-world deployment requires them to continually expand their capabilities, making Multimodal Continual Instruction Tuning (MCIT) essential. Recent methods leverage sparse expert routing to promote ",
    "url": "https://arxiv.org/abs/2602.01990",
    "source": "Arxiv AI"
  },
  {
    "title": "Optimizing Tensor Train Decomposition in DNNs for RISC-V Architectures Using Design Space Exploration and Compiler Optimizations",
    "summary": "arXiv:2602.01996v1 Announce Type: cross Abstract: Deep neural networks (DNNs) have become indispensable in many real-life applications like natural language processing, and autonomous systems. However, deploying DNNs on resource-constrained devices, e.g., in RISC-V platforms, remains challenging due to the high computational and memory demands of f",
    "url": "https://arxiv.org/abs/2602.01996",
    "source": "Arxiv AI"
  },
  {
    "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
    "summary": "arXiv:2602.01997v1 Announce Type: cross Abstract: Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks. Through a systematic ",
    "url": "https://arxiv.org/abs/2602.01997",
    "source": "Arxiv AI"
  },
  {
    "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
    "summary": "arXiv:2602.02000v2 Announce Type: cross Abstract: Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail ",
    "url": "https://arxiv.org/abs/2602.02000",
    "source": "Arxiv AI"
  },
  {
    "title": "Preserve-Then-Quantize: Balancing Rank Budgets for Quantization Error Reconstruction in LLMs",
    "summary": "arXiv:2602.02001v1 Announce Type: cross Abstract: Quantization Error Reconstruction (QER) reduces accuracy loss in Post-Training Quantization (PTQ) by approximating weights as $\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$, using a rank-$r$ correction to reconstruct quantization error. Prior methods devote the full rank budget to error reco",
    "url": "https://arxiv.org/abs/2602.02001",
    "source": "Arxiv AI"
  },
  {
    "title": "ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning",
    "summary": "arXiv:2602.02004v1 Announce Type: cross Abstract: Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input im",
    "url": "https://arxiv.org/abs/2602.02004",
    "source": "Arxiv AI"
  },
  {
    "title": "Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation",
    "summary": "arXiv:2602.02007v1 Announce Type: cross Abstract: Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly c",
    "url": "https://arxiv.org/abs/2602.02007",
    "source": "Arxiv AI"
  },
  {
    "title": "Rethinking Genomic Modeling Through Optical Character Recognition",
    "summary": "arXiv:2602.02014v1 Announce Type: cross Abstract: Recent genomic foundation models largely adopt large language model architectures that treat DNA as a one-dimensional token sequence. However, exhaustive sequential reading is structurally misaligned with sparse and discontinuous genomic semantics, leading to wasted computation on low-information ba",
    "url": "https://arxiv.org/abs/2602.02014",
    "source": "Arxiv AI"
  },
  {
    "title": "One Size, Many Fits: Aligning Diverse Group-Wise Click Preferences in Large-Scale Advertising Image Generation",
    "summary": "arXiv:2602.02033v1 Announce Type: cross Abstract: Advertising image generation has increasingly focused on online metrics like Click-Through Rate (CTR), yet existing approaches adopt a ``one-size-fits-all\" strategy that optimizes for overall CTR while neglecting preference diversity among user groups. This leads to suboptimal performance for specif",
    "url": "https://arxiv.org/abs/2602.02033",
    "source": "Arxiv AI"
  },
  {
    "title": "Bandwidth-Efficient Multi-Agent Communication through Information Bottleneck and Vector Quantization",
    "summary": "arXiv:2602.02035v1 Announce Type: cross Abstract: Multi-agent reinforcement learning systems deployed in real-world robotics applications face severe communication constraints that significantly impact coordination effectiveness. We present a framework that combines information bottleneck theory with vector quantization to enable selective, bandwid",
    "url": "https://arxiv.org/abs/2602.02035",
    "source": "Arxiv AI"
  },
  {
    "title": "Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models",
    "summary": "arXiv:2602.02043v1 Announce Type: cross Abstract: Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing \"a red cube and a blue sphere\" with \"a blue cube and a red sphere\". Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fin",
    "url": "https://arxiv.org/abs/2602.02043",
    "source": "Arxiv AI"
  },
  {
    "title": "FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor Rectification",
    "summary": "arXiv:2602.02055v1 Announce Type: cross Abstract: In Internet-of-Things systems, federated learning has advanced online reinforcement learning (RL) by enabling parallel policy training without sharing raw data. However, interacting with real environments online can be risky and costly, motivating offline federated RL (FRL), where local devices lear",
    "url": "https://arxiv.org/abs/2602.02055",
    "source": "Arxiv AI"
  },
  {
    "title": "FiLoRA: Focus-and-Ignore LoRA for Controllable Feature Reliance",
    "summary": "arXiv:2602.02060v1 Announce Type: cross Abstract: Multimodal foundation models integrate heterogeneous signals across modalities, yet it remains poorly understood how their predictions depend on specific internal feature groups and whether such reliance can be deliberately controlled. Existing studies of shortcut and spurious behavior largely rely ",
    "url": "https://arxiv.org/abs/2602.02060",
    "source": "Arxiv AI"
  },
  {
    "title": "See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers",
    "summary": "arXiv:2602.02063v1 Announce Type: cross Abstract: Automated vehicles lack natural communication channels with other road users, making external Human-Machine Interfaces (eHMIs) essential for conveying intent and maintaining trust in shared environments. However, most eHMI studies rely on developer-crafted message-action pairs, which are difficult t",
    "url": "https://arxiv.org/abs/2602.02063",
    "source": "Arxiv AI"
  },
  {
    "title": "Multi-View Stenosis Classification Leveraging Transformer-Based Multiple-Instance Learning Using Real-World Clinical Data",
    "summary": "arXiv:2602.02067v1 Announce Type: cross Abstract: Coronary artery stenosis is a leading cause of cardiovascular disease, diagnosed by analyzing the coronary arteries from multiple angiography views. Although numerous deep-learning models have been proposed for stenosis detection from a single angiography view, their performance heavily relies on ex",
    "url": "https://arxiv.org/abs/2602.02067",
    "source": "Arxiv AI"
  },
  {
    "title": "LEC-KG: An LLM-Embedding Collaborative Framework for Domain-Specific Knowledge Graph Construction -- A Case Study on SDGs",
    "summary": "arXiv:2602.02090v1 Announce Type: cross Abstract: Constructing domain-specific knowledge graphs from unstructured text remains challenging due to heterogeneous entity mentions, long-tail relation distributions, and the absence of standardized schemas. We present LEC-KG, a bidirectional collaborative framework that integrates the semantic understand",
    "url": "https://arxiv.org/abs/2602.02090",
    "source": "Arxiv AI"
  },
  {
    "title": "WADEPre: A Wavelet-based Decomposition Model for Extreme Precipitation Nowcasting with Multi-Scale Learning",
    "summary": "arXiv:2602.02096v1 Announce Type: cross Abstract: The heavy-tailed nature of precipitation intensity impedes precise precipitation nowcasting. Standard models that optimize pixel-wise losses are prone to regression-to-the-mean bias, which blurs extreme values. Existing Fourier-based methods also lack the spatial localization needed to resolve trans",
    "url": "https://arxiv.org/abs/2602.02096",
    "source": "Arxiv AI"
  },
  {
    "title": "Probabilistic Performance Guarantees for Multi-Task Reinforcement Learning",
    "summary": "arXiv:2602.02098v1 Announce Type: cross Abstract: Multi-task reinforcement learning trains generalist policies that can execute multiple tasks. While recent years have seen significant progress, existing approaches rarely provide formal performance guarantees, which are indispensable when deploying policies in safety-critical settings. We present a",
    "url": "https://arxiv.org/abs/2602.02098",
    "source": "Arxiv AI"
  },
  {
    "title": "The Verification Crisis: Expert Perceptions of GenAI Disinformation and the Case for Reproducible Provenance",
    "summary": "arXiv:2602.02100v1 Announce Type: cross Abstract: The growth of Generative Artificial Intelligence (GenAI) has shifted disinformation production from manual fabrication to automated, large-scale manipulation. This article presents findings from the first wave of a longitudinal expert perception survey (N=21) involving AI researchers, policymakers, ",
    "url": "https://arxiv.org/abs/2602.02100",
    "source": "Arxiv AI"
  },
  {
    "title": "Unifying Masked Diffusion Models with Various Generation Orders and Beyond",
    "summary": "arXiv:2602.02112v1 Announce Type: cross Abstract: Masked diffusion models (MDMs) are a potential alternative to autoregressive models (ARMs) for language generation, but generation quality depends critically on the generation order. Prior work either hard-codes an ordering (e.g., blockwise left-to-right) or learns an ordering policy for a pretraine",
    "url": "https://arxiv.org/abs/2602.02112",
    "source": "Arxiv AI"
  },
  {
    "title": "Toxicity Assessment in Preclinical Histopathology via Class-Aware Mahalanobis Distance for Known and Novel Anomalies",
    "summary": "arXiv:2602.02124v1 Announce Type: cross Abstract: Drug-induced toxicity remains a leading cause of failure in preclinical development and early clinical trials. Detecting adverse effects at an early stage is critical to reduce attrition and accelerate the development of safe medicines. Histopathological evaluation remains the gold standard for toxi",
    "url": "https://arxiv.org/abs/2602.02124",
    "source": "Arxiv AI"
  },
  {
    "title": "Two-Stage Grid Optimization for Group-wise Quantization of LLMs",
    "summary": "arXiv:2602.02126v1 Announce Type: cross Abstract: Group-wise quantization is an effective strategy for mitigating accuracy degradation in low-bit quantization of large language models (LLMs). Among existing methods, GPTQ has been widely adopted due to its efficiency; however, it neglects input statistics and inter-group correlations when determinin",
    "url": "https://arxiv.org/abs/2602.02126",
    "source": "Arxiv AI"
  },
  {
    "title": "Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics",
    "summary": "arXiv:2602.02128v1 Announce Type: cross Abstract: Molecular dynamics (MD) simulations remain the gold standard for studying protein dynamics, but their computational cost limits access to biologically relevant timescales. Recent generative models have shown promise in accelerating simulations, yet they struggle with long-horizon generation due to a",
    "url": "https://arxiv.org/abs/2602.02128",
    "source": "Arxiv AI"
  },
  {
    "title": "DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations",
    "summary": "arXiv:2602.02137v2 Announce Type: cross Abstract: Modern data centers (DCs) hosting artificial intelligence (AI)-dedicated devices operate at high power densities with rapidly varying workloads, making minute-level adaptation essential for safe and energy-efficient operation. However, manually designing piecewise deep reinforcement learning (DRL) a",
    "url": "https://arxiv.org/abs/2602.02137",
    "source": "Arxiv AI"
  },
  {
    "title": "CAM: A Causality-based Analysis Framework for Multi-Agent Code Generation Systems",
    "summary": "arXiv:2602.02138v1 Announce Type: cross Abstract: Despite the remarkable success that Multi-Agent Code Generation Systems (MACGS) have achieved, the inherent complexity of multi-agent architectures produces substantial volumes of intermediate outputs. To date, the individual importance of these intermediate outputs to the system correctness remains",
    "url": "https://arxiv.org/abs/2602.02138",
    "source": "Arxiv AI"
  },
  {
    "title": "EvoMU: Evolutionary Machine Unlearning",
    "summary": "arXiv:2602.02139v1 Announce Type: cross Abstract: Machine unlearning aims to unlearn specified training data (e.g. sensitive or copyrighted material). A prominent approach is to fine-tune an existing model with an unlearning loss that retains overall utility. The space of suitable unlearning loss functions is vast, making the search for an optimal ",
    "url": "https://arxiv.org/abs/2602.02139",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning Generative Selection for Best-of-N",
    "summary": "arXiv:2602.02143v1 Announce Type: cross Abstract: Scaling test-time compute via parallel sampling can substantially improve LLM reasoning, but is often limited by Best-of-N selection quality. Generative selection methods, such as GenSelect, address this bottleneck, yet strong selection performance remains largely limited to large models. We show th",
    "url": "https://arxiv.org/abs/2602.02143",
    "source": "Arxiv AI"
  },
  {
    "title": "Back to the Future: Look-ahead Augmentation and Parallel Self-Refinement for Time Series Forecasting",
    "summary": "arXiv:2602.02146v1 Announce Type: cross Abstract: Long-term time series forecasting (LTSF) remains challenging due to the trade-off between parallel efficiency and sequential modeling of temporal coherence. Direct multi-step forecasting (DMS) methods enable fast, parallel prediction of all future horizons but often lose temporal consistency across ",
    "url": "https://arxiv.org/abs/2602.02146",
    "source": "Arxiv AI"
  },
  {
    "title": "ECHO: Entropy-Confidence Hybrid Optimization for Test-Time Reinforcement Learning",
    "summary": "arXiv:2602.02150v1 Announce Type: cross Abstract: Test-time reinforcement learning generates multiple candidate answers via repeated rollouts and performs online updates using pseudo-labels constructed by majority voting. To reduce overhead and improve exploration, prior work introduces tree structured rollouts, which share reasoning prefixes and b",
    "url": "https://arxiv.org/abs/2602.02150",
    "source": "Arxiv AI"
  },
  {
    "title": "Self-Evolving Coordination Protocol in Multi-Agent AI Systems: An Exploratory Systems Feasibility Study",
    "summary": "arXiv:2602.02170v1 Announce Type: cross Abstract: Contemporary multi-agent systems increasingly rely on internal coordination mechanisms to combine, arbitrate, or constrain the outputs of heterogeneous components. In safety-critical and regulated domains such as finance, these mechanisms must satisfy strict formal requirements, remain auditable, an",
    "url": "https://arxiv.org/abs/2602.02170",
    "source": "Arxiv AI"
  },
  {
    "title": "SurvKAN: A Fully Parametric Survival Model Based on Kolmogorov-Arnold Networks",
    "summary": "arXiv:2602.02179v1 Announce Type: cross Abstract: Accurate prediction of time-to-event outcomes is critical for clinical decision-making, treatment planning, and resource allocation in modern healthcare. While classical survival models such as Cox remain widely adopted in standard practice, they rely on restrictive assumptions, including linear cov",
    "url": "https://arxiv.org/abs/2602.02179",
    "source": "Arxiv AI"
  },
  {
    "title": "Malware Detection Through Memory Analysis",
    "summary": "arXiv:2602.02184v1 Announce Type: cross Abstract: This paper summarizes the research conducted for a malware detection project using the Canadian Institute for Cybersecurity's MalMemAnalysis-2022 dataset. The purpose of the project was to explore the effectiveness and efficiency of machine learning techniques for the task of binary classification (",
    "url": "https://arxiv.org/abs/2602.02184",
    "source": "Arxiv AI"
  },
  {
    "title": "Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models",
    "summary": "arXiv:2602.02185v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations",
    "url": "https://arxiv.org/abs/2602.02185",
    "source": "Arxiv AI"
  },
  {
    "title": "State Rank Dynamics in Linear Attention LLMs",
    "summary": "arXiv:2602.02195v1 Announce Type: cross Abstract: Linear Attention Large Language Models (LLMs) offer a compelling recurrent formulation that compresses context into a fixed-size state matrix, enabling constant-time inference. However, the internal dynamics of this compressed state remain largely opaque. In this work, we present a comprehensive stu",
    "url": "https://arxiv.org/abs/2602.02195",
    "source": "Arxiv AI"
  },
  {
    "title": "Hierarchical Adaptive Eviction for KV Cache Management in Multimodal Language Models",
    "summary": "arXiv:2602.02197v1 Announce Type: cross Abstract: The integration of visual information into Large Language Models (LLMs) has enabled Multimodal LLMs (MLLMs), but the quadratic memory and computational costs of Transformer architectures remain a bottleneck. Existing KV cache eviction strategies fail to address the heterogeneous attention distributi",
    "url": "https://arxiv.org/abs/2602.02197",
    "source": "Arxiv AI"
  },
  {
    "title": "Cardinality-Preserving Structured Sparse Graph Transformers for Molecular Property Prediction",
    "summary": "arXiv:2602.02201v1 Announce Type: cross Abstract: Drug discovery motivates efficient molecular property prediction under limited labeled data. Chemical space is vast, often estimated at approximately 10^60 drug-like molecules, while only thousands of drugs have been approved. As a result, self-supervised pretraining on large unlabeled molecular cor",
    "url": "https://arxiv.org/abs/2602.02201",
    "source": "Arxiv AI"
  },
  {
    "title": "Towards AI Evaluation in Domain-Specific RAG Systems: The AgriHubi Case Study",
    "summary": "arXiv:2602.02208v1 Announce Type: cross Abstract: Large language models show promise for knowledge-intensive domains, yet their use in agriculture is constrained by weak grounding, English-centric training data, and limited real-world evaluation. These issues are amplified for low-resource languages, where high-quality domain documentation exists b",
    "url": "https://arxiv.org/abs/2602.02208",
    "source": "Arxiv AI"
  },
  {
    "title": "Generating Physically Sound Designs from Text and a Set of Physical Constraints",
    "summary": "arXiv:2602.02213v1 Announce Type: cross Abstract: We present TIDES, a text informed design approach for generating physically sound designs based on a textual description and a set of physical constraints. TIDES jointly optimizes structural (topology) and visual properties. A pre-trained text-image model is used to measure the design's visual align",
    "url": "https://arxiv.org/abs/2602.02213",
    "source": "Arxiv AI"
  },
  {
    "title": "Spectral Superposition: A Theory of Feature Geometry",
    "summary": "arXiv:2602.02224v1 Announce Type: cross Abstract: Neural networks represent more features than they have dimensions via superposition, forcing features to share representational space. Current methods decompose activations into sparse linear features but discard geometric structure. We develop a theory for studying the geometric structre of feature",
    "url": "https://arxiv.org/abs/2602.02224",
    "source": "Arxiv AI"
  },
  {
    "title": "SEDformer: Event-Synchronous Spiking Transformers for Irregular Telemetry Time Series Forecasting",
    "summary": "arXiv:2602.02230v2 Announce Type: cross Abstract: Telemetry streams from large-scale Internet-connected systems (e.g., IoT deployments and online platforms) naturally form an irregular multivariate time series (IMTS) whose accurate forecasting is operationally vital. A closer examination reveals a defining Sparsity-Event Duality (SED) property of I",
    "url": "https://arxiv.org/abs/2602.02230",
    "source": "Arxiv AI"
  },
  {
    "title": "Geometry- and Relation-Aware Diffusion for EEG Super-Resolution",
    "summary": "arXiv:2602.02238v1 Announce Type: cross Abstract: Recent electroencephalography (EEG) spatial super-resolution (SR) methods, while showing improved quality by either directly predicting missing signals from visible channels or adapting latent diffusion-based generative modeling to temporal data, often lack awareness of physiological spatial structu",
    "url": "https://arxiv.org/abs/2602.02238",
    "source": "Arxiv AI"
  },
  {
    "title": "OmniCode: A Benchmark for Evaluating Software Engineering Agents",
    "summary": "arXiv:2602.02262v1 Announce Type: cross Abstract: LLM-powered coding agents are redefining how real-world software is developed. To drive the research towards better coding agents, we require challenging benchmarks that can rigorously evaluate the ability of such agents to perform various software engineering tasks. However, popular coding benchmar",
    "url": "https://arxiv.org/abs/2602.02262",
    "source": "Arxiv AI"
  },
  {
    "title": "Unsupervised Physics-Informed Operator Learning through Multi-Stage Curriculum Training",
    "summary": "arXiv:2602.02264v1 Announce Type: cross Abstract: Solving partial differential equations remains a central challenge in scientific machine learning. Neural operators offer a promising route by learning mappings between function spaces and enabling resolution-independent inference, yet they typically require supervised data. Physics-informed neural ",
    "url": "https://arxiv.org/abs/2602.02264",
    "source": "Arxiv AI"
  },
  {
    "title": "OpenSeal: Good, Fast, and Cheap Construction of an Open-Source Southeast Asian LLM via Parallel Data",
    "summary": "arXiv:2602.02266v1 Announce Type: cross Abstract: Large language models (LLMs) have proven to be effective tools for a wide range of natural language processing (NLP) applications. Although many LLMs are multilingual, most remain English-centric and perform poorly on low-resource languages. Recently, several Southeast Asia-focused LLMs have been de",
    "url": "https://arxiv.org/abs/2602.02266",
    "source": "Arxiv AI"
  },
  {
    "title": "Bridging the Sim-to-Real Gap with multipanda ros2: A Real-Time ROS2 Framework for Multimanual Systems",
    "summary": "arXiv:2602.02269v1 Announce Type: cross Abstract: We present $multipanda\\_ros2$, a novel open-source ROS2 architecture for multi-robot control of Franka Robotics robots. Leveraging ros2 control, this framework provides native ROS2 interfaces for controlling any number of robots from a single process. Our core contributions address key challenges in",
    "url": "https://arxiv.org/abs/2602.02269",
    "source": "Arxiv AI"
  },
  {
    "title": "RACA: Representation-Aware Coverage Criteria for LLM Safety Testing",
    "summary": "arXiv:2602.02280v1 Announce Type: cross Abstract: Recent advancements in LLMs have led to significant breakthroughs in various AI applications. However, their sophisticated capabilities also introduce severe safety concerns, particularly the generation of harmful content through jailbreak attacks. Current safety testing for LLMs often relies on sta",
    "url": "https://arxiv.org/abs/2602.02280",
    "source": "Arxiv AI"
  },
  {
    "title": "Backpropagation as Physical Relaxation: Exact Gradients in Finite Time",
    "summary": "arXiv:2602.02281v1 Announce Type: cross Abstract: Backpropagation, the foundational algorithm for training neural networks, is typically understood as a symbolic computation that recursively applies the chain rule. We show it emerges exactly as the finite-time relaxation of a physical dynamical system. By formulating feedforward inference as a cont",
    "url": "https://arxiv.org/abs/2602.02281",
    "source": "Arxiv AI"
  },
  {
    "title": "DFKI-Speech System for WildSpoof Challenge: A robust framework for SASV In-the-Wild",
    "summary": "arXiv:2602.02286v1 Announce Type: cross Abstract: This paper presents the DFKI-Speech system developed for the WildSpoof Challenge under the Spoofing aware Automatic Speaker Verification (SASV) track. We propose a robust SASV framework in which a spoofing detector and a speaker verification (SV) network operate in tandem. The spoofing detector empl",
    "url": "https://arxiv.org/abs/2602.02286",
    "source": "Arxiv AI"
  },
  {
    "title": "An Optimization Method for Autoregressive Time Series Forecasting",
    "summary": "arXiv:2602.02288v1 Announce Type: cross Abstract: Current time-series forecasting models are primarily based on transformer-style neural networks. These models achieve long-term forecasting mainly by scaling up the model size rather than through genuinely autoregressive (AR) rollout. From the perspective of large language model training, the tradit",
    "url": "https://arxiv.org/abs/2602.02288",
    "source": "Arxiv AI"
  },
  {
    "title": "Hallucination or Creativity: How to Evaluate AI-Generated Scientific Stories?",
    "summary": "arXiv:2602.02290v1 Announce Type: cross Abstract: Generative AI can turn scientific articles into narratives for diverse audiences, but evaluating these stories remains challenging. Storytelling demands abstraction, simplification, and pedagogical creativity-qualities that are not often well-captured by standard summarization metrics. Meanwhile, fa",
    "url": "https://arxiv.org/abs/2602.02290",
    "source": "Arxiv AI"
  },
  {
    "title": "Decoupling Generalizability and Membership Privacy Risks in Neural Networks",
    "summary": "arXiv:2602.02296v1 Announce Type: cross Abstract: A deep learning model usually has to sacrifice some utilities when it acquires some other abilities or characteristics. Privacy preservation has such trade-off relationships with utilities. The loss disparity between various defense approaches implies the potential to decouple generalizability and p",
    "url": "https://arxiv.org/abs/2602.02296",
    "source": "Arxiv AI"
  },
  {
    "title": "Advancing General-Purpose Reasoning Models with Modular Gradient Surgery",
    "summary": "arXiv:2602.02301v1 Announce Type: cross Abstract: Reinforcement learning (RL) has played a central role in recent advances in large reasoning models (LRMs), yielding strong gains in verifiable and open-ended reasoning. However, training a single general-purpose LRM across diverse domains remains challenging due to pronounced domain heterogeneity. T",
    "url": "https://arxiv.org/abs/2602.02301",
    "source": "Arxiv AI"
  },
  {
    "title": "Spark: Modular Spiking Neural Networks",
    "summary": "arXiv:2602.02306v1 Announce Type: cross Abstract: Nowadays, neural networks act as a synonym for artificial intelligence. Present neural network models, although remarkably powerful, are inefficient both in terms of data and energy. Several alternative forms of neural networks have been proposed to address some of these problems. Specifically, spik",
    "url": "https://arxiv.org/abs/2602.02306",
    "source": "Arxiv AI"
  },
  {
    "title": "FragmentFlow: Scalable Transition State Generation for Large Molecules",
    "summary": "arXiv:2602.02310v1 Announce Type: cross Abstract: Transition states (TSs) are central to understanding and quantitatively predicting chemical reactivity and reaction mechanisms. Although traditional TS generation methods are computationally expensive, recent generative modeling approaches have enabled chemically meaningful TS prediction for relativ",
    "url": "https://arxiv.org/abs/2602.02310",
    "source": "Arxiv AI"
  },
  {
    "title": "A Large-Scale Dataset for Molecular Structure-Language Description via a Rule-Regularized Method",
    "summary": "arXiv:2602.02320v1 Announce Type: cross Abstract: Molecular function is largely determined by structure. Accurately aligning molecular structure with natural language is therefore essential for enabling large language models (LLMs) to reason about downstream chemical tasks. However, the substantial cost of human annotation makes it infeasible to co",
    "url": "https://arxiv.org/abs/2602.02320",
    "source": "Arxiv AI"
  },
  {
    "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
    "summary": "arXiv:2602.02331v1 Announce Type: cross Abstract: Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation",
    "url": "https://arxiv.org/abs/2602.02331",
    "source": "Arxiv AI"
  },
  {
    "title": "VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations",
    "summary": "arXiv:2602.02334v1 Announce Type: cross Abstract: Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided b",
    "url": "https://arxiv.org/abs/2602.02334",
    "source": "Arxiv AI"
  },
  {
    "title": "Building a Correct-by-Design Lakehouse. Data Contracts, Versioning, and Transactional Pipelines for Humans and Agents",
    "summary": "arXiv:2602.02335v1 Announce Type: cross Abstract: Lakehouses are the default cloud platform for analytics and AI, but they become unsafe when untrusted actors concurrently operate on production data: upstream-downstream mismatches surface only at runtime, and multi-table pipelines can leak partial effects. Inspired by software engineering, we desig",
    "url": "https://arxiv.org/abs/2602.02335",
    "source": "Arxiv AI"
  },
  {
    "title": "Rethinking Generative Recommender Tokenizer: Recsys-Native Encoding and Semantic Quantization Beyond LLMs",
    "summary": "arXiv:2602.02338v1 Announce Type: cross Abstract: Semantic ID (SID)-based recommendation is a promising paradigm for scaling sequential recommender systems, but existing methods largely follow a semantic-centric pipeline: item embeddings are learned from foundation models and discretized using generic quantization schemes. This design is misaligned",
    "url": "https://arxiv.org/abs/2602.02338",
    "source": "Arxiv AI"
  },
  {
    "title": "Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics",
    "summary": "arXiv:2602.02343v1 Announce Type: cross Abstract: Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these ",
    "url": "https://arxiv.org/abs/2602.02343",
    "source": "Arxiv AI"
  },
  {
    "title": "Artificial Intelligence and Symmetries: Learning, Encoding, and Discovering Structure in Physical Data",
    "summary": "arXiv:2602.02351v1 Announce Type: cross Abstract: Symmetries play a central role in physics, organizing dynamics, constraining interactions, and determining the effective number of physical degrees of freedom. In parallel, modern artificial intelligence methods have demonstrated a remarkable ability to extract low-dimensional structure from high-di",
    "url": "https://arxiv.org/abs/2602.02351",
    "source": "Arxiv AI"
  },
  {
    "title": "Implicit neural representation of textures",
    "summary": "arXiv:2602.02354v1 Announce Type: cross Abstract: Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space. Through t",
    "url": "https://arxiv.org/abs/2602.02354",
    "source": "Arxiv AI"
  },
  {
    "title": "SWE-Universe: Scale Real-World Verifiable Environments to Millions",
    "summary": "arXiv:2602.02361v1 Announce Type: cross Abstract: We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and proh",
    "url": "https://arxiv.org/abs/2602.02361",
    "source": "Arxiv AI"
  },
  {
    "title": "ReasonCACHE: Teaching LLMs To Reason Without Weight Updates",
    "summary": "arXiv:2602.02366v1 Announce Type: cross Abstract: Can Large language models (LLMs) learn to reason without any weight update and only through in-context learning (ICL)? ICL is strikingly sample-efficient, often learning from only a handful of demonstrations, but complex reasoning tasks typically demand many training examples to learn from. However,",
    "url": "https://arxiv.org/abs/2602.02366",
    "source": "Arxiv AI"
  },
  {
    "title": "From Sycophancy to Sensemaking: Premise Governance for Human-AI Decision Making",
    "summary": "arXiv:2602.02378v1 Announce Type: cross Abstract: As LLMs expand from assistance to decision support, a dangerous pattern emerges: fluent agreement without calibrated judgment. Low-friction assistants can become sycophantic, baking in implicit assumptions and pushing verification costs onto experts, while outcomes arrive too late to serve as reward",
    "url": "https://arxiv.org/abs/2602.02378",
    "source": "Arxiv AI"
  },
  {
    "title": "Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory",
    "summary": "arXiv:2602.02393v2 Announce Type: cross Abstract: We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradi",
    "url": "https://arxiv.org/abs/2602.02393",
    "source": "Arxiv AI"
  },
  {
    "title": "David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning",
    "summary": "arXiv:2602.02395v1 Announce Type: cross Abstract: The evolution of large language models into autonomous agents introduces adversarial failures that exploit legitimate tool privileges, transforming safety evaluation in tool-augmented environments from a subjective NLP task into an objective control problem. We formalize this threat model as Tag-Alo",
    "url": "https://arxiv.org/abs/2602.02395",
    "source": "Arxiv AI"
  },
  {
    "title": "SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation",
    "summary": "arXiv:2602.02402v1 Announce Type: cross Abstract: Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control,",
    "url": "https://arxiv.org/abs/2602.02402",
    "source": "Arxiv AI"
  },
  {
    "title": "Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning",
    "summary": "arXiv:2602.02405v1 Announce Type: cross Abstract: Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current fr",
    "url": "https://arxiv.org/abs/2602.02405",
    "source": "Arxiv AI"
  },
  {
    "title": "ReasonEdit: Editing Vision-Language Models using Human Reasoning",
    "summary": "arXiv:2602.02408v2 Announce Type: cross Abstract: Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images. We therefore p",
    "url": "https://arxiv.org/abs/2602.02408",
    "source": "Arxiv AI"
  },
  {
    "title": "Poly-attention: a general scheme for higher-order self-attention",
    "summary": "arXiv:2602.02422v1 Announce Type: cross Abstract: The self-attention mechanism, at the heart of the Transformer model, is able to effectively model pairwise interactions between tokens. However, numerous recent works have shown that it is unable to perform basic tasks involving detecting triples of correlated tokens, or compositional tasks where mu",
    "url": "https://arxiv.org/abs/2602.02422",
    "source": "Arxiv AI"
  },
  {
    "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
    "summary": "arXiv:2602.02437v1 Announce Type: cross Abstract: Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmon",
    "url": "https://arxiv.org/abs/2602.02437",
    "source": "Arxiv AI"
  },
  {
    "title": "Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization",
    "summary": "arXiv:2602.02451v1 Announce Type: cross Abstract: Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next. Traditional approaches such as random sampling, greedy information maximization, and round-robin cover",
    "url": "https://arxiv.org/abs/2602.02451",
    "source": "Arxiv AI"
  },
  {
    "title": "World-Gymnast: Training Robots with Reinforcement Learning in a World Model",
    "summary": "arXiv:2602.02454v1 Announce Type: cross Abstract: Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert d",
    "url": "https://arxiv.org/abs/2602.02454",
    "source": "Arxiv AI"
  },
  {
    "title": "Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models",
    "summary": "arXiv:2602.02462v1 Announce Type: cross Abstract: Large Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate r",
    "url": "https://arxiv.org/abs/2602.02462",
    "source": "Arxiv AI"
  },
  {
    "title": "Multi-head automated segmentation by incorporating detection head into the contextual layer neural network",
    "summary": "arXiv:2602.02471v1 Announce Type: cross Abstract: Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented wi",
    "url": "https://arxiv.org/abs/2602.02471",
    "source": "Arxiv AI"
  },
  {
    "title": "MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents",
    "summary": "arXiv:2602.02474v1 Announce Type: cross Abstract: Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long",
    "url": "https://arxiv.org/abs/2602.02474",
    "source": "Arxiv AI"
  },
  {
    "title": "Flow Policy Gradients for Robot Control",
    "summary": "arXiv:2602.02481v1 Announce Type: cross Abstract: Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradient",
    "url": "https://arxiv.org/abs/2602.02481",
    "source": "Arxiv AI"
  },
  {
    "title": "RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents",
    "summary": "arXiv:2602.02486v1 Announce Type: cross Abstract: LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient",
    "url": "https://arxiv.org/abs/2602.02486",
    "source": "Arxiv AI"
  },
  {
    "title": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
    "summary": "arXiv:2602.02493v1 Announce Type: cross Abstract: Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leavin",
    "url": "https://arxiv.org/abs/2602.02493",
    "source": "Arxiv AI"
  },
  {
    "title": "Reward-free Alignment for Conflicting Objectives",
    "summary": "arXiv:2602.02495v1 Announce Type: cross Abstract: Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, w",
    "url": "https://arxiv.org/abs/2602.02495",
    "source": "Arxiv AI"
  },
  {
    "title": "T-COL: Generating Counterfactual Explanations for General User Preferences on Variable Machine Learning Systems",
    "summary": "arXiv:2309.16146v3 Announce Type: replace Abstract: To address the interpretability challenge in machine learning (ML) systems, counterfactual explanations (CEs) have emerged as a promising solution. CEs are unique as they provide workable suggestions to users, instead of explaining why a certain outcome was predicted. The application of CEs encoun",
    "url": "https://arxiv.org/abs/2309.16146",
    "source": "Arxiv AI"
  },
  {
    "title": "Mastering NIM and Impartial Games with Weak Neural Networks: An AlphaZero-inspired Multi-Frame Approach",
    "summary": "arXiv:2411.06403v2 Announce Type: replace Abstract: We introduce a practical circuit-complexity model for fixed-precision neural networks to explain and overcome a persistent learnability barrier in impartial games like NIM. We show that bounded-depth, polynomial-size, fixed-precision neural inference, including recurrent and attention-style archit",
    "url": "https://arxiv.org/abs/2411.06403",
    "source": "Arxiv AI"
  },
  {
    "title": "Preference-Conditioned Gradient Variations for Multi-Objective Quality-Diversity",
    "summary": "arXiv:2411.12433v2 Announce Type: replace Abstract: In a variety of domains, from robotics to finance, Quality-Diversity algorithms have been used to generate collections of both diverse and high-performing solutions. Multi-Objective Quality-Diversity algorithms have emerged as a promising approach for applying these methods to complex, multi-objec",
    "url": "https://arxiv.org/abs/2411.12433",
    "source": "Arxiv AI"
  },
  {
    "title": "Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents",
    "summary": "arXiv:2503.24047v3 Announce Type: replace Abstract: As scientific research becomes increasingly complex, innovative tools are needed to manage vast data, facilitate interdisciplinary collaboration, and accelerate discovery. Large language models (LLMs) are now evolving into LLM-based scientific agents that automate critical tasks ranging from hypot",
    "url": "https://arxiv.org/abs/2503.24047",
    "source": "Arxiv AI"
  },
  {
    "title": "Past-Discounting is Key for Learning Markovian Fairness with Long Horizons",
    "summary": "arXiv:2504.01154v2 Announce Type: replace Abstract: Fairness is an important consideration for dynamic resource allocation in multi-agent systems. Many existing methods treat fairness as a one-shot problem without considering temporal dynamics, which misses the nuances of accumulating inequalities over time. Recent approaches overcome this limitati",
    "url": "https://arxiv.org/abs/2504.01154",
    "source": "Arxiv AI"
  },
  {
    "title": "Automated Archival Descriptions with Federated Intelligence of LLMs",
    "summary": "arXiv:2504.05711v2 Announce Type: replace Abstract: Enforcing archival standards requires specialized expertise, and manually creating metadata descriptions for archival materials is a tedious and error-prone task. This work aims at exploring the potential of agentic AI and large language models (LLMs) in addressing the challenges of implementing a",
    "url": "https://arxiv.org/abs/2504.05711",
    "source": "Arxiv AI"
  },
  {
    "title": "FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory",
    "summary": "arXiv:2504.14325v4 Announce Type: replace Abstract: Letting AI agents interact in multi-agent applications adds a layer of complexity to the interpretability and prediction of AI outcomes, with profound implications for their trustworthy adoption in research and society. Game theory offers powerful models to capture and interpret strategic interact",
    "url": "https://arxiv.org/abs/2504.14325",
    "source": "Arxiv AI"
  },
  {
    "title": "Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions",
    "summary": "arXiv:2505.11614v2 Announce Type: replace Abstract: A central goal of cognitive modeling is to develop models that not only predict human behavior but also provide insight into the underlying cognitive mechanisms. While neural network models trained on large-scale behavioral data often achieve strong predictive performance, they typically fall shor",
    "url": "https://arxiv.org/abs/2505.11614",
    "source": "Arxiv AI"
  },
  {
    "title": "Token-Importance Guided Direct Preference Optimization",
    "summary": "arXiv:2505.19653v2 Announce Type: replace Abstract: Aligning Large Language Models (LLMs) with human preferences is crucial for safe and effective AI interactions. While popular methods like Direct Preference Optimization (DPO) have simplified alignment, they remain sensitive to data noise and overlook the differential importance of individual toke",
    "url": "https://arxiv.org/abs/2505.19653",
    "source": "Arxiv AI"
  },
  {
    "title": "AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents",
    "summary": "arXiv:2506.00641v3 Announce Type: replace Abstract: Despite the rapid advancement of LLM-based agents, the reliable evaluation of their safety and security remains a significant challenge. Existing rule-based or LLM-based evaluators often miss dangers in agents' step-by-step actions, overlook subtle meanings, fail to see how small issues compound, ",
    "url": "https://arxiv.org/abs/2506.00641",
    "source": "Arxiv AI"
  },
  {
    "title": "Digital Simulations to Enhance Military Medical Evacuation Decision-Making",
    "summary": "arXiv:2507.06373v3 Announce Type: replace Abstract: Medical evacuation is one of the United States Army's most storied and critical mission sets, responsible for efficiently and expediently evacuating the battlefield ill and injured. Medical evacuation planning involves designing a robust network of medical platforms and facilities capable of movin",
    "url": "https://arxiv.org/abs/2507.06373",
    "source": "Arxiv AI"
  },
  {
    "title": "LoRA is All You Need for Safety Alignment of Reasoning LLMs",
    "summary": "arXiv:2507.17075v4 Announce Type: replace Abstract: Reasoning-capable LLMs have achieved major breakthroughs in solving complex problems, but recent work shows that acquiring and deploying strong reasoning can introduce significant safety risks. A common mitigation is to apply a secondary safety-alignment phase after reasoning is learned; however, ",
    "url": "https://arxiv.org/abs/2507.17075",
    "source": "Arxiv AI"
  },
  {
    "title": "MAC: Masked Agent Collaboration Boosts Large Language Model Medical Decision-Making",
    "summary": "arXiv:2507.21159v2 Announce Type: replace Abstract: Large language models (LLMs) have proven effective in artificial intelligence, where the multi-agent system (MAS) holds considerable promise for healthcare development by achieving the collaboration of LLMs. However, the absence of a systematic pipeline for agent construction and the rigidity of s",
    "url": "https://arxiv.org/abs/2507.21159",
    "source": "Arxiv AI"
  },
  {
    "title": "STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision",
    "summary": "arXiv:2508.08688v2 Announce Type: replace Abstract: Vision-language models (VLMs) have made significant strides in reasoning, yet they often struggle with complex multimodal tasks and tend to generate overly verbose outputs. A key limitation is their reliance on chain-of-thought (CoT) reasoning, despite many tasks benefiting from alternative topolo",
    "url": "https://arxiv.org/abs/2508.08688",
    "source": "Arxiv AI"
  },
  {
    "title": "AgentRAN: An Agentic AI Architecture for Autonomous Control of Open 6G Networks",
    "summary": "arXiv:2508.17778v2 Announce Type: replace Abstract: Despite the programmable architecture of Open RAN, today's deployments still rely heavily on static control and manual operations. To move beyond this limitation, we introduce AgentRAN, an AI-native, Open RAN-aligned agentic framework that generates and orchestrates a fabric of distributed AI agen",
    "url": "https://arxiv.org/abs/2508.17778",
    "source": "Arxiv AI"
  },
  {
    "title": "Instructional Agents: Reducing Teaching Faculty Workload through Multi-Agent Instructional Design",
    "summary": "arXiv:2508.19611v3 Announce Type: replace Abstract: Preparing high-quality instructional materials remains a labor-intensive process that often requires extensive coordination among teaching faculty, instructional designers, and teaching assistants. In this work, we present Instructional Agents, a multi-agent large language model framework designed",
    "url": "https://arxiv.org/abs/2508.19611",
    "source": "Arxiv AI"
  },
  {
    "title": "Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet",
    "summary": "arXiv:2509.06861v2 Announce Type: replace Abstract: Test-time scaling increases inference-time computation by allowing models to generate long reasoning chains, and has improved performance across many domains. However, in this work, we show that this approach is not yet effective for knowledge-intensive tasks. We evaluate 14 reasoning models on tw",
    "url": "https://arxiv.org/abs/2509.06861",
    "source": "Arxiv AI"
  },
  {
    "title": "p-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding",
    "summary": "arXiv:2509.23234v5 Announce Type: replace Abstract: Obtaining high-quality outputs from Large Language Models (LLMs) often depends upon the choice of a sampling-based decoding strategy to probabilistically choose the next token at each generation step. While a variety of such sampling methods have been proposed, their performance can be sensitive t",
    "url": "https://arxiv.org/abs/2509.23234",
    "source": "Arxiv AI"
  },
  {
    "title": "Unifying Agent Interaction and World Information for Multi-agent Coordination",
    "summary": "arXiv:2509.25550v4 Announce Type: replace Abstract: This work presents a novel representation learning framework, *interaction-world* latent (IWoL), to facilitate *team coordination* in multi-agent reinforcement learning (MARL). Building effective representation for team coordination is a challenging problem, due to the intricate dynamics emerging ",
    "url": "https://arxiv.org/abs/2509.25550",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning Reasoning Reward Models from Expert Demonstration via Inverse Reinforcement Learning",
    "summary": "arXiv:2510.01857v2 Announce Type: replace Abstract: Reasoning in large language models is typically trained via supervised fine-tuning (SFT) on expert traces, often framed as knowledge distillation, or reinforcement learning (RL) with outcome-based verifiable rewards. However, SFT focuses on imitation rather than optimisation, while outcome-based R",
    "url": "https://arxiv.org/abs/2510.01857",
    "source": "Arxiv AI"
  },
  {
    "title": "On The Statistical Limits of Self-Improving Agents",
    "summary": "arXiv:2510.04399v2 Announce Type: replace Abstract: As systems trend toward superintelligence, a natural modeling premise is that agents can self-improve along every facet of their own design. We formalize this with a five-axis decomposition and a decision layer, separating incentives from learning behavior and analyzing axes in isolation. Our cent",
    "url": "https://arxiv.org/abs/2510.04399",
    "source": "Arxiv AI"
  },
  {
    "title": "MARS: Co-evolving Dual-System Deep Research via Multi-Agent Reinforcement Learning",
    "summary": "arXiv:2510.04935v2 Announce Type: replace Abstract: Large Reasoning Models (LRMs) face two fundamental limitations: excessive token consumption when overanalyzing simple information processing tasks, and inability to access up-to-date knowledge beyond their training data. We introduce MARS (Multi-Agent System for Deep ReSearch), a novel co-evolutio",
    "url": "https://arxiv.org/abs/2510.04935",
    "source": "Arxiv AI"
  },
  {
    "title": "AI and Consciousness",
    "summary": "arXiv:2510.09858v3 Announce Type: replace Abstract: This is a skeptical overview of the literature on AI consciousness. We will soon create AI systems that are conscious according to some influential, mainstream theories of consciousness but are not conscious according to other influential, mainstream theories of consciousness. We will not be in a ",
    "url": "https://arxiv.org/abs/2510.09858",
    "source": "Arxiv AI"
  },
  {
    "title": "RGMem: Renormalization Group-inspired Memory Evolution for Language Agents",
    "summary": "arXiv:2510.16392v2 Announce Type: replace Abstract: Personalized and continuous interactions are critical for LLM-based conversational agents, yet finite context windows and static parametric memory hinder the modeling of long-term, cross-session user states. Existing approaches, including retrieval-augmented generation and explicit memory systems,",
    "url": "https://arxiv.org/abs/2510.16392",
    "source": "Arxiv AI"
  },
  {
    "title": "Extending RLVR to Open-Ended Tasks via Verifiable Multiple-Choice Reformulation",
    "summary": "arXiv:2511.02463v2 Announce Type: replace Abstract: Reinforcement Learning with Verifiable Rewards(RLVR) has demonstrated great potential in enhancing the reasoning capabilities of large language models (LLMs). However, its success has thus far been largely confined to the mathematical and programming domains with clear and automatically checkable ",
    "url": "https://arxiv.org/abs/2511.02463",
    "source": "Arxiv AI"
  },
  {
    "title": "IterResearch: Rethinking Long-Horizon Agents with Interaction Scaling",
    "summary": "arXiv:2511.07327v2 Announce Type: replace Abstract: Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to conte",
    "url": "https://arxiv.org/abs/2511.07327",
    "source": "Arxiv AI"
  },
  {
    "title": "Efficient Thought Space Exploration Through Strategic Intervention",
    "summary": "arXiv:2511.10038v2 Announce Type: replace Abstract: While large language models (LLMs) demonstrate emerging reasoning capabilities, current inference-time expansion methods incur prohibitive computational costs by exhaustive sampling. Through analyzing decoding trajectories, we observe that most next-token predictions align well with the golden out",
    "url": "https://arxiv.org/abs/2511.10038",
    "source": "Arxiv AI"
  },
  {
    "title": "Heterogeneous Multi-Agent Proximal Policy Optimization for Power Distribution System Restoration",
    "summary": "arXiv:2511.14730v5 Announce Type: replace Abstract: Restoring power distribution systems (PDSs) after large-scale outages requires sequential switching actions that reconfigure feeder topology and coordinate distributed energy resources (DERs) under nonlinear constraints, including power balance, voltage limits, and thermal ratings. These challenge",
    "url": "https://arxiv.org/abs/2511.14730",
    "source": "Arxiv AI"
  },
  {
    "title": "You Only Forward Once: An Efficient Compositional Judging Paradigm",
    "summary": "arXiv:2511.16600v3 Announce Type: replace Abstract: Multimodal large language models (MLLMs) show strong potential as judges. However, existing approaches face a fundamental trade-off: adapting MLLMs to output a single score misaligns with the generative nature of MLLMs and limits fine-grained requirement understanding, whereas autoregressively gen",
    "url": "https://arxiv.org/abs/2511.16600",
    "source": "Arxiv AI"
  },
  {
    "title": "CostNav: A Navigation Benchmark for Real-World Economic-Cost Evaluation of Physical AI Agents",
    "summary": "arXiv:2511.20216v2 Announce Type: replace Abstract: While current navigation benchmarks prioritize task success in simplified settings, they neglect the multidimensional economic constraints essential for the real-world commercialization of autonomous delivery systems. We introduce CostNav, an Economic Navigation Benchmark that evaluates physical A",
    "url": "https://arxiv.org/abs/2511.20216",
    "source": "Arxiv AI"
  },
  {
    "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
    "summary": "arXiv:2512.02589v2 Announce Type: replace Abstract: Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations",
    "url": "https://arxiv.org/abs/2512.02589",
    "source": "Arxiv AI"
  },
  {
    "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems",
    "summary": "arXiv:2512.06749v3 Announce Type: replace Abstract: Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradig",
    "url": "https://arxiv.org/abs/2512.06749",
    "source": "Arxiv AI"
  },
  {
    "title": "Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making",
    "summary": "arXiv:2512.08366v2 Announce Type: replace Abstract: Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) -- a demonstration-free f",
    "url": "https://arxiv.org/abs/2512.08366",
    "source": "Arxiv AI"
  },
  {
    "title": "Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance",
    "summary": "arXiv:2512.08492v2 Announce Type: replace Abstract: Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory struct",
    "url": "https://arxiv.org/abs/2512.08492",
    "source": "Arxiv AI"
  },
  {
    "title": "A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents",
    "summary": "arXiv:2512.20798v2 Announce Type: replace Abstract: As autonomous AI agents are increasingly deployed in high-stakes environments, ensuring their safety and alignment with human values has become a paramount concern. Current safety benchmarks primarily evaluate whether agents refuse explicitly harmful instructions or whether they can maintain proce",
    "url": "https://arxiv.org/abs/2512.20798",
    "source": "Arxiv AI"
  },
  {
    "title": "SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning",
    "summary": "arXiv:2601.04809v3 Announce Type: replace Abstract: Reinforcement learning (RL) offers a principled way to enhance the reasoning capabilities of large language models, yet its effectiveness hinges on training signals that remain informative as models evolve. In practice, RL progress often slows when task difficulty becomes poorly aligned with model",
    "url": "https://arxiv.org/abs/2601.04809",
    "source": "Arxiv AI"
  },
  {
    "title": "The Illusion of Human AI Parity Under Uncertainty: Navigating Elusive Ground Truth via a Probabilistic Paradigm",
    "summary": "arXiv:2601.05500v2 Announce Type: replace Abstract: Benchmarking the relative capabilities of AI systems, including Large Language Models (LLMs) and Vision Models, typically ignores the impact of uncertainty in the underlying ground truth answers from experts. This ambiguity is particularly consequential in medicine where uncertainty is pervasive. ",
    "url": "https://arxiv.org/abs/2601.05500",
    "source": "Arxiv AI"
  },
  {
    "title": "JudgeFlow: Agentic Workflow Optimization via Block Judge",
    "summary": "arXiv:2601.07477v2 Announce Type: replace Abstract: Optimizing LLM-based agentic workflows is challenging for scaling AI capabilities. Current methods rely on coarse, end-to-end evaluation signals and lack fine-grained signals on where to refine, often resulting in inefficient or low-impact modifications. To address these limitations, we propose Ju",
    "url": "https://arxiv.org/abs/2601.07477",
    "source": "Arxiv AI"
  },
  {
    "title": "Large-Scale Optimization Model Auto-Formulation: Harnessing LLM Flexibility via Structured Workflow",
    "summary": "arXiv:2601.09635v3 Announce Type: replace Abstract: Large-scale optimization is a key backbone of modern business decision-making. However, building these models is often labor-intensive and time-consuming. We address this by proposing LEAN-LLM-OPT, a LightwEight AgeNtic workflow construction framework for LLM-assisted large-scale OPTimization auto",
    "url": "https://arxiv.org/abs/2601.09635",
    "source": "Arxiv AI"
  },
  {
    "title": "NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models",
    "summary": "arXiv:2601.10457v2 Announce Type: replace Abstract: Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual b",
    "url": "https://arxiv.org/abs/2601.10457",
    "source": "Arxiv AI"
  },
  {
    "title": "Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment",
    "summary": "arXiv:2601.10520v2 Announce Type: replace Abstract: As AI agents become increasingly autonomous, widely deployed in consequential contexts, and efficacious in bringing about real-world impacts, ensuring that their decisions are not only instrumentally effective but also normatively aligned has become critical. We introduce a neuro-symbolic reason-b",
    "url": "https://arxiv.org/abs/2601.10520",
    "source": "Arxiv AI"
  },
  {
    "title": "Graph Neural Networks are Heuristics",
    "summary": "arXiv:2601.13465v3 Announce Type: replace Abstract: We demonstrate that a single training trajectory can transform a graph neural network into an unsupervised heuristic for combinatorial optimization. Focusing on the Travelling Salesman Problem, we show that encoding global structural constraints as an inductive bias enables a non-autoregressive mo",
    "url": "https://arxiv.org/abs/2601.13465",
    "source": "Arxiv AI"
  },
  {
    "title": "LangForce: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
    "summary": "arXiv:2601.15197v5 Announce Type: replace Abstract: Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datase",
    "url": "https://arxiv.org/abs/2601.15197",
    "source": "Arxiv AI"
  },
  {
    "title": "TransportAgents: a multi-agents LLM framework for traffic accident severity prediction",
    "summary": "arXiv:2601.15519v2 Announce Type: replace Abstract: Accurate prediction of traffic crash severity is critical for improving emergency response and public safety planning. Although recent large language models (LLMs) exhibit strong reasoning capabilities, their single-agent architectures often struggle with heterogeneous, domain-specific crash data ",
    "url": "https://arxiv.org/abs/2601.15519",
    "source": "Arxiv AI"
  },
  {
    "title": "SemanticALLI: Caching Reasoning, Not Just Responses, in Agentic Systems",
    "summary": "arXiv:2601.16286v2 Announce Type: replace Abstract: Agentic AI pipelines suffer from a hidden inefficiency: they frequently reconstruct identical intermediate logic, such as metric normalization or chart scaffolding, even when the user's natural language phrasing is entirely novel. Conventional boundary caching fails to capture this inefficiency be",
    "url": "https://arxiv.org/abs/2601.16286",
    "source": "Arxiv AI"
  },
  {
    "title": "LongCat-Flash-Thinking-2601 Technical Report",
    "summary": "arXiv:2601.16725v2 Announce Type: replace Abstract: We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, inclu",
    "url": "https://arxiv.org/abs/2601.16725",
    "source": "Arxiv AI"
  },
  {
    "title": "MAGNET: Towards Adaptive GUI Agents with Memory-Driven Knowledge Evolution",
    "summary": "arXiv:2601.19199v2 Announce Type: replace Abstract: Mobile GUI agents powered by large foundation models enable autonomous task execution, but frequent updates altering UI appearance and reorganizing workflows cause agents trained on historical data to fail. Despite surface changes, functional semantics and task intents remain fundamentally stable.",
    "url": "https://arxiv.org/abs/2601.19199",
    "source": "Arxiv AI"
  },
  {
    "title": "Insight Agents: An LLM-Based Multi-Agent System for Data Insights",
    "summary": "arXiv:2601.20048v2 Announce Type: replace Abstract: Today, E-commerce sellers face several key challenges, including difficulties in discovering and effectively utilizing available programs and tools, and struggling to understand and utilize rich data from various tools. We therefore aim to develop Insight Agents (IA), a conversational multi-agent ",
    "url": "https://arxiv.org/abs/2601.20048",
    "source": "Arxiv AI"
  },
  {
    "title": "AMA: Adaptive Memory via Multi-Agent Collaboration",
    "summary": "arXiv:2601.20352v2 Announce Type: replace Abstract: The rapid evolution of Large Language Model (LLM) agents has necessitated robust memory systems to support cohesive long-term interaction and complex reasoning. Benefiting from the strong capabilities of LLMs, recent research focus has shifted from simple context extension to the development of de",
    "url": "https://arxiv.org/abs/2601.20352",
    "source": "Arxiv AI"
  },
  {
    "title": "OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence",
    "summary": "arXiv:2601.21083v2 Announce Type: replace Abstract: As large language models improve, so do their offensive applications: frontier agents now generate working exploits for under $50 in compute (Heelan, 2026). Defensive incident response (IR) agents must keep pace, but existing benchmarks conflate action execution with correct execution, hiding cali",
    "url": "https://arxiv.org/abs/2601.21083",
    "source": "Arxiv AI"
  },
  {
    "title": "ChipBench: A Next-Step Benchmark for Evaluating LLM Performance in AI-Aided Chip Design",
    "summary": "arXiv:2601.21448v2 Announce Type: replace Abstract: While Large Language Models (LLMs) show significant potential in hardware engineering, current benchmarks suffer from saturation and limited task diversity, failing to reflect LLMs' performance in real industrial workflows. To address this gap, we propose a comprehensive benchmark for AI-aided chi",
    "url": "https://arxiv.org/abs/2601.21448",
    "source": "Arxiv AI"
  },
  {
    "title": "KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization",
    "summary": "arXiv:2601.21526v2 Announce Type: replace Abstract: We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable o",
    "url": "https://arxiv.org/abs/2601.21526",
    "source": "Arxiv AI"
  },
  {
    "title": "Language-based Trial and Error Falls Behind in the Era of Experience",
    "summary": "arXiv:2601.21754v2 Announce Type: replace Abstract: While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing ",
    "url": "https://arxiv.org/abs/2601.21754",
    "source": "Arxiv AI"
  },
  {
    "title": "Bridging Forecast Accuracy and Inventory KPIs: A Simulation-Based Software Framework",
    "summary": "arXiv:2601.21844v2 Announce Type: replace Abstract: Efficient management of spare parts inventory is crucial in the automotive aftermarket, where demand is highly intermittent and uncertainty drives substantial cost and service risks. Forecasting is therefore central, but the quality of forecasting models should be judged not by statistical accurac",
    "url": "https://arxiv.org/abs/2601.21844",
    "source": "Arxiv AI"
  },
  {
    "title": "Optimizing Agentic Workflows using Meta-tools",
    "summary": "arXiv:2601.22037v2 Announce Type: replace Abstract: Agentic AI enables LLM to dynamically reason, plan, and interact with tools to solve complex tasks. However, agentic workflows often require many iterative reasoning steps and tool invocations, leading to significant operational expense, end-to-end latency and failures due to hallucinations. This ",
    "url": "https://arxiv.org/abs/2601.22037",
    "source": "Arxiv AI"
  },
  {
    "title": "Semi-Autonomous Mathematics Discovery with Gemini: A Case Study on the Erd\\H{o}s Problems",
    "summary": "arXiv:2601.22401v2 Announce Type: replace Abstract: We present a case study in semi-autonomous mathematics discovery, using Gemini to systematically evaluate 700 conjectures labeled 'Open' in Bloom's Erd\\H{o}s Problems database. We employ a hybrid methodology: AI-driven natural language verification to narrow the search space, followed by human exp",
    "url": "https://arxiv.org/abs/2601.22401",
    "source": "Arxiv AI"
  },
  {
    "title": "Safe Control and Learning Using Generalized Action Governor",
    "summary": "arXiv:2211.12628v3 Announce Type: replace-cross Abstract: This paper introduces the Generalized Action Governor (AG), a supervisory scheme that augments a nominal closed-loop system with the capability to enforce state and input constraints through online action adjustment. We develop a generalized AG theory for discrete-time systems under bounded ",
    "url": "https://arxiv.org/abs/2211.12628",
    "source": "Arxiv AI"
  },
  {
    "title": "Real-Time Vibration-Based Bearing Fault Diagnosis Under Time-Varying Speed Conditions",
    "summary": "arXiv:2311.18547v2 Announce Type: replace-cross Abstract: Detection of rolling-element bearing faults is crucial for implementing proactive maintenance strategies and for minimizing the economic and operational consequences of unexpected failures. However, many existing techniques are developed and tested under strictly controlled conditions, limit",
    "url": "https://arxiv.org/abs/2311.18547",
    "source": "Arxiv AI"
  },
  {
    "title": "Unified Task and Motion Planning using Object-centric Abstractions of Motion Constraints",
    "summary": "arXiv:2312.17605v2 Announce Type: replace-cross Abstract: In task and motion planning (TAMP), the ambiguity and underdetermination of abstract descriptions used by task planning methods make it difficult to characterize physical constraints needed to successfully execute a task. The usual approach is to overlook such constraints at task planning le",
    "url": "https://arxiv.org/abs/2312.17605",
    "source": "Arxiv AI"
  },
  {
    "title": "Trajectory Data Management and Mining: A Survey from Deep Learning to the LLM Era",
    "summary": "arXiv:2403.14151v2 Announce Type: replace-cross Abstract: Trajectory computing is a pivotal domain encompassing trajectory data management and mining, garnering widespread attention due to its crucial role in various practical applications such as location services, urban traffic, and public safety. Traditional methods, focusing on simplistic spati",
    "url": "https://arxiv.org/abs/2403.14151",
    "source": "Arxiv AI"
  },
  {
    "title": "InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction",
    "summary": "arXiv:2403.19652v2 Announce Type: replace-cross Abstract: Text-conditioned human motion generation has experienced significant advancements with diffusion models trained on extensive motion capture data and corresponding textual annotations. However, extending such success to 3D dynamic human-object interaction (HOI) generation faces notable challe",
    "url": "https://arxiv.org/abs/2403.19652",
    "source": "Arxiv AI"
  },
  {
    "title": "Conditional diffusion models for downscaling and bias correction of Earth system model precipitation",
    "summary": "arXiv:2404.14416v2 Announce Type: replace-cross Abstract: Climate change exacerbates extreme weather events like heavy rainfall and flooding. As these events cause severe socioeconomic damage, accurate high-resolution simulation of precipitation is imperative. However, existing Earth System Models (ESMs) struggle to resolve small-scale dynamics and",
    "url": "https://arxiv.org/abs/2404.14416",
    "source": "Arxiv AI"
  },
  {
    "title": "Deep Transformer Network for Monocular Pose Estimation of Shipborne Unmanned Aerial Vehicle",
    "summary": "arXiv:2406.09260v3 Announce Type: replace-cross Abstract: This paper introduces a deep transformer network for estimating the relative 6D pose of a Unmanned Aerial Vehicle (UAV) with respect to a ship using monocular images. A synthetic dataset of ship images is created and annotated with 2D keypoints of multiple ship parts. A Transformer Neural Ne",
    "url": "https://arxiv.org/abs/2406.09260",
    "source": "Arxiv AI"
  },
  {
    "title": "Instance Temperature Knowledge Distillation",
    "summary": "arXiv:2407.00115v5 Announce Type: replace-cross Abstract: Knowledge distillation (KD) enhances the performance of a student network by allowing it to learn the knowledge transferred from a teacher network incrementally. Existing methods dynamically adjust the temperature to enable the student network to adapt to the varying learning difficulties at",
    "url": "https://arxiv.org/abs/2407.00115",
    "source": "Arxiv AI"
  },
  {
    "title": "Staircase Cascaded Fusion of Lightweight Local Pattern Recognition and Long-Range Dependencies for Structural Crack Segmentation",
    "summary": "arXiv:2408.12815v5 Announce Type: replace-cross Abstract: Accurately segmenting structural cracks at the pixel level remains a major hurdle, as existing methods fail to integrate local textures with pixel dependencies, often leading to fragmented and incomplete predictions. Moreover, their high parameter counts and substantial computational demands",
    "url": "https://arxiv.org/abs/2408.12815",
    "source": "Arxiv AI"
  },
  {
    "title": "Invariant Representation Guided Multimodal Sentiment Decoding with Sequential Variation Regularization",
    "summary": "arXiv:2409.00143v3 Announce Type: replace-cross Abstract: Achieving consistent sentiment representation across diverse modalities remains a key challenge in multimodal sentiment analysis. However, rapid emotional fluctuations over time often introduce instability, leading to compromised prediction performance. To address this challenge, we propose ",
    "url": "https://arxiv.org/abs/2409.00143",
    "source": "Arxiv AI"
  },
  {
    "title": "FPBoost: Fully Parametric Gradient Boosting for Survival Analysis",
    "summary": "arXiv:2409.13363v3 Announce Type: replace-cross Abstract: Survival analysis is a statistical framework for modeling time-to-event data. It plays a pivotal role in medicine, reliability engineering, and social science research, where understanding event dynamics even with few data samples is critical. Recent advancements in machine learning, particu",
    "url": "https://arxiv.org/abs/2409.13363",
    "source": "Arxiv AI"
  },
  {
    "title": "Exploiting Latent Linearity in LLMs Improves Explainable Molecular Representation Learning",
    "summary": "arXiv:2410.08829v2 Announce Type: replace-cross Abstract: Large language models (LLMs) have demonstrated broad utility across molecular domains, spanning drug discovery and materials design. Analyzing LLMs' latent representations is crucial for elucidating their underlying mechanisms, improving explainability, and ultimately advancing downstream pe",
    "url": "https://arxiv.org/abs/2410.08829",
    "source": "Arxiv AI"
  },
  {
    "title": "Optimizing Attention with Mirror Descent: Generalized Max-Margin Token Selection",
    "summary": "arXiv:2410.14581v5 Announce Type: replace-cross Abstract: Attention mechanisms have revolutionized several domains of artificial intelligence, such as natural language processing and computer vision, by enabling models to selectively focus on relevant parts of the input data. While recent work has characterized the optimization dynamics of gradient",
    "url": "https://arxiv.org/abs/2410.14581",
    "source": "Arxiv AI"
  },
  {
    "title": "SVIP: Towards Verifiable Inference of Open-source Large Language Models",
    "summary": "arXiv:2410.22307v3 Announce Type: replace-cross Abstract: The ever-increasing size of open-source Large Language Models (LLMs) renders local deployment impractical for individual users. Decentralized computing has emerged as a cost-effective solution, allowing individuals and small companies to perform LLM inference for users using surplus computat",
    "url": "https://arxiv.org/abs/2410.22307",
    "source": "Arxiv AI"
  },
  {
    "title": "U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills in LLMs",
    "summary": "arXiv:2412.03205v4 Announce Type: replace-cross Abstract: The current evaluation of mathematical skills in LLMs is limited, as existing benchmarks are either relatively small, primarily focus on elementary and high-school problems, or lack diversity in topics. Additionally, the inclusion of visual elements in tasks remains largely under-explored. T",
    "url": "https://arxiv.org/abs/2412.03205",
    "source": "Arxiv AI"
  },
  {
    "title": "Enhancing Human-Like Responses in Large Language Models",
    "summary": "arXiv:2501.05032v2 Announce Type: replace-cross Abstract: This paper explores the advancements in making large language models (LLMs) more human-like. We focus on techniques that enhance natural language understanding, conversational coherence, and emotional intelligence in AI systems. The study evaluates various approaches, including fine-tuning w",
    "url": "https://arxiv.org/abs/2501.05032",
    "source": "Arxiv AI"
  },
  {
    "title": "Conformal mapping based Physics-informed neural networks for designing neutral inclusions",
    "summary": "arXiv:2501.07809v2 Announce Type: replace-cross Abstract: We address the neutral inclusion problem with imperfect boundary conditions, focusing on designing interface functions for inclusions of arbitrary shapes. Traditional Physics-Informed Neural Networks (PINNs) struggle with this inverse problem, leading to the development of Conformal Mapping ",
    "url": "https://arxiv.org/abs/2501.07809",
    "source": "Arxiv AI"
  },
  {
    "title": "PIQL: Projective Implicit Q-Learning with Support Constraint for Offline Reinforcement Learning",
    "summary": "arXiv:2501.08907v2 Announce Type: replace-cross Abstract: Offline Reinforcement Learning (RL) faces a fundamental challenge of extrapolation errors caused by out-of-distribution (OOD) actions. Implicit Q-Learning (IQL) employs expectile regression to achieve in-sample learning. Nevertheless, IQL relies on a fixed expectile hyperparameter and a dens",
    "url": "https://arxiv.org/abs/2501.08907",
    "source": "Arxiv AI"
  },
  {
    "title": "BMG-Q: Localized Bipartite Match Graph Attention Q-Learning for Ride-Pooling Order Dispatch",
    "summary": "arXiv:2501.13448v2 Announce Type: replace-cross Abstract: This paper introduces Localized Bipartite Match Graph Attention Q-Learning (BMG-Q), a novel Multi-Agent Reinforcement Learning (MARL) algorithm framework tailored for ride-pooling order dispatch. BMG-Q advances ride-pooling decision-making process with the localized bipartite match graph und",
    "url": "https://arxiv.org/abs/2501.13448",
    "source": "Arxiv AI"
  },
  {
    "title": "Decoding Generalization from Memorization in Deep Neural Networks",
    "summary": "arXiv:2501.14687v2 Announce Type: replace-cross Abstract: Overparameterized deep networks that generalize well have been key to the dramatic success of deep learning in recent years. The reasons for their remarkable ability to generalize are not well understood yet. When class labels in the training set are shuffled to varying degrees, it is known ",
    "url": "https://arxiv.org/abs/2501.14687",
    "source": "Arxiv AI"
  },
  {
    "title": "CFT-RAG: An Entity Tree Based Retrieval Augmented Generation Algorithm With Cuckoo Filter",
    "summary": "arXiv:2501.15098v3 Announce Type: replace-cross Abstract: Although retrieval-augmented generation(RAG) significantly improves generation quality by retrieving external knowledge bases and integrating generated content, it faces computational efficiency bottlenecks, particularly in knowledge retrieval tasks involving hierarchical structures for Tree",
    "url": "https://arxiv.org/abs/2501.15098",
    "source": "Arxiv AI"
  },
  {
    "title": "Achieving Time Series Reasoning Requires Rethinking Model Design, Tasks Formulation, and Evaluation",
    "summary": "arXiv:2502.01477v2 Announce Type: replace-cross Abstract: Understanding time series data is fundamental to many real-world applications. Recent work explores multimodal large language models (MLLMs) to enhance time series understanding with contextual information beyond numerical signals. This area has grown from 7 papers in 2023 to over 580 in 202",
    "url": "https://arxiv.org/abs/2502.01477",
    "source": "Arxiv AI"
  },
  {
    "title": "LEAD: An EEG Foundation Model for Alzheimer's Disease Detection",
    "summary": "arXiv:2502.01678v4 Announce Type: replace-cross Abstract: Electroencephalography (EEG) provides a non-invasive, highly accessible, and cost-effective approach for detecting Alzheimer's disease (AD). However, existing methods, whether based on handcrafted feature engineering or standard deep learning, face three major challenges: 1) the lack of larg",
    "url": "https://arxiv.org/abs/2502.01678",
    "source": "Arxiv AI"
  },
  {
    "title": "Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies",
    "summary": "arXiv:2502.02533v2 Announce Type: replace-cross Abstract: Large language models, employed as multiple agents that interact and collaborate with each other, have excelled at solving complex tasks. The agents are programmed with prompts that declare their functionality, along with the topologies that orchestrate interactions across agents. Designing ",
    "url": "https://arxiv.org/abs/2502.02533",
    "source": "Arxiv AI"
  },
  {
    "title": "How does a Multilingual LM Handle Multiple Languages?",
    "summary": "arXiv:2502.04269v2 Announce Type: replace-cross Abstract: Multilingual language models have significantly advanced due to rapid progress in natural language processing. Models like BLOOM 1.7B, trained on diverse multilingual datasets, aim to bridge linguistic gaps. However, their effectiveness in capturing linguistic knowledge, particularly for low",
    "url": "https://arxiv.org/abs/2502.04269",
    "source": "Arxiv AI"
  },
  {
    "title": "SCALM: Detecting Bad Practices in Smart Contracts Through LLMs",
    "summary": "arXiv:2502.04347v2 Announce Type: replace-cross Abstract: As the Ethereum platform continues to mature and gain widespread usage, it is crucial to maintain high standards of smart contract writing practices. While bad practices in smart contracts may not directly lead to security issues, they do elevate the risk of encountering problems. Therefore,",
    "url": "https://arxiv.org/abs/2502.04347",
    "source": "Arxiv AI"
  },
  {
    "title": "Large Multimodal Models for Low-Resource Languages: A Survey",
    "summary": "arXiv:2502.05568v4 Announce Type: replace-cross Abstract: In this survey, we systematically analyze techniques used to adapt large multimodal models (LMMs) for low-resource (LR) languages, examining approaches ranging from visual enhancement and data creation to cross-modal transfer and fusion strategies. Through a comprehensive analysis of 117 stu",
    "url": "https://arxiv.org/abs/2502.05568",
    "source": "Arxiv AI"
  },
  {
    "title": "Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging",
    "summary": "arXiv:2502.06876v4 Announce Type: replace-cross Abstract: Achieving balanced alignment of large language models (LLMs) in terms of Helpfulness, Honesty, and Harmlessness (3H optimization) constitutes a cornerstone of responsible AI. Existing methods like data mixture strategies face limitations, including heavy reliance on expert knowledge and conf",
    "url": "https://arxiv.org/abs/2502.06876",
    "source": "Arxiv AI"
  },
  {
    "title": "UniZyme: A Unified Protein Cleavage Site Predictor Enhanced with Enzyme Active-Site Knowledge",
    "summary": "arXiv:2502.06914v3 Announce Type: replace-cross Abstract: Enzyme-catalyzed protein cleavage is essential for many biological functions. Accurate prediction of cleavage sites can facilitate various applications such as drug development, enzyme design, and a deeper understanding of biological mechanisms. However, most existing models are restricted t",
    "url": "https://arxiv.org/abs/2502.06914",
    "source": "Arxiv AI"
  },
  {
    "title": "On the Importance of Pretraining Data Alignment for Atomic Property Prediction",
    "summary": "arXiv:2502.11085v2 Announce Type: replace-cross Abstract: This paper challenges the recent paradigm in atomic property prediction that links progress to growing dataset sizes and computational resources. We show that pretraining on a carefully selected task-aligned dataset can match or even surpass large-scale joint pretraining while using only 1/2",
    "url": "https://arxiv.org/abs/2502.11085",
    "source": "Arxiv AI"
  },
  {
    "title": "Sparse Autoencoder Features for Classifications and Transferability",
    "summary": "arXiv:2502.11367v2 Announce Type: replace-cross Abstract: Sparse Autoencoders (SAEs) provide potentials for uncovering structured, human-interpretable representations in Large Language Models (LLMs), making them a crucial tool for transparent and controllable AI systems. We systematically analyze SAE for interpretable feature extraction from LLMs i",
    "url": "https://arxiv.org/abs/2502.11367",
    "source": "Arxiv AI"
  },
  {
    "title": "Teaching LLMs According to Their Aptitude: Adaptive Reasoning for Mathematical Problem Solving",
    "summary": "arXiv:2502.12022v4 Announce Type: replace-cross Abstract: Existing approaches to mathematical reasoning with large language models (LLMs) rely on Chain-of-Thought (CoT) for generalizability or Tool-Integrated Reasoning (TIR) for precise computation. While efforts have been made to combine these methods, they primarily rely on post-selection or pred",
    "url": "https://arxiv.org/abs/2502.12022",
    "source": "Arxiv AI"
  },
  {
    "title": "How Much Do LLMs Hallucinate across Languages? On Realistic Multilingual Estimation of LLM Hallucination",
    "summary": "arXiv:2502.12769v4 Announce Type: replace-cross Abstract: In the age of misinformation, hallucination - the tendency of Large Language Models (LLMs) to generate non-factual or unfaithful responses - represents the main risk for their global utility. Despite LLMs becoming increasingly multilingual, the vast majority of research on detecting and quan",
    "url": "https://arxiv.org/abs/2502.12769",
    "source": "Arxiv AI"
  },
  {
    "title": "Entropy-Lens: Uncovering Decision Strategies in LLMs",
    "summary": "arXiv:2502.16570v3 Announce Type: replace-cross Abstract: In large language models (LLMs), each block operates on the residual stream to map input token sequences to output token distributions. However, most of the interpretability literature focuses on internal latent representations, leaving token-space dynamics underexplored. The high dimensiona",
    "url": "https://arxiv.org/abs/2502.16570",
    "source": "Arxiv AI"
  },
  {
    "title": "Sample-Efficient Diffusion-based Control of Complex Physics Systems",
    "summary": "arXiv:2502.17893v2 Announce Type: replace-cross Abstract: Controlling complex physics systems is important in diverse domains. While diffusion-based methods have demonstrated advantages over classical model-based approaches and myopic sequential learning methods in achieving global trajectory consistency, they are limited by sample efficiency.This ",
    "url": "https://arxiv.org/abs/2502.17893",
    "source": "Arxiv AI"
  },
  {
    "title": "Mixtera: A Data Plane for Foundation Model Training",
    "summary": "arXiv:2502.19790v3 Announce Type: replace-cross Abstract: State-of-the-art large language and vision models are trained over trillions of tokens that are aggregated from a large variety of sources. As training data collections grow, manually managing the samples becomes time-consuming, tedious, and prone to errors. Yet recent research shows that th",
    "url": "https://arxiv.org/abs/2502.19790",
    "source": "Arxiv AI"
  },
  {
    "title": "Causally Reliable Concept Bottleneck Models",
    "summary": "arXiv:2503.04363v4 Announce Type: replace-cross Abstract: Concept-based models are an emerging paradigm in deep learning that constrains the inference process to operate through human-interpretable variables, facilitating explainability and human interaction. However, these architectures, on par with popular opaque neural models, fail to account fo",
    "url": "https://arxiv.org/abs/2503.04363",
    "source": "Arxiv AI"
  },
  {
    "title": "Towards Next-Generation Recommender Systems: A Benchmark for Personalized Recommendation Assistant with LLMs",
    "summary": "arXiv:2503.09382v2 Announce Type: replace-cross Abstract: Recommender systems (RecSys) are widely used across various modern digital platforms and have garnered significant attention. Traditional recommender systems usually focus only on fixed and simple recommendation scenarios, making it difficult to generalize to new and unseen recommendation ta",
    "url": "https://arxiv.org/abs/2503.09382",
    "source": "Arxiv AI"
  },
  {
    "title": "Large-Scale Auto-bidding with Nash Equilibrium Constraints",
    "summary": "arXiv:2503.10304v2 Announce Type: replace-cross Abstract: Auto-bidding has become a cornerstone of modern online advertising platforms, enabling many advertisers to automate bidding at scale and optimize campaign performance. However, prevailing industrial systems rely on single-agent auto-bidding methods that are scalable but overlook the strategi",
    "url": "https://arxiv.org/abs/2503.10304",
    "source": "Arxiv AI"
  },
  {
    "title": "1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities",
    "summary": "arXiv:2503.14858v4 Announce Type: replace-cross Abstract: Scaling up self-supervised learning has driven breakthroughs in language and vision, yet comparable progress has remained elusive in reinforcement learning (RL). In this paper, we study building blocks for self-supervised RL that unlock substantial improvements in scalability, with network d",
    "url": "https://arxiv.org/abs/2503.14858",
    "source": "Arxiv AI"
  },
  {
    "title": "DenseFormer: Learning Dense Depth Map from Sparse Depth and Image via Conditional Diffusion Model",
    "summary": "arXiv:2503.23993v2 Announce Type: replace-cross Abstract: The depth completion task is a critical problem in autonomous driving, involving the generation of dense depth maps from sparse depth maps and RGB images. Most existing methods employ a spatial propagation network to iteratively refine the depth map after obtaining an initial dense depth. In",
    "url": "https://arxiv.org/abs/2503.23993",
    "source": "Arxiv AI"
  },
  {
    "title": "DiTOX: Fault Detection and Localization in the ONNX Optimizer",
    "summary": "arXiv:2505.01892v5 Announce Type: replace-cross Abstract: The ONNX Optimizer, part of the official ONNX repository and widely adopted for graph-level model optimizations, is used by default to optimize ONNX models. Despite its popularity, its ability to preserve model correctness has not been systematically evaluated. We present DiTOX, an automated",
    "url": "https://arxiv.org/abs/2505.01892",
    "source": "Arxiv AI"
  },
  {
    "title": "Minimisation of Quasar-Convex Functions Using Random Zeroth-Order Oracles",
    "summary": "arXiv:2505.02281v2 Announce Type: replace-cross Abstract: This paper explores the performance of a random Gaussian smoothing zeroth-order (ZO) scheme for minimising quasar-convex (QC) and strongly quasar-convex (SQC) functions in both unconstrained and constrained settings. For the unconstrained problem, we establish the ZO algorithm's convergence ",
    "url": "https://arxiv.org/abs/2505.02281",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning-Augmented Power System Operations: A Unified Optimization View",
    "summary": "arXiv:2505.05203v2 Announce Type: replace-cross Abstract: With the increasing penetration of renewable energy, traditional physics-based power system operation faces growing challenges in achieving economic efficiency, stability, and robustness. Machine learning (ML) has emerged as a powerful tool for modeling complex system dynamics to address the",
    "url": "https://arxiv.org/abs/2505.05203",
    "source": "Arxiv AI"
  },
  {
    "title": "UniSymNet: A Unified Symbolic Network Guided by Transformer",
    "summary": "arXiv:2505.06091v2 Announce Type: replace-cross Abstract: Symbolic Regression (SR) is a powerful technique for automatically discovering mathematical expressions from input data. Mainstream SR algorithms search for the optimal symbolic tree in a vast function space, but the increasing complexity of the tree structure limits their performance. Inspi",
    "url": "https://arxiv.org/abs/2505.06091",
    "source": "Arxiv AI"
  },
  {
    "title": "Sparse Latent Factor Forecaster (SLFF) with Iterative Inference for Transparent Multi-Horizon Commodity Futures Prediction",
    "summary": "arXiv:2505.06795v4 Announce Type: replace-cross Abstract: Commodity futures are volatile. Forecasting across horizons with interpretable drivers remains challenging. We propose the Sparse Latent Factor Forecaster with Iterative Inference (SLFF), a structured prediction latent variable model that combines sparse coding, unrolled optimization, and am",
    "url": "https://arxiv.org/abs/2505.06795",
    "source": "Arxiv AI"
  },
  {
    "title": "Graph-Based Floor Separation Using Node Embeddings and Clustering of WiFi Trajectories",
    "summary": "arXiv:2505.08088v3 Announce Type: replace-cross Abstract: Vertical localization, particularly floor separation, remains a major challenge in indoor positioning systems operating in GPS-denied multistory environments. This paper proposes a fully data-driven, graph-based framework for blind floor separation using only Wi-Fi fingerprint trajectories, ",
    "url": "https://arxiv.org/abs/2505.08088",
    "source": "Arxiv AI"
  },
  {
    "title": "Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents",
    "summary": "arXiv:2505.11891v3 Announce Type: replace-cross Abstract: VLM-based mobile agents are increasingly popular due to their capabilities to interact with smartphone GUIs and XML-structured texts and to complete daily tasks. However, existing online benchmarks struggle with obtaining stable reward signals due to dynamic environmental changes. Offline be",
    "url": "https://arxiv.org/abs/2505.11891",
    "source": "Arxiv AI"
  },
  {
    "title": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics",
    "summary": "arXiv:2505.13192v3 Announce Type: replace-cross Abstract: Complex, temporally evolving phenomena, from climate to brain activity, are governed by dynamical systems (DS). DS reconstruction (DSR) seeks to infer generative surrogate models of these from observed data, reproducing their long-term behavior. Existing DSR approaches require purpose-traini",
    "url": "https://arxiv.org/abs/2505.13192",
    "source": "Arxiv AI"
  },
  {
    "title": "Code-Mixed Phonetic Perturbations for Red-Teaming LLMs",
    "summary": "arXiv:2505.14226v4 Announce Type: replace-cross Abstract: Large language models (LLMs) continue to be demonstrably unsafe despite sophisticated safety alignment techniques and multilingual red-teaming. However, recent red-teaming work has focused on incremental gains in attack success over identifying underlying architectural vulnerabilities in mod",
    "url": "https://arxiv.org/abs/2505.14226",
    "source": "Arxiv AI"
  },
  {
    "title": "RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection",
    "summary": "arXiv:2505.15386v3 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have become powerful, but hallucinations remain a vital obstacle to their trustworthy use. Previous works improved the capability of hallucination detection by measuring uncertainty. But they can not explain the provenance behind why hallucinations occur, particu",
    "url": "https://arxiv.org/abs/2505.15386",
    "source": "Arxiv AI"
  },
  {
    "title": "DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection",
    "summary": "arXiv:2505.16530v2 Announce Type: replace-cross Abstract: Large language models (LLMs) are considered valuable Intellectual Properties (IP) for legitimate owners due to the enormous computational cost of training. It is crucial to protect the IP of LLMs from malicious stealing or unauthorized deployment. Despite existing efforts in watermarking and",
    "url": "https://arxiv.org/abs/2505.16530",
    "source": "Arxiv AI"
  },
  {
    "title": "HyBattNet: Hybrid Framework for Predicting the Remaining Useful Life of Lithium-Ion Batteries",
    "summary": "arXiv:2505.16664v3 Announce Type: replace-cross Abstract: Accurate prediction of the Remaining Useful Life (RUL) is essential for enabling timely maintenance of lithium-ion batteries, impacting the operational efficiency of electric applications that rely on them. This paper proposes a RUL prediction approach that leverages data from recent charge-",
    "url": "https://arxiv.org/abs/2505.16664",
    "source": "Arxiv AI"
  },
  {
    "title": "Small Models, Smarter Learning: The Power of Joint Task Training",
    "summary": "arXiv:2505.18369v2 Announce Type: replace-cross Abstract: Multi-task learning improves generalization, but when does it reduce the model capacity required to learn? We provide a systematic study of how joint training affects the learning transition, the minimum model size at which a task can be learned, using nested arithmetic (ListOps) and permuta",
    "url": "https://arxiv.org/abs/2505.18369",
    "source": "Arxiv AI"
  },
  {
    "title": "ePC: Fast and Deep Predictive Coding for Digital Hardware",
    "summary": "arXiv:2505.20137v4 Announce Type: replace-cross Abstract: Predictive Coding (PC) offers a brain-inspired alternative to backpropagation for neural network training, described as a physical system minimizing its internal energy. However, in practice, PC is predominantly digitally simulated, requiring excessive amounts of compute while struggling to ",
    "url": "https://arxiv.org/abs/2505.20137",
    "source": "Arxiv AI"
  },
  {
    "title": "Experience-based Knowledge Correction for Robust Planning in Minecraft",
    "summary": "arXiv:2505.24157v2 Announce Type: replace-cross Abstract: Large Language Model (LLM)-based planning has advanced embodied agents in long-horizon environments such as Minecraft, where acquiring latent knowledge of goal (or item) dependencies and feasible actions is critical. However, LLMs often begin with flawed priors and fail to correct them throu",
    "url": "https://arxiv.org/abs/2505.24157",
    "source": "Arxiv AI"
  },
  {
    "title": "HueManity: Probing Fine-Grained Visual Perception in MLLMs",
    "summary": "arXiv:2506.03194v5 Announce Type: replace-cross Abstract: Recent Multimodal Large Language Models (MLLMs) demonstrate strong high-level visual reasoning on tasks such as visual question answering and image captioning. Yet existing benchmarks largely overlook their ability to capture fine-grained perceptual details. As MLLMs are increasingly deploye",
    "url": "https://arxiv.org/abs/2506.03194",
    "source": "Arxiv AI"
  },
  {
    "title": "Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce",
    "summary": "arXiv:2506.06576v3 Announce Type: replace-cross Abstract: The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the labor market, raising concerns about job displacement, diminished human agency, and overreliance on automation. Yet, we lack a systematic understanding of the evolving landscape. In this paper, we address this gap by ",
    "url": "https://arxiv.org/abs/2506.06576",
    "source": "Arxiv AI"
  },
  {
    "title": "KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache",
    "summary": "arXiv:2506.08018v3 Announce Type: replace-cross Abstract: The high memory demands of the Key-Value (KV) Cache during the inference of Large Language Models (LLMs) severely restrict their deployment in resource-constrained platforms. Quantization can effectively alleviate the memory pressure caused by KV Cache. However, existing methods either rely ",
    "url": "https://arxiv.org/abs/2506.08018",
    "source": "Arxiv AI"
  },
  {
    "title": "Draft-based Approximate Inference for LLMs",
    "summary": "arXiv:2506.08373v3 Announce Type: replace-cross Abstract: Optimizing inference for long-context large language models (LLMs) is increasingly important due to the quadratic compute and linear memory cost of Transformers. Existing approximate inference methods, including key-value (KV) cache dropping, sparse attention, and prompt compression, typical",
    "url": "https://arxiv.org/abs/2506.08373",
    "source": "Arxiv AI"
  },
  {
    "title": "PAL: Probing Audio Encoders via LLMs -- Audio Information Transfer into LLMs",
    "summary": "arXiv:2506.10423v3 Announce Type: replace-cross Abstract: Integration of audio perception into large language models (LLMs) is an emerging research area for enabling machine listening applications, yet efficient transfer of rich audio semantics from audio encoders to LLMs remains underexplored. The most widely used integration paradigm projects aud",
    "url": "https://arxiv.org/abs/2506.10423",
    "source": "Arxiv AI"
  },
  {
    "title": "Adaptive Shielding for Safe Reinforcement Learning under Hidden-Parameter Dynamics Shifts",
    "summary": "arXiv:2506.11033v2 Announce Type: replace-cross Abstract: Unseen shifts in environment dynamics, driven by hidden parameters such as friction or gravity, create a challenge for maintaining safety. We address this challenge by proposing Adaptive Shielding, a framework for safe reinforcement learning in constrained hidden-parameter Markov decision pr",
    "url": "https://arxiv.org/abs/2506.11033",
    "source": "Arxiv AI"
  },
  {
    "title": "NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust Vision-Language Models",
    "summary": "arXiv:2506.12706v2 Announce Type: replace-cross Abstract: Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capabilities in understanding relationships between visual and textual data through joint embedding spaces. Despite their effectiveness, these models remain vulnerable to adversarial attacks, particularly in the image mo",
    "url": "https://arxiv.org/abs/2506.12706",
    "source": "Arxiv AI"
  },
  {
    "title": "Identifiability of Deep Polynomial Neural Networks",
    "summary": "arXiv:2506.17093v3 Announce Type: replace-cross Abstract: Polynomial Neural Networks (PNNs) possess a rich algebraic and geometric structure. However, their identifiability -- a key property for ensuring interpretability -- remains poorly understood. In this work, we present a comprehensive analysis of the identifiability of deep PNNs, including ar",
    "url": "https://arxiv.org/abs/2506.17093",
    "source": "Arxiv AI"
  },
  {
    "title": "HalluRNN: Mitigating Hallucinations via Recurrent Cross-Layer Reasoning in Large Vision-Language Models",
    "summary": "arXiv:2506.17587v2 Announce Type: replace-cross Abstract: Though Large Vision-Language Models (LVLMs) have achieved remarkable performance across various tasks, they are still prone to hallucinations-generating outputs that are textually plausible but visually ungrounded. While prior approaches generally address this issue through data-centric fine",
    "url": "https://arxiv.org/abs/2506.17587",
    "source": "Arxiv AI"
  },
  {
    "title": "Transferring Visual Explainability of Self-Explaining Models to Prediction-Only Models without Additional Training",
    "summary": "arXiv:2507.04380v2 Announce Type: replace-cross Abstract: In image classification scenarios where both prediction and explanation efficiency are required, self-explaining models that perform both tasks in a single inference are effective. However, for users who already have prediction-only models, training a new self-explaining model from scratch i",
    "url": "https://arxiv.org/abs/2507.04380",
    "source": "Arxiv AI"
  },
  {
    "title": "DP-Fusion: Token-Level Differentially Private Inference for Large Language Models",
    "summary": "arXiv:2507.04531v4 Announce Type: replace-cross Abstract: Large language models (LLMs) do not preserve privacy at inference-time. The LLM's outputs can inadvertently reveal information about the model's context, which presents a privacy challenge when the LLM is augmented via tools or databases containing sensitive information. Existing privacy-pre",
    "url": "https://arxiv.org/abs/2507.04531",
    "source": "Arxiv AI"
  },
  {
    "title": "AblationBench: Evaluating Automated Planning of Ablations in Empirical AI Research",
    "summary": "arXiv:2507.08038v2 Announce Type: replace-cross Abstract: Language model agents are increasingly used to automate scientific research, yet evaluating their scientific contributions remains a challenge. A key mechanism to obtain such insights is through ablation experiments. To this end, we introduce AblationBench, a benchmark suite for evaluating a",
    "url": "https://arxiv.org/abs/2507.08038",
    "source": "Arxiv AI"
  },
  {
    "title": "A Selective Quantization Tuner for ONNX Models",
    "summary": "arXiv:2507.12196v2 Announce Type: replace-cross Abstract: Quantization reduces the precision of deep neural networks to lower model size and computational demands, but often at the expense of accuracy. Fully quantized models can suffer significant accuracy degradation, and resource-constrained hardware accelerators may not support all quantized ope",
    "url": "https://arxiv.org/abs/2507.12196",
    "source": "Arxiv AI"
  },
  {
    "title": "R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning",
    "summary": "arXiv:2507.17307v5 Announce Type: replace-cross Abstract: Chain-of-thought (CoT) enhances the problem-solving ability of large language models (LLMs) but incurs substantial inference cost due to long autoregressive trajectories. Existing acceleration strategies either shorten traces via early stopping or compression, or adopt speculative decoding w",
    "url": "https://arxiv.org/abs/2507.17307",
    "source": "Arxiv AI"
  },
  {
    "title": "Information Security Based on LLM Approaches: A Review",
    "summary": "arXiv:2507.18215v2 Announce Type: replace-cross Abstract: Information security is facing increasingly severe challenges, and traditional protection means are difficult to cope with complex and changing threats. In recent years, as an emerging intelligent technology, large language models (LLMs) have shown a broad application prospect in the field o",
    "url": "https://arxiv.org/abs/2507.18215",
    "source": "Arxiv AI"
  },
  {
    "title": "Multivariate Standardized Residuals for Conformal Prediction",
    "summary": "arXiv:2507.20941v3 Announce Type: replace-cross Abstract: While split conformal prediction guarantees marginal coverage, approaching the stronger property of conditional coverage is essential for reliable uncertainty quantification. Naive conformal scores, however, suffer from poor conditional coverage in heteroskedastic settings. In univariate reg",
    "url": "https://arxiv.org/abs/2507.20941",
    "source": "Arxiv AI"
  },
  {
    "title": "MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge",
    "summary": "arXiv:2507.21183v4 Announce Type: replace-cross Abstract: As the era of large language models (LLMs) unfolds, Preference Optimization (PO) methods have become a central approach to aligning LLMs with human preferences and improving performance. We propose Maximum a Posteriori Preference Optimization (MaPPO), a methodology for learning from preferen",
    "url": "https://arxiv.org/abs/2507.21183",
    "source": "Arxiv AI"
  },
  {
    "title": "ECGTwin: Personalized ECG Generation Using Controllable Diffusion Model",
    "summary": "arXiv:2508.02720v2 Announce Type: replace-cross Abstract: Personalized electrocardiogram (ECG) generation is to simulate a patient's ECG digital twins tailored to specific conditions. It has the potential to transform traditional healthcare into a more accurate individualized paradigm, while preserving the key benefits of conventional population-le",
    "url": "https://arxiv.org/abs/2508.02720",
    "source": "Arxiv AI"
  },
  {
    "title": "DeepGB-TB: A Risk-Balanced Cross-Attention Gradient-Boosted Convolutional Network for Rapid, Interpretable Tuberculosis Screening",
    "summary": "arXiv:2508.02741v2 Announce Type: replace-cross Abstract: Large-scale tuberculosis (TB) screening is limited by the high cost and operational complexity of traditional diagnostics, creating a need for artificial-intelligence solutions. We propose DeepGB-TB, a non-invasive system that instantly assigns TB risk scores using only cough audio and basic",
    "url": "https://arxiv.org/abs/2508.02741",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning Robust Intervention Representations with Delta Embeddings",
    "summary": "arXiv:2508.04492v2 Announce Type: replace-cross Abstract: Causal representation learning has attracted significant research interest during the past few years, as a means for improving model generalization and robustness. Causal representations of interventional image pairs (also called ``actionable counterfactuals'' in the literature), have the pr",
    "url": "https://arxiv.org/abs/2508.04492",
    "source": "Arxiv AI"
  },
  {
    "title": "BiasGym: A Simple and Generalizable Framework for Analyzing and Removing Biases through Elicitation",
    "summary": "arXiv:2508.08855v4 Announce Type: replace-cross Abstract: Understanding biases and stereotypes encoded in the weights of Large Language Models (LLMs) is crucial for developing effective mitigation strategies. However, biased behaviour is often subtle and non-trivial to isolate, even when deliberately elicited, making systematic analysis and debiasi",
    "url": "https://arxiv.org/abs/2508.08855",
    "source": "Arxiv AI"
  },
  {
    "title": "SACO: Sequence-Aware Constrained Optimization Framework for Coupon Distribution in E-commerce",
    "summary": "arXiv:2508.09198v2 Announce Type: replace-cross Abstract: Coupon distribution is a critical marketing strategy used by online platforms to boost revenue and enhance user engagement. Regrettably, existing coupon distribution strategies fall far short of effectively leveraging the complex sequential interactions between platforms and users. This crit",
    "url": "https://arxiv.org/abs/2508.09198",
    "source": "Arxiv AI"
  },
  {
    "title": "Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models",
    "summary": "arXiv:2508.10030v2 Announce Type: replace-cross Abstract: Prompt optimization methods have demonstrated significant effectiveness in aligning black-box large language models (LLMs). In parallel, inference scaling strategies such as Best-of-N Sampling and Majority Voting have likewise been shown to improve alignment and performance by trading additi",
    "url": "https://arxiv.org/abs/2508.10030",
    "source": "Arxiv AI"
  },
  {
    "title": "Quantum Flow Matching",
    "summary": "arXiv:2508.12413v3 Announce Type: replace-cross Abstract: The flow matching has rapidly become a dominant paradigm in classical generative modeling, offering an efficient way to interpolate between two complex distributions. We extend this idea to the quantum realm and introduce the Quantum Flow Matching (QFM), a quantum-circuit realization that of",
    "url": "https://arxiv.org/abs/2508.12413",
    "source": "Arxiv AI"
  },
  {
    "title": "ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction",
    "summary": "arXiv:2508.12685v2 Announce Type: replace-cross Abstract: Agentic task-solving with Large Language Models (LLMs) requires multi-turn, multi-step interactions, often involving complex function calls and dynamic user-agent exchanges. Existing simulation-based data generation methods for such scenarios rely heavily on costly autoregressive interaction",
    "url": "https://arxiv.org/abs/2508.12685",
    "source": "Arxiv AI"
  },
  {
    "title": "Hallucination-Resistant Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement",
    "summary": "arXiv:2508.14391v2 Announce Type: replace-cross Abstract: Relation extraction (RE) enables the construction of structured knowledge for many downstream applications. While large language models (LLMs) have shown great promise in this task, they often struggle to reliably determine whether a relation exists, particularly in sentences with complex sy",
    "url": "https://arxiv.org/abs/2508.14391",
    "source": "Arxiv AI"
  },
  {
    "title": "End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning",
    "summary": "arXiv:2508.15746v2 Announce Type: replace-cross Abstract: The integration of Large Language Models (LLMs) into healthcare is constrained by knowledge limitations, hallucinations, and a disconnect from Evidence-Based Medicine (EBM). While Retrieval-Augmented Generation (RAG) offers a solution, current systems often rely on static workflows that miss",
    "url": "https://arxiv.org/abs/2508.15746",
    "source": "Arxiv AI"
  },
  {
    "title": "RDDM: Practicing RAW Domain Diffusion Model for Real-world Image Restoration",
    "summary": "arXiv:2508.19154v2 Announce Type: replace-cross Abstract: We present the RAW domain diffusion model (RDDM), an end-to-end diffusion model that restores photo-realistic images directly from the sensor RAW data. While recent sRGB-domain diffusion methods achieve impressive results, they are caught in a dilemma between high fidelity and image generati",
    "url": "https://arxiv.org/abs/2508.19154",
    "source": "Arxiv AI"
  },
  {
    "title": "FoMEMO: Towards Foundation Models for Expensive Multi-objective Optimization",
    "summary": "arXiv:2509.03244v2 Announce Type: replace-cross Abstract: Expensive multi-objective optimization is a prevalent and crucial concern in many real-world scenarios, where sample-efficiency is vital due to the limited evaluations to recover the true Pareto front for decision making. Existing works either involve rebuilding Gaussian process surrogates f",
    "url": "https://arxiv.org/abs/2509.03244",
    "source": "Arxiv AI"
  },
  {
    "title": "Incorporating AI incident reporting into telecommunications law and policy: Insights from India",
    "summary": "arXiv:2509.09508v2 Announce Type: replace-cross Abstract: The integration of artificial intelligence (AI) into telecommunications infrastructure introduces novel risks, such as algorithmic bias and unpredictable system behavior, that fall outside the scope of traditional cybersecurity and data protection frameworks. This paper introduces a precise ",
    "url": "https://arxiv.org/abs/2509.09508",
    "source": "Arxiv AI"
  },
  {
    "title": "Meta-Learning Reinforcement Learning for Crypto-Return Prediction",
    "summary": "arXiv:2509.09751v2 Announce Type: replace-cross Abstract: Predicting cryptocurrency returns is notoriously difficult: price movements are driven by a fast-shifting blend of on-chain activity, news flow, and social sentiment, while labeled training data are scarce and expensive. In this paper, we present Meta-RL-Crypto, a unified transformer-based a",
    "url": "https://arxiv.org/abs/2509.09751",
    "source": "Arxiv AI"
  },
  {
    "title": "Estimating Respiratory Effort from Nocturnal Breathing Sounds for Obstructive Sleep Apnoea Screening",
    "summary": "arXiv:2509.14944v2 Announce Type: replace-cross Abstract: Obstructive sleep apnoea (OSA) is a prevalent condition with significant health consequences, yet many patients remain undiagnosed due to the complexity and cost of over-night polysomnography. Acoustic-based screening provides a scalable alternative, yet performance is limited by environment",
    "url": "https://arxiv.org/abs/2509.14944",
    "source": "Arxiv AI"
  },
  {
    "title": "From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations",
    "summary": "arXiv:2509.16584v2 Announce Type: replace-cross Abstract: Large language models (LLMs) have demonstrated promising performance on medical benchmarks; however, their ability to perform medical calculations, a crucial aspect of clinical decision-making, remains underexplored and poorly evaluated. Existing benchmarks often assess only the final answer",
    "url": "https://arxiv.org/abs/2509.16584",
    "source": "Arxiv AI"
  },
  {
    "title": "StefaLand: An Efficient Geoscience Foundation Model That Improves Dynamic Land-Surface Predictions",
    "summary": "arXiv:2509.17942v3 Announce Type: replace-cross Abstract: Managing natural resources and mitigating risks from floods, droughts, wildfires, and landslides require models that can accurately predict climate-driven land-surface responses. Traditional models often struggle with spatial generalization because they are trained or calibrated on limited o",
    "url": "https://arxiv.org/abs/2509.17942",
    "source": "Arxiv AI"
  },
  {
    "title": "Intra-Cluster Mixup: An Effective Data Augmentation Technique for Complementary-Label Learning",
    "summary": "arXiv:2509.17971v2 Announce Type: replace-cross Abstract: In this paper, we investigate the challenges of complementary-label learning (CLL), a specialized form of weakly-supervised learning (WSL) where models are trained with labels indicating classes to which instances do not belong, rather than standard ordinary labels. This alternative supervis",
    "url": "https://arxiv.org/abs/2509.17971",
    "source": "Arxiv AI"
  },
  {
    "title": "Frictional Q-Learning",
    "summary": "arXiv:2509.19771v3 Announce Type: replace-cross Abstract: Off-policy reinforcement learning suffers from extrapolation errors when a learned policy selects actions that are weakly supported in the replay buffer. In this study, we address this issue by drawing an analogy to static friction in classical mechanics. From this perspective, the replay bu",
    "url": "https://arxiv.org/abs/2509.19771",
    "source": "Arxiv AI"
  },
  {
    "title": "Advancing Universal Deep Learning for Electronic-Structure Hamiltonian Prediction of Materials",
    "summary": "arXiv:2509.19877v3 Announce Type: replace-cross Abstract: Deep learning methods for electronic-structure Hamiltonian prediction has offered significant computational efficiency advantages over traditional DFT methods, yet the diversity of atomic types, structural patterns, and the high-dimensional complexity of Hamiltonians pose substantial challen",
    "url": "https://arxiv.org/abs/2509.19877",
    "source": "Arxiv AI"
  },
  {
    "title": "CHOIR: A Chatbot-mediated Organizational Memory Leveraging Communication in University Research Labs",
    "summary": "arXiv:2509.20512v3 Announce Type: replace-cross Abstract: University research labs often rely on chat-based platforms for communication and project management, where valuable knowledge surfaces but is easily lost in message streams. Documentation can preserve knowledge, but it requires ongoing maintenance and is challenging to navigate. Drawing on ",
    "url": "https://arxiv.org/abs/2509.20512",
    "source": "Arxiv AI"
  },
  {
    "title": "FS-DFM: Fast and Accurate Long Text Generation with Few-Step Diffusion Language Models",
    "summary": "arXiv:2509.20624v3 Announce Type: replace-cross Abstract: Autoregressive language models (ARMs) deliver strong likelihoods, but are inherently serial: they generate one token per forward pass, which limits throughput and inflates latency for long sequences. Diffusion Language Models (DLMs) parallelize across positions and thus appear promising for ",
    "url": "https://arxiv.org/abs/2509.20624",
    "source": "Arxiv AI"
  },
  {
    "title": "SupCLAP: Controlling Optimization Trajectory Drift in Audio-Text Contrastive Learning with Support Vector Regularization",
    "summary": "arXiv:2509.21033v2 Announce Type: replace-cross Abstract: Contrastive language-audio pretraining, which aims to unify multimodal representations in a shared embedding space, serves as a cornerstone for building a wide range of applications, from cross-modal retrieval to cutting-edge multimodal large language models. However, we find that the perpen",
    "url": "https://arxiv.org/abs/2509.21033",
    "source": "Arxiv AI"
  },
  {
    "title": "Quokka: Accelerating Program Verification with LLMs via Invariant Synthesis",
    "summary": "arXiv:2509.21629v2 Announce Type: replace-cross Abstract: Program verification relies on loop invariants, yet automatically discovering strong invariants remains a long-standing challenge. We investigate whether large language models (LLMs) can accelerate program verification by generating useful loop invariants. We introduce Quokka, a first-order ",
    "url": "https://arxiv.org/abs/2509.21629",
    "source": "Arxiv AI"
  },
  {
    "title": "ChaosNexus: A Foundation Model for ODE-based Chaotic System Forecasting with Hierarchical Multi-scale Awareness",
    "summary": "arXiv:2509.21802v2 Announce Type: replace-cross Abstract: Foundation models have shown great promise in achieving zero-shot or few-shot forecasting for ODE-based chaotic systems via large-scale pretraining. However, existing architectures often fail to capture the multi-scale temporal structures and distinct spectral characteristics of chaotic dyna",
    "url": "https://arxiv.org/abs/2509.21802",
    "source": "Arxiv AI"
  },
  {
    "title": "Reinforcement Learning for Durable Algorithmic Recourse",
    "summary": "arXiv:2509.22102v2 Announce Type: replace-cross Abstract: Algorithmic recourse seeks to provide individuals with actionable recommendations that increase their chances of receiving favorable outcomes from automated decision systems (e.g., loan approvals). While prior research has emphasized robustness to model updates, considerably less attention h",
    "url": "https://arxiv.org/abs/2509.22102",
    "source": "Arxiv AI"
  },
  {
    "title": "Mental Health Impacts of AI Companions: Triangulating Social Media Quasi-Experiments, User Perspectives, and Relational Theory",
    "summary": "arXiv:2509.22505v2 Announce Type: replace-cross Abstract: AI-powered companion chatbots (AICCs) such as Replika are increasingly popular, offering empathetic interactions, yet their psychosocial impacts remain unclear. We examined how engaging with AICCs shaped wellbeing and how users perceived these experiences. First, we conducted a large-scale q",
    "url": "https://arxiv.org/abs/2509.22505",
    "source": "Arxiv AI"
  },
  {
    "title": "Beyond Model Ranking: Predictability-Aligned Evaluation for Time Series Forecasting",
    "summary": "arXiv:2509.23074v2 Announce Type: replace-cross Abstract: In the era of increasingly complex AI models for time series forecasting, progress is often measured by marginal improvements on benchmark leaderboards. However, this approach suffers from a fundamental flaw: standard evaluation metrics conflate a model's performance with the data's intrinsi",
    "url": "https://arxiv.org/abs/2509.23074",
    "source": "Arxiv AI"
  },
  {
    "title": "Dense associative memory for Gaussian distributions",
    "summary": "arXiv:2509.23162v2 Announce Type: replace-cross Abstract: Dense associative memories (DAMs) store and retrieve patterns via energy-function based fixed points, but existing models are limited to vector representations. We extend DAMs to Gaussian densities equipped with the 2-Wasserstein distance. Our framework defines a log-sum-exp energy over stor",
    "url": "https://arxiv.org/abs/2509.23162",
    "source": "Arxiv AI"
  },
  {
    "title": "Revisiting Multivariate Time Series Forecasting with Missing Values",
    "summary": "arXiv:2509.23494v3 Announce Type: replace-cross Abstract: Missing values are common in real-world time series, and multivariate time series forecasting with missing values (MTSF-M) has become a crucial area of research for ensuring reliable predictions. To address the challenge of missing data, current approaches have developed an imputation-then-p",
    "url": "https://arxiv.org/abs/2509.23494",
    "source": "Arxiv AI"
  },
  {
    "title": "Putnam-like dataset summary: LLMs as mathematical competition contestants",
    "summary": "arXiv:2509.24827v3 Announce Type: replace-cross Abstract: In this paper we summarize the results of the Putnam-like benchmark published by Google DeepMind. This dataset consists of 96 original problems in the spirit of the Putnam Competition and 576 solutions generated by LLMs. We analyze the performance of models on this set of problems to verify ",
    "url": "https://arxiv.org/abs/2509.24827",
    "source": "Arxiv AI"
  },
  {
    "title": "GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs",
    "summary": "arXiv:2509.25178v3 Announce Type: replace-cross Abstract: Object hallucination in Multimodal Large Language Models (MLLMs) is a persistent failure mode that causes the model to perceive objects absent in the image. This weakness of MLLMs is currently studied using static benchmarks with fixed visual scenarios, which preempts the possibility of unco",
    "url": "https://arxiv.org/abs/2509.25178",
    "source": "Arxiv AI"
  },
  {
    "title": "STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents",
    "summary": "arXiv:2509.25624v2 Announce Type: replace-cross Abstract: As LLMs advance into autonomous agents with tool-use capabilities, they introduce security challenges that extend beyond traditional content-based LLM safety concerns. This paper introduces Sequential Tool Attack Chaining (STAC), a novel multi-turn attack framework that exploits agent tool u",
    "url": "https://arxiv.org/abs/2509.25624",
    "source": "Arxiv AI"
  },
  {
    "title": "Point2RBox-v3: Self-Bootstrapping from Point Annotations via Integrated Pseudo-Label Refinement and Utilization",
    "summary": "arXiv:2509.26281v3 Announce Type: replace-cross Abstract: Driven by the growing need for Oriented Object Detection (OOD), learning from point annotations under a weakly-supervised framework has emerged as a promising alternative to costly and laborious manual labeling. In this paper, we discuss two deficiencies in existing point-supervised methods:",
    "url": "https://arxiv.org/abs/2509.26281",
    "source": "Arxiv AI"
  },
  {
    "title": "Game-Time: Evaluating Temporal Dynamics in Spoken Language Models",
    "summary": "arXiv:2509.26388v2 Announce Type: replace-cross Abstract: Conversational Spoken Language Models (SLMs) are emerging as a promising paradigm for real-time speech interaction. However, their capacity of temporal dynamics, including the ability to manage timing, tempo and simultaneous speaking, remains a critical and unevaluated challenge for conversa",
    "url": "https://arxiv.org/abs/2509.26388",
    "source": "Arxiv AI"
  },
  {
    "title": "AI-Based Stroke Rehabilitation Domiciliary Assessment System with ST_GCN Attention",
    "summary": "arXiv:2510.00049v2 Announce Type: replace-cross Abstract: Effective stroke recovery requires continuous rehabilitation integrated with daily living. To support this need, we propose a home-based rehabilitation exercise and feedback system. The system consists of (1) hardware setup with RGB-D camera and wearable sensors to capture stroke movements, ",
    "url": "https://arxiv.org/abs/2510.00049",
    "source": "Arxiv AI"
  },
  {
    "title": "Planning-Augmented Sampling with Early Guidance for High-Reward Discovery",
    "summary": "arXiv:2510.00805v3 Announce Type: replace-cross Abstract: Generative Flow Networks (GFlowNets) enable structured generation with inherent diversity, but existing sampling strategies often rely on weak guided exploration, slowing early discovery of high-reward candidates. In tasks such as molecular design, rapid and consistent generation of high-rew",
    "url": "https://arxiv.org/abs/2510.00805",
    "source": "Arxiv AI"
  },
  {
    "title": "Do Bias Benchmarks Generalise? Evidence from Voice-based Evaluation of Gender Bias in SpeechLLMs",
    "summary": "arXiv:2510.01254v2 Announce Type: replace-cross Abstract: Recent work in benchmarking bias and fairness in speech large language models (SpeechLLMs) has relied heavily on multiple-choice question answering (MCQA) formats. The model is tasked to choose between stereotypical, anti-stereotypical, or neutral/irrelevant answers given an input speech pro",
    "url": "https://arxiv.org/abs/2510.01254",
    "source": "Arxiv AI"
  },
  {
    "title": "AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees",
    "summary": "arXiv:2510.01268v5 Announce Type: replace-cross Abstract: We study the problem of determining whether a piece of text has been authored by a human or by a large language model (LLM). Existing state of the art logits-based detectors make use of statistics derived from the log-probability of the observed text evaluated using the distribution function",
    "url": "https://arxiv.org/abs/2510.01268",
    "source": "Arxiv AI"
  },
  {
    "title": "LLM-Based Multi-Agent Blackboard System for Information Discovery in Data Science",
    "summary": "arXiv:2510.01285v2 Announce Type: replace-cross Abstract: Advances in large language models (LLMs) have created new opportunities in data science, but their deployment is often limited by the challenge of finding relevant data in large data lakes. Existing methods struggle with this: both single- and multi-agent systems are quickly overwhelmed by l",
    "url": "https://arxiv.org/abs/2510.01285",
    "source": "Arxiv AI"
  },
  {
    "title": "The Three Regimes of Offline-to-Online Reinforcement Learning",
    "summary": "arXiv:2510.01460v3 Announce Type: replace-cross Abstract: Offline-to-online reinforcement learning (RL) has emerged as a practical paradigm that leverages offline datasets for pretraining and online interactions for fine-tuning. However, its empirical behavior is highly inconsistent: design choices of online fine-tuning that work well in one settin",
    "url": "https://arxiv.org/abs/2510.01460",
    "source": "Arxiv AI"
  },
  {
    "title": "DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning",
    "summary": "arXiv:2510.02341v2 Announce Type: replace-cross Abstract: Real-world large language model deployments (e.g., conversational AI systems, code generation assistants) naturally generate abundant implicit user dissatisfaction (DSAT) signals, as users iterate toward better answers through refinements, corrections, and expressed preferences, while explic",
    "url": "https://arxiv.org/abs/2510.02341",
    "source": "Arxiv AI"
  },
  {
    "title": "F-scheduler: illuminating the free-lunch design space for fast sampling of diffusion models",
    "summary": "arXiv:2510.02390v3 Announce Type: replace-cross Abstract: Diffusion models are the state-of-the-art generative models for high-resolution images, but sampling from pretrained models is computationally expensive, motivating interest in fast sampling. Although Free-U Net is a training-free enhancement for improving image quality, we find it ineffecti",
    "url": "https://arxiv.org/abs/2510.02390",
    "source": "Arxiv AI"
  },
  {
    "title": "How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models",
    "summary": "arXiv:2510.02453v2 Announce Type: replace-cross Abstract: Frontier language models are deployed as black-box services, where model weights cannot be modified and customization is limited to prompting. We introduce Advisor Models, a method to train small open-weight models to generate dynamic, per-instance natural language advice that improves the c",
    "url": "https://arxiv.org/abs/2510.02453",
    "source": "Arxiv AI"
  },
  {
    "title": "Fusing Multi- and Hyperspectral Satellite Data for Harmful Algal Bloom Monitoring with Self-Supervised and Hierarchical Deep Learning",
    "summary": "arXiv:2510.02763v3 Announce Type: replace-cross Abstract: We present a self-supervised machine learning framework for detecting and mapping the severity and speciation of harmful algal blooms (HABs) using multi-sensor satellite data. By fusing reflectance data from operational polar-orbiting satellite-based instruments (VIIRS, MODIS, OLCI, and OCI)",
    "url": "https://arxiv.org/abs/2510.02763",
    "source": "Arxiv AI"
  },
  {
    "title": "Simple Policy Gradients for Reasoning with Diffusion Language Models",
    "summary": "arXiv:2510.04019v2 Announce Type: replace-cross Abstract: Diffusion large language models (dLLMs), which offer a promising alternative to traditional autoregressive LLMs, have recently shown strong results in pretraining. However, due to their lack of tractable sequence-level likelihoods, they have yet to benefit from modern LLM post-training techn",
    "url": "https://arxiv.org/abs/2510.04019",
    "source": "Arxiv AI"
  },
  {
    "title": "MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering",
    "summary": "arXiv:2510.04217v3 Announce Type: replace-cross Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable capabilities across vision-language tasks, yet their large-scale deployment raises pressing concerns about memorized private data, outdated knowledge, and harmful content. Existing unlearning approaches for MLLMs typically",
    "url": "https://arxiv.org/abs/2510.04217",
    "source": "Arxiv AI"
  },
  {
    "title": "PatternKV: Flattening KV Representation Expands Quantization Headroom",
    "summary": "arXiv:2510.05176v2 Announce Type: replace-cross Abstract: KV cache in autoregressive LLMs eliminates redundant recomputation but has emerged as the dominant memory and bandwidth bottleneck during inference, notably with long contexts and test-time scaling. KV quantization is a key lever for reducing cache cost, but accuracy drops sharply as the nat",
    "url": "https://arxiv.org/abs/2510.05176",
    "source": "Arxiv AI"
  },
  {
    "title": "HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection",
    "summary": "arXiv:2510.05609v2 Announce Type: replace-cross Abstract: Recent human-object interaction detection (HOID) methods highly require prior knowledge from vision-language models (VLMs) to enhance the interaction recognition capabilities. The training strategies and model architectures for connecting the knowledge from VLMs to the HOI instance represent",
    "url": "https://arxiv.org/abs/2510.05609",
    "source": "Arxiv AI"
  },
  {
    "title": "iPEAR: Iterative Pyramid Estimation with Attention and Residuals for Deformable Medical Image Registration",
    "summary": "arXiv:2510.07666v3 Announce Type: replace-cross Abstract: Existing pyramid registration networks may accumulate anatomical misalignments and lack an effective mechanism to dynamically determine the number of optimization iterations under varying deformation requirements across images, leading to degraded performance. To solve these limitations, we ",
    "url": "https://arxiv.org/abs/2510.07666",
    "source": "Arxiv AI"
  },
  {
    "title": "When Search Goes Wrong: Red-Teaming Web-Augmented Large Language Models",
    "summary": "arXiv:2510.09689v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have been augmented with web search to overcome the limitations of the static knowledge boundary by accessing up-to-date information from the open Internet. While this integration enhances model capability, it also introduces a distinct safety threat surface: the",
    "url": "https://arxiv.org/abs/2510.09689",
    "source": "Arxiv AI"
  },
  {
    "title": "Myopic Bayesian Decision Theory for Batch Active Learning with Partial Batch Label Sampling",
    "summary": "arXiv:2510.09877v2 Announce Type: replace-cross Abstract: Over the past couple of decades, many active learning acquisition functions have been proposed, leaving practitioners with an unclear choice of which to use. Bayesian Decision Theory (BDT) offers a universal principle to guide decision-making. In this work, we derive BDT for (Bayesian) activ",
    "url": "https://arxiv.org/abs/2510.09877",
    "source": "Arxiv AI"
  },
  {
    "title": "AnyBCQ: Hardware Efficient Flexible Binary-Coded Quantization for Multi-Precision LLMs",
    "summary": "arXiv:2510.10467v2 Announce Type: replace-cross Abstract: The deployment of large language models (LLMs) is increasingly constrained by memory and latency bottlenecks, motivating the need for quantization techniques that flexibly balance accuracy and efficiency. Recent work has introduced multi-precision models, which enable inference at multiple p",
    "url": "https://arxiv.org/abs/2510.10467",
    "source": "Arxiv AI"
  },
  {
    "title": "Enhancing LLM Reasoning via Non-Human-Like Reasoning Path Preference Optimization",
    "summary": "arXiv:2510.11104v2 Announce Type: replace-cross Abstract: Current approaches for strengthening LLM reasoning tend to introduce a training bias toward human-like reasoning trajectories. In step-wise preference optimization, in particular, dependence on human or higher-capacity model annotations for intermediate steps limits exploration of alternativ",
    "url": "https://arxiv.org/abs/2510.11104",
    "source": "Arxiv AI"
  },
  {
    "title": "Message Passing on the Edge: Towards Scalable and Expressive GNNs",
    "summary": "arXiv:2510.13615v2 Announce Type: replace-cross Abstract: Graph neural networks (GNNs) are widely used in graph learning and most architectures propagate information by passing messages between vertices. In this work, we shift our attention to GNNs that perform message passing on edges and introduce EB-1WL, an edge-based color-refinement test, and ",
    "url": "https://arxiv.org/abs/2510.13615",
    "source": "Arxiv AI"
  },
  {
    "title": "Entropy Meets Importance: A Unified Head Importance-Entropy Score for Stable and Efficient Transformer Pruning",
    "summary": "arXiv:2510.13832v2 Announce Type: replace-cross Abstract: Transformer-based models have achieved remarkable performance in NLP tasks. However, their structural characteristics-multiple layers and attention heads-introduce efficiency challenges in inference and deployment. To address these challenges, various pruning methods have recently been propo",
    "url": "https://arxiv.org/abs/2510.13832",
    "source": "Arxiv AI"
  },
  {
    "title": "BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation",
    "summary": "arXiv:2510.13853v2 Announce Type: replace-cross Abstract: Large language models (LLMs) have been successfully applied to many tasks, including text-to-SQL generation. However, much of this work has focused on publicly available datasets, such as Fiben, Spider, and Bird. Our earlier work showed that LLMs are much less effective in querying large pri",
    "url": "https://arxiv.org/abs/2510.13853",
    "source": "Arxiv AI"
  },
  {
    "title": "Context-Selective State Space Models: Feedback is All You Need",
    "summary": "arXiv:2510.14027v2 Announce Type: replace-cross Abstract: Transformers, powered by the attention mechanism, are the backbone of most foundation models, yet they suffer from quadratic complexity and difficulties in dealing with long-range dependencies in the input sequence. Recent work has shown that state space models (SSMs) provide a promising alt",
    "url": "https://arxiv.org/abs/2510.14027",
    "source": "Arxiv AI"
  },
  {
    "title": "Model-agnostic Selective Labeling with Provable Statistical Guarantees",
    "summary": "arXiv:2510.14581v2 Announce Type: replace-cross Abstract: Obtaining high-quality labels for large datasets is expensive, requiring massive annotations from human experts. While AI models offer a cost-effective alternative by predicting labels, their label quality is compromised by the unavoidable labeling errors. Existing methods mitigate this issu",
    "url": "https://arxiv.org/abs/2510.14581",
    "source": "Arxiv AI"
  },
  {
    "title": "LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems",
    "summary": "arXiv:2510.15969v2 Announce Type: replace-cross Abstract: Reformulating nonlinear optimization problems into solver-ready linear optimization problems is often necessary for practical applications, but the process is often manual and requires domain expertise. We propose LinearizeLLM, an agent-based LLM framework that produces solver-ready linear r",
    "url": "https://arxiv.org/abs/2510.15969",
    "source": "Arxiv AI"
  },
  {
    "title": "Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning",
    "summary": "arXiv:2510.16882v3 Announce Type: replace-cross Abstract: Supervised fine-tuning (SFT) is a commonly used technique to adapt large language models (LLMs) to downstream tasks. In practice, SFT on a full dataset is computationally expensive and sometimes suffers from overfitting or bias amplification. This facilitates the rise of data curation in SFT",
    "url": "https://arxiv.org/abs/2510.16882",
    "source": "Arxiv AI"
  },
  {
    "title": "Can Transformer Memory Be Corrupted? Investigating Cache-Side Vulnerabilities in Large Language Models",
    "summary": "arXiv:2510.17098v2 Announce Type: replace-cross Abstract: Even when prompts and parameters are secured, transformer language models remain vulnerable because their key-value (KV) cache during inference constitutes an overlooked attack surface. This paper introduces Malicious Token Injection (MTI), a modular framework that systematically perturbs ca",
    "url": "https://arxiv.org/abs/2510.17098",
    "source": "Arxiv AI"
  },
  {
    "title": "Are Large Language Models Sensitive to the Motives Behind Communication?",
    "summary": "arXiv:2510.19687v2 Announce Type: replace-cross Abstract: Human communication is motivated: people speak, write, and create content with a particular communicative intent in mind. As a result, information that large language models (LLMs) and AI agents process is inherently framed by humans' intentions and incentives. People are adept at navigating",
    "url": "https://arxiv.org/abs/2510.19687",
    "source": "Arxiv AI"
  },
  {
    "title": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention",
    "summary": "arXiv:2510.19875v2 Announce Type: replace-cross Abstract: As Large Language Models (LLMs) scale to million-token contexts, traditional Mechanistic Interpretability techniques for analyzing attention scale quadratically with context length, demanding terabytes of memory beyond 100,000 tokens. We introduce Sparse Tracing, a novel technique that lever",
    "url": "https://arxiv.org/abs/2510.19875",
    "source": "Arxiv AI"
  },
  {
    "title": "Human-Inspired Neuro-Symbolic World Modeling and Logic Reasoning for Interpretable Safe UAV Landing Site Assessment",
    "summary": "arXiv:2510.22204v2 Announce Type: replace-cross Abstract: Reliable assessment of safe landing sites in unstructured environments is essential for deploying Unmanned Aerial Vehicles (UAVs) in real-world applications such as delivery, inspection, and surveillance. Existing learning-based approaches often degrade under covariate shift and offer limite",
    "url": "https://arxiv.org/abs/2510.22204",
    "source": "Arxiv AI"
  },
  {
    "title": "VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations",
    "summary": "arXiv:2510.22373v2 Announce Type: replace-cross Abstract: Visualization, a domain-specific yet widely used form of imagery, is an effective way to turn complex datasets into intuitive insights, and its value depends on whether data are faithfully represented, clearly communicated, and aesthetically designed. However, evaluating visualization qualit",
    "url": "https://arxiv.org/abs/2510.22373",
    "source": "Arxiv AI"
  },
  {
    "title": "A Survey on Efficient Vision-Language-Action Models",
    "summary": "arXiv:2510.24795v2 Announce Type: replace-cross Abstract: Vision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. Despite their remarkable performance, foundational VLAs are hindered by the prohibitive computational and data demands inherent ",
    "url": "https://arxiv.org/abs/2510.24795",
    "source": "Arxiv AI"
  },
  {
    "title": "BOLT-GAN: Bayes-Error-Motivated Objective for Stable GAN Training",
    "summary": "arXiv:2510.25609v2 Announce Type: replace-cross Abstract: We introduce BOLT-GAN, a novel framework for stable GAN training using the Bayes optimal learning threshold (BOLT). The discriminator is trained via the BOLT loss under a standard 1-Lipschitz constraint. This guides the generator to maximize the Bayes error of the discrimination task. We sho",
    "url": "https://arxiv.org/abs/2510.25609",
    "source": "Arxiv AI"
  },
  {
    "title": "How Similar Are Grokipedia and Wikipedia? A Multi-Dimensional Textual and Structural Comparison",
    "summary": "arXiv:2510.26899v4 Announce Type: replace-cross Abstract: The launch of Grokipedia, an AI-generated encyclopedia developed by Elon Musk's xAI, was presented as a response to perceived ideological and structural biases in Wikipedia, aiming to produce \"truthful\" entries using the Grok large language model. Yet whether an AI-driven alternative can esc",
    "url": "https://arxiv.org/abs/2510.26899",
    "source": "Arxiv AI"
  },
  {
    "title": "A Proof of Learning Rate Transfer under $\\mu$P",
    "summary": "arXiv:2511.01734v2 Announce Type: replace-cross Abstract: We provide the first proof of learning rate transfer with width in a linear multi-layer perceptron (MLP) parametrized with $\\mu$P, a neural network parameterization designed to ``maximize'' feature learning in the infinite-width limit. We show that under $\\mu P$, the optimal learning rate co",
    "url": "https://arxiv.org/abs/2511.01734",
    "source": "Arxiv AI"
  },
  {
    "title": "Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks",
    "summary": "arXiv:2511.04689v2 Announce Type: replace-cross Abstract: Evaluating large language models (LLMs) typically requires thousands of benchmark items, making the process expensive, slow, and increasingly impractical at scale. Existing evaluation protocols rely on average accuracy over fixed item sets, treating all items as equally informative despite s",
    "url": "https://arxiv.org/abs/2511.04689",
    "source": "Arxiv AI"
  },
  {
    "title": "Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets",
    "summary": "arXiv:2511.06356v3 Announce Type: replace-cross Abstract: Chemical reaction prediction remains a fundamental challenge in organic chemistry, where existing machine learning models face two critical limitations: sensitivity to input permutations (molecule/atom orderings) and inadequate modeling of substructural interactions governing reactivity. The",
    "url": "https://arxiv.org/abs/2511.06356",
    "source": "Arxiv AI"
  },
  {
    "title": "TabRAG: Improving Tabular Document Question Answering for Retrieval Augmented Generation via Structured Representations",
    "summary": "arXiv:2511.06582v2 Announce Type: replace-cross Abstract: Incorporating external knowledge bases in traditional retrieval-augmented generation (RAG) relies on parsing the document, followed by querying a language model with the parsed information via in-context learning. While effective for text-based documents, question answering on tabular docume",
    "url": "https://arxiv.org/abs/2511.06582",
    "source": "Arxiv AI"
  },
  {
    "title": "Multi-period Learning for Financial Time Series Forecasting",
    "summary": "arXiv:2511.08622v2 Announce Type: replace-cross Abstract: Time series forecasting is important in finance domain. Financial time series (TS) patterns are influenced by both short-term public opinions and medium-/long-term policy and market trends. Hence, processing multi-period inputs becomes crucial for accurate financial time series forecasting (",
    "url": "https://arxiv.org/abs/2511.08622",
    "source": "Arxiv AI"
  },
  {
    "title": "Introduction to Automated Negotiation",
    "summary": "arXiv:2511.08659v2 Announce Type: replace-cross Abstract: This book is an introductory textbook targeted towards computer science students who are completely new to the topic of automated negotiation. It does not require any prerequisite knowledge, except for elementary mathematics and basic programming skills. This book comes with an simple toy-wo",
    "url": "https://arxiv.org/abs/2511.08659",
    "source": "Arxiv AI"
  },
  {
    "title": "Beyond Cosine Similarity: Magnitude-Aware CLIP for No-Reference Image Quality Assessment",
    "summary": "arXiv:2511.09948v3 Announce Type: replace-cross Abstract: Recent efforts have repurposed the Contrastive Language-Image Pre-training (CLIP) model for No-Reference Image Quality Assessment (NR-IQA) by measuring the cosine similarity between the image embedding and textual prompts such as \"a good photo\" or \"a bad photo.\" However, this semantic simila",
    "url": "https://arxiv.org/abs/2511.09948",
    "source": "Arxiv AI"
  },
  {
    "title": "ImAgent: A Unified Multimodal Agent Framework for Test-Time Scalable Image Generation",
    "summary": "arXiv:2511.11483v3 Announce Type: replace-cross Abstract: Recent text-to-image (T2I) models have made remarkable progress in generating visually realistic and semantically coherent images. However, they still suffer from randomness and inconsistency with the given prompts, particularly when textual descriptions are vague or underspecified. Existing",
    "url": "https://arxiv.org/abs/2511.11483",
    "source": "Arxiv AI"
  },
  {
    "title": "Global Feature Enhancing and Fusion Framework for Strain Gauge Time Series Classification",
    "summary": "arXiv:2511.11629v2 Announce Type: replace-cross Abstract: Strain Gauge Status (SGS) time series recognition is crucial in the field of intelligent manufacturing based on the Internet of Things, as accurate identification helps timely detection of failed mechanical components, avoiding accidents. The loading and unloading sequences generated by stra",
    "url": "https://arxiv.org/abs/2511.11629",
    "source": "Arxiv AI"
  },
  {
    "title": "Physics-Based Benchmarking Metrics for Multimodal Synthetic Images",
    "summary": "arXiv:2511.15204v2 Announce Type: replace-cross Abstract: Current state of the art measures like BLEU, CIDEr, VQA score, SigLIP-2 and CLIPScore are often unable to capture semantic or structural accuracy, especially for domain-specific or context-dependent scenarios. For this, this paper proposes a Physics-Constrained Multimodal Data Evaluation (PC",
    "url": "https://arxiv.org/abs/2511.15204",
    "source": "Arxiv AI"
  },
  {
    "title": "EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control",
    "summary": "arXiv:2511.15248v2 Announce Type: replace-cross Abstract: Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, ",
    "url": "https://arxiv.org/abs/2511.15248",
    "source": "Arxiv AI"
  },
  {
    "title": "GEO-Bench-2: From Performance to Capability, Rethinking Evaluation in Geospatial AI",
    "summary": "arXiv:2511.15658v2 Announce Type: replace-cross Abstract: Geospatial Foundation Models (GeoFMs) are transforming Earth Observation (EO), but evaluation lacks standardized protocols. GEO-Bench-2 addresses this with a comprehensive framework spanning classification, segmentation, regression, object detection, and instance segmentation across 19 permi",
    "url": "https://arxiv.org/abs/2511.15658",
    "source": "Arxiv AI"
  },
  {
    "title": "Your Latent Reasoning is Secretly Policy Improvement Operator",
    "summary": "arXiv:2511.16886v3 Announce Type: replace-cross Abstract: Recently, small models with latent recursion have obtained promising results on complex reasoning tasks. These results are typically explained by the theory that such recursion increases a networks depth, allowing it to compactly emulate the capacity of larger models. However, the performanc",
    "url": "https://arxiv.org/abs/2511.16886",
    "source": "Arxiv AI"
  },
  {
    "title": "Geometric-disentangelment Unlearning",
    "summary": "arXiv:2511.17100v4 Announce Type: replace-cross Abstract: Large language models (LLMs) can internalize private or harmful content, motivating unlearning that removes a forget set while preserving retaining knowledge. However, forgetting updates often cause collateral degradation on retaining knowledge, creating a persistent trade-off. Existing LLM ",
    "url": "https://arxiv.org/abs/2511.17100",
    "source": "Arxiv AI"
  },
  {
    "title": "Decomposed Trust: Exploring Privacy, Adversarial Robustness, Fairness, and Ethics of Low-Rank LLMs",
    "summary": "arXiv:2511.22099v2 Announce Type: replace-cross Abstract: Large language models (LLMs) have driven major advances across domains, yet their massive size hinders deployment in resource-constrained settings. Model compression addresses this challenge, with low-rank factorization emerging as a particularly effective method for reducing size, memory, a",
    "url": "https://arxiv.org/abs/2511.22099",
    "source": "Arxiv AI"
  },
  {
    "title": "MAS-Shield: A Defense Framework for Secure and Efficient LLM MAS",
    "summary": "arXiv:2511.22924v2 Announce Type: replace-cross Abstract: Large Language Model (LLM)-based Multi-Agent Systems (MAS) are susceptible to linguistic attacks that can trigger cascading failures across the network. Existing defenses face a fundamental dilemma: lightweight single-auditor methods are prone to single points of failure, while robust commit",
    "url": "https://arxiv.org/abs/2511.22924",
    "source": "Arxiv AI"
  },
  {
    "title": "On The Finetuning of MLIPs Through the Lens of Iterated Maps With BPTT",
    "summary": "arXiv:2512.01067v2 Announce Type: replace-cross Abstract: Accurate structural relaxation is critical for advanced materials design. Traditional approaches built on physics-derived first-principles calculations are computationally expensive, motivating the creation of machine-learning interatomic potentials (MLIPs), which strive to faithfully reprod",
    "url": "https://arxiv.org/abs/2512.01067",
    "source": "Arxiv AI"
  },
  {
    "title": "SocialFusion: Addressing Social Degradation in Pre-trained Vision-Language Models",
    "summary": "arXiv:2512.01148v2 Announce Type: replace-cross Abstract: Understanding social interactions from visual cues is a fundamental challenge for a socially competent AI. While powerful pre-trained vision-language models (VLMs) have shown remarkable general capabilities, they surprisingly struggle to unify and learn multiple social perception tasks simul",
    "url": "https://arxiv.org/abs/2512.01148",
    "source": "Arxiv AI"
  },
  {
    "title": "Language as a Wave Phenomenon: Semantic Phase Locking and Interference in Neural Networks",
    "summary": "arXiv:2512.01208v3 Announce Type: replace-cross Abstract: In standard Transformer architectures, semantic importance is often conflated with activation magnitude, obscuring the geometric structure of latent representations. To disentangle these factors, we introduce PRISM, a complex-valued architecture designed to isolate the computational role of ",
    "url": "https://arxiv.org/abs/2512.01208",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems",
    "summary": "arXiv:2512.01661v3 Announce Type: replace-cross Abstract: Ensuring large language model (LLM) reliability requires distinguishing objective unsolvability (inherent contradictions) from subjective capability limitations (tasks exceeding model competence). Current LLMs often conflate these dimensions, leading to hallucinations in which they return co",
    "url": "https://arxiv.org/abs/2512.01661",
    "source": "Arxiv AI"
  },
  {
    "title": "Agentic Policy Optimization via Instruction-Policy Co-Evolution",
    "summary": "arXiv:2512.01945v2 Announce Type: replace-cross Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-integrated reasoning. While instructions serve as the primary protocol for defining agents, R",
    "url": "https://arxiv.org/abs/2512.01945",
    "source": "Arxiv AI"
  },
  {
    "title": "Measuring Agents in Production",
    "summary": "arXiv:2512.04123v3 Announce Type: replace-cross Abstract: LLM-based agents already operate in production across many industries, yet we lack an understanding of what technical methods make deployments successful. We present the first systematic study of Measuring Agents in Production, MAP, using first-hand data from agent developers. We conducted 2",
    "url": "https://arxiv.org/abs/2512.04123",
    "source": "Arxiv AI"
  },
  {
    "title": "GuidNoise: Single-Pair Guided Diffusion for Generalized Noise Synthesis",
    "summary": "arXiv:2512.04456v2 Announce Type: replace-cross Abstract: Recent image denoising methods have leveraged generative modeling for real noise synthesis to address the costly acquisition of real-world noisy data. However, these generative models typically require camera metadata and extensive target-specific noisy-clean image pairs, often showing limit",
    "url": "https://arxiv.org/abs/2512.04456",
    "source": "Arxiv AI"
  },
  {
    "title": "Model-Free Assessment of Simulator Fidelity via Quantile Curves",
    "summary": "arXiv:2512.05024v2 Announce Type: replace-cross Abstract: As generative AI models are increasingly used to simulate real-world systems, quantifying the ``sim-to-real'' gap is critical. The distributional discrepancy between real and simulated outputs is a random variable driven by the stochastic input scenario. A fundamental challenge is that for a",
    "url": "https://arxiv.org/abs/2512.05024",
    "source": "Arxiv AI"
  },
  {
    "title": "When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models",
    "summary": "arXiv:2512.06343v2 Announce Type: replace-cross Abstract: Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of chosen and rejected responses. In this work, we analyze the per-sample ",
    "url": "https://arxiv.org/abs/2512.06343",
    "source": "Arxiv AI"
  },
  {
    "title": "When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks",
    "summary": "arXiv:2512.07684v2 Announce Type: replace-cross Abstract: Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches o",
    "url": "https://arxiv.org/abs/2512.07684",
    "source": "Arxiv AI"
  },
  {
    "title": "MixLM: High-Throughput and Effective LLM Ranking via Text-Embedding Mix-Interaction",
    "summary": "arXiv:2512.07846v2 Announce Type: replace-cross Abstract: Large language models (LLMs) excel at capturing semantic nuances and therefore show impressive relevance ranking performance in modern recommendation and search systems. However, they suffer from high computational overhead under industrial latency and throughput requirements. In particular,",
    "url": "https://arxiv.org/abs/2512.07846",
    "source": "Arxiv AI"
  },
  {
    "title": "SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale",
    "summary": "arXiv:2512.10922v2 Announce Type: replace-cross Abstract: The resource requirements of neural networks can be significantly reduced through pruning - the removal of seemingly less important parameters. However, for LLMs, full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as magnitud",
    "url": "https://arxiv.org/abs/2512.10922",
    "source": "Arxiv AI"
  },
  {
    "title": "WebSTAR: Scalable Data Synthesis for Computer Use Agents with Step-Level Filtering",
    "summary": "arXiv:2512.10962v2 Announce Type: replace-cross Abstract: Computer use agents (CUAs) can operate real-world digital interfaces but remain difficult to train due to the high cost of graphical user interface (GUI) interaction and the scarcity of high-quality trajectory data. Existing datasets rely on human demonstrations, limiting scalability. A natu",
    "url": "https://arxiv.org/abs/2512.10962",
    "source": "Arxiv AI"
  },
  {
    "title": "Robust MLLM Unlearning via Visual Knowledge Distillation",
    "summary": "arXiv:2512.11325v2 Announce Type: replace-cross Abstract: Recently, machine unlearning approaches have been proposed to remove sensitive information from well-trained large models. However, most existing methods are tailored for LLMs, while MLLM-oriented unlearning remains at its early stage. Inspired by recent studies exploring the internal mechan",
    "url": "https://arxiv.org/abs/2512.11325",
    "source": "Arxiv AI"
  },
  {
    "title": "Position: Universal Aesthetic Alignment Narrows Artistic Expression",
    "summary": "arXiv:2512.11883v2 Announce Type: replace-cross Abstract: Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when \"anti-aesthetic\" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic p",
    "url": "https://arxiv.org/abs/2512.11883",
    "source": "Arxiv AI"
  },
  {
    "title": "Diffusion Language Model Inference with Monte Carlo Tree Search",
    "summary": "arXiv:2512.12168v2 Announce Type: replace-cross Abstract: Diffusion language models (DLMs) have recently emerged as a compelling alternative to autoregressive generation, offering parallel generation and improved global coherence. During inference, DLMs generate text by iteratively denoising masked sequences in parallel; however, determining which ",
    "url": "https://arxiv.org/abs/2512.12168",
    "source": "Arxiv AI"
  },
  {
    "title": "PIS: A Generalized Physical Inversion Solver for Arbitrary Sparse Observations via Set Conditioned Flow Matching",
    "summary": "arXiv:2512.13732v2 Announce Type: replace-cross Abstract: The estimation of high-dimensional physical parameters constrained by partial differential equations (PDEs) from limited and indirect measurements is a highly ill-posed problem. Traditional methods face significant accuracy and efficiency bottlenecks, particularly when observations are spars",
    "url": "https://arxiv.org/abs/2512.13732",
    "source": "Arxiv AI"
  },
  {
    "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
    "summary": "arXiv:2512.19673v2 Announce Type: replace-cross Abstract: Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a unified policy, overlooking their internal mechanisms. In this paper, we decompose the LLM-based policy into Internal Layer Policies and Internal Modular Policies via Transformer's residual stream. Our en",
    "url": "https://arxiv.org/abs/2512.19673",
    "source": "Arxiv AI"
  },
  {
    "title": "Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieval",
    "summary": "arXiv:2512.20042v2 Announce Type: replace-cross Abstract: Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital",
    "url": "https://arxiv.org/abs/2512.20042",
    "source": "Arxiv AI"
  },
  {
    "title": "Performative Policy Gradient: Optimality in Performative Reinforcement Learning",
    "summary": "arXiv:2512.20576v2 Announce Type: replace-cross Abstract: Post-deployment machine learning algorithms often influence the environments they act in, and thus shift the underlying dynamics that the standard reinforcement learning (RL) methods ignore. While designing optimal algorithms in this performative setting has recently been studied in supervis",
    "url": "https://arxiv.org/abs/2512.20576",
    "source": "Arxiv AI"
  },
  {
    "title": "Flexible Multitask Learning with Factorized Diffusion Policy",
    "summary": "arXiv:2512.21898v2 Announce Type: replace-cross Abstract: Multitask learning poses significant challenges due to the highly multimodal and diverse nature of robot action distributions. However, effectively fitting policies to these complex task distributions is often difficult, and existing monolithic models often underfit the action distribution a",
    "url": "https://arxiv.org/abs/2512.21898",
    "source": "Arxiv AI"
  },
  {
    "title": "Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model",
    "summary": "arXiv:2512.21917v2 Announce Type: replace-cross Abstract: Aligning large language models (LLMs) to preference data typically assumes a known link function between observed preferences and latent rewards (e.g., a logistic Bradley-Terry link). Misspecification of this link can bias inferred rewards and misalign learned policies. We study preference a",
    "url": "https://arxiv.org/abs/2512.21917",
    "source": "Arxiv AI"
  },
  {
    "title": "Unifying Learning Dynamics and Generalization in Transformers Scaling Law",
    "summary": "arXiv:2512.22088v2 Announce Type: replace-cross Abstract: The scaling law, a cornerstone of Large Language Model (LLM) development, predicts improvements in model performance with increasing computational resources. Yet, while empirically validated, its theoretical underpinnings remain poorly understood. This work formalizes the learning dynamics o",
    "url": "https://arxiv.org/abs/2512.22088",
    "source": "Arxiv AI"
  },
  {
    "title": "EvoXplain: When Machine Learning Models Agree on Predictions but Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs",
    "summary": "arXiv:2512.22240v3 Announce Type: replace-cross Abstract: Machine learning models are primarily judged by predictive performance, especially in applied settings. Once a model reaches high accuracy, its explanation is often assumed to be correct and trustworthy. This assumption raises an overlooked question: when two models achieve high accuracy, do",
    "url": "https://arxiv.org/abs/2512.22240",
    "source": "Arxiv AI"
  },
  {
    "title": "High-Dimensional Search, Low-Dimensional Solution: Decoupling Optimization from Representation",
    "summary": "arXiv:2512.23410v2 Announce Type: replace-cross Abstract: State-of-the-art models rely on massive widths despite exhibiting low Intrinsic Dimension (ID). We posit that this redundancy serves the non-convex optimization search rather than the final representation. We validate this hypothesis by decoupling the solution geometry via data-independent r",
    "url": "https://arxiv.org/abs/2512.23410",
    "source": "Arxiv AI"
  },
  {
    "title": "A Survey of AI Methods for Geometry Preparation and Mesh Generation in Engineering Simulation",
    "summary": "arXiv:2512.23719v2 Announce Type: replace-cross Abstract: Artificial intelligence is beginning to reduce the manual effort in the CAD-to-mesh pipeline. Written for meshing and geometry practitioners with limited AI background, this survey organizes recent work by workflow step. We cover part classification and segmentation, mesh quality prediction,",
    "url": "https://arxiv.org/abs/2512.23719",
    "source": "Arxiv AI"
  },
  {
    "title": "A Community-Aware Framework for Influence Maximization with Explicit Accounting for Inter-Community Influence",
    "summary": "arXiv:2512.23973v2 Announce Type: replace-cross Abstract: Influence Maximization (IM) seeks to identify a small set of seed nodes in a social network to maximize expected information spread under a diffusion model. While community-based approaches improve scalability by exploiting modular structure, they typically assume independence between commun",
    "url": "https://arxiv.org/abs/2512.23973",
    "source": "Arxiv AI"
  },
  {
    "title": "Generative Video Compression: Towards 0.01% Compression Rate for Video Transmission",
    "summary": "arXiv:2512.24300v2 Announce Type: replace-cross Abstract: Whether a video can be compressed at an extreme compression rate as low as 0.01%? To this end, we achieve the compression rate as 0.02% at some cases by introducing Generative Video Compression (GVC), a new framework that redefines the limits of video compression by leveraging modern generat",
    "url": "https://arxiv.org/abs/2512.24300",
    "source": "Arxiv AI"
  },
  {
    "title": "FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering",
    "summary": "arXiv:2601.00269v3 Announce Type: replace-cross Abstract: Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on",
    "url": "https://arxiv.org/abs/2601.00269",
    "source": "Arxiv AI"
  },
  {
    "title": "Probability-Aware Parking Selection",
    "summary": "arXiv:2601.00521v2 Announce Type: replace-cross Abstract: Current navigation systems conflate time-to-drive with the true time-to-arrive by ignoring parking search duration and the final walking leg. Such underestimation can significantly affect user experience, mode choice, congestion, and emissions. To address this issue, this paper introduces th",
    "url": "https://arxiv.org/abs/2601.00521",
    "source": "Arxiv AI"
  },
  {
    "title": "Correctness isnt Efficiency: Runtime Memory Divergence in LLM-Generated Code",
    "summary": "arXiv:2601.01215v2 Announce Type: replace-cross Abstract: Large language models (LLMs) can generate programs that pass unit tests, but passing tests does not guarantee reliable runtime behavior. We find that different correct solutions to the same task can show very different memory and performance patterns, which can lead to hidden operational ris",
    "url": "https://arxiv.org/abs/2601.01215",
    "source": "Arxiv AI"
  },
  {
    "title": "Reliable Grid Forecasting: State Space Models for Safety-Critical Energy Systems",
    "summary": "arXiv:2601.01410v5 Announce Type: replace-cross Abstract: Accurate grid load forecasting is safety-critical: under-predictions risk supply shortfalls, while symmetric error metrics can mask this operational asymmetry. We introduce an operator-legible evaluation framework -- Under-Prediction Rate (UPR), tail Reserve$_{99.5}^{\\%}$ requirements, and e",
    "url": "https://arxiv.org/abs/2601.01410",
    "source": "Arxiv AI"
  },
  {
    "title": "MHRC-Bench: A Multilingual Hardware Repository-Level Code Completion benchmark",
    "summary": "arXiv:2601.03708v2 Announce Type: replace-cross Abstract: Large language models (LLMs) have achieved strong performance on code completion tasks in general-purpose programming languages. However, existing repository-level code completion benchmarks focus almost exclusively on software code and largely overlook hardware description languages. In thi",
    "url": "https://arxiv.org/abs/2601.03708",
    "source": "Arxiv AI"
  },
  {
    "title": "Clinical Data Goes MEDS? Let's OWL make sense of it",
    "summary": "arXiv:2601.04164v2 Announce Type: replace-cross Abstract: The application of machine learning on healthcare data is often hindered by the lack of standardized and semantically explicit representation, leading to limited interoperability and reproducibility across datasets and experiments. The Medical Event Data Standard (MEDS) addresses these issue",
    "url": "https://arxiv.org/abs/2601.04164",
    "source": "Arxiv AI"
  },
  {
    "title": "UNIC: Learning Unified Multimodal Extrinsic Contact Estimation",
    "summary": "arXiv:2601.04356v2 Announce Type: replace-cross Abstract: Contact-rich manipulation requires reliable estimation of extrinsic contacts-the interactions between a grasped object and its environment which provide essential contextual information for planning, control, and policy learning. However, existing approaches often rely on restrictive assumpt",
    "url": "https://arxiv.org/abs/2601.04356",
    "source": "Arxiv AI"
  },
  {
    "title": "Decoding Workload and Agreement From EEG During Spoken Dialogue With Conversational AI",
    "summary": "arXiv:2601.05825v2 Announce Type: replace-cross Abstract: Passive brain-computer interfaces offer a potential source of implicit feedback for alignment of large language models, but most mental state decoding has been done in controlled tasks. This paper investigates whether established EEG classifiers for mental workload and implicit agreement can",
    "url": "https://arxiv.org/abs/2601.05825",
    "source": "Arxiv AI"
  },
  {
    "title": "FastSLM: Hierarchical Frame Q-Former for Effective Speech Modality Adaptation",
    "summary": "arXiv:2601.06199v2 Announce Type: replace-cross Abstract: Although Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision, language, and video understanding tasks, scaling them to long-form speech remains a critical bottleneck due to the explosive growth of input tokens. Existing speech-language models typicall",
    "url": "https://arxiv.org/abs/2601.06199",
    "source": "Arxiv AI"
  },
  {
    "title": "DYCP: Dynamic Context Pruning for Long-Form Dialogue with LLMs",
    "summary": "arXiv:2601.07994v3 Announce Type: replace-cross Abstract: Large Language Models (LLMs) increasingly operate over long-form dialogues with frequent topic shifts. While recent LLMs support extended context windows, efficient management of dialogue history in practice is needed due to inference cost and latency constraints. We present DyCP, a lightwei",
    "url": "https://arxiv.org/abs/2601.07994",
    "source": "Arxiv AI"
  },
  {
    "title": "TP-Blend: Textual-Prompt Attention Pairing for Precise Object-Style Blending in Diffusion Models",
    "summary": "arXiv:2601.08011v3 Announce Type: replace-cross Abstract: Current text-conditioned diffusion editors handle single object replacement well but struggle when a new object and a new style must be introduced simultaneously. We present Twin-Prompt Attention Blend (TP-Blend), a lightweight training-free framework that receives two separate textual promp",
    "url": "https://arxiv.org/abs/2601.08011",
    "source": "Arxiv AI"
  },
  {
    "title": "A.X K1 Technical Report",
    "summary": "arXiv:2601.09200v3 Announce Type: replace-cross Abstract: We introduce A.X K1, a 519B-parameter Mixture-of-Experts (MoE) language model trained from scratch. Our design leverages scaling laws to optimize training configurations and vocabulary size under fixed computational budgets. A.X K1 is pre-trained on a corpus of approximately 10T tokens, cura",
    "url": "https://arxiv.org/abs/2601.09200",
    "source": "Arxiv AI"
  },
  {
    "title": "SimMerge: Learning to Select Merge Operators from Similarity Signals",
    "summary": "arXiv:2601.09473v2 Announce Type: replace-cross Abstract: Model merging combines multiple models into a single model with aggregated capabilities, making it a powerful tool for large language model (LLM) development. However, scaling model merging is challenging: performance depends on the choice of merge operator, model subset, and merge order, of",
    "url": "https://arxiv.org/abs/2601.09473",
    "source": "Arxiv AI"
  },
  {
    "title": "Aesthetics as Structural Harm: Algorithmic Lookism Across Text-to-Image Generation and Classification",
    "summary": "arXiv:2601.11651v2 Announce Type: replace-cross Abstract: This paper examines algorithmic lookism-the systematic preferential treatment based on physical appearance-in text-to-image (T2I) generative AI and a downstream gender classification task. Through the analysis of 26,400 synthetic faces created with Stable Diffusion 2.1 and 3.5 Medium, we dem",
    "url": "https://arxiv.org/abs/2601.11651",
    "source": "Arxiv AI"
  },
  {
    "title": "Mixed Precision PointPillars for Efficient 3D Object Detection with TensorRT",
    "summary": "arXiv:2601.12638v2 Announce Type: replace-cross Abstract: LIDAR 3D object detection is one of the important tasks for autonomous vehicles. Ensuring that this task operates in real-time is crucial. Toward this, model quantization can be used to accelerate the runtime. However, directly applying model quantization often leads to performance degradati",
    "url": "https://arxiv.org/abs/2601.12638",
    "source": "Arxiv AI"
  },
  {
    "title": "AI-generated data contamination erodes pathological variability and diagnostic reliability",
    "summary": "arXiv:2601.12946v4 Announce Type: replace-cross Abstract: Generative artificial intelligence (AI) is rapidly populating medical records with synthetic content, creating a feedback loop where future models are increasingly at risk of training on uncurated AI-generated data. However, the clinical consequences of this AI-generated data contamination r",
    "url": "https://arxiv.org/abs/2601.12946",
    "source": "Arxiv AI"
  },
  {
    "title": "A Learnable Wavelet Transformer for Long-Short Equity Trading and Risk-Adjusted Return Optimization",
    "summary": "arXiv:2601.13435v2 Announce Type: replace-cross Abstract: Learning profitable intraday trading policies from financial time series is challenging due to heavy noise, non-stationarity, and strong cross-sectional dependence among related assets. We propose \\emph{WaveLSFormer}, a learnable wavelet-based long-short Transformer that jointly performs mul",
    "url": "https://arxiv.org/abs/2601.13435",
    "source": "Arxiv AI"
  },
  {
    "title": "GeoDynamics: A Geometric State-Space Neural Network for Understanding Brain Dynamics on Riemannian Manifolds",
    "summary": "arXiv:2601.13570v2 Announce Type: replace-cross Abstract: State-space models (SSMs) have become a cornerstone for unraveling brain dynamics, revealing how latent neural states evolve over time and give rise to observed signals. By combining the flexibility of deep learning with the principled dynamical structure of SSMs, recent studies have achieve",
    "url": "https://arxiv.org/abs/2601.13570",
    "source": "Arxiv AI"
  },
  {
    "title": "ConceptCaps: a Distilled Concept Dataset for Interpretability in Music Models",
    "summary": "arXiv:2601.14157v2 Announce Type: replace-cross Abstract: Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure: tags are sparse, noisy, or ill-defined. We introduce ConceptCaps, a dataset of 23k music-caption-audio triplets with ex",
    "url": "https://arxiv.org/abs/2601.14157",
    "source": "Arxiv AI"
  },
  {
    "title": "HELIOS: Hierarchical Graph Abstraction for Structure-Aware LLM Decompilation",
    "summary": "arXiv:2601.14598v2 Announce Type: replace-cross Abstract: Large language models (LLMs) have recently been applied to binary decompilation, yet they still treat code as plain text and ignore the graphs that govern program control flow. This limitation often yields syntactically fragile and logically inconsistent output, especially for optimized bina",
    "url": "https://arxiv.org/abs/2601.14598",
    "source": "Arxiv AI"
  },
  {
    "title": "Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data",
    "summary": "arXiv:2601.15158v3 Announce Type: replace-cross Abstract: Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive policy gradient to discover such systematic reasoning remains",
    "url": "https://arxiv.org/abs/2601.15158",
    "source": "Arxiv AI"
  },
  {
    "title": "Why Inference in Large Models Becomes Decomposable After Training",
    "summary": "arXiv:2601.15871v2 Announce Type: replace-cross Abstract: Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems ",
    "url": "https://arxiv.org/abs/2601.15871",
    "source": "Arxiv AI"
  },
  {
    "title": "DMV-AVP: Distributed Multi-Vehicle Autonomous Valet Parking Using Autoware",
    "summary": "arXiv:2601.16327v2 Announce Type: replace-cross Abstract: This paper presents DMV-AVP, a distributed simulation of Multi-Vehicle Autonomous Valet Parking (AVP). The system was implemented as an application of the Distributed Multi-Autonomous Vehicle Architecture (DMAVA) for synchronized multi-host execution. Most existing simulation approaches rely",
    "url": "https://arxiv.org/abs/2601.16327",
    "source": "Arxiv AI"
  },
  {
    "title": "DMAVA: Distributed Multi-Autonomous Vehicle Architecture Using Autoware",
    "summary": "arXiv:2601.16336v2 Announce Type: replace-cross Abstract: Simulating and validating coordination among multiple autonomous vehicles remains challenging, as many existing simulation architectures are limited to single-vehicle operation or rely on centralized control. This paper presents the Distributed Multi-Autonomous Vehicle Architecture (DMAVA), ",
    "url": "https://arxiv.org/abs/2601.16336",
    "source": "Arxiv AI"
  },
  {
    "title": "Model-Centric Diagnostics: A Framework for Internal State Readouts",
    "summary": "arXiv:2601.16874v2 Announce Type: replace-cross Abstract: We present a model-centric diagnostic framework that treats training state as a latent variable and unifies a family of internal readouts -- head-gradient norms, confidence, entropy, margin, and related signals -- as anchor-relative projections of that state. A preliminary version of this wo",
    "url": "https://arxiv.org/abs/2601.16874",
    "source": "Arxiv AI"
  },
  {
    "title": "A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs",
    "summary": "arXiv:2601.16979v2 Announce Type: replace-cross Abstract: Understanding the curvature evolution of the loss landscape is fundamental to analyzing the training dynamics of neural networks. The most commonly studied measure, Hessian sharpness ($\\lambda_{\\max}^H$) -- the largest eigenvalue of the loss Hessian -- determines local training stability and",
    "url": "https://arxiv.org/abs/2601.16979",
    "source": "Arxiv AI"
  },
  {
    "title": "When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs",
    "summary": "arXiv:2601.18350v3 Announce Type: replace-cross Abstract: Large language models can exhibit surprising adapter interference when combining domain adaptation and instruction alignment in safety-critical settings. We study a two-stage LoRA pipeline for medical LLMs, where domain-oriented pre-training (PT) and supervised fine-tuning (SFT) are trained ",
    "url": "https://arxiv.org/abs/2601.18350",
    "source": "Arxiv AI"
  },
  {
    "title": "Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning",
    "summary": "arXiv:2601.18352v2 Announce Type: replace-cross Abstract: LLMs struggle with Semantic Inertia: the inability to inhibit pre-trained priors (e.g., \"Lava is Dangerous\") when dynamic, in-context rules contradict them. We probe this phenomenon using Baba Is You, where physical laws are mutable text rules, enabling precise evaluation of models' ability ",
    "url": "https://arxiv.org/abs/2601.18352",
    "source": "Arxiv AI"
  },
  {
    "title": "From Fuzzy to Exact: The Halo Architecture for Infinite-Depth Reasoning via Rational Arithmetic",
    "summary": "arXiv:2601.18702v2 Announce Type: replace-cross Abstract: The pursuit of scale in deep learning has entrenched a trade-off: computational throughput is prioritized at the expense of numerical precision. We argue this compromise is fundamentally at odds with the requirements of general intelligence. We propose the \\textbf{Exactness Hypothesis}: high",
    "url": "https://arxiv.org/abs/2601.18702",
    "source": "Arxiv AI"
  },
  {
    "title": "SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification",
    "summary": "arXiv:2601.18739v3 Announce Type: replace-cross Abstract: Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex c",
    "url": "https://arxiv.org/abs/2601.18739",
    "source": "Arxiv AI"
  },
  {
    "title": "Language Family Matters: Evaluating LLM-Based ASR Across Linguistic Boundaries",
    "summary": "arXiv:2601.18899v2 Announce Type: replace-cross Abstract: Large Language Model (LLM)-powered Automatic Speech Recognition (ASR) systems achieve strong performance with limited resources by linking a frozen speech encoder to a pretrained LLM via a lightweight connector. Prior work trains a separate connector per language, overlooking linguistic rela",
    "url": "https://arxiv.org/abs/2601.18899",
    "source": "Arxiv AI"
  },
  {
    "title": "Toward Learning POMDPs Beyond Full-Rank Actions and State Observability",
    "summary": "arXiv:2601.18930v3 Announce Type: replace-cross Abstract: We are interested in enabling autonomous agents to learn and reason about systems with hidden states, such as locking mechanisms. We cast this problem as learning the parameters of a discrete Partially Observable Markov Decision Process (POMDP). The agent begins with knowledge of the POMDP's",
    "url": "https://arxiv.org/abs/2601.18930",
    "source": "Arxiv AI"
  },
  {
    "title": "FloydNet: A Learning Paradigm for Global Relational Reasoning",
    "summary": "arXiv:2601.19094v2 Announce Type: replace-cross Abstract: Developing models capable of complex, multi-step reasoning is a central goal in artificial intelligence. While representing problems as graphs is a powerful approach, Graph Neural Networks (GNNs) are fundamentally constrained by their message-passing mechanism, which imposes a local bottlene",
    "url": "https://arxiv.org/abs/2601.19094",
    "source": "Arxiv AI"
  },
  {
    "title": "A Scalable Inter-edge Correlation Modeling in CopulaGNN for Link Sign Prediction",
    "summary": "arXiv:2601.19175v3 Announce Type: replace-cross Abstract: Link sign prediction on a signed graph is a task to determine whether the relationship represented by an edge is positive or negative. Since the presence of negative edges violates the graph homophily assumption that adjacent nodes are similar, regular graph methods have not been applicable ",
    "url": "https://arxiv.org/abs/2601.19175",
    "source": "Arxiv AI"
  },
  {
    "title": "Membership Inference Attacks Against Fine-tuned Diffusion Language Models",
    "summary": "arXiv:2601.20125v2 Announce Type: replace-cross Abstract: Diffusion Language Models (DLMs) represent a promising alternative to autoregressive language models, using bidirectional masked token prediction. Yet their susceptibility to privacy leakage via Membership Inference Attacks (MIA) remains critically underexplored. This paper presents the firs",
    "url": "https://arxiv.org/abs/2601.20125",
    "source": "Arxiv AI"
  },
  {
    "title": "How AI Impacts Skill Formation",
    "summary": "arXiv:2601.20245v2 Announce Type: replace-cross Abstract: AI assistance produces significant productivity gains across professional domains, particularly for novice workers. Yet how this assistance affects the development of skills required to effectively supervise AI remains unclear. Novice workers who rely heavily on AI to complete unfamiliar tas",
    "url": "https://arxiv.org/abs/2601.20245",
    "source": "Arxiv AI"
  },
  {
    "title": "Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification",
    "summary": "arXiv:2601.21244v2 Announce Type: replace-cross Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from pr",
    "url": "https://arxiv.org/abs/2601.21244",
    "source": "Arxiv AI"
  },
  {
    "title": "Music Plagiarism Detection: Problem Formulation and a Segment-based Solution",
    "summary": "arXiv:2601.21260v2 Announce Type: replace-cross Abstract: Recently, the problem of music plagiarism has emerged as an even more pressing social issue. As music information retrieval research advances, there is a growing effort to address issues related to music plagiarism. However, many studies, including our previous work, have conducted research ",
    "url": "https://arxiv.org/abs/2601.21260",
    "source": "Arxiv AI"
  },
  {
    "title": "HER: Human-like Reasoning and Reinforcement Learning for LLM Role-playing",
    "summary": "arXiv:2601.21459v2 Announce Type: replace-cross Abstract: LLM role-playing, i.e., using LLMs to simulate specific personas, has emerged as a key capability in various applications, such as companionship, content creation, and digital games. While current models effectively capture character tones and knowledge, simulating the inner thoughts behind ",
    "url": "https://arxiv.org/abs/2601.21459",
    "source": "Arxiv AI"
  },
  {
    "title": "Shaping capabilities with token-level data filtering",
    "summary": "arXiv:2601.21571v2 Announce Type: replace-cross Abstract: Current approaches to reducing undesired capabilities in language models are largely post hoc, and can thus be easily bypassed by adversaries. A natural alternative is to shape capabilities during pretraining itself. On the proxy task of removing medical capabilities, we show that the simple",
    "url": "https://arxiv.org/abs/2601.21571",
    "source": "Arxiv AI"
  },
  {
    "title": "TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention",
    "summary": "arXiv:2601.21900v2 Announce Type: replace-cross Abstract: Despite their capabilities, large foundation models (LFMs) remain susceptible to adversarial manipulation. Current defenses predominantly rely on the \"locality hypothesis\", suppressing isolated neurons or features. However, harmful semantics act as distributed, cross-layer circuits, renderin",
    "url": "https://arxiv.org/abs/2601.21900",
    "source": "Arxiv AI"
  },
  {
    "title": "Latent Adversarial Regularization for Offline Preference Optimization",
    "summary": "arXiv:2601.22083v2 Announce Type: replace-cross Abstract: Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral si",
    "url": "https://arxiv.org/abs/2601.22083",
    "source": "Arxiv AI"
  },
  {
    "title": "EUGens: Efficient, Unified, and General Dense Layers",
    "summary": "arXiv:2601.22563v2 Announce Type: replace-cross Abstract: Efficient neural networks are essential for scaling machine learning models to real-time applications and resource-constrained environments. Fully-connected feedforward layers (FFLs) introduce computation and parameter count bottlenecks within neural network architectures. To address this ch",
    "url": "https://arxiv.org/abs/2601.22563",
    "source": "Arxiv AI"
  },
  {
    "title": "Gated Relational Alignment via Confidence-based Distillation for Efficient VLMs",
    "summary": "arXiv:2601.22709v2 Announce Type: replace-cross Abstract: Vision-Language Models (VLMs) achieve strong multimodal performance but are costly to deploy, and post-training quantization often causes significant accuracy loss. Despite its potential, quantization-aware training for VLMs remains underexplored. We propose GRACE, a framework unifying knowl",
    "url": "https://arxiv.org/abs/2601.22709",
    "source": "Arxiv AI"
  },
  {
    "title": "MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering",
    "summary": "arXiv:2601.22859v2 Announce Type: replace-cross Abstract: The evolution of Large Language Model (LLM) agents for software engineering (SWE) is constrained by the scarcity of verifiable datasets, a bottleneck stemming from the complexity of constructing executable environments across diverse languages. To address this, we introduce MEnvAgent, a Mult",
    "url": "https://arxiv.org/abs/2601.22859",
    "source": "Arxiv AI"
  },
  {
    "title": "Bias Beyond Borders: Political Ideology Evaluation and Steering in Multilingual LLMs",
    "summary": "arXiv:2601.23001v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) increasingly shape global discourse, making fairness and ideological neutrality essential for responsible AI deployment. Despite growing attention to political bias in LLMs, prior work largely focuses on high-resource, Western languages or narrow multilingual set",
    "url": "https://arxiv.org/abs/2601.23001",
    "source": "Arxiv AI"
  },
  {
    "title": "Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference",
    "summary": "arXiv:2601.23039v2 Announce Type: replace-cross Abstract: Differentiable matching layers, often implemented via entropy-regularized Optimal Transport, serve as a critical approximate inference mechanism in structural prediction. However, recovering discrete permutations via annealing $\\epsilon \\to 0$ is notoriously unstable. We identify a fundament",
    "url": "https://arxiv.org/abs/2601.23039",
    "source": "Arxiv AI"
  },
  {
    "title": "To See Far, Look Close: Evolutionary Forecasting for Long-term Time Series",
    "summary": "arXiv:2601.23114v2 Announce Type: replace-cross Abstract: The prevailing Direct Forecasting (DF) paradigm dominates Long-term Time Series Forecasting (LTSF) by forcing models to predict the entire future horizon in a single forward pass. While efficient, this rigid coupling of output and evaluation horizons necessitates computationally prohibitive ",
    "url": "https://arxiv.org/abs/2601.23114",
    "source": "Arxiv AI"
  },
  {
    "title": "TEON: Tensorized Orthonormalization Beyond Layer-Wise Muon for Large Language Model Pre-Training",
    "summary": "arXiv:2601.23261v2 Announce Type: replace-cross Abstract: The Muon optimizer has demonstrated strong empirical performance in pre-training large language models by performing matrix-level gradient (or momentum) orthogonalization in each layer independently. In this work, we propose TEON, a principled generalization of Muon that extends orthogonaliz",
    "url": "https://arxiv.org/abs/2601.23261",
    "source": "Arxiv AI"
  },
  {
    "title": "UniRG: Scaling medical imaging report generation with multimodal reinforcement learning",
    "summary": "AI can help generate medical image reports, but today’s models struggle with varying reporting schemes. Learn how UniRG uses reinforcement learning to boost performance of medical vision-language models. The post UniRG: Scaling medical imaging report generation with multimodal reinforcement learning appeared first on Microsoft Research.",
    "url": "https://www.microsoft.com/en-us/research/blog/unirg-scaling-medical-imaging-report-generation-with-multimodal-reinforcement-learning/",
    "source": "Microsoft Research"
  },
  {
    "title": "Multimodal reinforcement learning with agentic verifier for AI agents",
    "summary": "Argos improves multimodal RL by evaluating whether an agent’s reasoning aligns with what it observes over time. The approach reduces visual hallucinations and produces more reliable, data-efficient agents for real-world applications. The post Multimodal reinforcement learning with agentic verifier for AI agents appeared first on Microsoft Research.",
    "url": "https://www.microsoft.com/en-us/research/blog/multimodal-reinforcement-learning-with-agentic-verifier-for-ai-agents/",
    "source": "Microsoft Research"
  },
  {
    "title": "OptiMind: A small language model with optimization expertise",
    "summary": "OptiMind is a small language model that converts business operation challenges, described naturally, into mathematical formulations that optimization software can solve. It reduces formulation time &#038; errors &#038; enables fast, privacy-preserving local use. The post OptiMind: A small language model with optimization expertise appeared first on",
    "url": "https://www.microsoft.com/en-us/research/blog/optimind-a-small-language-model-with-optimization-expertise/",
    "source": "Microsoft Research"
  },
  {
    "title": "Agent Lightning: Adding reinforcement learning to AI agents without code rewrites",
    "summary": "By decoupling how agents work from how they’re trained, Agent Lightning turns each step an agent takes into data for reinforcement learning. This makes it easy for developers to improve agent performance with almost zero code changes. The post Agent Lightning: Adding reinforcement learning to AI agents without code rewrites appeared first on Micros",
    "url": "https://www.microsoft.com/en-us/research/blog/agent-lightning-adding-reinforcement-learning-to-ai-agents-without-code-rewrites/",
    "source": "Microsoft Research"
  },
  {
    "title": "Promptions helps make AI prompting more precise with dynamic UI controls",
    "summary": "Promptions helps developers add dynamic, context-aware controls to chat interfaces so users can guide generative AI responses. It lets users shape outputs quickly without writing long instructions. The post Promptions helps make AI prompting more precise with dynamic UI controls appeared first on Microsoft Research.",
    "url": "https://www.microsoft.com/en-us/research/blog/promptions-helps-make-ai-prompting-more-precise-with-dynamic-ui-controls/",
    "source": "Microsoft Research"
  },
  {
    "title": "GigaTIME: Scaling tumor microenvironment modeling using virtual population generated by multimodal AI",
    "summary": "Using AI-generated virtual populations, Microsoft researchers uncovered hidden cellular patterns that could reshape how we understand and treat cancer. The post GigaTIME: Scaling tumor microenvironment modeling using virtual population generated by multimodal AI appeared first on Microsoft Research.",
    "url": "https://www.microsoft.com/en-us/research/blog/gigatime-scaling-tumor-microenvironment-modeling-using-virtual-population-generated-by-multimodal-ai/",
    "source": "Microsoft Research"
  },
  {
    "title": "Ideas: Community building, machine learning, and the future of AI",
    "summary": "As the Women in Machine Learning Workshop (WiML) marks its 20th annual gathering, cofounders, friends, and collaborators Jenn Wortman Vaughan and Hanna Wallach reflect on WiML’s evolution, navigating the field of ML, and their work in responsible AI. The post Ideas: Community building, machine learning, and the future of AI appeared first on Micros",
    "url": "https://www.microsoft.com/en-us/research/podcast/ideas-community-building-machine-learning-and-the-future-of-ai/",
    "source": "Microsoft Research"
  },
  {
    "title": "Reducing Privacy leaks in AI: Two approaches to contextual integrity",
    "summary": "New research explores two ways to give AI agents stronger privacy safeguards grounded in contextual integrity. One adds lightweight, inference-time checks; the other builds contextual awareness directly into models through reasoning and RL. The post Reducing Privacy leaks in AI: Two approaches to contextual integrity appeared first on Microsoft Res",
    "url": "https://www.microsoft.com/en-us/research/blog/reducing-privacy-leaks-in-ai-two-approaches-to-contextual-integrity/",
    "source": "Microsoft Research"
  },
  {
    "title": "Fara-7B: An Efficient Agentic Model for Computer Use",
    "summary": "Fara-7B is our first agentic small language model for computer use. This experimental model includes robust safety measures to aid responsible deployment. Despite its size, Fara-7B holds its own against larger, more resource-intensive agentic systems. The post Fara-7B: An Efficient Agentic Model for Computer Use appeared first on Microsoft Research",
    "url": "https://www.microsoft.com/en-us/research/blog/fara-7b-an-efficient-agentic-model-for-computer-use/",
    "source": "Microsoft Research"
  },
  {
    "title": "MMCTAgent: Enabling multimodal reasoning over large video and image collections",
    "summary": "MMCTAgent enables dynamic multimodal reasoning with iterative planning and reflection. Built on Microsoft’s AutoGen framework, it integrates language, vision, and temporal understanding for complex tasks like long video and image analysis. The post MMCTAgent: Enabling multimodal reasoning over large video and image collections appeared first on Mic",
    "url": "https://www.microsoft.com/en-us/research/blog/mmctagent-enabling-multimodal-reasoning-over-large-video-and-image-collections/",
    "source": "Microsoft Research"
  },
  {
    "title": "Nemotron ColEmbed V2: Raising the Bar for Multimodal Retrieval with ViDoRe V3’s Top Model",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/nemotron-colembed-v2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "H Company's new Holo2 model takes the lead in UI Localization",
    "summary": "",
    "url": "https://huggingface.co/blog/Hcompany/introducing-holo2-235b-a22b",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Future of the Global Open-Source AI Ecosystem: From DeepSeek to AI+",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment-blog-3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Training Design for Text-to-Image Models: Lessons from Ablations",
    "summary": "",
    "url": "https://huggingface.co/blog/Photoroom/prx-part2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Daggr: Chain apps programmatically, inspect visually",
    "summary": "",
    "url": "https://huggingface.co/blog/daggr",
    "source": "Hugging Face Blog"
  },
  {
    "title": "We Got Claude to Build CUDA Kernels and teach open models!",
    "summary": "",
    "url": "https://huggingface.co/blog/upskill",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Architectural Choices in China's Open-Source AI Ecosystem: Building Beyond DeepSeek",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment-blog-2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Alyah ⭐️: Toward Robust Evaluation of Emirati Dialect Capabilities in Arabic LLMs",
    "summary": "",
    "url": "https://huggingface.co/blog/tiiuae/emirati-benchmarks",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Unlocking Agentic RL Training for GPT-OSS: A Practical Retrospective",
    "summary": "",
    "url": "https://huggingface.co/blog/LinkedIn/gpt-oss-agentic-rl",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AssetOpsBench: Bridging the Gap Between AI Agent Benchmarks and Industrial Reality",
    "summary": "",
    "url": "https://huggingface.co/blog/ibm-research/assetopsbench-playground-on-hugging-face",
    "source": "Hugging Face Blog"
  },
  {
    "title": "One Year Since the “DeepSeek Moment”",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Differential Transformer V2",
    "summary": "",
    "url": "https://huggingface.co/blog/microsoft/diff-attn-v2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Waypoint-1: Real-time interactive video diffusion from Overworld",
    "summary": "",
    "url": "https://huggingface.co/blog/waypoint-1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open Responses: What you need to know",
    "summary": "",
    "url": "https://huggingface.co/blog/open-responses",
    "source": "Hugging Face Blog"
  },
  {
    "title": "NVIDIA Cosmos Reason 2 Brings Advanced Reasoning To Physical AI",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/nvidia-cosmos-reason-2-brings-advanced-reasoning",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Falcon-H1-Arabic: Pushing the Boundaries of Arabic Language AI with Hybrid Architecture",
    "summary": "",
    "url": "https://huggingface.co/blog/tiiuae/falcon-h1-arabic",
    "source": "Hugging Face Blog"
  },
  {
    "title": "NVIDIA brings agents to life with DGX Spark and Reachy Mini",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia-reachy-mini",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AprielGuard: A Guardrail for Safety and Adversarial Robustness in Modern LLM Systems",
    "summary": "",
    "url": "https://huggingface.co/blog/ServiceNow-AI/aprielguard",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Tokenization in Transformers v5: Simpler, Clearer, and More Modular",
    "summary": "",
    "url": "https://huggingface.co/blog/tokenizers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Open Evaluation Standard: Benchmarking NVIDIA Nemotron 3 Nano with NeMo Evaluator",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe",
    "source": "Hugging Face Blog"
  },
  {
    "title": "CUGA on Hugging Face: Democratizing Configurable AI Agents",
    "summary": "",
    "url": "https://huggingface.co/blog/ibm-research/cuga-on-hugging-face",
    "source": "Hugging Face Blog"
  },
  {
    "title": "New in llama.cpp: Model Management",
    "summary": "",
    "url": "https://huggingface.co/blog/ggml-org/model-management-in-llamacpp",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Codex is Open Sourcing AI models",
    "summary": "",
    "url": "https://huggingface.co/blog/hf-skills-training-codex",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing swift-huggingface: The Complete Swift Client for Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/swift-huggingface",
    "source": "Hugging Face Blog"
  },
  {
    "title": "DeepMath: A lightweight math reasoning Agent with smolagents",
    "summary": "",
    "url": "https://huggingface.co/blog/intel-deepmath",
    "source": "Hugging Face Blog"
  },
  {
    "title": "We Got Claude to Fine-Tune an Open Source LLM",
    "summary": "",
    "url": "https://huggingface.co/blog/hf-skills-training",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Transformers v5: Simple model definitions powering the AI ecosystem",
    "summary": "",
    "url": "https://huggingface.co/blog/transformers-v5",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Diffusers welcomes FLUX-2",
    "summary": "",
    "url": "https://huggingface.co/blog/flux-2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Continuous batching from first principles",
    "summary": "",
    "url": "https://huggingface.co/blog/continuous_batching",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Building Deep Research: How we Achieved State of the Art",
    "summary": "",
    "url": "https://huggingface.co/blog/Tavily/tavily-deep-research",
    "source": "Hugging Face Blog"
  },
  {
    "title": "OVHcloud on Hugging Face Inference Providers 🔥",
    "summary": "",
    "url": "https://huggingface.co/blog/OVHcloud/inference-providers-ovhcloud",
    "source": "Hugging Face Blog"
  },
  {
    "title": "20x Faster TRL Fine-tuning with RapidFire AI",
    "summary": "",
    "url": "https://huggingface.co/blog/rapidfireai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open ASR Leaderboard: Trends and Insights with New Multilingual & Long-Form Tracks",
    "summary": "",
    "url": "https://huggingface.co/blog/open-asr-leaderboard",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing AnyLanguageModel: One API for Local and Remote LLMs on Apple Platforms",
    "summary": "",
    "url": "https://huggingface.co/blog/anylanguagemodel",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Apriel-H1: The Surprising Key to Distilling Efficient Reasoning Models",
    "summary": "",
    "url": "https://huggingface.co/blog/ServiceNow-AI/apriel-h1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Easily Build and Share ROCm Kernels with Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/build-rocm-kernels",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Join the AMD Open Robotics Hackathon",
    "summary": "",
    "url": "https://huggingface.co/blog/amd/openroboticshackathon",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Building for an Open Future - our new partnership with Google Cloud",
    "summary": "",
    "url": "https://huggingface.co/blog/google-cloud",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Aligning to What? Rethinking Agent Generalization in MiniMax M2",
    "summary": "",
    "url": "https://huggingface.co/blog/MiniMax-AI/aligning-to-what",
    "source": "Hugging Face Blog"
  },
  {
    "title": "On the Shifting Global Compute Landscape",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface/shifting-compute-landscape",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Building a Healthcare Robot from Simulation to Deployment with NVIDIA Isaac",
    "summary": "",
    "url": "https://huggingface.co/blog/lerobotxnvidia-healthcare",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How to Build a Healthcare Robot from Simulation to Deployment with NVIDIA Isaac for Healthcare",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/nvidia-isaac-for-healthcare",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Granite 4.0 Nano: Just how small can you go?",
    "summary": "",
    "url": "https://huggingface.co/blog/ibm-granite/granite-4-nano",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Voice Cloning with Consent",
    "summary": "",
    "url": "https://huggingface.co/blog/voice-consent-gate",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Streaming datasets: 100x More Efficient",
    "summary": "",
    "url": "https://huggingface.co/blog/streaming-datasets",
    "source": "Hugging Face Blog"
  },
  {
    "title": "huggingface_hub v1.0: Five Years of Building the Foundation of Open Machine Learning",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface-hub-v1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "LeRobot v0.4.0: Supercharging OSS Robot Learning",
    "summary": "",
    "url": "https://huggingface.co/blog/lerobot-release-v040",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Building the Open Agent Ecosystem Together: Introducing OpenEnv",
    "summary": "",
    "url": "https://huggingface.co/blog/openenv",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face and VirusTotal collaborate to strengthen AI security",
    "summary": "",
    "url": "https://huggingface.co/blog/virustotal",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Sentence Transformers is joining Hugging Face!",
    "summary": "",
    "url": "https://huggingface.co/blog/sentence-transformers-joins-hf",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Supercharge your OCR Pipelines with Open Models",
    "summary": "",
    "url": "https://huggingface.co/blog/ocr-open-models",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Unlock the power of images with AI Sheets",
    "summary": "",
    "url": "https://huggingface.co/blog/aisheets-unlock-images",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AI for Food Allergies",
    "summary": "",
    "url": "https://huggingface.co/blog/hugging-science/ai-for-food-allergies",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Google Cloud C4 Brings a 70% TCO improvement on GPT OSS with Intel and Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/gpt-oss-on-intel-xeon",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Get your VLM running in 3 simple steps on Intel CPUs",
    "summary": "",
    "url": "https://huggingface.co/blog/openvino-vlm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Nemotron-Personas-India: Synthesized Data for Sovereign AI",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/nemotron-personas-india",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Arm will be @ PyTorch Conference, Join Us!",
    "summary": "",
    "url": "https://huggingface.co/blog/Arm/arm-at-pytorch-conference",
    "source": "Hugging Face Blog"
  },
  {
    "title": "BigCodeArena: Judging code generations end to end with code executions",
    "summary": "",
    "url": "https://huggingface.co/blog/bigcode/arena",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SOTA OCR with Core ML and dots.ocr",
    "summary": "",
    "url": "https://huggingface.co/blog/dots-ocr-ne",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing RTEB: A New Standard for Retrieval Evaluation",
    "summary": "",
    "url": "https://huggingface.co/blog/rteb",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerating Qwen3-8B Agent on Intel® Core™ Ultra with Depth-Pruned Draft Models",
    "summary": "",
    "url": "https://huggingface.co/blog/intel-qwen3-agent",
    "source": "Hugging Face Blog"
  },
  {
    "title": "VibeGame: Exploring Vibe Coding Games",
    "summary": "",
    "url": "https://huggingface.co/blog/vibegame",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Nemotron-Personas-Japan: ソブリン AI のための合成データセット",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/nemotron-personas-japan-ja",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Swift Transformers Reaches 1.0 – and Looks to the Future",
    "summary": "",
    "url": "https://huggingface.co/blog/swift-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Smol2Operator: Post-Training GUI Agents for Computer Use",
    "summary": "",
    "url": "https://huggingface.co/blog/smol2operator",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SyGra: The One-Stop Framework for Building Data for LLMs and SLMs",
    "summary": "",
    "url": "https://huggingface.co/blog/ServiceNow-AI/sygra-data-gen-framework",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Gaia2 and ARE: Empowering the community to study agents",
    "summary": "",
    "url": "https://huggingface.co/blog/gaia2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Scaleway on Hugging Face Inference Providers 🔥",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-providers-scaleway",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Democratizing AI Safety with RiskRubric.ai",
    "summary": "",
    "url": "https://huggingface.co/blog/riskrubric",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Public AI on Hugging Face Inference Providers 🔥",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-providers-publicai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "`LeRobotDataset:v3.0`: Bringing large-scale datasets to `lerobot`",
    "summary": "",
    "url": "https://huggingface.co/blog/lerobot-datasets-v3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Visible Watermarking with Gradio",
    "summary": "",
    "url": "https://huggingface.co/blog/watermarking-with-gradio",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Palmyra-mini family: Powerful, lightweight, and ready to reason!",
    "summary": "",
    "url": "https://huggingface.co/blog/Writer/announcing-palmyra-mini",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Tricks from OpenAI gpt-oss YOU 🫵 can use with transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/faster-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-tune Any LLM from the Hugging Face Hub with Together AI",
    "summary": "",
    "url": "https://huggingface.co/blog/togethercomputer/together-ft",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Jupyter Agents: training LLMs to reason with notebooks",
    "summary": "",
    "url": "https://huggingface.co/blog/jupyter-agent-2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "mmBERT: ModernBERT goes Multilingual",
    "summary": "",
    "url": "https://huggingface.co/blog/mmbert",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome EmbeddingGemma, Google's new efficient embedding model",
    "summary": "",
    "url": "https://huggingface.co/blog/embeddinggemma",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SAIR: Accelerating Pharma R&D with AI-Powered Structural Intelligence",
    "summary": "",
    "url": "https://huggingface.co/blog/SandboxAQ/sair-data-accelerating-drug-discovery-with-ai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Make your ZeroGPU Spaces go brrr with ahead-of-time compilation",
    "summary": "",
    "url": "https://huggingface.co/blog/zerogpu-aoti",
    "source": "Hugging Face Blog"
  },
  {
    "title": "NVIDIA Releases 6 Million Multi-Lingual Reasoning Dataset",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/multilingual-reasoning-v1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Generate Images with Claude and Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/claude-and-mcp",
    "source": "Hugging Face Blog"
  },
  {
    "title": "From Zero to GPU: A Guide to Building and Scaling Production-Ready CUDA Kernels",
    "summary": "",
    "url": "https://huggingface.co/blog/kernel-builder",
    "source": "Hugging Face Blog"
  },
  {
    "title": "MCP for Research: How to Connect AI to Research Tools",
    "summary": "",
    "url": "https://huggingface.co/blog/mcp-for-research",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Kimina-Prover-RL",
    "summary": "",
    "url": "https://huggingface.co/blog/AI-MO/kimina-prover-rl",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Arm & ExecuTorch 0.7: Bringing Generative AI to the masses",
    "summary": "",
    "url": "https://huggingface.co/blog/Arm/executorch-0-dot-7",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Neural Super Sampling is here!",
    "summary": "",
    "url": "https://huggingface.co/blog/Arm/neural-super-sampling",
    "source": "Hugging Face Blog"
  },
  {
    "title": "TextQuests: How Good are LLMs at Text-Based Video Games?",
    "summary": "",
    "url": "https://huggingface.co/blog/textquests",
    "source": "Hugging Face Blog"
  },
  {
    "title": "🇵🇭 FilBench - Can LLMs Understand and Generate Filipino?",
    "summary": "",
    "url": "https://huggingface.co/blog/filbench",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing AI Sheets: a tool to work with datasets using open AI models!",
    "summary": "",
    "url": "https://huggingface.co/blog/aisheets",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerate ND-Parallel: A guide to Efficient Multi-GPU Training",
    "summary": "",
    "url": "https://huggingface.co/blog/accelerate-nd-parallel",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Vision Language Model Alignment in TRL ⚡️",
    "summary": "",
    "url": "https://huggingface.co/blog/trl-vlm-alignment",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome GPT OSS, the new open-source model family from OpenAI!",
    "summary": "",
    "url": "https://huggingface.co/blog/welcome-openai-gpt-oss",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Measuring Open-Source Llama Nemotron Models on DeepResearch Bench",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/ai-q-top-ranking-open-portable-deep-research-agent",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Implementing MCP Servers in Python: An AI Shopping Assistant with Gradio",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-vton-mcp",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Trackio: A Lightweight Experiment Tracking Library from Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/trackio",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Say hello to `hf`: a faster, friendlier Hugging Face CLI ✨",
    "summary": "",
    "url": "https://huggingface.co/blog/hf-cli",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Parquet Content-Defined Chunking",
    "summary": "",
    "url": "https://huggingface.co/blog/parquet-cdc",
    "source": "Hugging Face Blog"
  },
  {
    "title": "TimeScope: How Long Can Your Video Large Multimodal Model Go?",
    "summary": "",
    "url": "https://huggingface.co/blog/timescope-video-lmm-benchmark",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fast LoRA inference for Flux with Diffusers and PEFT",
    "summary": "",
    "url": "https://huggingface.co/blog/lora-fast",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerate a World of LLMs on Hugging Face with NVIDIA NIM",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/multi-llm-nim",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Arc Virtual Cell Challenge: A Primer",
    "summary": "",
    "url": "https://huggingface.co/blog/virtual-cell-challenge",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Consilium: When Multiple LLMs Collaborate",
    "summary": "",
    "url": "https://huggingface.co/blog/consilium-multi-llm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Back to The Future: Evaluating AI Agents on Predicting Future Events",
    "summary": "",
    "url": "https://huggingface.co/blog/futurebench",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Five Big Improvements to Gradio MCP Servers",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-mcp-updates",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Ettin Suite: SoTA Paired Encoders and Decoders",
    "summary": "",
    "url": "https://huggingface.co/blog/ettin",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Migrating the Hub from Git LFS to Xet",
    "summary": "",
    "url": "https://huggingface.co/blog/migrating-the-hub-to-xet",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Kimina-Prover: Applying Test-time RL Search on Large Formal Reasoning Models",
    "summary": "",
    "url": "https://huggingface.co/blog/AI-MO/kimina-prover",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Asynchronous Robot Inference: Decoupling Action Prediction and Execution",
    "summary": "",
    "url": "https://huggingface.co/blog/async-robot-inference",
    "source": "Hugging Face Blog"
  },
  {
    "title": "ScreenEnv: Deploy your full stack Desktop Agent",
    "summary": "",
    "url": "https://huggingface.co/blog/screenenv",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Building the Hugging Face MCP Server",
    "summary": "",
    "url": "https://huggingface.co/blog/building-hf-mcp",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Reachy Mini - The Open-Source Robot for Today's and Tomorrow's AI Builders",
    "summary": "",
    "url": "https://huggingface.co/blog/reachy-mini",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Creating custom kernels for the AMD MI300",
    "summary": "",
    "url": "https://huggingface.co/blog/mi300kernels",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Upskill your LLMs With Gradio MCP Servers",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-mcp-servers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SmolLM3: smol, multilingual, long-context reasoner",
    "summary": "",
    "url": "https://huggingface.co/blog/smollm3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Three Mighty Alerts Supporting Hugging Face’s Production Infrastructure",
    "summary": "",
    "url": "https://huggingface.co/blog/infrastructure-alerting",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Efficient MultiModal Data Pipeline",
    "summary": "",
    "url": "https://huggingface.co/blog/mmdp",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Announcing  NeurIPS 2025 E2LM Competition: Early Training Evaluation of Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/tiiuae/e2lm-competition",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Training and Finetuning Sparse Embedding Models with Sentence Transformers v5",
    "summary": "",
    "url": "https://huggingface.co/blog/train-sparse-encoder",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome the NVIDIA Llama Nemotron Nano VLM to Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/llama-nemotron-nano-vl",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Gemma 3n fully available in the open-source ecosystem!",
    "summary": "",
    "url": "https://huggingface.co/blog/gemma3n",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Transformers backend integration in SGLang",
    "summary": "",
    "url": "https://huggingface.co/blog/transformers-backend-sglang",
    "source": "Hugging Face Blog"
  },
  {
    "title": "(LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware",
    "summary": "",
    "url": "https://huggingface.co/blog/flux-qlora",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Groq on Hugging Face Inference Providers 🔥",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-providers-groq",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How Long Prompts Block Other Requests - Optimizing LLM Performance",
    "summary": "",
    "url": "https://huggingface.co/blog/tngtech/llm-performance-blocked-by-long-prompts",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Learn the Hugging Face Kernel Hub in 5 Minutes",
    "summary": "",
    "url": "https://huggingface.co/blog/hello-hf-kernels",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Featherless AI on Hugging Face Inference Providers 🔥",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-providers-featherless",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Post-Training Isaac GR00T N1.5 for LeRobot SO-101 Arm",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/gr00t-n1-5-so101-tuning",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Training Cluster as a Service - a new collaboration with NVIDIA",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia-training-cluster",
    "source": "Hugging Face Blog"
  },
  {
    "title": "ScreenSuite - The most comprehensive evaluation suite for GUI Agents!",
    "summary": "",
    "url": "https://huggingface.co/blog/screensuite",
    "source": "Hugging Face Blog"
  },
  {
    "title": "KV Cache from scratch in nanoVLM",
    "summary": "",
    "url": "https://huggingface.co/blog/kv-cache",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Real-Time AI Sound Generation on Arm: A Personal Tool for Creative Freedom",
    "summary": "",
    "url": "https://huggingface.co/blog/Arm/ai-sound-gen-on-arm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Holo1: New family of GUI automation VLMs powering GUI agent Surfer-H",
    "summary": "",
    "url": "https://huggingface.co/blog/Hcompany/holo1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SmolVLA: Efficient Vision-Language-Action Model trained on Lerobot Community Data",
    "summary": "",
    "url": "https://huggingface.co/blog/smolvla",
    "source": "Hugging Face Blog"
  },
  {
    "title": "No GPU left behind: Unlocking Efficiency with Co-located vLLM in TRL",
    "summary": "",
    "url": "https://huggingface.co/blog/vllm-colocate",
    "source": "Hugging Face Blog"
  },
  {
    "title": "CodeAgents + Structure: A Better Way to Execute Actions",
    "summary": "",
    "url": "https://huggingface.co/blog/structured-codeagent",
    "source": "Hugging Face Blog"
  },
  {
    "title": "🐯 Liger GRPO meets TRL",
    "summary": "",
    "url": "https://huggingface.co/blog/liger-grpo",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Dell Enterprise Hub is all you need to build AI on premises",
    "summary": "",
    "url": "https://huggingface.co/blog/dell-ai-applications",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Tiny Agents in Python: a MCP-powered agent in ~70 lines of code",
    "summary": "",
    "url": "https://huggingface.co/blog/python-tiny-agents",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance",
    "summary": "",
    "url": "https://huggingface.co/blog/tiiuae/falcon-h1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Falcon-Arabic: A Breakthrough in Arabic Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/tiiuae/falcon-arabic",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Exploring Quantization Backends in Diffusers",
    "summary": "",
    "url": "https://huggingface.co/blog/diffusers-quantization",
    "source": "Hugging Face Blog"
  },
  {
    "title": "nanoVLM: The simplest repository to train your VLM in pure PyTorch",
    "summary": "",
    "url": "https://huggingface.co/blog/nanovlm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Microsoft and Hugging Face expand collaboration",
    "summary": "",
    "url": "https://huggingface.co/blog/azure-ai-foundry",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Falcon-Edge: A series of powerful, universal, fine-tunable 1.58bit language models.",
    "summary": "",
    "url": "https://huggingface.co/blog/tiiuae/falcon-edge",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Transformers Library: standardizing model definitions",
    "summary": "",
    "url": "https://huggingface.co/blog/transformers-model-definition",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Improving Hugging Face Model Access for Kaggle Users",
    "summary": "",
    "url": "https://huggingface.co/blog/kaggle-integration",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Blazingly fast whisper transcriptions with Inference Endpoints",
    "summary": "",
    "url": "https://huggingface.co/blog/fast-whisper-endpoints",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Vision Language Models (Better, faster, stronger)",
    "summary": "",
    "url": "https://huggingface.co/blog/vlms-2025",
    "source": "Hugging Face Blog"
  },
  {
    "title": "LeRobot Community Datasets: The “ImageNet” of Robotics — When and How?",
    "summary": "",
    "url": "https://huggingface.co/blog/lerobot-datasets",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How to Build an MCP Server with Gradio",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-mcp",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The 4 Things Qwen-3’s Chat Template Teaches Us",
    "summary": "",
    "url": "https://huggingface.co/blog/qwen-3-chat-template-deep-dive",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcoming Llama Guard 4 on Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/llama-guard-4",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing AutoRound: Intel’s Advanced Quantization for LLMs and VLMs",
    "summary": "",
    "url": "https://huggingface.co/blog/autoround",
    "source": "Hugging Face Blog"
  },
  {
    "title": "PipelineRL",
    "summary": "",
    "url": "https://huggingface.co/blog/ServiceNow/pipelinerl",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Tiny Agents: an MCP-powered agent in 50 lines of code",
    "summary": "",
    "url": "https://huggingface.co/blog/tiny-agents",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Finetuning olmOCR to be a faithful OCR-Engine",
    "summary": "",
    "url": "https://huggingface.co/blog/tngtech/finetuning-olmocr-to-be-a-faithful-ocr-engine",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Prefill and Decode for Concurrent Requests - Optimizing LLM Performance",
    "summary": "",
    "url": "https://huggingface.co/blog/tngtech/llm-performance-prefill-decode-concurrent-requests",
    "source": "Hugging Face Blog"
  },
  {
    "title": "17 Reasons Why Gradio Isn't Just Another UI Library",
    "summary": "",
    "url": "https://huggingface.co/blog/why-gradio-stands-out",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Cohere on Hugging Face Inference Providers 🔥",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-providers-cohere",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing HELMET: Holistically Evaluating Long-context Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/helmet",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face to sell open-source robots thanks to Pollen Robotics acquisition 🤖",
    "summary": "",
    "url": "https://huggingface.co/blog/hugging-face-pollen-robotics-acquisition",
    "source": "Hugging Face Blog"
  },
  {
    "title": "4M Models Scanned: Protect AI + Hugging Face 6 Months In",
    "summary": "",
    "url": "https://huggingface.co/blog/pai-6-month",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Visual Salamandra: Pushing the Boundaries of Multimodal Understanding",
    "summary": "",
    "url": "https://huggingface.co/blog/BSC-LT/visualsalamandra7b",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face and Cloudflare Partner to Make Real-Time Speech and Video Seamless with FastRTC",
    "summary": "",
    "url": "https://huggingface.co/blog/fastrtc-cloudflare",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Arabic Leaderboards: Introducing Arabic Instruction Following, Updating AraGen, and More",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-3c3h-aragen-ifeval",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome Llama 4 Maverick & Scout on Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/llama4-release",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Journey to 1 Million Gradio Users!",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-1m",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The NLP Course is becoming the LLM Course",
    "summary": "",
    "url": "https://huggingface.co/blog/llm-course",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Efficient Request Queueing – Optimizing LLM Performance",
    "summary": "",
    "url": "https://huggingface.co/blog/tngtech/llm-performance-request-queueing",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How Hugging Face Scaled Secrets Management for AI Infrastructure",
    "summary": "",
    "url": "https://huggingface.co/blog/scaling-secrets-management",
    "source": "Hugging Face Blog"
  },
  {
    "title": "🚀 Accelerating LLM Inference with TGI on Intel Gaudi",
    "summary": "",
    "url": "https://huggingface.co/blog/intel-gaudi-backend-for-tgi",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open R1: Update #4",
    "summary": "",
    "url": "https://huggingface.co/blog/open-r1/update-4",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Training and Finetuning Reranker Models with Sentence Transformers v4",
    "summary": "",
    "url": "https://huggingface.co/blog/train-reranker",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Gradio's new Dataframe!",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-dataframe-upgrade",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The New and Fresh analytics in Inference Endpoints",
    "summary": "",
    "url": "https://huggingface.co/blog/endpoint-analytics",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open R1: How to use OlympicCoder locally for coding",
    "summary": "",
    "url": "https://huggingface.co/blog/olympic-coder-lmstudio",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AI Policy @🤗: Response to the White House AI Action Plan RFI",
    "summary": "",
    "url": "https://huggingface.co/blog/ai-action-wh-2025",
    "source": "Hugging Face Blog"
  },
  {
    "title": "NVIDIA's GTC 2025 Announcement for Physical AI Developers: New Open Models and Datasets",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia-physical-ai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Xet is on the Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/xet-on-the-hub",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome Gemma 3: Google's all new multimodal, multilingual, long context open LLM",
    "summary": "",
    "url": "https://huggingface.co/blog/gemma3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open R1: Update #3",
    "summary": "",
    "url": "https://huggingface.co/blog/open-r1/update-3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "LeRobot goes to driving school: World’s largest open-source self-driving dataset",
    "summary": "",
    "url": "https://huggingface.co/blog/lerobot-goes-to-driving-school",
    "source": "Hugging Face Blog"
  },
  {
    "title": "LLM Inference on Edge: A Fun and Easy Guide to run LLMs via React Native on your Phone!",
    "summary": "",
    "url": "https://huggingface.co/blog/llm-inference-on-edge",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face and JFrog partner to make AI Security more transparent",
    "summary": "",
    "url": "https://huggingface.co/blog/jfrog",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A Deepdive into Aya Vision: Advancing the Frontier of Multilingual Multimodality",
    "summary": "",
    "url": "https://huggingface.co/blog/aya-vision",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Trace & Evaluate your Agent with Arize Phoenix",
    "summary": "",
    "url": "https://huggingface.co/blog/smolagents-phoenix",
    "source": "Hugging Face Blog"
  },
  {
    "title": "HuggingFace, IISc partner to supercharge model building on India's diverse languages",
    "summary": "",
    "url": "https://huggingface.co/blog/iisc-huggingface-collab",
    "source": "Hugging Face Blog"
  },
  {
    "title": "FastRTC: The Real-Time Communication Library for Python",
    "summary": "",
    "url": "https://huggingface.co/blog/fastrtc",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Remote VAEs for decoding with Inference Endpoints 🤗",
    "summary": "",
    "url": "https://huggingface.co/blog/remote_vae",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SigLIP 2: A better multilingual vision language encoder",
    "summary": "",
    "url": "https://huggingface.co/blog/siglip2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SmolVLM2: Bringing Video Understanding to Every Device",
    "summary": "",
    "url": "https://huggingface.co/blog/smolvlm2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "PaliGemma 2 Mix - New Instruction Vision Language Models by Google",
    "summary": "",
    "url": "https://huggingface.co/blog/paligemma2mix",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Three New Serverless Inference Providers: Hyperbolic, Nebius AI Studio, and Novita 🔥",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-providers-nebius-novita-hyperbolic",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome Fireworks.ai on the Hub 🎆",
    "summary": "",
    "url": "https://huggingface.co/blog/fireworks-ai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fixing Open LLM Leaderboard with Math-Verify",
    "summary": "",
    "url": "https://huggingface.co/blog/math_verify_leaderboard",
    "source": "Hugging Face Blog"
  },
  {
    "title": "1 Billion Classifications",
    "summary": "",
    "url": "https://huggingface.co/blog/billion-classifications",
    "source": "Hugging Face Blog"
  },
  {
    "title": "From Chunks to Blocks: Accelerating Uploads and Downloads on the Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/from-chunks-to-blocks",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Build awesome datasets for video generation",
    "summary": "",
    "url": "https://huggingface.co/blog/vid_ds_scripts",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open R1: Update #2",
    "summary": "",
    "url": "https://huggingface.co/blog/open-r1/update-2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Open Arabic LLM Leaderboard 2",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-arabic-v2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open-source DeepResearch – Freeing our search agents",
    "summary": "",
    "url": "https://huggingface.co/blog/open-deep-research",
    "source": "Hugging Face Blog"
  },
  {
    "title": "π0 and π0-FAST: Vision-Language-Action Models for General Robot Control",
    "summary": "",
    "url": "https://huggingface.co/blog/pi0",
    "source": "Hugging Face Blog"
  },
  {
    "title": "DABStep: Data Agent Benchmark for Multi-step Reasoning",
    "summary": "",
    "url": "https://huggingface.co/blog/dabstep",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open-R1: Update #1",
    "summary": "",
    "url": "https://huggingface.co/blog/open-r1/update-1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Mini-R1: Reproduce Deepseek R1 „aha moment“ a RL tutorial",
    "summary": "",
    "url": "https://huggingface.co/blog/open-r1/mini-r1-contdown-game",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The AI tools for Art Newsletter - Issue 1",
    "summary": "",
    "url": "https://huggingface.co/blog/ai-art-newsletter-jan-25",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How to deploy and fine-tune DeepSeek models on AWS",
    "summary": "",
    "url": "https://huggingface.co/blog/deepseek-r1-aws",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome to Inference Providers on the Hub 🔥",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-providers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open-R1: a fully open reproduction of DeepSeek-R1",
    "summary": "",
    "url": "https://huggingface.co/blog/open-r1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "State of open video generation models in Diffusers",
    "summary": "",
    "url": "https://huggingface.co/blog/video_gen",
    "source": "Hugging Face Blog"
  },
  {
    "title": "We now support VLMs in smolagents!",
    "summary": "",
    "url": "https://huggingface.co/blog/smolagents-can-see",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Mastering Long Contexts in LLMs with KVPress",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/kvpress",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SmolVLM Grows Smaller – Introducing the 256M & 500M Models!",
    "summary": "",
    "url": "https://huggingface.co/blog/smolervlm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face and FriendliAI partner to supercharge model deployment on the Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/friendliai-partnership",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Yay! Organizations can now publish blog Articles",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface/blog-articles-for-orgs",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Timm ❤️ Transformers: Use any timm model with transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/timm-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing multi-backends (TRT-LLM, vLLM) support for Text Generation Inference",
    "summary": "",
    "url": "https://huggingface.co/blog/tgi-multi-backend",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Train 400x faster Static Embedding Models with Sentence Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/static-embeddings",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AI Agents Are Here. What Now?",
    "summary": "",
    "url": "https://huggingface.co/blog/ethics-soc-7",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Visual Document Retrieval Goes Multilingual",
    "summary": "",
    "url": "https://huggingface.co/blog/vdr-2b-multilingual",
    "source": "Hugging Face Blog"
  },
  {
    "title": "CO₂ Emissions and Models Performance: Insights from the Open LLM Leaderboard",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-emissions-analysis",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing smolagents: simple agents that write actions in code.",
    "summary": "",
    "url": "https://huggingface.co/blog/smolagents",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Visualize and understand GPU memory in PyTorch",
    "summary": "",
    "url": "https://huggingface.co/blog/train_memory",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Controlling Language Model Generation with NVIDIA's LogitsProcessorZoo",
    "summary": "",
    "url": "https://huggingface.co/blog/logits-processor-zoo",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Evaluating Audio Reasoning with Big Bench Audio",
    "summary": "",
    "url": "https://huggingface.co/blog/big-bench-audio-release",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Finally, a Replacement for BERT: Introducing ModernBERT",
    "summary": "",
    "url": "https://huggingface.co/blog/modernbert",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Bamba: Inference-Efficient Hybrid Mamba2 Model",
    "summary": "",
    "url": "https://huggingface.co/blog/bamba",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome to the Falcon 3 Family of Open Models!",
    "summary": "",
    "url": "https://huggingface.co/blog/falcon3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Benchmarking Language Model Performance on 5th Gen Xeon at GCP",
    "summary": "",
    "url": "https://huggingface.co/blog/intel-gcp-c4",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Synthetic Data Generator - Build Datasets with Natural Language",
    "summary": "",
    "url": "https://huggingface.co/blog/synthetic-data-generator",
    "source": "Hugging Face Blog"
  },
  {
    "title": "LeMaterial: an open source initiative to accelerate materials discovery and research",
    "summary": "",
    "url": "https://huggingface.co/blog/lematerial",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face models in Amazon Bedrock",
    "summary": "",
    "url": "https://huggingface.co/blog/bedrock-marketplace",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open Preference Dataset for Text-to-Image Generation by the 🤗 Community",
    "summary": "",
    "url": "https://huggingface.co/blog/image-preferences",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome PaliGemma 2 – New vision language models by Google",
    "summary": "",
    "url": "https://huggingface.co/blog/paligemma2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How good are LLMs at fixing their mistakes? A chatbot arena experiment with Keras and TPUs",
    "summary": "",
    "url": "https://huggingface.co/blog/keras-chatbot-arena",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Rethinking LLM Evaluation with 3C3H: AraGen Benchmark and Leaderboard",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-3c3h-aragen",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Investing in Performance: Fine-tune small models with LLM insights  - a CFM case study",
    "summary": "",
    "url": "https://huggingface.co/blog/cfm-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open Source Developers Guide to the EU AI Act",
    "summary": "",
    "url": "https://huggingface.co/blog/eu-ai-act-for-oss-developers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Rearchitecting Hugging Face Uploads and Downloads",
    "summary": "",
    "url": "https://huggingface.co/blog/rearchitecting-uploads-and-downloads",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SmolVLM - small yet mighty Vision Language Model",
    "summary": "",
    "url": "https://huggingface.co/blog/smolvlm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "You could have designed state of the art positional encoding",
    "summary": "",
    "url": "https://huggingface.co/blog/designing-positional-encoding",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Letting Large Models Debate: The First Multilingual LLM Debate Competition",
    "summary": "",
    "url": "https://huggingface.co/blog/debate",
    "source": "Hugging Face Blog"
  },
  {
    "title": "From Files to Chunks: Improving HF Storage Efficiency",
    "summary": "",
    "url": "https://huggingface.co/blog/from-files-to-chunks",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Faster Text Generation with Self-Speculative Decoding",
    "summary": "",
    "url": "https://huggingface.co/blog/layerskip",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Open Leaderboard for Japanese LLMs!",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-japanese",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Judge Arena: Benchmarking LLMs as Evaluators",
    "summary": "",
    "url": "https://huggingface.co/blog/arena-atla",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Share your open ML datasets on Hugging Face Hub!",
    "summary": "",
    "url": "https://huggingface.co/blog/researcher-dataset-sharing",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face + PyCharm",
    "summary": "",
    "url": "https://huggingface.co/blog/pycharm-integration",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Argilla 2.4: Easily Build Fine-Tuning and Evaluation Datasets on the Hub — No Code Required",
    "summary": "",
    "url": "https://huggingface.co/blog/argilla-ui-hub",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Universal Assisted Generation: Faster Decoding with Any Assistant Model",
    "summary": "",
    "url": "https://huggingface.co/blog/universal_assisted_generation",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Expert Support case study: Bolstering a RAG app with LLM-as-a-Judge",
    "summary": "",
    "url": "https://huggingface.co/blog/digital-green-llm-judge",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A Deepdive into Aya Expanse: Advancing the Frontier of Multilinguality",
    "summary": "",
    "url": "https://huggingface.co/blog/aya-expanse",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing SynthID Text",
    "summary": "",
    "url": "https://huggingface.co/blog/synthid-text",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing HUGS - Scale your AI with Open Models",
    "summary": "",
    "url": "https://huggingface.co/blog/hugs",
    "source": "Hugging Face Blog"
  },
  {
    "title": "CinePile 2.0 - making stronger datasets with adversarial refinement",
    "summary": "",
    "url": "https://huggingface.co/blog/cinepile2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face Teams Up with Protect AI: Enhancing Model Security for the ML Community",
    "summary": "",
    "url": "https://huggingface.co/blog/protectai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Transformers.js v3: WebGPU Support, New Models & Tasks, and More…",
    "summary": "",
    "url": "https://huggingface.co/blog/transformersjs-v3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Diffusers welcomes Stable Diffusion 3.5 Large",
    "summary": "",
    "url": "https://huggingface.co/blog/sd3-5",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Releasing Outlines-core 0.1.0: structured generation in Rust and Python",
    "summary": "",
    "url": "https://huggingface.co/blog/outlines-core",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploying Speech-to-Speech on Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/s2s_endpoint",
    "source": "Hugging Face Blog"
  },
  {
    "title": "“Llama 3.2 in Keras”",
    "summary": "",
    "url": "https://huggingface.co/blog/keras-llama-32",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fixing Gradient Accumulation",
    "summary": "",
    "url": "https://huggingface.co/blog/gradient_accumulation",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the AMD 5th Gen EPYC™ CPU",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface-amd-turin",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A Security Review of Gradio 5",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-5-security",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome, Gradio 5",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-5",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Scaling AI-based Data Processing with Hugging Face + Dask",
    "summary": "",
    "url": "https://huggingface.co/blog/dask-scaling",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Faster Assisted Generation with Dynamic Speculation",
    "summary": "",
    "url": "https://huggingface.co/blog/dynamic_speculation_lookahead",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Improving Parquet Dedupe on Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/improve_parquet_dedupe",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Open FinLLM Leaderboard",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-finbench",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A Short Summary of Chinese AI Global Expansion",
    "summary": "",
    "url": "https://huggingface.co/blog/chinese-ai-expansion",
    "source": "Hugging Face Blog"
  },
  {
    "title": "🇨🇿 BenCzechMark - Can your LLM Understand Czech?",
    "summary": "",
    "url": "https://huggingface.co/blog/benczechmark",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Converting Vertex-Colored Meshes to Textured Meshes",
    "summary": "",
    "url": "https://huggingface.co/blog/vertex-colored-to-textured-mesh",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Llama can now see and run on your device - welcome Llama 3.2",
    "summary": "",
    "url": "https://huggingface.co/blog/llama32",
    "source": "Hugging Face Blog"
  },
  {
    "title": "FineVideo: behind the scenes",
    "summary": "",
    "url": "https://huggingface.co/blog/fine-video",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Exploring the Daily Papers Page on Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/daily-papers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Optimize and deploy with Optimum-Intel and OpenVINO GenAI",
    "summary": "",
    "url": "https://huggingface.co/blog/deploy-with-openvino",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-tuning LLMs to 1.58bit: extreme quantization made easy",
    "summary": "",
    "url": "https://huggingface.co/blog/1_58_llm_extreme_quantization",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the SQL Console on Datasets",
    "summary": "",
    "url": "https://huggingface.co/blog/sql-console",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Community Tools on HuggingChat",
    "summary": "",
    "url": "https://huggingface.co/blog/community-tools",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerate 1.0.0",
    "summary": "",
    "url": "https://huggingface.co/blog/accelerate-v1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face partners with TruffleHog to Scan for Secrets",
    "summary": "",
    "url": "https://huggingface.co/blog/trufflesecurity-partnership",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Scaling robotics datasets with video encoding",
    "summary": "",
    "url": "https://huggingface.co/blog/video-encoding",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The 5 Most Under-Rated Tools on Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/unsung-heroes",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Improving Hugging Face Training Efficiency Through Packing with Flash Attention 2",
    "summary": "",
    "url": "https://huggingface.co/blog/packing-with-FA2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploy Meta Llama 3.1 405B on Google Cloud Vertex AI",
    "summary": "",
    "url": "https://huggingface.co/blog/llama31-on-vertex-ai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A failed experiment: Infini-Attention, and why we should keep trying?",
    "summary": "",
    "url": "https://huggingface.co/blog/infini-attention",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introduction to ggml",
    "summary": "",
    "url": "https://huggingface.co/blog/introduction-to-ggml",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome Falcon Mamba: The first strong attention-free 7B model",
    "summary": "",
    "url": "https://huggingface.co/blog/falconmamba",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Tool Use, Unified",
    "summary": "",
    "url": "https://huggingface.co/blog/unified-tool-use",
    "source": "Hugging Face Blog"
  },
  {
    "title": "XetHub is joining Hugging Face!",
    "summary": "",
    "url": "https://huggingface.co/blog/xethub-joins-hf",
    "source": "Hugging Face Blog"
  },
  {
    "title": "2024 Security Feature Highlights",
    "summary": "",
    "url": "https://huggingface.co/blog/2024-security-features",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing TextImage Augmentation for Document Images",
    "summary": "",
    "url": "https://huggingface.co/blog/doc_aug_hf_alb",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Google releases Gemma 2 2B, ShieldGemma and Gemma Scope",
    "summary": "",
    "url": "https://huggingface.co/blog/gemma-july-update",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Memory-efficient Diffusion Transformers with Quanto and Diffusers",
    "summary": "",
    "url": "https://huggingface.co/blog/quanto-diffusers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Serverless Inference with Hugging Face and NVIDIA NIM",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-dgx-cloud",
    "source": "Hugging Face Blog"
  },
  {
    "title": "LAVE: Zero-shot VQA Evaluation on Docmatix with LLMs - Do We Still Need Fine-Tuning?",
    "summary": "",
    "url": "https://huggingface.co/blog/zero-shot-vqa-docmatix",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Llama 3.1 - 405B, 70B & 8B with multilinguality and long context",
    "summary": "",
    "url": "https://huggingface.co/blog/llama31",
    "source": "Hugging Face Blog"
  },
  {
    "title": "WWDC 24: Running Mistral 7B with Core ML",
    "summary": "",
    "url": "https://huggingface.co/blog/mistral-coreml",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Docmatix - a huge dataset for Document Visual Question Answering",
    "summary": "",
    "url": "https://huggingface.co/blog/docmatix",
    "source": "Hugging Face Blog"
  },
  {
    "title": "TGI Multi-LoRA: Deploy Once, Serve 30 Models",
    "summary": "",
    "url": "https://huggingface.co/blog/multi-lora-serving",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SmolLM - blazingly fast and remarkably powerful",
    "summary": "",
    "url": "https://huggingface.co/blog/smollm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How we leveraged distilabel to create an Argilla 2.0 Chatbot",
    "summary": "",
    "url": "https://huggingface.co/blog/argilla-chatbot",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How NuminaMath Won the 1st AIMO Progress Prize",
    "summary": "",
    "url": "https://huggingface.co/blog/winning-aimo-progress-prize",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Announcing New Hugging Face and KerasHub integration",
    "summary": "",
    "url": "https://huggingface.co/blog/keras-hub-integration",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Experimenting with Automatic PII Detection on the Hub using Presidio",
    "summary": "",
    "url": "https://huggingface.co/blog/presidio-pii-detection",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Preference Optimization for Vision Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/dpo_vlm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Google Cloud TPUs made available to Hugging Face users",
    "summary": "",
    "url": "https://huggingface.co/blog/tpu-inference-endpoints-spaces",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Banque des Territoires (CDC Group) x Polyconseil x Hugging Face: Enhancing a Major French Environmental Program with a Sovereign Data Solution",
    "summary": "",
    "url": "https://huggingface.co/blog/sovereign-data-solution-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Announcing New Dataset Search Features",
    "summary": "",
    "url": "https://huggingface.co/blog/datasets-filters",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerating Protein Language Model ProtST on Intel Gaudi 2",
    "summary": "",
    "url": "https://huggingface.co/blog/intel-protein-language-model-protst",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Our Transformers Code Agent beats the GAIA benchmark 🏅",
    "summary": "",
    "url": "https://huggingface.co/blog/beating-gaia",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome Gemma 2 - Google’s new open LLM",
    "summary": "",
    "url": "https://huggingface.co/blog/gemma2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "XLSCOUT Unveils ParaEmbed 2.0: a Powerful Embedding Model Tailored for Patents and IP with Expert Support from Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/xlscout-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-tuning Florence-2 - Microsoft's Cutting-edge Vision Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/finetune-florence2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Ethics and Society Newsletter #6: Building Better AI: The Importance of Data Quality",
    "summary": "",
    "url": "https://huggingface.co/blog/ethics-soc-6",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Data Is Better Together: A Look Back and Forward",
    "summary": "",
    "url": "https://huggingface.co/blog/dibt",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Going multimodal: How Prezi is leveraging the Hub and the Expert Support Program to accelerate their ML roadmap",
    "summary": "",
    "url": "https://huggingface.co/blog/prezi-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "BigCodeBench: The Next Generation of HumanEval",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-bigcodebench",
    "source": "Hugging Face Blog"
  },
  {
    "title": "From DeepSpeed to FSDP and Back Again with Hugging Face Accelerate",
    "summary": "",
    "url": "https://huggingface.co/blog/deepspeed-to-fsdp-and-back",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Diffusers welcomes Stable Diffusion 3",
    "summary": "",
    "url": "https://huggingface.co/blog/sd3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Putting RL back in RLHF",
    "summary": "",
    "url": "https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Making sense of this mess",
    "summary": "",
    "url": "https://huggingface.co/blog/transformers-docs-redesign",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Hugging Face Embedding Container for Amazon SageMaker",
    "summary": "",
    "url": "https://huggingface.co/blog/sagemaker-huggingface-embedding",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Launching the Artificial Analysis Text to Image Leaderboard & Arena",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-artificial-analysis2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing NPC-Playground, a 3D playground to interact with LLM-powered NPCs",
    "summary": "",
    "url": "https://huggingface.co/blog/npc-gigax-cubzh",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Faster assisted generation support for Intel Gaudi",
    "summary": "",
    "url": "https://huggingface.co/blog/assisted-generation-support-gaudi",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Space secrets security update",
    "summary": "",
    "url": "https://huggingface.co/blog/space-secrets-disclosure",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Benchmarking Text Generation Inference",
    "summary": "",
    "url": "https://huggingface.co/blog/tgi-benchmarking",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Training and Finetuning Embedding Models with Sentence Transformers v3",
    "summary": "",
    "url": "https://huggingface.co/blog/train-sentence-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages",
    "summary": "",
    "url": "https://huggingface.co/blog/falcon2-11b",
    "source": "Hugging Face Blog"
  },
  {
    "title": "CyberSecEval 2 - A Comprehensive Evaluation Framework for Cybersecurity Risks and Capabilities of Large Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-llamaguard",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploy models on AWS Inferentia2 from Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/inferentia-inference-endpoints",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Spaces Dev Mode for a seamless developer experience",
    "summary": "",
    "url": "https://huggingface.co/blog/spaces-dev-mode",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Build AI on premise with Dell Enterprise Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/dell-enterprise-hub",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face on AMD Instinct MI300 GPU",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface-amd-mi300",
    "source": "Hugging Face Blog"
  },
  {
    "title": "From cloud to developers: Hugging Face and Microsoft Deepen Collaboration",
    "summary": "",
    "url": "https://huggingface.co/blog/microsoft-collaboration",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Unlocking Longer Generation with Key-Value Cache Quantization",
    "summary": "",
    "url": "https://huggingface.co/blog/kv-cache-quantization",
    "source": "Hugging Face Blog"
  },
  {
    "title": "PaliGemma – Google's Cutting-Edge Open Vision Language Model",
    "summary": "",
    "url": "https://huggingface.co/blog/paligemma",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face x LangChain : A new partner package",
    "summary": "",
    "url": "https://huggingface.co/blog/langchain",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Open Arabic LLM Leaderboard",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-arabic",
    "source": "Hugging Face Blog"
  },
  {
    "title": "License to Call: Introducing Transformers Agents 2.0",
    "summary": "",
    "url": "https://huggingface.co/blog/agents",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Subscribe to Enterprise Hub with your AWS Account",
    "summary": "",
    "url": "https://huggingface.co/blog/enterprise-hub-aws-marketplace",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Building Cost-Efficient Enterprise RAG applications with Intel Gaudi 2 and Intel Xeon",
    "summary": "",
    "url": "https://huggingface.co/blog/cost-efficient-rag-applications-with-intel",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Open Leaderboard for Hebrew LLMs!",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-hebrew",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Bringing the Artificial Analysis LLM Performance Leaderboard to Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-artificial-analysis",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Powerful ASR + diarization + speculative decoding with Hugging Face Inference Endpoints",
    "summary": "",
    "url": "https://huggingface.co/blog/asr-diarization",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Improving Prompt Consistency with Structured Generations",
    "summary": "",
    "url": "https://huggingface.co/blog/evaluation-structured-outputs",
    "source": "Hugging Face Blog"
  },
  {
    "title": "StarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for Code Generation",
    "summary": "",
    "url": "https://huggingface.co/blog/sc2-instruct",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Open Chain of Thought Leaderboard",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-cot",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent",
    "summary": "",
    "url": "https://huggingface.co/blog/jat",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Open Medical-LLM Leaderboard: Benchmarking Large Language Models in Healthcare",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-medicalllm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome Llama 3 - Meta's new open LLM",
    "summary": "",
    "url": "https://huggingface.co/blog/llama3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AI Apps in a Flash with Gradio's Reload Mode",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-reload",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the LiveCodeBench Leaderboard - Holistic and Contamination-Free Evaluation of Code LLMs",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-livecodebench",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Running Privacy-Preserving Inferences on Hugging Face Endpoints",
    "summary": "",
    "url": "https://huggingface.co/blog/fhe-endpoints",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Ryght’s Journey to Empower Healthcare and Life Sciences with Expert Support from Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/ryght-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Idefics2: A Powerful 8B Vision-Language Model for the community",
    "summary": "",
    "url": "https://huggingface.co/blog/idefics2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Vision Language Models Explained",
    "summary": "",
    "url": "https://huggingface.co/blog/vlms",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Making thousands of open LLMs bloom in the Vertex AI Model Garden",
    "summary": "",
    "url": "https://huggingface.co/blog/google-cloud-model-garden",
    "source": "Hugging Face Blog"
  },
  {
    "title": "CodeGemma - an official Google release for code LLMs",
    "summary": "",
    "url": "https://huggingface.co/blog/codegemma",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Public Policy at Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/policy-blog",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face partners with Wiz Research to Improve AI Security",
    "summary": "",
    "url": "https://huggingface.co/blog/hugging-face-wiz-security-blog",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Text2SQL using Hugging Face Dataset Viewer API and Motherduck DuckDB-NSQL-7B",
    "summary": "",
    "url": "https://huggingface.co/blog/duckdb-nsql-7b",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Blazing Fast SetFit Inference with 🤗 Optimum Intel on Xeon",
    "summary": "",
    "url": "https://huggingface.co/blog/setfit-optimum-intel",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Bringing serverless GPU inference to Hugging Face users",
    "summary": "",
    "url": "https://huggingface.co/blog/cloudflare-workers-ai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Pollen-Vision: Unified interface for Zero-Shot vision models in robotics",
    "summary": "",
    "url": "https://huggingface.co/blog/pollen-vision",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Total noob’s intro to Hugging Face Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/noob_intro_transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Binary and Scalar Embedding Quantization for Significantly Faster & Cheaper Retrieval",
    "summary": "",
    "url": "https://huggingface.co/blog/embedding-quantization",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Chatbot Guardrails Arena",
    "summary": "",
    "url": "https://huggingface.co/blog/arena-lighthouz",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A Chatbot on your Laptop: Phi-2 on Intel Meteor Lake",
    "summary": "",
    "url": "https://huggingface.co/blog/phi2-intel-meteor-lake",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Cosmopedia: how to create large-scale synthetic data for pre-training Large Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/cosmopedia",
    "source": "Hugging Face Blog"
  },
  {
    "title": "GaLore: Advancing Large Model Training on Consumer-grade Hardware",
    "summary": "",
    "url": "https://huggingface.co/blog/galore",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Easily Train Models with H100 GPUs on NVIDIA DGX Cloud",
    "summary": "",
    "url": "https://huggingface.co/blog/train-dgx-cloud",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Quanto: a PyTorch quantization backend for Optimum",
    "summary": "",
    "url": "https://huggingface.co/blog/quanto-introduction",
    "source": "Hugging Face Blog"
  },
  {
    "title": "CPU Optimized Embeddings with 🤗 Optimum Intel and fastRAG",
    "summary": "",
    "url": "https://huggingface.co/blog/intel-fast-embedding",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset",
    "summary": "",
    "url": "https://huggingface.co/blog/websight",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing ConTextual: How well can your Multimodal model jointly reason over text and image in text-rich scenes?",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-contextual",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Data is better together: Enabling communities to collectively build better datasets together using Argilla and Hugging Face Spaces",
    "summary": "",
    "url": "https://huggingface.co/blog/community-datasets",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Text-Generation Pipeline on Intel® Gaudi® 2 AI Accelerator",
    "summary": "",
    "url": "https://huggingface.co/blog/textgen-pipe-gaudi",
    "source": "Hugging Face Blog"
  },
  {
    "title": "StarCoder2 and The Stack v2",
    "summary": "",
    "url": "https://huggingface.co/blog/starcoder2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "TTS Arena: Benchmarking Text-to-Speech Models in the Wild",
    "summary": "",
    "url": "https://huggingface.co/blog/arena-tts",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AI Watermarking 101: Tools and Techniques",
    "summary": "",
    "url": "https://huggingface.co/blog/watermarking",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-Tuning Gemma Models in Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/gemma-peft",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Red-Teaming Resistance Leaderboard",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-haizelab",
    "source": "Hugging Face Blog"
  },
  {
    "title": "🪆 Introduction to Matryoshka Embedding Models",
    "summary": "",
    "url": "https://huggingface.co/blog/matryoshka",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome Gemma - Google’s new open LLM",
    "summary": "",
    "url": "https://huggingface.co/blog/gemma",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Open Ko-LLM Leaderboard: Leading the Korean LLM Evaluation Ecosystem",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-upstage",
    "source": "Hugging Face Blog"
  },
  {
    "title": "🤗 PEFT welcomes new merging methods",
    "summary": "",
    "url": "https://huggingface.co/blog/peft_merging",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Synthetic data: save money, time and carbon with open source",
    "summary": "",
    "url": "https://huggingface.co/blog/synthetic-data-save-costs",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AMD Pervasive AI Developer Contest!",
    "summary": "",
    "url": "https://huggingface.co/blog/amd_pervasive_developer_ai_contest",
    "source": "Hugging Face Blog"
  },
  {
    "title": "From OpenAI to Open LLMs with Messages API on Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/tgi-messages-api",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SegMoE: Segmind Mixture of Diffusion Experts",
    "summary": "",
    "url": "https://huggingface.co/blog/segmoe",
    "source": "Hugging Face Blog"
  },
  {
    "title": "NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexity Classes and Dynamic Updates",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-nphardeval",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Constitutional AI with Open LLMs",
    "summary": "",
    "url": "https://huggingface.co/blog/constitutional_ai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face Text Generation Inference available for AWS Inferentia2",
    "summary": "",
    "url": "https://huggingface.co/blog/text-generation-inference-on-inferentia2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Patch Time Series Transformer in Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/patchtst",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Enterprise Scenarios Leaderboard: a Leaderboard for Real World Use Cases",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-patronus",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerate StarCoder with 🤗 Optimum Intel on Xeon: Q8/Q4 and Speculative Decoding",
    "summary": "",
    "url": "https://huggingface.co/blog/intel-starcoder-quantization",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Hallucinations Leaderboard, an Open Effort to Measure Hallucinations in Large Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-hallucinations",
    "source": "Hugging Face Blog"
  },
  {
    "title": "An Introduction to AI Secure LLM Safety Leaderboard",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-decodingtrust",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face and Google partner for open AI collaboration",
    "summary": "",
    "url": "https://huggingface.co/blog/gcp-partnership",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open-source LLMs as LangChain Agents",
    "summary": "",
    "url": "https://huggingface.co/blog/open-source-llms-as-agents",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-Tune W2V2-Bert for low-resource ASR with 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/fine-tune-w2v2-bert",
    "source": "Hugging Face Blog"
  },
  {
    "title": "PatchTSMixer in HuggingFace",
    "summary": "",
    "url": "https://huggingface.co/blog/patchtsmixer",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Preference Tuning LLMs with Direct Preference Optimization Methods",
    "summary": "",
    "url": "https://huggingface.co/blog/pref-tuning",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerating SD Turbo and SDXL Turbo Inference with ONNX Runtime and Olive",
    "summary": "",
    "url": "https://huggingface.co/blog/sdxl_ort_inference",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Run ComfyUI workflows for free with Gradio on Hugging Face Spaces",
    "summary": "",
    "url": "https://huggingface.co/blog/run-comfyui-workflows-on-spaces",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A guide to setting up your own Hugging Face leaderboard: an end-to-end example with Vectara's hallucination leaderboard",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-vectara",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Make LLM Fine-tuning 2x faster with Unsloth and 🤗 TRL",
    "summary": "",
    "url": "https://huggingface.co/blog/unsloth-trl",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome aMUSEd: Efficient Text-to-Image Generation",
    "summary": "",
    "url": "https://huggingface.co/blog/amused",
    "source": "Hugging Face Blog"
  },
  {
    "title": "LoRA training scripts of the world, unite!",
    "summary": "",
    "url": "https://huggingface.co/blog/sdxl_lora_advanced_script",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Speculative Decoding for 2x Faster Whisper Inference",
    "summary": "",
    "url": "https://huggingface.co/blog/whisper-speculative-decoding",
    "source": "Hugging Face Blog"
  },
  {
    "title": "2023, year of open LLMs",
    "summary": "",
    "url": "https://huggingface.co/blog/2023-in-llms",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome Mixtral - a SOTA Mixture of Experts on Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/mixtral",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Mixture of Experts Explained",
    "summary": "",
    "url": "https://huggingface.co/blog/moe",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SetFitABSA: Few-Shot Aspect Based Sentiment Analysis using SetFit",
    "summary": "",
    "url": "https://huggingface.co/blog/setfit-absa",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AMD + 🤗: Large Language Models Out-of-the-Box Acceleration with AMD GPU",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface-and-optimum-amd",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Optimum-NVIDIA Unlocking blazingly fast LLM inference in just 1 line of code",
    "summary": "",
    "url": "https://huggingface.co/blog/optimum-nvidia",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Goodbye cold boot - how we made LoRA Inference 300% faster",
    "summary": "",
    "url": "https://huggingface.co/blog/lora-adapters-dynamic-loading",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open LLM Leaderboard: DROP deep dive",
    "summary": "",
    "url": "https://huggingface.co/blog/open-llm-leaderboard-drop",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SDXL in 4 steps with Latent Consistency LoRAs",
    "summary": "",
    "url": "https://huggingface.co/blog/lcm_lora",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Make your llama generation time fly with AWS Inferentia2",
    "summary": "",
    "url": "https://huggingface.co/blog/inferentia-llama2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Prodigy-HF: a direct integration with Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/prodigy-hf",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tweets Analysis with Lora",
    "summary": "",
    "url": "https://huggingface.co/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Storage Regions on the HF Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/regions",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Personal Copilot: Train Your Own Coding Assistant",
    "summary": "",
    "url": "https://huggingface.co/blog/personal-copilot",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Interactively explore your Huggingface dataset with one line of code",
    "summary": "",
    "url": "https://huggingface.co/blog/scalable-data-inspection",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploy Embedding Models with Hugging Face Inference Endpoints",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-endpoints-embeddings",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The N Implementation Details of RLHF with PPO",
    "summary": "",
    "url": "https://huggingface.co/blog/the_n_implementation_details_of_rlhf_with_ppo",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Exploring simple optimizations for SDXL",
    "summary": "",
    "url": "https://huggingface.co/blog/simple_sdxl_optimizations",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Gradio-Lite: Serverless Gradio Running Entirely in Your Browser",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-lite",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerating over 130,000 Hugging Face models with ONNX Runtime",
    "summary": "",
    "url": "https://huggingface.co/blog/ort-accelerating-hf-models",
    "source": "Hugging Face Blog"
  },
  {
    "title": "🧨 Accelerating Stable Diffusion XL Inference with JAX on Cloud TPU v5e",
    "summary": "",
    "url": "https://huggingface.co/blog/sdxl_jax",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Chat Templates: An End to the Silent Performance Killer",
    "summary": "",
    "url": "https://huggingface.co/blog/chat-templates",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploying the AI Comic Factory using the Inference API",
    "summary": "",
    "url": "https://huggingface.co/blog/ai-comic-factory",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Ethics and Society Newsletter #5: Hugging Face Goes To Washington and Other Summer 2023 Musings",
    "summary": "",
    "url": "https://huggingface.co/blog/ethics-soc-5",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Finetune Stable Diffusion Models with DDPO via TRL",
    "summary": "",
    "url": "https://huggingface.co/blog/trl-ddpo",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Non-engineers guide: Train a LLaMA 2 chatbot",
    "summary": "",
    "url": "https://huggingface.co/blog/Llama2-for-non-engineers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Llama 2 on Amazon SageMaker a Benchmark",
    "summary": "",
    "url": "https://huggingface.co/blog/llama-sagemaker-benchmark",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Inference for PROs",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-pro",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Rocket Money x Hugging Face: Scaling Volatile ML Models in Production​",
    "summary": "",
    "url": "https://huggingface.co/blog/rocketmoney-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introduction to 3D Gaussian Splatting",
    "summary": "",
    "url": "https://huggingface.co/blog/gaussian-splatting",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Object Detection Leaderboard",
    "summary": "",
    "url": "https://huggingface.co/blog/object-detection-leaderboard",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Optimizing your LLM in production",
    "summary": "",
    "url": "https://huggingface.co/blog/optimize-llm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Würstchen: Fast Diffusion for Image Generation",
    "summary": "",
    "url": "https://huggingface.co/blog/wuerstchen",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-tuning Llama 2 70B using PyTorch FSDP",
    "summary": "",
    "url": "https://huggingface.co/blog/ram-efficient-pytorch-fsdp",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Overview of natively supported quantization schemes in 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/overview-quantization-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SafeCoder vs. Closed-source Code Assistants",
    "summary": "",
    "url": "https://huggingface.co/blog/safecoder-vs-closed-source-code-assistants",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Efficient Controllable Generation for SDXL with T2I-Adapters",
    "summary": "",
    "url": "https://huggingface.co/blog/t2i-sdxl-adapters",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Spread Your Wings: Falcon 180B is here",
    "summary": "",
    "url": "https://huggingface.co/blog/falcon-180b",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fetch Cuts ML Processing Latency by 50% Using Amazon SageMaker & Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/fetch-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AudioLDM 2, but faster ⚡️",
    "summary": "",
    "url": "https://huggingface.co/blog/audioldm2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Code Llama: Llama 2 learns to code",
    "summary": "",
    "url": "https://huggingface.co/blog/codellama",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deprecation of Git Authentication using password",
    "summary": "",
    "url": "https://huggingface.co/blog/password-git-deprecation",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Making LLMs lighter with AutoGPTQ and transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/gptq-integration",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing SafeCoder",
    "summary": "",
    "url": "https://huggingface.co/blog/safecoder",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing IDEFICS: An Open Reproduction of State-of-the-art Visual Langage Model",
    "summary": "",
    "url": "https://huggingface.co/blog/idefics",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face Hub on the AWS Marketplace: Pay with your AWS Account",
    "summary": "",
    "url": "https://huggingface.co/blog/aws-marketplace",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Optimizing Bark using 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/optimizing-bark",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploying Hugging Face Models with BentoML: DeepFloyd IF in Action",
    "summary": "",
    "url": "https://huggingface.co/blog/deploy-deepfloydif-using-bentoml",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-tune Llama 2 with DPO",
    "summary": "",
    "url": "https://huggingface.co/blog/dpo-trl",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Releasing Swift Transformers: Run On-Device LLMs in Apple Devices",
    "summary": "",
    "url": "https://huggingface.co/blog/swift-coreml-llm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploy MusicGen in no time with Inference Endpoints",
    "summary": "",
    "url": "https://huggingface.co/blog/run-musicgen-as-an-api",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/huggy-lingo",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Towards Encrypted Large Language Models with FHE",
    "summary": "",
    "url": "https://huggingface.co/blog/encrypted-llm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Practical 3D Asset Generation: A Step-by-Step Guide",
    "summary": "",
    "url": "https://huggingface.co/blog/3d-assets",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open-sourcing Knowledge Distillation Code and Weights of SD-Small and SD-Tiny",
    "summary": "",
    "url": "https://huggingface.co/blog/sd_distillation",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Stable Diffusion XL on Mac with Advanced Core ML Quantization",
    "summary": "",
    "url": "https://huggingface.co/blog/stable-diffusion-xl-coreml",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AI Policy @🤗: Open ML Considerations in the EU AI Act",
    "summary": "",
    "url": "https://huggingface.co/blog/eu-ai-act-oss",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Agents.js: Give tools to your LLMs using JavaScript",
    "summary": "",
    "url": "https://huggingface.co/blog/agents-js",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Results of the Open Source AI Game Jam",
    "summary": "",
    "url": "https://huggingface.co/blog/game-jam-first-edition-results",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Happy 1st anniversary 🤗 Diffusers!",
    "summary": "",
    "url": "https://huggingface.co/blog/diffusers-turns-1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Llama 2 is here - get it on Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/llama2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Building an AI WebTV",
    "summary": "",
    "url": "https://huggingface.co/blog/ai-webtv",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open-Source Text Generation & LLM Ecosystem at Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/os-llms",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-tuning Stable Diffusion models on Intel CPUs",
    "summary": "",
    "url": "https://huggingface.co/blog/stable-diffusion-finetuning-intel",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Making ML-powered web games with Transformers.js",
    "summary": "",
    "url": "https://huggingface.co/blog/ml-web-games",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploy LLMs with Hugging Face Inference Endpoints",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-endpoints-llm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Making a web app generator with open ML models",
    "summary": "",
    "url": "https://huggingface.co/blog/text-to-webapp",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Leveraging Hugging Face for complex generative AI use cases",
    "summary": "",
    "url": "https://huggingface.co/blog/writer-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerating Vision-Language Models: BridgeTower on Habana Gaudi2",
    "summary": "",
    "url": "https://huggingface.co/blog/bridgetower",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Ethics and Society Newsletter #4: Bias in Text-to-Image Models",
    "summary": "",
    "url": "https://huggingface.co/blog/ethics-soc-4",
    "source": "Hugging Face Blog"
  },
  {
    "title": "What's going on with the Open LLM Leaderboard?",
    "summary": "",
    "url": "https://huggingface.co/blog/open-llm-leaderboard-mmlu",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Panel on Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/panel-on-hugging-face",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-Tune MMS Adapter Models for low-resource ASR",
    "summary": "",
    "url": "https://huggingface.co/blog/mms_adapters",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer)",
    "summary": "",
    "url": "https://huggingface.co/blog/autoformer",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Faster Stable Diffusion with Core ML on iPhone, iPad, and Mac",
    "summary": "",
    "url": "https://huggingface.co/blog/fast-diffusers-coreml",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploy Livebook notebooks as apps to Hugging Face Spaces",
    "summary": "",
    "url": "https://huggingface.co/blog/livebook-app-deployment",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Announcing our new Content Guidelines and Policy",
    "summary": "",
    "url": "https://huggingface.co/blog/content-guidelines-update",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face and AMD partner on accelerating state-of-the-art models for CPU and GPU platforms",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface-and-amd",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Can foundation models label data like humans?",
    "summary": "",
    "url": "https://huggingface.co/blog/open-llm-leaderboard-rlhf",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Hugging Face Hub for Galleries, Libraries, Archives and Museums",
    "summary": "",
    "url": "https://huggingface.co/blog/hf-hub-glam-guide",
    "source": "Hugging Face Blog"
  },
  {
    "title": "DuckDB: analyze 50,000+ datasets stored on the Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/hub-duckdb",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome fastText to the Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/fasttext",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Falcon has landed in the Hugging Face ecosystem",
    "summary": "",
    "url": "https://huggingface.co/blog/falcon",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AI Speech Recognition in Unity",
    "summary": "",
    "url": "https://huggingface.co/blog/unity-asr",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Announcing the Open Source AI Game Jam 🎮",
    "summary": "",
    "url": "https://huggingface.co/blog/game-jam",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Hugging Face LLM Inference Container for Amazon SageMaker",
    "summary": "",
    "url": "https://huggingface.co/blog/sagemaker-huggingface-llm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing BERTopic Integration with the Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/bertopic",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Optimizing Stable Diffusion for Intel CPUs with NNCF and 🤗 Optimum",
    "summary": "",
    "url": "https://huggingface.co/blog/train-optimize-sd-intel",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA",
    "summary": "",
    "url": "https://huggingface.co/blog/4bit-transformers-bitsandbytes",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure",
    "summary": "",
    "url": "https://huggingface.co/blog/hugging-face-endpoints-on-azure",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face and IBM partner on watsonx.ai, the next-generation enterprise studio for AI builders",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface-and-ibm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "🐶Safetensors audited as really safe and becoming the default",
    "summary": "",
    "url": "https://huggingface.co/blog/safetensors-security-audit",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Instruction-tuning Stable Diffusion with InstructPix2Pix",
    "summary": "",
    "url": "https://huggingface.co/blog/instruction-tuning-sd",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Large-scale Near-deduplication Behind BigCode",
    "summary": "",
    "url": "https://huggingface.co/blog/dedup",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Smaller is better: Q8-Chat, an efficient generative AI experience on Xeon",
    "summary": "",
    "url": "https://huggingface.co/blog/generative-ai-models-on-intel-cpu",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face Selected for the French Data Protection Agency Enhanced Support Program",
    "summary": "",
    "url": "https://huggingface.co/blog/cnil",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Run a Chatgpt-like Chatbot on a Single GPU with ROCm",
    "summary": "",
    "url": "https://huggingface.co/blog/chatbot-amd-gpu",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing RWKV - An RNN with the advantages of a transformer",
    "summary": "",
    "url": "https://huggingface.co/blog/rwkv",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Assisted Generation: a new direction toward low-latency text generation",
    "summary": "",
    "url": "https://huggingface.co/blog/assisted-generation",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Creating a Coding Assistant with StarCoder",
    "summary": "",
    "url": "https://huggingface.co/blog/starchat-alpha",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A Dive into Text-to-Video Models",
    "summary": "",
    "url": "https://huggingface.co/blog/text-to-video",
    "source": "Hugging Face Blog"
  },
  {
    "title": "StarCoder: A State-of-the-Art LLM for Code",
    "summary": "",
    "url": "https://huggingface.co/blog/starcoder",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How to Install and Use the Hugging Face Unity API",
    "summary": "",
    "url": "https://huggingface.co/blog/unity-api",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Training a language model with 🤗 Transformers using TensorFlow and TPUs",
    "summary": "",
    "url": "https://huggingface.co/blog/tf_tpu",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Running IF with 🧨 diffusers on a Free Tier Google Colab",
    "summary": "",
    "url": "https://huggingface.co/blog/if",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Databricks ❤️ Hugging Face: up to 40% faster training and tuning of Large Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/databricks-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing HuggingFace blog for Chinese speakers: Fostering Collaboration with the Chinese AI community",
    "summary": "",
    "url": "https://huggingface.co/blog/chinese-language-blog",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How to host a Unity game in a Space",
    "summary": "",
    "url": "https://huggingface.co/blog/unity-in-spaces",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerating Hugging Face Transformers with AWS Inferentia2",
    "summary": "",
    "url": "https://huggingface.co/blog/accelerate-transformers-with-inferentia2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Graph Classification with Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/graphml-classification",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Creating Privacy Preserving AI with Substra",
    "summary": "",
    "url": "https://huggingface.co/blog/owkin-substra",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Snorkel AI x Hugging Face: unlock foundation models for enterprises",
    "summary": "",
    "url": "https://huggingface.co/blog/snorkel-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "StackLLaMA: A hands-on guide to train LLaMA with RLHF",
    "summary": "",
    "url": "https://huggingface.co/blog/stackllama",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Ethics and Society Newsletter #3: Ethical Openness at Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/ethics-soc-3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator",
    "summary": "",
    "url": "https://huggingface.co/blog/habana-gaudi-2-bloom",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerating Stable Diffusion Inference on Intel CPUs",
    "summary": "",
    "url": "https://huggingface.co/blog/stable-diffusion-inference-intel",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Federated Learning using Hugging Face and Flower",
    "summary": "",
    "url": "https://huggingface.co/blog/fl-with-flower",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Train your ControlNet with diffusers",
    "summary": "",
    "url": "https://huggingface.co/blog/train-your-controlnet",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Jupyter X Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/notebooks-hub",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Multivariate Probabilistic Time Series Forecasting with Informer",
    "summary": "",
    "url": "https://huggingface.co/blog/informer",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU",
    "summary": "",
    "url": "https://huggingface.co/blog/trl-peft",
    "source": "Hugging Face Blog"
  },
  {
    "title": "New ViT and ALIGN Models From Kakao Brain",
    "summary": "",
    "url": "https://huggingface.co/blog/vit-align",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Using Machine Learning to Aid Survivors and Race through Time",
    "summary": "",
    "url": "https://huggingface.co/blog/using-ml-for-disasters",
    "source": "Hugging Face Blog"
  },
  {
    "title": "ControlNet in 🧨 Diffusers",
    "summary": "",
    "url": "https://huggingface.co/blog/controlnet",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Ethical Guidelines for developing the Diffusers library",
    "summary": "",
    "url": "https://huggingface.co/blog/ethics-diffusers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How Hugging Face Accelerated Development of Witty Works Writing Assistant",
    "summary": "",
    "url": "https://huggingface.co/blog/classification-use-cases",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Red-Teaming Large Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/red-teaming",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Swift 🧨Diffusers - Fast Stable Diffusion for Mac",
    "summary": "",
    "url": "https://huggingface.co/blog/fast-mac-diffusers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fetch Consolidates AI Tools and Saves 30% Development Time with Hugging Face on AWS",
    "summary": "",
    "url": "https://huggingface.co/blog/fetch-eap-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face and AWS partner to make AI more accessible",
    "summary": "",
    "url": "https://huggingface.co/blog/aws-partnership",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Zero-shot image-to-text generation with BLIP-2",
    "summary": "",
    "url": "https://huggingface.co/blog/blip-2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Why we’re switching to Hugging Face Inference Endpoints, and maybe you should too",
    "summary": "",
    "url": "https://huggingface.co/blog/mantis-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Parameter-Efficient Fine-Tuning using 🤗 PEFT",
    "summary": "",
    "url": "https://huggingface.co/blog/peft",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Speech Synthesis, Recognition, and More With SpeechT5",
    "summary": "",
    "url": "https://huggingface.co/blog/speecht5",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Generating Stories: AI for Game Development #5",
    "summary": "",
    "url": "https://huggingface.co/blog/ml-for-games-5",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing ⚔️ AI vs. AI ⚔️ a deep reinforcement learning multi-agents competition system",
    "summary": "",
    "url": "https://huggingface.co/blog/aivsai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerating PyTorch Transformers with Intel Sapphire Rapids - part 2",
    "summary": "",
    "url": "https://huggingface.co/blog/intel-sapphire-rapids-inference",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A Dive into Vision-Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/vision_language_pretraining",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The State of Computer Vision at Hugging Face 🤗",
    "summary": "",
    "url": "https://huggingface.co/blog/cv_state",
    "source": "Hugging Face Blog"
  },
  {
    "title": "2D Asset Generation: AI for Game Development #4",
    "summary": "",
    "url": "https://huggingface.co/blog/ml-for-games-4",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Using LoRA for Efficient Stable Diffusion Fine-Tuning",
    "summary": "",
    "url": "https://huggingface.co/blog/lora",
    "source": "Hugging Face Blog"
  },
  {
    "title": "What Makes a Dialog Agent Useful?",
    "summary": "",
    "url": "https://huggingface.co/blog/dialog-agents",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Optimum+ONNX Runtime - Easier, Faster training for your Hugging Face models",
    "summary": "",
    "url": "https://huggingface.co/blog/optimum-onnxruntime-training",
    "source": "Hugging Face Blog"
  },
  {
    "title": "3D Asset Generation: AI for Game Development #3",
    "summary": "",
    "url": "https://huggingface.co/blog/ml-for-games-3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Universal Image Segmentation with Mask2Former and OneFormer",
    "summary": "",
    "url": "https://huggingface.co/blog/mask2former",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome PaddlePaddle to the Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/paddlepaddle",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Image Similarity with Hugging Face Datasets and Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/image-similarity",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AI for Game Development: Creating a Farming Game in 5 Days. Part 2",
    "summary": "",
    "url": "https://huggingface.co/blog/ml-for-games-2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introduction to Graph Machine Learning",
    "summary": "",
    "url": "https://huggingface.co/blog/intro-graphml",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Zero-shot image segmentation with CLIPSeg",
    "summary": "",
    "url": "https://huggingface.co/blog/clipseg-zero-shot",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Model Cards",
    "summary": "",
    "url": "https://huggingface.co/blog/model-cards",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Let's talk about biases in machine learning! Ethics and Society Newsletter #2",
    "summary": "",
    "url": "https://huggingface.co/blog/ethics-soc-2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A Complete Guide to Audio Datasets",
    "summary": "",
    "url": "https://huggingface.co/blog/audio-datasets",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Faster Training and Inference: Habana Gaudi®2 vs Nvidia A100 80GB",
    "summary": "",
    "url": "https://huggingface.co/blog/habana-gaudi-2-benchmark",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Illustrating Reinforcement Learning from Human Feedback (RLHF)",
    "summary": "",
    "url": "https://huggingface.co/blog/rlhf",
    "source": "Hugging Face Blog"
  },
  {
    "title": "From GPT2 to Stable Diffusion: Hugging Face arrives to the Elixir community",
    "summary": "",
    "url": "https://huggingface.co/blog/elixir-bumblebee",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deep Learning with Proteins",
    "summary": "",
    "url": "https://huggingface.co/blog/deep-learning-with-proteins",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Using Stable Diffusion with Core ML on Apple Silicon",
    "summary": "",
    "url": "https://huggingface.co/blog/diffusers-coreml",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Probabilistic Time Series Forecasting with 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/time-series-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "VQ-Diffusion",
    "summary": "",
    "url": "https://huggingface.co/blog/vq-diffusion",
    "source": "Hugging Face Blog"
  },
  {
    "title": "We are hiring interns!",
    "summary": "",
    "url": "https://huggingface.co/blog/interns-2023",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Diffusion Models Live Event",
    "summary": "",
    "url": "https://huggingface.co/blog/diffusion-models-event",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Director of Machine Learning Insights [Part 4]",
    "summary": "",
    "url": "https://huggingface.co/blog/ml-director-insights-4",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerating Document AI",
    "summary": "",
    "url": "https://huggingface.co/blog/document-ai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "An overview of inference solutions on Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-update",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face Machine Learning Demos on arXiv",
    "summary": "",
    "url": "https://huggingface.co/blog/arxiv",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Sentiment Analysis on Encrypted Data with Homomorphic Encryption",
    "summary": "",
    "url": "https://huggingface.co/blog/sentiment-analysis-fhe",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Generating Human-level Text with Contrastive Search in Transformers 🤗",
    "summary": "",
    "url": "https://huggingface.co/blog/introducing-csearch",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing our new pricing",
    "summary": "",
    "url": "https://huggingface.co/blog/pricing-update",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Training Stable Diffusion with Dreambooth using Diffusers",
    "summary": "",
    "url": "https://huggingface.co/blog/dreambooth",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-Tune Whisper For Multilingual ASR with 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/fine-tune-whisper",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerate your models with 🤗 Optimum Intel and OpenVINO",
    "summary": "",
    "url": "https://huggingface.co/blog/openvino",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Evaluating Language Model Bias with 🤗 Evaluate",
    "summary": "",
    "url": "https://huggingface.co/blog/evaluating-llm-bias",
    "source": "Hugging Face Blog"
  },
  {
    "title": "From PyTorch DDP to Accelerate to Trainer, mastery of distributed training with ease",
    "summary": "",
    "url": "https://huggingface.co/blog/pytorch-ddp-accelerate-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "MTEB: Massive Text Embedding Benchmark",
    "summary": "",
    "url": "https://huggingface.co/blog/mteb",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Getting Started with Hugging Face Inference Endpoints",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-endpoints",
    "source": "Hugging Face Blog"
  },
  {
    "title": "🧨 Stable Diffusion  in JAX / Flax !",
    "summary": "",
    "url": "https://huggingface.co/blog/stable_diffusion_jax",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Optimization story: Bloom inference",
    "summary": "",
    "url": "https://huggingface.co/blog/bloom-inference-optimization",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing DOI: the Digital Object Identifier to Datasets and Models",
    "summary": "",
    "url": "https://huggingface.co/blog/introducing-doi",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Japanese Stable Diffusion",
    "summary": "",
    "url": "https://huggingface.co/blog/japanese-stable-diffusion",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Very Large Language Models and How to Evaluate Them",
    "summary": "",
    "url": "https://huggingface.co/blog/zero-shot-eval-on-the-hub",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Image Classification with AutoTrain",
    "summary": "",
    "url": "https://huggingface.co/blog/autotrain-image-classification",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How 🤗 Accelerate runs very large models thanks to PyTorch",
    "summary": "",
    "url": "https://huggingface.co/blog/accelerate-large-models",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SetFit: Efficient Few-Shot Learning Without Prompts",
    "summary": "",
    "url": "https://huggingface.co/blog/setfit",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Ethics and Society Newsletter #1",
    "summary": "",
    "url": "https://huggingface.co/blog/ethics-soc-1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate",
    "summary": "",
    "url": "https://huggingface.co/blog/bloom-inference-pytorch-scripts",
    "source": "Hugging Face Blog"
  },
  {
    "title": "What's new in Diffusers? 🎨",
    "summary": "",
    "url": "https://huggingface.co/blog/diffusers-2nd-month",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Train your first Decision Transformer",
    "summary": "",
    "url": "https://huggingface.co/blog/train-decision-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How to train a Language Model with Megatron-LM",
    "summary": "",
    "url": "https://huggingface.co/blog/megatron-training",
    "source": "Hugging Face Blog"
  },
  {
    "title": "OpenRAIL: Towards open and responsible AI licensing frameworks",
    "summary": "",
    "url": "https://huggingface.co/blog/open_rail",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Visualize proteins on Hugging Face Spaces",
    "summary": "",
    "url": "https://huggingface.co/blog/spaces_3dmoljs",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Stable Diffusion with 🧨 Diffusers",
    "summary": "",
    "url": "https://huggingface.co/blog/stable_diffusion",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Pre-Train BERT with Hugging Face Transformers and Habana Gaudi",
    "summary": "",
    "url": "https://huggingface.co/blog/pretraining-bert",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploying 🤗 ViT on Vertex AI",
    "summary": "",
    "url": "https://huggingface.co/blog/deploy-vertex-ai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore",
    "summary": "",
    "url": "https://huggingface.co/blog/vision-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using transformers, accelerate and bitsandbytes",
    "summary": "",
    "url": "https://huggingface.co/blog/hf-bitsandbytes-integration",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Skops",
    "summary": "",
    "url": "https://huggingface.co/blog/skops",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face's TensorFlow Philosophy",
    "summary": "",
    "url": "https://huggingface.co/blog/tensorflow-philosophy",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploying 🤗 ViT on Kubernetes with TF Serving",
    "summary": "",
    "url": "https://huggingface.co/blog/deploy-tfserving-kubernetes",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Train and Fine-Tune Sentence Transformers Models",
    "summary": "",
    "url": "https://huggingface.co/blog/how-to-train-sentence-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Proximal Policy Optimization (PPO)",
    "summary": "",
    "url": "https://huggingface.co/blog/deep-rl-ppo",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Private Hub: A New Way to Build With Machine Learning",
    "summary": "",
    "url": "https://huggingface.co/blog/introducing-private-hub",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method",
    "summary": "",
    "url": "https://huggingface.co/blog/nystromformer",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Comments on U.S. National AI Research Resource Interim Report",
    "summary": "",
    "url": "https://huggingface.co/blog/us-national-ai-research-resource",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing new audio and vision documentation in 🤗 Datasets",
    "summary": "",
    "url": "https://huggingface.co/blog/datasets-docs-update",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Faster Text Generation with TensorFlow and XLA",
    "summary": "",
    "url": "https://huggingface.co/blog/tf-xla-generate",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploying TensorFlow Vision Models in Hugging Face with TF Serving",
    "summary": "",
    "url": "https://huggingface.co/blog/tf-serving-vision",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Advantage Actor Critic (A2C)",
    "summary": "",
    "url": "https://huggingface.co/blog/deep-rl-a2c",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How to train your model dynamically using adversarial data",
    "summary": "",
    "url": "https://huggingface.co/blog/mnist-adversarial",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Technology Behind BLOOM Training",
    "summary": "",
    "url": "https://huggingface.co/blog/bloom-megatron-deepspeed",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Building a Playlist Generator with Sentence Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/playlist-generator",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing The World's Largest Open Multilingual Language Model: BLOOM",
    "summary": "",
    "url": "https://huggingface.co/blog/bloom",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Getting Started with Sentiment Analysis on Twitter",
    "summary": "",
    "url": "https://huggingface.co/blog/sentiment-analysis-twitter",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Policy Gradient with PyTorch",
    "summary": "",
    "url": "https://huggingface.co/blog/deep-rl-pg",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Liftoff! How to get started with your first ML project 🚀",
    "summary": "",
    "url": "https://huggingface.co/blog/your-first-ml-project",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerate Large Model Training using DeepSpeed",
    "summary": "",
    "url": "https://huggingface.co/blog/accelerate-deepspeed",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Announcing Evaluation on the Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/eval-on-the-hub",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Getting Started With Embeddings",
    "summary": "",
    "url": "https://huggingface.co/blog/getting-started-with-embeddings",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Convert Transformers to ONNX with Hugging Face Optimum",
    "summary": "",
    "url": "https://huggingface.co/blog/convert-transformers-to-onnx",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration",
    "summary": "",
    "url": "https://huggingface.co/blog/intel",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Annotated Diffusion Model",
    "summary": "",
    "url": "https://huggingface.co/blog/annotated-diffusion",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deep Q-Learning with Space Invaders",
    "summary": "",
    "url": "https://huggingface.co/blog/deep-rl-dqn",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Graphcore and Hugging Face Launch New Lineup of IPU-Ready Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/graphcore-update",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Pull Requests and Discussions 🥳",
    "summary": "",
    "url": "https://huggingface.co/blog/community-update",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Efficient Table Pre-training without Real Data: An Introduction to TAPEX",
    "summary": "",
    "url": "https://huggingface.co/blog/tapex",
    "source": "Hugging Face Blog"
  },
  {
    "title": "An Introduction to Q-Learning Part 2/2",
    "summary": "",
    "url": "https://huggingface.co/blog/deep-rl-q-part2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How Sempre Health is leveraging the Expert Acceleration Program to accelerate their ML roadmap",
    "summary": "",
    "url": "https://huggingface.co/blog/sempre-health-eap-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Putting ethical principles at the core of the research lifecycle",
    "summary": "",
    "url": "https://huggingface.co/blog/ethical-charter-multimodal",
    "source": "Hugging Face Blog"
  },
  {
    "title": "An Introduction to Q-Learning Part 1",
    "summary": "",
    "url": "https://huggingface.co/blog/deep-rl-q-part1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Machine Learning Experts - Sasha Luccioni",
    "summary": "",
    "url": "https://huggingface.co/blog/sasha-luccioni-interview",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Announcing the Hugging Face Fellowship Program",
    "summary": "",
    "url": "https://huggingface.co/blog/fellowship",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Gradio 3.0 is Out!",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-blocks",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Student Ambassador Program’s call for applications is open!",
    "summary": "",
    "url": "https://huggingface.co/blog/ambassadors",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerated Inference with Optimum and Transformers Pipelines",
    "summary": "",
    "url": "https://huggingface.co/blog/optimum-inference",
    "source": "Hugging Face Blog"
  },
  {
    "title": "We Raised $100 Million for Open & Collaborative Machine Learning 🚀",
    "summary": "",
    "url": "https://huggingface.co/blog/series-c",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome fastai to the Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/fastai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "An Introduction to Deep Reinforcement Learning",
    "summary": "",
    "url": "https://huggingface.co/blog/deep-rl-intro",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel",
    "summary": "",
    "url": "https://huggingface.co/blog/pytorch-fsdp",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Opinion Classification with Kili and HuggingFace AutoTrain",
    "summary": "",
    "url": "https://huggingface.co/blog/opinion-classification-with-kili",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Director of Machine Learning Insights",
    "summary": "",
    "url": "https://huggingface.co/blog/ml-director-insights",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Getting Started with Transformers on Habana Gaudi",
    "summary": "",
    "url": "https://huggingface.co/blog/getting-started-habana",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Hugging Face for Education 🤗",
    "summary": "",
    "url": "https://huggingface.co/blog/education",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Supercharged Customer Service with Machine Learning",
    "summary": "",
    "url": "https://huggingface.co/blog/supercharge-customer-service-with-machine-learning",
    "source": "Hugging Face Blog"
  },
  {
    "title": "CO2 Emissions and the 🤗 Hub: Leading the Charge",
    "summary": "",
    "url": "https://huggingface.co/blog/carbon-emissions-on-the-hub",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Machine Learning Experts - Lewis Tunstall",
    "summary": "",
    "url": "https://huggingface.co/blog/lewis-tunstall-interview",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training",
    "summary": "",
    "url": "https://huggingface.co/blog/habana",
    "source": "Hugging Face Blog"
  },
  {
    "title": "~Don't~ Repeat Yourself",
    "summary": "",
    "url": "https://huggingface.co/blog/transformers-design-philosophy",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Decision Transformers on Hugging Face 🤗",
    "summary": "",
    "url": "https://huggingface.co/blog/decision-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Machine Learning Experts - Margaret Mitchell",
    "summary": "",
    "url": "https://huggingface.co/blog/meg-mitchell-interview",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Announcing the 🤗 AI Research Residency Program",
    "summary": "",
    "url": "https://huggingface.co/blog/ai-residency",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-Tune a Semantic Segmentation Model with a Custom Dataset",
    "summary": "",
    "url": "https://huggingface.co/blog/fine-tune-segformer",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerate BERT inference with Hugging Face Transformers and AWS Inferentia",
    "summary": "",
    "url": "https://huggingface.co/blog/bert-inferentia-sagemaker",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Image search with 🤗 datasets",
    "summary": "",
    "url": "https://huggingface.co/blog/image-search-datasets",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Guiding Text Generation with Constrained Beam Search in 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/constrained-beam-search",
    "source": "Hugging Face Blog"
  },
  {
    "title": "BERT 101 - State Of The Art NLP Model Explained",
    "summary": "",
    "url": "https://huggingface.co/blog/bert-101",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-Tune ViT for Image Classification with 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/fine-tune-vit",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Getting Started with Sentiment Analysis using Python",
    "summary": "",
    "url": "https://huggingface.co/blog/sentiment-analysis-python",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Making automatic speech recognition work on large files with Wav2Vec2 in 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/asr-chunking",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Supercharged Searching on the 🤗 Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/searching-the-hub",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome Stable-baselines3 to the Hugging Face Hub 🤗",
    "summary": "",
    "url": "https://huggingface.co/blog/sb3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs",
    "summary": "",
    "url": "https://huggingface.co/blog/infinity-cpu-performance",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Boosting Wav2Vec2 with n-grams in 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/wav2vec2-with-ngram",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploy GPT-J 6B for inference using  Hugging Face Transformers and Amazon SageMaker",
    "summary": "",
    "url": "https://huggingface.co/blog/gptj-sagemaker",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Active Learning with AutoNLP and Prodigy",
    "summary": "",
    "url": "https://huggingface.co/blog/autonlp-prodigy",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Gradio is joining Hugging Face!",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-joins-hf",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Perceiver IO: a scalable, fully-attentional model that works on any modality",
    "summary": "",
    "url": "https://huggingface.co/blog/perceiver",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Training CodeParrot 🦜 from Scratch",
    "summary": "",
    "url": "https://huggingface.co/blog/codeparrot",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Snowball Fight ☃️, our first ML-Agents environment",
    "summary": "",
    "url": "https://huggingface.co/blog/snowball-fight",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Getting Started with Hugging Face Transformers for IPUs with Optimum",
    "summary": "",
    "url": "https://huggingface.co/blog/graphcore-getting-started",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Data Measurements Tool: an Interactive Tool for Looking at Datasets",
    "summary": "",
    "url": "https://huggingface.co/blog/data-measurements-tool",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerating PyTorch distributed fine-tuning with Intel technologies",
    "summary": "",
    "url": "https://huggingface.co/blog/accelerating-pytorch",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-Tune XLSR-Wav2Vec2 for low-resource ASR with 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/fine-tune-xlsr-wav2vec2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Scaling up BERT-like model Inference on modern CPU  - Part 2",
    "summary": "",
    "url": "https://huggingface.co/blog/bert-cpu-scaling-part-2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Course Launch Community Event",
    "summary": "",
    "url": "https://huggingface.co/blog/course-launch-event",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Large Language Models: A New Moore's Law?",
    "summary": "",
    "url": "https://huggingface.co/blog/large-language-models",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Train a Sentence Embedding Model with 1B Training Pairs",
    "summary": "",
    "url": "https://huggingface.co/blog/1b-sentence-embeddings",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Age of Machine Learning As Code Has Arrived",
    "summary": "",
    "url": "https://huggingface.co/blog/the-age-of-ml-as-code",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine tuning CLIP with Remote Sensing (Satellite) images and captions",
    "summary": "",
    "url": "https://huggingface.co/blog/fine-tune-clip-rsicd",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hosting your Models and Datasets on Hugging Face Spaces using Streamlit",
    "summary": "",
    "url": "https://huggingface.co/blog/streamlit-spaces",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Showcase Your Projects in Spaces using Gradio",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-spaces",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Summer at Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/summer-at-huggingface",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face and Graphcore partner for IPU-optimized Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/graphcore",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Optimum: The Optimization Toolkit for Transformers at Scale",
    "summary": "",
    "url": "https://huggingface.co/blog/hardware-partners-program",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deep Learning over the Internet: Training Language Models Collaboratively",
    "summary": "",
    "url": "https://huggingface.co/blog/collaborative-training",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome spaCy to the Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/spacy",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploy Hugging Face models easily with Amazon SageMaker",
    "summary": "",
    "url": "https://huggingface.co/blog/deploy-hugging-face-models-easily-with-amazon-sagemaker",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Sentence Transformers in the Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/sentence-transformers-in-the-hub",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Few-shot learning in practice: GPT-Neo and the 🤗 Accelerated Inference API",
    "summary": "",
    "url": "https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Using & Mixing Hugging Face Models with Gradio 2.0",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Scaling-up BERT Inference on CPU (Part 1)",
    "summary": "",
    "url": "https://huggingface.co/blog/bert-cpu-scaling-part-1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing 🤗 Accelerate",
    "summary": "",
    "url": "https://huggingface.co/blog/accelerate-library",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Distributed Training: Train BART/T5 for Summarization using 🤗 Transformers and Amazon SageMaker",
    "summary": "",
    "url": "https://huggingface.co/blog/sagemaker-distributed-training-seq2seq",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Understanding BigBird's Block Sparse Attention",
    "summary": "",
    "url": "https://huggingface.co/blog/big-bird",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Partnership: Amazon SageMaker and Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face",
    "source": "Hugging Face Blog"
  },
  {
    "title": "My Journey to a serverless transformers pipeline on Google Cloud",
    "summary": "",
    "url": "https://huggingface.co/blog/how-to-deploy-a-pipeline-to-google-clouds",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-Tune Wav2Vec2 for English ASR in Hugging Face with 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/fine-tune-wav2vec2-english",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face Reads, Feb. 2021 - Long-range Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/long-range-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Simple considerations for simple people building fancy neural networks",
    "summary": "",
    "url": "https://huggingface.co/blog/simple-considerations",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Retrieval Augmented Generation with Huggingface Transformers and Ray",
    "summary": "",
    "url": "https://huggingface.co/blog/ray-rag",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face on PyTorch / XLA TPUs",
    "summary": "",
    "url": "https://huggingface.co/blog/pytorch-xla",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Faster TensorFlow models in Hugging Face Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/tf-serving",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fit More and Train Faster With ZeRO via DeepSpeed and FairScale",
    "summary": "",
    "url": "https://huggingface.co/blog/zero-deepspeed-fairscale",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How we sped up transformer inference 100x for 🤗 API customers",
    "summary": "",
    "url": "https://huggingface.co/blog/accelerated-inference",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Leveraging Pre-trained Language Model Checkpoints for Encoder-Decoder Models",
    "summary": "",
    "url": "https://huggingface.co/blog/warm-starting-encoder-decoder",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Porting fairseq wmt19 translation system to transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/porting-fsmt",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hyperparameter Search with Transformers and Ray Tune",
    "summary": "",
    "url": "https://huggingface.co/blog/ray-tune",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Transformer-based Encoder-Decoder Models",
    "summary": "",
    "url": "https://huggingface.co/blog/encoder-decoder",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Block Sparse Matrices for Smaller and Faster Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/pytorch_block_sparse",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Reformer - Pushing the limits of language modeling",
    "summary": "",
    "url": "https://huggingface.co/blog/reformer",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How to generate text: using different decoding methods for language generation with Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/how-to-generate",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How to train a new language model from scratch using Transformers and Tokenizers",
    "summary": "",
    "url": "https://huggingface.co/blog/how-to-train",
    "source": "Hugging Face Blog"
  }
]