[
  {
    "title": "OpenAI Is Nuking Its 4o Model. China\u2019s ChatGPT Fans Aren\u2019t OK",
    "summary": "As OpenAI removed access to GPT-4o in its app on Friday, people who have come to rely on the chatbot for companionship are mourning the loss all over the world.",
    "url": "https://www.wired.com/story/openai-nuking-4o-model-china-chatgpt-fans-arent-ok/",
    "source": "Wired AI",
    "published_at": "2026-02-13T21:56:18+00:00"
  },
  {
    "title": "Zillow Has Gone Wild\u2014for AI",
    "summary": "As the housing market stalls, Zillow\u2019s CEO sees AI as \u201can ingredient rather than a threat\u201d that can both help the company protect its turf and reinvent how people search for homes.",
    "url": "https://www.wired.com/story/backchannel-how-artificial-intelligence-changed-zillow/",
    "source": "Wired AI",
    "published_at": "2026-02-13T16:14:25+00:00"
  },
  {
    "title": "Inside the New York City Date Night for AI Lovers",
    "summary": "EVA AI created a pop-up romantic date night at a Manhattan wine bar to help making AI-human relationships a \u201cnew normal.\u201d",
    "url": "https://www.wired.com/story/inside-the-new-york-city-date-night-for-ai-lovers/",
    "source": "Wired AI",
    "published_at": "2026-02-13T11:30:00+00:00"
  },
  {
    "title": "Explaining AI Without Code: A User Study on Explainable AI",
    "summary": "arXiv:2602.11159v1 Announce Type: new Abstract: The increasing use of Machine Learning (ML) in sensitive domains such as healthcare, finance, and public policy has raised concerns about the transparency of automated decisions. Explainable AI (XAI) addresses this by clarifying how models generate predictions, yet most methods demand technical expert",
    "url": "https://arxiv.org/abs/2602.11159",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Latent Generative Solvers for Generalizable Long-Term Physics Simulation",
    "summary": "arXiv:2602.11229v1 Announce Type: new Abstract: We study long-horizon surrogate simulation across heterogeneous PDE systems. We introduce Latent Generative Solvers (LGS), a two-stage framework that (i) maps diverse PDE states into a shared latent physics space with a pretrained VAE, and (ii) learns probabilistic latent dynamics with a Transformer t",
    "url": "https://arxiv.org/abs/2602.11229",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "On Decision-Valued Maps and Representational Dependence",
    "summary": "arXiv:2602.11295v1 Announce Type: new Abstract: A computational engine applied to different representations of the same data can produce different discrete outcomes, with some representations preserving the result and others changing it entirely. A decision-valued map records which representations preserve the outcome and which change it, associati",
    "url": "https://arxiv.org/abs/2602.11295",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Voxtral Realtime",
    "summary": "arXiv:2602.11298v1 Announce Type: new Abstract: We introduce Voxtral Realtime, a natively streaming automatic speech recognition model that matches offline transcription quality at sub-second latency. Unlike approaches that adapt offline models through chunking or sliding windows, Voxtral Realtime is trained end-to-end for streaming, with explicit ",
    "url": "https://arxiv.org/abs/2602.11298",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "The PBSAI Governance Ecosystem: A Multi-Agent AI Reference Architecture for Securing Enterprise AI Estates",
    "summary": "arXiv:2602.11301v1 Announce Type: new Abstract: Enterprises are rapidly deploying large language models, retrieval augmented generation pipelines, and tool using agents into production, often on shared high performance computing clusters and cloud accelerator platforms that also support defensive analytics. These systems increasingly function not a",
    "url": "https://arxiv.org/abs/2602.11301",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Dissecting Subjectivity and the \"Ground Truth\" Illusion in Data Annotation",
    "summary": "arXiv:2602.11318v1 Announce Type: new Abstract: In machine learning, \"ground truth\" refers to the assumed correct labels used to train and evaluate models. However, the foundational \"ground truth\" paradigm rests on a positivistic fallacy that treats human disagreement as technical noise rather than a vital sociotechnical signal. This systematic lit",
    "url": "https://arxiv.org/abs/2602.11318",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Bi-Level Prompt Optimization for Multimodal LLM-as-a-Judge",
    "summary": "arXiv:2602.11340v1 Announce Type: new Abstract: Large language models (LLMs) have become widely adopted as automated judges for evaluating AI-generated content. Despite their success, aligning LLM-based evaluations with human judgments remains challenging. While supervised fine-tuning on human-labeled data can improve alignment, it is costly and in",
    "url": "https://arxiv.org/abs/2602.11340",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "AgentNoiseBench: Benchmarking Robustness of Tool-Using LLM Agents Under Noisy Condition",
    "summary": "arXiv:2602.11348v1 Announce Type: new Abstract: Recent advances in large language models have enabled LLM-based agents to achieve strong performance on a variety of benchmarks. However, their performance in real-world deployments often that observed on benchmark settings, especially in complex and imperfect environments. This discrepancy largely ar",
    "url": "https://arxiv.org/abs/2602.11348",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Pushing Forward Pareto Frontiers of Proactive Agents with Behavioral Agentic Optimization",
    "summary": "arXiv:2602.11351v1 Announce Type: new Abstract: Proactive large language model (LLM) agents aim to actively plan, query, and interact over multiple turns, enabling efficient task completion beyond passive instruction following and making them essential for real-world, user-centric applications. Agentic reinforcement learning (RL) has recently emerg",
    "url": "https://arxiv.org/abs/2602.11351",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "ReplicatorBench: Benchmarking LLM Agents for Replicability in Social and Behavioral Sciences",
    "summary": "arXiv:2602.11354v1 Announce Type: new Abstract: The literature has witnessed an emerging interest in AI agents for automated assessment of scientific papers. Existing benchmarks focus primarily on the computational aspect of this task, testing agents' ability to reproduce or replicate research outcomes when having access to the code and data. This ",
    "url": "https://arxiv.org/abs/2602.11354",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Causal-JEPA: Learning World Models through Object-Level Latent Interventions",
    "summary": "arXiv:2602.11389v1 Announce Type: new Abstract: World models require robust relational understanding to support prediction, reasoning, and control. While object-centric representations provide a useful abstraction, they are not sufficient to capture interaction-dependent dynamics. We therefore propose C-JEPA, a simple and flexible object-centric wo",
    "url": "https://arxiv.org/abs/2602.11389",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "GHOST: Unmasking Phantom States in Mamba2 via Grouped Hidden-state Output-aware Selection & Truncation",
    "summary": "arXiv:2602.11408v1 Announce Type: new Abstract: While Mamba2's expanded state dimension enhances temporal modeling, it incurs substantial inference overhead that saturates bandwidth during autoregressive generation. Standard pruning methods fail to address this bottleneck: unstructured sparsity leaves activations dense, magnitude-based selection ig",
    "url": "https://arxiv.org/abs/2602.11408",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "TRACER: Trajectory Risk Aggregation for Critical Episodes in Agentic Reasoning",
    "summary": "arXiv:2602.11409v1 Announce Type: new Abstract: Estimating uncertainty for AI agents in real-world multi-turn tool-using interaction with humans is difficult because failures are often triggered by sparse critical episodes (e.g., looping, incoherent tool use, or user-agent miscoordination) even when local generation appears confident. Existing unce",
    "url": "https://arxiv.org/abs/2602.11409",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Distributionally Robust Cooperative Multi-Agent Reinforcement Learning via Robust Value Factorization",
    "summary": "arXiv:2602.11437v1 Announce Type: new Abstract: Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution, where value-factorization methods enforce the individual-global-maximum (IGM) principle so that decentralized greedy actions recover the team-optimal joint action. However, the reli",
    "url": "https://arxiv.org/abs/2602.11437",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Credit Where It is Due: Cross-Modality Connectivity Drives Precise Reinforcement Learning for MLLM Reasoning",
    "summary": "arXiv:2602.11455v1 Announce Type: new Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), yet how visual evidence is integrated during reasoning remains poorly understood. We explore multimodal RLVR through the lens of cross-modal attentio",
    "url": "https://arxiv.org/abs/2602.11455",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "AgentLeak: A Full-Stack Benchmark for Privacy Leakage in Multi-Agent LLM Systems",
    "summary": "arXiv:2602.11510v1 Announce Type: new Abstract: Multi-agent Large Language Model (LLM) systems create privacy risks that current benchmarks cannot measure. When agents coordinate on tasks, sensitive data passes through inter-agent messages, shared memory, and tool arguments; pathways that output-only audits never inspect. We introduce AgentLeak, to",
    "url": "https://arxiv.org/abs/2602.11510",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Human-Inspired Continuous Learning of Internal Reasoning Processes: Learning How to Think for Adaptive AI Systems",
    "summary": "arXiv:2602.11516v1 Announce Type: new Abstract: Learning internal reasoning processes is crucial for developing AI systems capable of sustained adaptation in dynamic real-world environments. However, most existing approaches primarily emphasize learning task-specific outputs or static knowledge representations, while overlooking the continuous refi",
    "url": "https://arxiv.org/abs/2602.11516",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "CausalAgent: A Conversational Multi-Agent System for End-to-End Causal Inference",
    "summary": "arXiv:2602.11527v1 Announce Type: new Abstract: Causal inference holds immense value in fields such as healthcare, economics, and social sciences. However, traditional causal analysis workflows impose significant technical barriers, requiring researchers to possess dual backgrounds in statistics and computer science, while manually selecting algori",
    "url": "https://arxiv.org/abs/2602.11527",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use",
    "summary": "arXiv:2602.11541v1 Announce Type: new Abstract: We study budget-constrained tool-augmented agents, where a large language model must solve multi-step tasks by invoking external tools under a strict monetary budget. We formalize this setting as sequential decision making in context space with priced and stochastic tool executions, making direct plan",
    "url": "https://arxiv.org/abs/2602.11541",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "SemaPop: Semantic-Persona Conditioned Population Synthesis",
    "summary": "arXiv:2602.11569v1 Announce Type: new Abstract: Population synthesis is a critical component of individual-level socio-economic simulation, yet remains challenging due to the need to jointly represent statistical structure and latent behavioral semantics. Existing population synthesis approaches predominantly rely on structured attributes and stati",
    "url": "https://arxiv.org/abs/2602.11569",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Learning to Configure Agentic AI Systems",
    "summary": "arXiv:2602.11574v1 Announce Type: new Abstract: Configuring LLM-based agent systems involves choosing workflows, tools, token budgets, and prompts from a large combinatorial design space, and is typically handled today by fixed large templates or hand-tuned heuristics. This leads to brittle behavior and unnecessary compute, since the same cumbersom",
    "url": "https://arxiv.org/abs/2602.11574",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "The Five Ws of Multi-Agent Communication: Who Talks to Whom, When, What, and Why -- A Survey from MARL to Emergent Language and LLMs",
    "summary": "arXiv:2602.11583v1 Announce Type: new Abstract: Multi-agent sequential decision-making powers many real-world systems, from autonomous vehicles and robotics to collaborative AI assistants. In dynamic, partially observable environments, communication is often what reduces uncertainty and makes collaboration possible. This survey reviews multi-agent ",
    "url": "https://arxiv.org/abs/2602.11583",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "MAPLE: Modality-Aware Post-training and Learning Ecosystem",
    "summary": "arXiv:2602.11596v1 Announce Type: new Abstract: Multimodal language models now integrate text, audio, and video for unified reasoning. Yet existing RL post-training pipelines treat all input signals as equally relevant, ignoring which modalities each task actually requires. This modality-blind training inflates policy-gradient variance, slows conve",
    "url": "https://arxiv.org/abs/2602.11596",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery",
    "summary": "arXiv:2602.11609v1 Announce Type: new Abstract: We present scPilot, the first systematic framework to practice omics-native reasoning: a large language model (LLM) converses in natural language while directly inspecting single-cell RNA-seq data and on-demand bioinformatics tools. scPilot converts core single-cell analyses, i.e., cell-type annotatio",
    "url": "https://arxiv.org/abs/2602.11609",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "When Agents Disagree With Themselves: Measuring Behavioral Consistency in LLM-Based Agents",
    "summary": "arXiv:2602.11619v1 Announce Type: new Abstract: Run the same LLM agent on the same task twice: do you get the same behavior? We find the answer is often no. In a study of 3,000 agent runs across three models (Llama 3.1 70B, GPT-4o, and Claude Sonnet 4.5) on HotpotQA, we observe that ReAct-style agents produce 2.0--4.2 distinct action sequences per ",
    "url": "https://arxiv.org/abs/2602.11619",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Neuro-Symbolic Multitasking: A Unified Framework for Discovering Generalizable Solutions to PDE Families",
    "summary": "arXiv:2602.11630v1 Announce Type: new Abstract: Solving Partial Differential Equations (PDEs) is fundamental to numerous scientific and engineering disciplines. A common challenge arises from solving the PDE families, which are characterized by sharing an identical mathematical structure but varying in specific parameters. Traditional numerical met",
    "url": "https://arxiv.org/abs/2602.11630",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Do MLLMs Really Understand Space? A Mathematical Reasoning Evaluation",
    "summary": "arXiv:2602.11635v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) have achieved strong performance on perception-oriented tasks, yet their ability to perform mathematical spatial reasoning, defined as the capacity to parse and manipulate two- and three-dimensional relations, remains unclear. Humans easily solve textbook-style",
    "url": "https://arxiv.org/abs/2602.11635",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Quark Medical Alignment: A Holistic Multi-Dimensional Alignment and Collaborative Optimization Paradigm",
    "summary": "arXiv:2602.11661v1 Announce Type: new Abstract: While reinforcement learning for large language model alignment has progressed rapidly in recent years, transferring these paradigms to high-stakes medical question answering reveals a fundamental paradigm mismatch. Reinforcement Learning from Human Feedback relies on preference annotations that are p",
    "url": "https://arxiv.org/abs/2602.11661",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "PhyNiKCE: A Neurosymbolic Agentic Framework for Autonomous Computational Fluid Dynamics",
    "summary": "arXiv:2602.11666v1 Announce Type: new Abstract: The deployment of autonomous agents for Computational Fluid Dynamics (CFD), is critically limited by the probabilistic nature of Large Language Models (LLMs), which struggle to enforce the strict conservation laws and numerical stability required for physics-based simulations. Reliance on purely seman",
    "url": "https://arxiv.org/abs/2602.11666",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Benchmark Health Index: A Systematic Framework for Benchmarking the Benchmarks of LLMs",
    "summary": "arXiv:2602.11674v1 Announce Type: new Abstract: Large Language Models (LLMs) are advancing rapidly, yet the benchmarks used to measure this progress are becoming increasingly unreliable. Score inflation and selective reporting have eroded the authority of standard benchmarks, leaving the community uncertain about which evaluation results remain tru",
    "url": "https://arxiv.org/abs/2602.11674",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Right for the Wrong Reasons: Epistemic Regret Minimization for Causal Rung Collapse in LLMs",
    "summary": "arXiv:2602.11675v1 Announce Type: new Abstract: Machine learning systems that are \"right for the wrong reasons\" achieve high performance through shortcuts that collapse under distributional shift. We show this pathology has a precise causal origin: autoregressive training provides no gradient signal to distinguish association P(Y|X) from interventi",
    "url": "https://arxiv.org/abs/2602.11675",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Beyond Pixels: Vector-to-Graph Transformation for Reliable Schematic Auditing",
    "summary": "arXiv:2602.11678v1 Announce Type: new Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable progress in visual understanding, yet they suffer from a critical limitation: structural blindness. Even state-of-the-art models fail to capture topology and symbolic logic in engineering schematics, as their pixel-driven paradigm discards",
    "url": "https://arxiv.org/abs/2602.11678",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces",
    "summary": "arXiv:2602.11683v1 Announce Type: new Abstract: Recent work explores latent reasoning to improve reasoning efficiency by replacing explicit reasoning trajectories with continuous representations in a latent space, yet its effectiveness varies across settings. Analysis of model confidence dynamics under latent reasoning reveals that thinking traject",
    "url": "https://arxiv.org/abs/2602.11683",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Beyond Parameter Arithmetic: Sparse Complementary Fusion for Distribution-Aware Model Merging",
    "summary": "arXiv:2602.11717v1 Announce Type: new Abstract: Model merging has emerged as a promising paradigm for composing the capabilities of large language models by directly operating in weight space, enabling the integration of specialized models without costly retraining. However, existing merging methods largely rely on parameter-space heuristics, which",
    "url": "https://arxiv.org/abs/2602.11717",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs",
    "summary": "arXiv:2602.11729v1 Announce Type: new Abstract: Model diffing, the process of comparing models' internal representations to identify their differences, is a promising approach for uncovering safety-critical behaviors in new models. However, its application has so far been primarily focused on comparing a base model with its finetune. Since new LLM ",
    "url": "https://arxiv.org/abs/2602.11729",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Text2GQL-Bench: A Text to Graph Query Language Benchmark [Experiment, Analysis & Benchmark]",
    "summary": "arXiv:2602.11745v1 Announce Type: new Abstract: Graph models are fundamental to data analysis in domains rich with complex relationships. Text-to-Graph-Query-Language (Text-to-GQL) systems act as a translator, converting natural language into executable graph queries. This capability allows Large Language Models (LLMs) to directly analyze and manip",
    "url": "https://arxiv.org/abs/2602.11745",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "AIR: Improving Agent Safety through Incident Response",
    "summary": "arXiv:2602.11749v1 Announce Type: new Abstract: Large Language Model (LLM) agents are increasingly deployed in practice across a wide range of autonomous applications. Yet current safety mechanisms for LLM agents focus almost exclusively on preventing failures in advance, providing limited capabilities for responding to, containing, or recovering f",
    "url": "https://arxiv.org/abs/2602.11749",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "TSR: Trajectory-Search Rollouts for Multi-Turn RL of LLM Agents",
    "summary": "arXiv:2602.11767v1 Announce Type: new Abstract: Advances in large language models (LLMs) are driving a shift toward using reinforcement learning (RL) to train agents from iterative, multi-turn interactions across tasks. However, multi-turn RL remains challenging as rewards are often sparse or delayed, and environments can be stochastic. In this reg",
    "url": "https://arxiv.org/abs/2602.11767",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "How to Optimize Multispecies Set Predictions in Presence-Absence Modeling ?",
    "summary": "arXiv:2602.11771v1 Announce Type: new Abstract: Species distribution models (SDMs) commonly produce probabilistic occurrence predictions that must be converted into binary presence-absence maps for ecological inference and conservation planning. However, this binarization step is typically heuristic and can substantially distort estimates of specie",
    "url": "https://arxiv.org/abs/2602.11771",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "RELATE: A Reinforcement Learning-Enhanced LLM Framework for Advertising Text Generation",
    "summary": "arXiv:2602.11780v1 Announce Type: new Abstract: In online advertising, advertising text plays a critical role in attracting user engagement and driving advertiser value. Existing industrial systems typically follow a two-stage paradigm, where candidate texts are first generated and subsequently aligned with online performance metrics such as click-",
    "url": "https://arxiv.org/abs/2602.11780",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "FlowMind: Execute-Summarize for Structured Workflow Generation from LLM Reasoning",
    "summary": "arXiv:2602.11782v1 Announce Type: new Abstract: LLMs can solve complex tasks through reasoning and tool use, but accurately translating these solutions into structured workflows remains challenging. We model workflows as sequences of tool use and reformulate the problem as designing a mechanism that can both solve tasks and reliably construct workf",
    "url": "https://arxiv.org/abs/2602.11782",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Beyond End-to-End Video Models: An LLM-Based Multi-Agent System for Educational Video Generation",
    "summary": "arXiv:2602.11790v1 Announce Type: new Abstract: Although recent end-to-end video generation models demonstrate impressive performance in visually oriented content creation, they remain limited in scenarios that require strict logical rigor and precise knowledge representation, such as instructional and educational media. To address this problem, we",
    "url": "https://arxiv.org/abs/2602.11790",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Detecting RLVR Training Data via Structural Convergence of Reasoning",
    "summary": "arXiv:2602.11792v1 Announce Type: new Abstract: Reinforcement learning with verifiable rewards (RLVR) is central to training modern reasoning models, but the undisclosed training data raises concerns about benchmark contamination. Unlike pretraining methods, which optimize models using token-level probabilities, RLVR fine-tunes models based on rewa",
    "url": "https://arxiv.org/abs/2602.11792",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Hi-SAM: A Hierarchical Structure-Aware Multi-modal Framework for Large-Scale Recommendation",
    "summary": "arXiv:2602.11799v1 Announce Type: new Abstract: Multi-modal recommendation has gained traction as items possess rich attributes like text and images. Semantic ID-based approaches effectively discretize this information into compact tokens. However, two challenges persist: (1) Suboptimal Tokenization: existing methods (e.g., RQ-VAE) lack disentangle",
    "url": "https://arxiv.org/abs/2602.11799",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "PuYun-LDM: A Latent Diffusion Model for High-Resolution Ensemble Weather Forecasts",
    "summary": "arXiv:2602.11807v1 Announce Type: new Abstract: Latent diffusion models (LDMs) suffer from limited diffusability in high-resolution (<=0.25{\\deg}) ensemble weather forecasting, where diffusability characterizes how easily a latent data distribution can be modeled by a diffusion process. Unlike natural image fields, meteorological fields lack task-a",
    "url": "https://arxiv.org/abs/2602.11807",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Predicting LLM Output Length via Entropy-Guided Representations",
    "summary": "arXiv:2602.11812v1 Announce Type: new Abstract: The long-tailed distribution of sequence lengths in LLM serving and reinforcement learning (RL) sampling causes significant computational waste due to excessive padding in batched inference. Existing methods rely on auxiliary models for static length prediction, but they incur high overhead, generaliz",
    "url": "https://arxiv.org/abs/2602.11812",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Revis: Sparse Latent Steering to Mitigate Object Hallucination in Large Vision-Language Models",
    "summary": "arXiv:2602.11824v1 Announce Type: new Abstract: Despite the advanced capabilities of Large Vision-Language Models (LVLMs), they frequently suffer from object hallucination. One reason is that visual features and pretrained textual representations often become intertwined in the deeper network layers. To address this, we propose REVIS, a training-fr",
    "url": "https://arxiv.org/abs/2602.11824",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Prototype Transformer: Towards Language Model Architectures Interpretable by Design",
    "summary": "arXiv:2602.11852v1 Announce Type: new Abstract: While state-of-the-art language models (LMs) surpass the vast majority of humans in certain domains, their reasoning remains largely opaque, undermining trust in their output. Furthermore, while autoregressive LMs can output explicit reasoning, their true reasoning process is opaque, which introduces ",
    "url": "https://arxiv.org/abs/2602.11852",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Talk2DM: Enabling Natural Language Querying and Commonsense Reasoning for Vehicle-Road-Cloud Integrated Dynamic Maps with Large Language Models",
    "summary": "arXiv:2602.11860v1 Announce Type: new Abstract: Dynamic maps (DM) serve as the fundamental information infrastructure for vehicle-road-cloud (VRC) cooperative autonomous driving in China and Japan. By providing comprehensive traffic scene representations, DM overcome the limitations of standalone autonomous driving systems (ADS), such as physical o",
    "url": "https://arxiv.org/abs/2602.11860",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Intelligent AI Delegation",
    "summary": "arXiv:2602.11865v1 Announce Type: new Abstract: AI agents are able to tackle increasingly complex tasks. To achieve more ambitious goals, AI agents need to be able to meaningfully decompose problems into manageable sub-components, and safely delegate their completion across to other AI agents and humans alike. Yet, existing task decomposition and d",
    "url": "https://arxiv.org/abs/2602.11865",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "From Atoms to Trees: Building a Structured Feature Forest with Hierarchical Sparse Autoencoders",
    "summary": "arXiv:2602.11881v1 Announce Type: new Abstract: Sparse autoencoders (SAEs) have proven effective for extracting monosemantic features from large language models (LLMs), yet these features are typically identified in isolation. However, broad evidence suggests that LLMs capture the intrinsic structure of natural language, where the phenomenon of \"fe",
    "url": "https://arxiv.org/abs/2602.11881",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "When Should LLMs Be Less Specific? Selective Abstraction for Reliable Long-Form Text Generation",
    "summary": "arXiv:2602.11908v1 Announce Type: new Abstract: LLMs are widely used, yet they remain prone to factual errors that erode user trust and limit adoption in high-risk settings. One approach to mitigate this risk is to equip models with uncertainty estimation mechanisms that abstain when confidence is low. However, this binary \"all-or-nothing\" approach",
    "url": "https://arxiv.org/abs/2602.11908",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "AlphaPROBE: Alpha Mining via Principled Retrieval and On-graph biased evolution",
    "summary": "arXiv:2602.11917v1 Announce Type: new Abstract: Extracting signals through alpha factor mining is a fundamental challenge in quantitative finance. Existing automated methods primarily follow two paradigms: Decoupled Factor Generation, which treats factor discovery as isolated events, and Iterative Factor Evolution, which focuses on local parent-chi",
    "url": "https://arxiv.org/abs/2602.11917",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "MEME: Modeling the Evolutionary Modes of Financial Markets",
    "summary": "arXiv:2602.11918v1 Announce Type: new Abstract: LLMs have demonstrated significant potential in quantitative finance by processing vast unstructured data to emulate human-like analytical workflows. However, current LLM-based methods primarily follow either an Asset-Centric paradigm focused on individual stock prediction or a Market-Centric approach",
    "url": "https://arxiv.org/abs/2602.11918",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Gaia2: Benchmarking LLM Agents on Dynamic and Asynchronous Environments",
    "summary": "arXiv:2602.11964v1 Announce Type: new Abstract: We introduce Gaia2, a benchmark for evaluating large language model agents in realistic, asynchronous environments. Unlike prior static or synchronous evaluations, Gaia2 introduces scenarios where environments evolve independently of agent actions, requiring agents to operate under temporal constraint",
    "url": "https://arxiv.org/abs/2602.11964",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "CSEval: A Framework for Evaluating Clinical Semantics in Text-to-Image Generation",
    "summary": "arXiv:2602.12004v1 Announce Type: new Abstract: Text-to-image generation has been increasingly applied in medical domains for various purposes such as data augmentation and education. Evaluating the quality and clinical reliability of these generated images is essential. However, existing methods mainly assess image realism or diversity, while fail",
    "url": "https://arxiv.org/abs/2602.12004",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "InjectRBP: Steering Large Language Model Reasoning Behavior via Pattern Injection",
    "summary": "arXiv:2602.12013v1 Announce Type: new Abstract: Reasoning can significantly enhance the performance of Large Language Models. While recent studies have exploited behavior-related prompts adjustment to enhance reasoning, these designs remain largely intuitive and lack a systematic analysis of the underlying behavioral patterns. Motivated by this, we",
    "url": "https://arxiv.org/abs/2602.12013",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Multi UAVs Preflight Planning in a Shared and Dynamic Airspace",
    "summary": "arXiv:2602.12055v1 Announce Type: new Abstract: Preflight planning for large-scale Unmanned Aerial Vehicle (UAV) fleets in dynamic, shared airspace presents significant challenges, including temporal No-Fly Zones (NFZs), heterogeneous vehicle profiles, and strict delivery deadlines. While Multi-Agent Path Finding (MAPF) provides a formal framework,",
    "url": "https://arxiv.org/abs/2602.12055",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "LawThinker: A Deep Research Legal Agent in Dynamic Environments",
    "summary": "arXiv:2602.12056v1 Announce Type: new Abstract: Legal reasoning requires not only correct outcomes but also procedurally compliant reasoning processes. However, existing methods lack mechanisms to verify intermediate reasoning steps, allowing errors such as inapplicable statute citations to propagate undetected through the reasoning chain. To addre",
    "url": "https://arxiv.org/abs/2602.12056",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Tiny Recursive Reasoning with Mamba-2 Attention Hybrid",
    "summary": "arXiv:2602.12078v1 Announce Type: new Abstract: Recent work on recursive reasoning models like TRM demonstrates that tiny networks (7M parameters) can achieve strong performance on abstract reasoning tasks through latent recursion -- iterative refinement in hidden representation space without emitting intermediate tokens. This raises a natural ques",
    "url": "https://arxiv.org/abs/2602.12078",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Differentiable Modal Logic for Multi-Agent Diagnosis, Orchestration and Communication",
    "summary": "arXiv:2602.12083v1 Announce Type: new Abstract: As multi-agent AI systems evolve from simple chatbots to autonomous swarms, debugging semantic failures requires reasoning about knowledge, belief, causality, and obligation, precisely what modal logic was designed to formalize. However, traditional modal logic requires manual specification of relatio",
    "url": "https://arxiv.org/abs/2602.12083",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context",
    "summary": "arXiv:2602.12108v1 Announce Type: new Abstract: In the world of Harry Potter, when Dumbledore's mind is overburdened, he extracts memories into a Pensieve to be revisited later. In the world of AI, while we possess the Pensieve-mature databases and retrieval systems, our models inexplicably lack the \"wand\" to operate it. They remain like a Dumbledo",
    "url": "https://arxiv.org/abs/2602.12108",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Stop Unnecessary Reflection: Training LRMs for Efficient Reasoning with Adaptive Reflection and Length Coordinated Penalty",
    "summary": "arXiv:2602.12113v1 Announce Type: new Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks by employing test-time scaling. However, they often generate over-long chains-of-thought that, driven by substantial reflections such as repetitive self-questioning and circular reasoning, lead to high to",
    "url": "https://arxiv.org/abs/2602.12113",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Commencing-Student Enrolment Forecasting Under Data Sparsity with Time Series Foundation Models",
    "summary": "arXiv:2602.12120v1 Announce Type: new Abstract: Many universities face increasing financial pressure and rely on accurate forecasts of commencing enrolments. However, enrolment forecasting in higher education is often data-sparse; annual series are short and affected by reporting changes and regime shifts. Popular classical approaches can be unreli",
    "url": "https://arxiv.org/abs/2602.12120",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "HLA: Hadamard Linear Attention",
    "summary": "arXiv:2602.12128v1 Announce Type: new Abstract: The attention mechanism is an important reason for the success of transformers. It relies on computing pairwise relations between tokens. To reduce the high computational cost of standard quadratic attention, linear attention has been proposed as an efficient approximation. It employs kernel functions",
    "url": "https://arxiv.org/abs/2602.12128",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Neutral Prompts, Non-Neutral People: Quantifying Gender and Skin-Tone Bias in Gemini Flash 2.5 Image and GPT Image 1.5",
    "summary": "arXiv:2602.12133v1 Announce Type: new Abstract: This study quantifies gender and skin-tone bias in two widely deployed commercial image generators - Gemini Flash 2.5 Image (NanoBanana) and GPT Image 1.5 - to test the assumption that neutral prompts yield demographically neutral outputs. We generated 3,200 photorealistic images using four semantical",
    "url": "https://arxiv.org/abs/2602.12133",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Value Alignment Tax: Measuring Value Trade-offs in LLM Alignment",
    "summary": "arXiv:2602.12134v1 Announce Type: new Abstract: Existing work on value alignment typically characterizes value relations statically, ignoring how interventions - such as prompting, fine-tuning, or preference optimization - reshape the broader value system. We introduce the Value Alignment Tax (VAT), a framework that measures how alignment-induced c",
    "url": "https://arxiv.org/abs/2602.12134",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "STAR : Bridging Statistical and Agentic Reasoning for Large Model Performance Prediction",
    "summary": "arXiv:2602.12143v1 Announce Type: new Abstract: As comprehensive large model evaluation becomes prohibitively expensive, predicting model performance from limited observations has become essential. However, existing statistical methods struggle with pattern shifts, data sparsity, and lack of explanation, while pure LLM methods remain unreliable. We",
    "url": "https://arxiv.org/abs/2602.12143",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Seq2Seq2Seq: Lossless Data Compression via Discrete Latent Transformers and Reinforcement Learning",
    "summary": "arXiv:2602.12146v1 Announce Type: new Abstract: Efficient lossless compression is essential for minimizing storage costs and transmission overhead while preserving data integrity. Traditional compression techniques, such as dictionary-based and statistical methods, often struggle to optimally exploit the structure and redundancy in complex data for",
    "url": "https://arxiv.org/abs/2602.12146",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "GPT-4o Lacks Core Features of Theory of Mind",
    "summary": "arXiv:2602.12150v1 Announce Type: new Abstract: Do Large Language Models (LLMs) possess a Theory of Mind (ToM)? Research into this question has focused on evaluating LLMs against benchmarks and found success across a range of social tasks. However, these evaluations do not test for the actual representations posited by ToM: namely, a causal model o",
    "url": "https://arxiv.org/abs/2602.12150",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision",
    "summary": "arXiv:2602.12164v1 Announce Type: new Abstract: Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in",
    "url": "https://arxiv.org/abs/2602.12164",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Statistical Parsing for Logical Information Retrieval",
    "summary": "arXiv:2602.12170v1 Announce Type: new Abstract: In previous work (Coppola, 2024) we introduced the Quantified Boolean Bayesian Network (QBBN), a logical graphical model that implements the forward fragment of natural deduction (Prawitz, 1965) as a probabilistic factor graph. That work left two gaps: no negation/backward reasoning, and no parser for",
    "url": "https://arxiv.org/abs/2602.12170",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation",
    "summary": "arXiv:2602.12172v1 Announce Type: new Abstract: Knowledge distillation from Large Language Models (LLMs) to smaller models has emerged as a critical technique for deploying efficient AI systems. However, current methods for distillation via synthetic data lack pedagogical awareness, treating knowledge transfer as a one-off data synthesis and traini",
    "url": "https://arxiv.org/abs/2602.12172",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "SAM3-LiteText: An Anatomical Study of the SAM3 Text Encoder for Efficient Vision-Language Segmentation",
    "summary": "arXiv:2602.12173v1 Announce Type: new Abstract: Vision-language segmentation models such as SAM3 enable flexible, prompt-driven visual grounding, but inherit large, general-purpose text encoders originally designed for open-ended language understanding. In practice, segmentation prompts are short, structured, and semantically constrained, leading t",
    "url": "https://arxiv.org/abs/2602.12173",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "\"Sorry, I Didn't Catch That\": How Speech Models Miss What Matters Most",
    "summary": "arXiv:2602.12249v1 Announce Type: new Abstract: Despite speech recognition systems achieving low word error rates on standard benchmarks, they often fail on short, high-stakes utterances in real-world deployments. Here, we study this failure mode in a high-stakes task: the transcription of U.S. street names as spoken by U.S. participants. We evalua",
    "url": "https://arxiv.org/abs/2602.12249",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Think like a Scientist: Physics-guided LLM Agent for Equation Discovery",
    "summary": "arXiv:2602.12259v1 Announce Type: new Abstract: Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existi",
    "url": "https://arxiv.org/abs/2602.12259",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use",
    "summary": "arXiv:2602.12268v1 Announce Type: new Abstract: AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behavio",
    "url": "https://arxiv.org/abs/2602.12268",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Agentic Test-Time Scaling for WebAgents",
    "summary": "arXiv:2602.12276v1 Announce Type: new Abstract: Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly incr",
    "url": "https://arxiv.org/abs/2602.12276",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Improving Medical Visual Reinforcement Fine-Tuning via Perception and Reasoning Augmentation",
    "summary": "arXiv:2602.10619v1 Announce Type: cross Abstract: While recent advances in Reinforcement Fine-Tuning (RFT) have shown that rule-based reward schemes can enable effective post-training for large language models, their extension to cross-modal, vision-centric domains remains largely underexplored. This limitation is especially pronounced in the medic",
    "url": "https://arxiv.org/abs/2602.10619",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "HybridRAG: A Practical LLM-based ChatBot Framework based on Pre-Generated Q&A over Raw Unstructured Documents",
    "summary": "arXiv:2602.11156v1 Announce Type: cross Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for grounding Large Language Model (LLM)-based chatbot responses on external knowledge. However, existing RAG studies typically assume well-structured textual sources (e.g. Wikipedia or curated datasets) and perform retrieval an",
    "url": "https://arxiv.org/abs/2602.11156",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Methodological Variation in Studying Staff and Student Perceptions of AI",
    "summary": "arXiv:2602.11158v1 Announce Type: cross Abstract: In this paper, we compare methodological approaches for comparing student and staff perceptions, and ask: how much do these measures vary across different approaches? We focus on the case of AI perceptions, which are generally assessed via a single quantitative or qualitative measure, or with a mixe",
    "url": "https://arxiv.org/abs/2602.11158",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "BIRD: A Museum Open Dataset Combining Behavior Patterns and Identity Types to Better Model Visitors' Experience",
    "summary": "arXiv:2602.11160v1 Announce Type: cross Abstract: Lack of data is a recurring problem in Artificial Intelligence, as it is essential for training and validating models. This is particularly true in the field of cultural heritage, where the number of open datasets is relatively limited and where the data collected does not always allow for holistic ",
    "url": "https://arxiv.org/abs/2602.11160",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Nested Named Entity Recognition in Plasma Physics Research Articles",
    "summary": "arXiv:2602.11163v1 Announce Type: cross Abstract: Named Entity Recognition (NER) is an important task in natural language processing that aims to identify and extract key entities from unstructured text. We present a novel application of NER in plasma physics research articles and address the challenges of extracting specialized entities from scien",
    "url": "https://arxiv.org/abs/2602.11163",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Automated Optimization Modeling via a Localizable Error-Driven Perspective",
    "summary": "arXiv:2602.11164v1 Announce Type: cross Abstract: Automated optimization modeling via Large Language Models (LLMs) has emerged as a promising approach to assist complex human decision-making. While post-training has become a pivotal technique to enhance LLMs' capabilities in this domain, its effectiveness is severely constrained by the scarcity and",
    "url": "https://arxiv.org/abs/2602.11164",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Assessing LLM Reliability on Temporally Recent Open-Domain Questions",
    "summary": "arXiv:2602.11165v1 Announce Type: cross Abstract: Large Language Models (LLMs) are increasingly deployed for open-domain question answering, yet their alignment with human perspectives on temporally recent information remains underexplored. We introduce RECOM (Reddit Evaluation for Correspondence of Models), a benchmark dataset of 15,000 recent Red",
    "url": "https://arxiv.org/abs/2602.11165",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Small Updates, Big Doubts: Does Parameter-Efficient Fine-tuning Enhance Hallucination Detection ?",
    "summary": "arXiv:2602.11166v1 Announce Type: cross Abstract: Parameter-efficient fine-tuning (PEFT) methods are widely used to adapt large language models (LLMs) to downstream tasks and are often assumed to improve factual correctness. However, how the parameter-efficient fine-tuning methods affect hallucination behavior remains insufficiently understood, esp",
    "url": "https://arxiv.org/abs/2602.11166",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Visualizing and Benchmarking LLM Factual Hallucination Tendencies via Internal State Analysis and Clustering",
    "summary": "arXiv:2602.11167v1 Announce Type: cross Abstract: Large Language Models (LLMs) often hallucinate, generating nonsensical or false information that can be especially harmful in sensitive fields such as medicine or law. To study this phenomenon systematically, we introduce FalseCite, a curated dataset designed to capture and benchmark hallucinated re",
    "url": "https://arxiv.org/abs/2602.11167",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Enhancing SDG-Text Classification with Combinatorial Fusion Analysis and Generative AI",
    "summary": "arXiv:2602.11168v1 Announce Type: cross Abstract: (Natural Language Processing) NLP techniques such as text classification and topic discovery are very useful in many application areas including information retrieval, knowledge discovery, policy formulation, and decision-making. However, it remains a challenging problem in cases where the categorie",
    "url": "https://arxiv.org/abs/2602.11168",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Disentangling Direction and Magnitude in Transformer Representations: A Double Dissociation Through L2-Matched Perturbation Analysis",
    "summary": "arXiv:2602.11169v1 Announce Type: cross Abstract: Transformer hidden states encode information as high-dimensional vectors, yet whether direction (orientation in representational space) and magnitude (vector norm) serve distinct functional roles remains unclear. Studying Pythia-family models, we discover a striking cross-over dissociation: angular ",
    "url": "https://arxiv.org/abs/2602.11169",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Efficient Hyper-Parameter Search for LoRA via Language-aided Bayesian Optimization",
    "summary": "arXiv:2602.11171v1 Announce Type: cross Abstract: Fine-tuning Large Language Models (LLMs) with Low-Rank Adaptation (LoRA) enables resource-efficient personalization or specialization, but it comes at the expense of additional hyperparameter tuning. Although LoRA makes fine-tuning efficient, it is highly sensitive to the choice of hyperparameters, ",
    "url": "https://arxiv.org/abs/2602.11171",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "The Script Tax: Measuring Tokenization-Driven Efficiency and Latency Disparities in Multilingual Language Models",
    "summary": "arXiv:2602.11174v1 Announce Type: cross Abstract: Pretrained multilingual language models are often assumed to be script-agnostic, yet their tokenizers can impose systematic costs on certain writing systems. We quantify this script tax by comparing two orthographic variants with identical linguistic content. Across mBERT and XLM-R, the higher-fragm",
    "url": "https://arxiv.org/abs/2602.11174",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Evaluating Few-Shot Temporal Reasoning of LLMs for Human Activity Prediction in Smart Environments",
    "summary": "arXiv:2602.11176v1 Announce Type: cross Abstract: Anticipating human activities and their durations is essential in applications such as smart-home automation, simulation-based architectural and urban design, activity-based transportation system simulation, and human-robot collaboration, where adaptive systems must respond to human activities. Exis",
    "url": "https://arxiv.org/abs/2602.11176",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "What Do LLMs Know About Alzheimer's Disease? Fine-Tuning, Probing, and Data Synthesis for AD Detection",
    "summary": "arXiv:2602.11177v1 Announce Type: cross Abstract: Reliable early detection of Alzheimer's disease (AD) is challenging, particularly due to limited availability of labeled data. While large language models (LLMs) have shown strong transfer capabilities across domains, adapting them to the AD domain through supervised fine-tuning remains largely unex",
    "url": "https://arxiv.org/abs/2602.11177",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "From Instruction to Output: The Role of Prompting in Modern NLG",
    "summary": "arXiv:2602.11179v1 Announce Type: cross Abstract: Prompt engineering has emerged as an integral technique for extending the strengths and abilities of Large Language Models (LLMs) to gain significant performance gains in various Natural Language Processing (NLP) tasks. This approach, which requires instructions to be composed in natural language to",
    "url": "https://arxiv.org/abs/2602.11179",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "KBVQ-MoE: KLT-guided SVD with Bias-Corrected Vector Quantization for MoE Large Language Models",
    "summary": "arXiv:2602.11184v1 Announce Type: cross Abstract: Mixture of Experts (MoE) models have achieved great success by significantly improving performance while maintaining computational efficiency through sparse expert activation. However, their enormous parameter sizes and memory demands pose major challenges for deployment in resource-constrained envi",
    "url": "https://arxiv.org/abs/2602.11184",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Spectra: Rethinking Optimizers for LLMs Under Spectral Anisotropy",
    "summary": "arXiv:2602.11185v1 Announce Type: cross Abstract: Gradient signals in LLM training are highly anisotropic: recurrent linguistic structure concentrates energy into a small set of dominant spectral directions, while context specific information resides in a long tail. We show that this spike tail separation persists throughout training, with the spik",
    "url": "https://arxiv.org/abs/2602.11185",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "TDPNavigator-Placer: Thermal- and Wirelength-Aware Chiplet Placement in 2.5D Systems Through Multi-Agent Reinforcement Learning",
    "summary": "arXiv:2602.11187v1 Announce Type: cross Abstract: The rapid growth of electronics has accelerated the adoption of 2.5D integrated circuits, where effective automated chiplet placement is essential as systems scale to larger and more heterogeneous chiplet assemblies. Existing placement methods typically focus on minimizing wirelength or transforming",
    "url": "https://arxiv.org/abs/2602.11187",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "MuCO: Generative Peptide Cyclization Empowered by Multi-stage Conformation Optimization",
    "summary": "arXiv:2602.11189v1 Announce Type: cross Abstract: Modeling peptide cyclization is critical for the virtual screening of candidate peptides with desirable physical and pharmaceutical properties. This task is challenging because a cyclic peptide often exhibits diverse, ring-shaped conformations, which cannot be well captured by deterministic predicti",
    "url": "https://arxiv.org/abs/2602.11189",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Time-TK: A Multi-Offset Temporal Interaction Framework Combining Transformer and Kolmogorov-Arnold Networks for Time Series Forecasting",
    "summary": "arXiv:2602.11190v1 Announce Type: cross Abstract: Time series forecasting is crucial for the World Wide Web and represents a core technical challenge in ensuring the stable and efficient operation of modern web services, such as intelligent transportation and website throughput. However, we have found that existing methods typically employ a strate",
    "url": "https://arxiv.org/abs/2602.11190",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "MELINOE: Fine-Tuning Enables Memory-Efficient Inference for Mixture-of-Experts Models",
    "summary": "arXiv:2602.11192v1 Announce Type: cross Abstract: Mixture-of-Experts (MoE) model architectures can significantly reduce the number of activated parameters per token, enabling computationally efficient training and inference. However, their large overall parameter counts and model sizes have precluded their widespread usage in resource-constrained s",
    "url": "https://arxiv.org/abs/2602.11192",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Position-Aware Self-supervised Representation Learning for Cross-mode Radar Signal Recognition",
    "summary": "arXiv:2602.11196v1 Announce Type: cross Abstract: Radar signal recognition in open electromagnetic environments is challenging due to diverse operating modes and unseen radar types. Existing methods often overlook position relations in pulse sequences, limiting their ability to capture semantic dependencies over time. We propose RadarPos, a positio",
    "url": "https://arxiv.org/abs/2602.11196",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Hybrid operator learning of wave scattering maps in high-contrast media",
    "summary": "arXiv:2602.11197v1 Announce Type: cross Abstract: Surrogate modeling of wave propagation and scattering (i.e. the wave speed and source to wave field map) in heterogeneous media has significant potential in applications such as seismic imaging and inversion. High-contrast settings, such as subsurface models with salt bodies, exhibit strong scatteri",
    "url": "https://arxiv.org/abs/2602.11197",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "DDL2PropBank Agent: Benchmarking Multi-Agent Frameworks' Developer Experience Through a Novel Relational Schema Mapping Task",
    "summary": "arXiv:2602.11198v1 Announce Type: cross Abstract: Multi-agent frameworks promise to simplify LLM-driven software development, yet there is no principled way to evaluate their developer experience in a controlled setting. We introduce DDL2PropBank, a novel benchmark task that maps relational database schemas to PropBank rolesets, requiring autonomou",
    "url": "https://arxiv.org/abs/2602.11198",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "interwhen: A Generalizable Framework for Verifiable Reasoning with Test-time Monitors",
    "summary": "arXiv:2602.11202v1 Announce Type: cross Abstract: We present a test-time verification framework, interwhen, that ensures that the output of a reasoning model is valid wrt. a given set of verifiers. Verified reasoning is an important goal in high-stakes scenarios such as deploying agents in the physical world or in domains such as law and finance. H",
    "url": "https://arxiv.org/abs/2602.11202",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Zero-Sacrifice Persistent-Robustness Adversarial Defense for Pre-Trained Encoders",
    "summary": "arXiv:2602.11204v1 Announce Type: cross Abstract: The widespread use of publicly available pre-trained encoders from self-supervised learning (SSL) has exposed a critical vulnerability: their susceptibility to downstream-agnostic adversarial examples (DAEs), which are crafted without knowledge of the downstream tasks but capable of misleading downs",
    "url": "https://arxiv.org/abs/2602.11204",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "UltraLIF: Fully Differentiable Spiking Neural Networks via Ultradiscretization and Max-Plus Algebra",
    "summary": "arXiv:2602.11206v1 Announce Type: cross Abstract: Spiking Neural Networks (SNNs) offer energy-efficient, biologically plausible computation but suffer from non-differentiable spike generation, necessitating reliance on heuristic surrogate gradients. This paper introduces UltraLIF, a principled framework that replaces surrogate gradients with ultrad",
    "url": "https://arxiv.org/abs/2602.11206",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "SWE-MiniSandbox: Container-Free Reinforcement Learning for Building Software Engineering Agents",
    "summary": "arXiv:2602.11210v1 Announce Type: cross Abstract: Reinforcement learning (RL) has become a key paradigm for training software engineering (SWE) agents, but existing pipelines typically rely on per-task containers for isolation. At scale, pre-built container images incur substantial storage overhead, slow environment setup, and require container-man",
    "url": "https://arxiv.org/abs/2602.11210",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Credal Concept Bottleneck Models: Structural Separation of Epistemic and Aleatoric Uncertainty",
    "summary": "arXiv:2602.11219v1 Announce Type: cross Abstract: Decomposing predictive uncertainty into epistemic (model ignorance) and aleatoric (data ambiguity) components is central to reliable decision making, yet most methods estimate both from the same predictive distribution. Recent empirical and theoretical results show these estimates are typically stro",
    "url": "https://arxiv.org/abs/2602.11219",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "AI-Driven Clinical Decision Support System for Enhanced Diabetes Diagnosis and Management",
    "summary": "arXiv:2602.11237v1 Announce Type: cross Abstract: Identifying type 2 diabetes mellitus can be challenging, particularly for primary care physicians. Clinical decision support systems incorporating artificial intelligence (AI-CDSS) can assist medical professionals in diagnosing type 2 diabetes with high accuracy. This study aimed to assess an AI-CDS",
    "url": "https://arxiv.org/abs/2602.11237",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Toward Reliable Tea Leaf Disease Diagnosis Using Deep Learning Model: Enhancing Robustness With Explainable AI and Adversarial Training",
    "summary": "arXiv:2602.11239v1 Announce Type: cross Abstract: Tea is a valuable asset for the economy of Bangladesh. So, tea cultivation plays an important role to boost the economy. These valuable plants are vulnerable to various kinds of leaf infections which may cause less production and low quality. It is not so easy to detect these diseases manually. It m",
    "url": "https://arxiv.org/abs/2602.11239",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "How Many Features Can a Language Model Store Under the Linear Representation Hypothesis?",
    "summary": "arXiv:2602.11246v1 Announce Type: cross Abstract: We introduce a mathematical framework for the linear representation hypothesis (LRH), which asserts that intermediate layers of language models store features linearly. We separate the hypothesis into two claims: linear representation (features are linearly embedded in neuron activations) and linear",
    "url": "https://arxiv.org/abs/2602.11246",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "DeepRed: an architecture for redshift estimation",
    "summary": "arXiv:2602.11281v1 Announce Type: cross Abstract: Estimating redshift is a central task in astrophysics, but its measurement is costly and time-consuming. In addition, current image-based methods are often validated on homogeneous datasets. The development and comparison of networks able generalize across different morphologies, ranging from galaxi",
    "url": "https://arxiv.org/abs/2602.11281",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "HiFloat4 Format for Language Model Inference",
    "summary": "arXiv:2602.11287v1 Announce Type: cross Abstract: This paper introduces HiFloat4 (HiF4), a block floating-point data format tailored for deep learning. Each HiF4 unit packs 64 4-bit elements with 32 bits of shared scaling metadata, averaging 4.5 bits per value. The metadata specifies a three-level scaling hierarchy, capturing inter- and intra-group",
    "url": "https://arxiv.org/abs/2602.11287",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "CryptoAnalystBench: Failures in Multi-Tool Long-Form LLM Analysis",
    "summary": "arXiv:2602.11304v1 Announce Type: cross Abstract: Modern analyst agents must reason over complex, high token inputs, including dozens of retrieved documents, tool outputs, and time sensitive data. While prior work has produced tool calling benchmarks and examined factuality in knowledge augmented systems, relatively little work studies their inters",
    "url": "https://arxiv.org/abs/2602.11304",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Predictive Associative Memory: Retrieval Beyond Similarity Through Temporal Co-occurrence",
    "summary": "arXiv:2602.11322v1 Announce Type: cross Abstract: Current approaches to memory in neural systems rely on similarity-based retrieval: given a query, find the most representationally similar stored state. This assumption -- that useful memories are similar memories -- fails to capture a fundamental property of biological memory: association through t",
    "url": "https://arxiv.org/abs/2602.11322",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Security Threat Modeling for Emerging AI-Agent Protocols: A Comparative Analysis of MCP, A2A, Agora, and ANP",
    "summary": "arXiv:2602.11327v1 Announce Type: cross Abstract: The rapid development of the AI agent communication protocols, including the Model Context Protocol (MCP), Agent2Agent (A2A), Agora, and Agent Network Protocol (ANP), is reshaping how AI agents communicate with tools, services, and each other. While these protocols support scalable multi-agent inter",
    "url": "https://arxiv.org/abs/2602.11327",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation",
    "summary": "arXiv:2602.11337v1 Announce Type: cross Abstract: Deploying robots at scale demands robustness to the long tail of everyday situations. The countless variations in scene layout, object geometry, and task specifications that characterize real environments are vast and underrepresented in existing robot benchmarks. Measuring this level of generalizat",
    "url": "https://arxiv.org/abs/2602.11337",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Situated, Dynamic, and Subjective: Envisioning the Design of Theory-of-Mind-Enabled Everyday AI with Industry Practitioners",
    "summary": "arXiv:2602.11342v1 Announce Type: cross Abstract: Theory of Mind (ToM) -- the ability to infer what others are thinking (e.g., intentions) from observable cues -- is traditionally considered fundamental to human social interactions. This has sparked growing efforts in building and benchmarking AI's ToM capability, yet little is known about how such",
    "url": "https://arxiv.org/abs/2602.11342",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Divide and Learn: Multi-Objective Combinatorial Optimization at Scale",
    "summary": "arXiv:2602.11346v1 Announce Type: cross Abstract: Multi-objective combinatorial optimization seeks Pareto-optimal solutions over exponentially large discrete spaces, yet existing methods sacrifice generality, scalability, or theoretical guarantees. We reformulate it as an online learning problem over a decomposed decision space, solving position-wi",
    "url": "https://arxiv.org/abs/2602.11346",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "When Models Examine Themselves: Vocabulary-Activation Correspondence in Self-Referential Processing",
    "summary": "arXiv:2602.11358v1 Announce Type: cross Abstract: Large language models produce rich introspective language when prompted for self-examination, but whether this language reflects internal computation or sophisticated confabulation has remained unclear. We show that self-referential vocabulary tracks concurrent activation dynamics, and that this cor",
    "url": "https://arxiv.org/abs/2602.11358",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Bootstrapping-based Regularisation for Reducing Individual Prediction Instability in Clinical Risk Prediction Models",
    "summary": "arXiv:2602.11360v1 Announce Type: cross Abstract: Clinical prediction models are increasingly used to support patient care, yet many deep learning-based approaches remain unstable, as their predictions can vary substantially when trained on different samples from the same population. Such instability undermines reliability and limits clinical adopt",
    "url": "https://arxiv.org/abs/2602.11360",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Finding the Cracks: Improving LLMs Reasoning with Paraphrastic Probing and Consistency Verification",
    "summary": "arXiv:2602.11361v1 Announce Type: cross Abstract: Large language models have demonstrated impressive performance across a variety of reasoning tasks. However, their problem-solving ability often declines on more complex tasks due to hallucinations and the accumulation of errors within these intermediate steps. Recent work has introduced the notion ",
    "url": "https://arxiv.org/abs/2602.11361",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "The Energy of Falsehood: Detecting Hallucinations via Diffusion Model Likelihoods",
    "summary": "arXiv:2602.11364v1 Announce Type: cross Abstract: Large Language Models (LLMs) frequently hallucinate plausible but incorrect assertions, a vulnerability often missed by uncertainty metrics when models are confidently wrong. We propose DiffuTruth, an unsupervised framework that reconceptualizes fact verification via non equilibrium thermodynamics, ",
    "url": "https://arxiv.org/abs/2602.11364",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "The Manifold of the Absolute: Religious Perennialism as Generative Inference",
    "summary": "arXiv:2602.11368v1 Announce Type: cross Abstract: This paper formalizes religious epistemology through the mathematics of Variational Autoencoders. We model religious traditions as distinct generative mappings from a shared, low-dimensional latent space to the high-dimensional space of observable cultural forms, and define three competing generativ",
    "url": "https://arxiv.org/abs/2602.11368",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Retrieval-Aware Distillation for Transformer-SSM Hybrids",
    "summary": "arXiv:2602.11374v1 Announce Type: cross Abstract: State-space models (SSMs) offer efficient sequence modeling but lag behind Transformers on benchmarks that require in-context retrieval. Prior work links this gap to a small set of attention heads, termed Gather-and-Aggregate (G&amp;A), which SSMs struggle to reproduce. We propose *retrieval-aware d",
    "url": "https://arxiv.org/abs/2602.11374",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "General and Efficient Steering of Unconditional Diffusion",
    "summary": "arXiv:2602.11395v1 Announce Type: cross Abstract: Guiding unconditional diffusion models typically requires either retraining with conditional inputs or per-step gradient computations (e.g., classifier-based guidance), both of which incur substantial computational overhead. We present a general recipe for efficiently steering unconditional diffusio",
    "url": "https://arxiv.org/abs/2602.11395",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Can We Really Learn One Representation to Optimize All Rewards?",
    "summary": "arXiv:2602.11399v1 Announce Type: cross Abstract: As machine learning has moved towards leveraging large models as priors for downstream tasks, the community has debated the right form of prior for solving reinforcement learning (RL) problems. If one were to try to prefetch as much computation as possible, they would attempt to learn a prior over t",
    "url": "https://arxiv.org/abs/2602.11399",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "When Visibility Outpaces Verification: Delayed Verification and Narrative Lock-in in Agentic AI Discourse",
    "summary": "arXiv:2602.11412v1 Announce Type: cross Abstract: Agentic AI systems-autonomous entities capable of independent planning and execution-reshape the landscape of human-AI trust. Long before direct system exposure, user expectations are mediated through high-stakes public discourse on social platforms. However, platform-mediated engagement signals (e.",
    "url": "https://arxiv.org/abs/2602.11412",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Gradients Must Earn Their Influence: Unifying SFT with Generalized Entropic Objectives",
    "summary": "arXiv:2602.11424v1 Announce Type: cross Abstract: Standard negative log-likelihood (NLL) for Supervised Fine-Tuning (SFT) applies uniform token-level weighting. This rigidity creates a two-fold failure mode: (i) overemphasizing low-probability targets can amplify gradients on noisy supervision and disrupt robust priors, and (ii) uniform weighting p",
    "url": "https://arxiv.org/abs/2602.11424",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Fighting MRI Anisotropy: Learning Multiple Cardiac Shapes From a Single Implicit Neural Representation",
    "summary": "arXiv:2602.11436v1 Announce Type: cross Abstract: The anisotropic nature of short-axis (SAX) cardiovascular magnetic resonance imaging (CMRI) limits cardiac shape analysis. To address this, we propose to leverage near-isotropic, higher resolution computed tomography angiography (CTA) data of the heart. We use this data to train a single neural impl",
    "url": "https://arxiv.org/abs/2602.11436",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Towards Reliable Machine Translation: Scaling LLMs for Critical Error Detection and Safety",
    "summary": "arXiv:2602.11444v1 Announce Type: cross Abstract: Machine Translation (MT) plays a pivotal role in cross-lingual information access, public policy communication, and equitable knowledge dissemination. However, critical meaning errors, such as factual distortions, intent reversals, or biased translations, can undermine the reliability, fairness, and",
    "url": "https://arxiv.org/abs/2602.11444",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Enhanced Portable Ultra Low-Field Diffusion Tensor Imaging with Bayesian Artifact Correction and Deep Learning-Based Super-Resolution",
    "summary": "arXiv:2602.11446v1 Announce Type: cross Abstract: Portable, ultra-low-field (ULF) magnetic resonance imaging has the potential to expand access to neuroimaging but currently suffers from coarse spatial and angular resolutions and low signal-to-noise ratios. Diffusion tensor imaging (DTI), a sequence tailored to detect and reconstruct white matter t",
    "url": "https://arxiv.org/abs/2602.11446",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "From Noise to Order: Learning to Rank via Denoising Diffusion",
    "summary": "arXiv:2602.11453v1 Announce Type: cross Abstract: In information retrieval (IR), learning-to-rank (LTR) methods have traditionally limited themselves to discriminative machine learning approaches that model the probability of the document being relevant to the query given some feature representation of the query-document pair. In this work, we prop",
    "url": "https://arxiv.org/abs/2602.11453",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "EM-Aware Physical Synthesis: Neural Inductor Modeling and Intelligent Placement & Routing for RF Circuits",
    "summary": "arXiv:2602.11461v1 Announce Type: cross Abstract: This paper presents an ML-driven framework for automated RF physical synthesis that transforms circuit netlists into manufacturable GDSII layouts. While recent ML approaches demonstrate success in topology selection and parameter optimization, they fail to produce manufacturable layouts due to overs",
    "url": "https://arxiv.org/abs/2602.11461",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Compiler-Guided Inference-Time Adaptation: Improving GPT-5 Programming Performance in Idris",
    "summary": "arXiv:2602.11481v1 Announce Type: cross Abstract: GPT-5, a state of the art large language model from OpenAI, demonstrates strong performance in widely used programming languages such as Python, C++, and Java; however, its ability to operate in low resource or less commonly used languages remains underexplored. This work investigates whether GPT-5 ",
    "url": "https://arxiv.org/abs/2602.11481",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Understanding Persuasive Interactions between Generative Social Agents and Humans: The Knowledge-based Persuasion Model (KPM)",
    "summary": "arXiv:2602.11483v1 Announce Type: cross Abstract: Generative social agents (GSAs) use artificial intelligence to autonomously communicate with human users in a natural and adaptive manner. Currently, there is a lack of theorizing regarding interactions with GSAs, and likewise, few guidelines exist for studying how they influence user attitudes and ",
    "url": "https://arxiv.org/abs/2602.11483",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "RooflineBench: A Benchmarking Framework for On-Device LLMs via Roofline Analysis",
    "summary": "arXiv:2602.11506v1 Announce Type: cross Abstract: The transition toward localized intelligence through Small Language Models (SLMs) has intensified the need for rigorous performance characterization on resource-constrained edge hardware. However, objectively measuring the theoretical performance ceilings of diverse architectures across heterogeneou",
    "url": "https://arxiv.org/abs/2602.11506",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Multimodal Fact-Level Attribution for Verifiable Reasoning",
    "summary": "arXiv:2602.11509v1 Announce Type: cross Abstract: Multimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual factual claims. However, existing multimodal groundi",
    "url": "https://arxiv.org/abs/2602.11509",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Differentially Private and Communication Efficient Large Language Model Split Inference via Stochastic Quantization and Soft Prompt",
    "summary": "arXiv:2602.11513v1 Announce Type: cross Abstract: Large Language Models (LLMs) have achieved remarkable performance and received significant research interest. The enormous computational demands, however, hinder the local deployment on devices with limited resources. The current prevalent LLM inference paradigms require users to send queries to the",
    "url": "https://arxiv.org/abs/2602.11513",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "How Smart Is Your GUI Agent? A Framework for the Future of Software Interaction",
    "summary": "arXiv:2602.11514v1 Announce Type: cross Abstract: GUI agents are rapidly becoming a new interaction to software, allowing people to navigate web, desktop and mobile rather than execute them click by click. Yet ``agent'' is described with radically different degrees of autonomy, obscuring capability, responsibility and risk. We call for conceptual c",
    "url": "https://arxiv.org/abs/2602.11514",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Locally Interpretable Individualized Treatment Rules for Black-Box Decision Models",
    "summary": "arXiv:2602.11520v1 Announce Type: cross Abstract: Individualized treatment rules (ITRs) aim to optimize healthcare by tailoring treatment decisions to patient-specific characteristics. Existing methods typically rely on either interpretable but inflexible models or highly flexible black-box approaches that sacrifice interpretability; moreover, most",
    "url": "https://arxiv.org/abs/2602.11520",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Adaptive Milestone Reward for GUI Agents",
    "summary": "arXiv:2602.11524v1 Announce Type: cross Abstract: Reinforcement Learning (RL) has emerged as a mainstream paradigm for training Mobile GUI Agents, yet it struggles with the temporal credit assignment problem inherent in long-horizon tasks. A primary challenge lies in the trade-off between reward fidelity and density: outcome reward offers high fide",
    "url": "https://arxiv.org/abs/2602.11524",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Stop Tracking Me! Proactive Defense Against Attribute Inference Attack in LLMs",
    "summary": "arXiv:2602.11528v1 Announce Type: cross Abstract: Recent studies have shown that large language models (LLMs) can infer private user attributes (e.g., age, location, gender) from user-generated text shared online, enabling rapid and large-scale privacy breaches. Existing anonymization-based defenses are coarse-grained, lacking word-level precision ",
    "url": "https://arxiv.org/abs/2602.11528",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "AltTS: A Dual-Path Framework with Alternating Optimization for Multivariate Time Series Forecasting",
    "summary": "arXiv:2602.11533v1 Announce Type: cross Abstract: Multivariate time series forecasting involves two qualitatively distinct factors: (i) stable within-series autoregressive (AR) dynamics, and (ii) intermittent cross-dimension interactions that can become spurious over long horizons. We argue that fitting a single model to capture both effects create",
    "url": "https://arxiv.org/abs/2602.11533",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Krause Synchronization Transformers",
    "summary": "arXiv:2602.11534v1 Announce Type: cross Abstract: Self-attention in Transformers relies on globally normalized softmax weights, causing all tokens to compete for influence at every layer. When composed across depth, this interaction pattern induces strong synchronization dynamics that favor convergence toward a dominant mode, a behavior associated ",
    "url": "https://arxiv.org/abs/2602.11534",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Native Reasoning Models: Training Language Models to Reason on Unverifiable Data",
    "summary": "arXiv:2602.11549v1 Announce Type: cross Abstract: The prevailing paradigm for training large reasoning models--combining Supervised Fine-Tuning (SFT) with Reinforcement Learning with Verifiable Rewards (RLVR)--is fundamentally constrained by its reliance on high-quality, human-annotated reasoning data and external verifiers. This dependency incurs ",
    "url": "https://arxiv.org/abs/2602.11549",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "TS-Memory: Plug-and-Play Memory for Time Series Foundation Models",
    "summary": "arXiv:2602.11550v1 Announce Type: cross Abstract: Time Series Foundation Models (TSFMs) achieve strong zero-shot forecasting through large-scale pre-training, but adapting them to downstream domains under distribution shift remains challenging. Existing solutions face a trade-off: Parametric Adaptation can cause catastrophic forgetting and requires",
    "url": "https://arxiv.org/abs/2602.11550",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Perception-based Image Denoising via Generative Compression",
    "summary": "arXiv:2602.11553v1 Announce Type: cross Abstract: Image denoising aims to remove noise while preserving structural details and perceptual realism, yet distortion-driven methods often produce over-smoothed reconstructions, especially under strong noise and distribution shift. This paper proposes a generative compression framework for perception-base",
    "url": "https://arxiv.org/abs/2602.11553",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "ReaDy-Go: Real-to-Sim Dynamic 3D Gaussian Splatting Simulation for Environment-Specific Visual Navigation with Moving Obstacles",
    "summary": "arXiv:2602.11575v1 Announce Type: cross Abstract: Visual navigation models often struggle in real-world dynamic environments due to limited robustness to the sim-to-real gap and the difficulty of training policies tailored to target deployment environments (e.g., households, restaurants, and factories). Although real-to-sim navigation simulation us",
    "url": "https://arxiv.org/abs/2602.11575",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Analytical Search",
    "summary": "arXiv:2602.11581v1 Announce Type: cross Abstract: Analytical information needs, such as trend analysis and causal impact assessment, are prevalent across various domains including law, finance, science, and much more. However, existing information retrieval paradigms, whether based on relevance-oriented document ranking or retrieval-augmented gener",
    "url": "https://arxiv.org/abs/2602.11581",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Gradient Compression May Hurt Generalization: A Remedy by Synthetic Data Guided Sharpness Aware Minimization",
    "summary": "arXiv:2602.11584v1 Announce Type: cross Abstract: It is commonly believed that gradient compression in federated learning (FL) enjoys significant improvement in communication efficiency with negligible performance degradation. In this paper, we find that gradient compression induces sharper loss landscapes in federated learning, particularly under ",
    "url": "https://arxiv.org/abs/2602.11584",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation",
    "summary": "arXiv:2602.11598v1 Announce Type: cross Abstract: Embodied navigation has long been fragmented by task-specific architectures. We introduce ABot-N0, a unified Vision-Language-Action (VLA) foundation model that achieves a ``Grand Unification'' across 5 core tasks: Point-Goal, Object-Goal, Instruction-Following, POI-Goal, and Person-Following. ABot-N",
    "url": "https://arxiv.org/abs/2602.11598",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "PLOT-CT: Pre-log Voronoi Decomposition Assisted Generation for Low-dose CT Reconstruction",
    "summary": "arXiv:2602.11625v1 Announce Type: cross Abstract: Low-dose computed tomography (LDCT) reconstruction is fundamentally challenged by severe noise and compromised data fidelity under reduced radiation exposure. Most existing methods operate either in the image or post-log projection domain, which fails to fully exploit the rich structural information",
    "url": "https://arxiv.org/abs/2602.11625",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "ArGEnT: Arbitrary Geometry-encoded Transformer for Operator Learning",
    "summary": "arXiv:2602.11626v1 Announce Type: cross Abstract: Learning solution operators for systems with complex, varying geometries and parametric physical settings is a central challenge in scientific machine learning. In many-query regimes such as design optimization, control and inverse problems, surrogate modeling must generalize across geometries while",
    "url": "https://arxiv.org/abs/2602.11626",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning",
    "summary": "arXiv:2602.11636v1 Announce Type: cross Abstract: Large-scale Visual Instruction Tuning (VIT) has become a key paradigm for advancing the performance of vision-language models (VLMs) across various multimodal tasks. However, training on the large-scale datasets is computationally expensive and inefficient due to redundancy in the data, which motiva",
    "url": "https://arxiv.org/abs/2602.11636",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Variation-aware Flexible 3D Gaussian Editing",
    "summary": "arXiv:2602.11638v1 Announce Type: cross Abstract: Indirect editing methods for 3D Gaussian Splatting (3DGS) have recently witnessed significant advancements. These approaches operate by first applying edits in the rendered 2D space and subsequently projecting the modifications back into 3D. However, this paradigm inevitably introduces cross-view in",
    "url": "https://arxiv.org/abs/2602.11638",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning",
    "summary": "arXiv:2602.11643v1 Announce Type: cross Abstract: Tactile information plays a crucial role in human manipulation tasks and has recently garnered increasing attention in robotic manipulation. However, existing approaches mostly focus on the alignment of visual and tactile features and the integration mechanism tends to be direct concatenation. Conse",
    "url": "https://arxiv.org/abs/2602.11643",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Brain Tumor Classifiers Under Attack: Robustness of ResNet Variants Against Transferable FGSM and PGD Attacks",
    "summary": "arXiv:2602.11646v1 Announce Type: cross Abstract: Adversarial robustness in deep learning models for brain tumor classification remains an underexplored yet critical challenge, particularly for clinical deployment scenarios involving MRI data. In this work, we investigate the susceptibility and resilience of several ResNet-based architectures, refe",
    "url": "https://arxiv.org/abs/2602.11646",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "DMind-3: A Sovereign Edge--Local--Cloud AI System with Controlled Deliberation and Correction-Based Tuning for Safe, Low-Latency Transaction Execution",
    "summary": "arXiv:2602.11651v1 Announce Type: cross Abstract: This paper introduces DMind-3, a sovereign Edge-Local-Cloud intelligence stack designed to secure irreversible financial execution in Web3 environments against adversarial risks and strict latency constraints. While existing cloud-centric assistants compromise privacy and fail under network congesti",
    "url": "https://arxiv.org/abs/2602.11651",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "LoRA-based Parameter-Efficient LLMs for Continuous Learning in Edge-based Malware Detection",
    "summary": "arXiv:2602.11655v1 Announce Type: cross Abstract: The proliferation of edge devices has created an urgent need for security solutions capable of detecting malware in real time while operating under strict computational and memory constraints. Recently, Large Language Models (LLMs) have demonstrated remarkable capabilities in recognizing complex pat",
    "url": "https://arxiv.org/abs/2602.11655",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "SToRM: Supervised Token Reduction for Multi-modal LLMs toward efficient end-to-end autonomous driving",
    "summary": "arXiv:2602.11656v1 Announce Type: cross Abstract: In autonomous driving, end-to-end (E2E) driving systems that predict control commands directly from sensor data have achieved significant advancements. For safe driving in unexpected scenarios, these systems may additionally rely on human interventions such as natural language instructions. Using a ",
    "url": "https://arxiv.org/abs/2602.11656",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Provable Offline Reinforcement Learning for Structured Cyclic MDPs",
    "summary": "arXiv:2602.11679v1 Announce Type: cross Abstract: We introduce a novel cyclic Markov decision process (MDP) framework for multi-step decision problems with heterogeneous stage-specific dynamics, transitions, and discount factors across the cycle. In this setting, offline learning is challenging: optimizing a policy at any stage shifts the state dis",
    "url": "https://arxiv.org/abs/2602.11679",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "PatientHub: A Unified Framework for Patient Simulation",
    "summary": "arXiv:2602.11684v1 Announce Type: cross Abstract: As Large Language Models increasingly power role-playing applications, simulating patients has become a valuable tool for training counselors and scaling therapeutic assessment. However, prior work is fragmented: existing approaches rely on incompatible, non-standardized data formats, prompts, and e",
    "url": "https://arxiv.org/abs/2602.11684",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "DRACO: a Cross-Domain Benchmark for Deep Research Accuracy, Completeness, and Objectivity",
    "summary": "arXiv:2602.11685v1 Announce Type: cross Abstract: We present DRACO (Deep Research Accuracy, Completeness, and Objectivity), a benchmark of complex deep research tasks. These tasks, which span 10 domains and draw on information sources from 40 countries, originate from anonymized real-world usage patterns within a large-scale deep research system. T",
    "url": "https://arxiv.org/abs/2602.11685",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "ANML: Attribution-Native Machine Learning with Guaranteed Robustness",
    "summary": "arXiv:2602.11690v1 Announce Type: cross Abstract: Frontier AI systems increasingly train on specialized expert data, from clinical records to proprietary research to curated datasets, yet current training pipelines treat all samples identically. A Nobel laureate's contribution receives the same weight as an unverified submission. We introduce ANML ",
    "url": "https://arxiv.org/abs/2602.11690",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "OMEGA-Avatar: One-shot Modeling of 360{\\deg} Gaussian Avatars",
    "summary": "arXiv:2602.11693v1 Announce Type: cross Abstract: Creating high-fidelity, animatable 3D avatars from a single image remains a formidable challenge. We identified three desirable attributes of avatar generation: 1) the method should be feed-forward, 2) model a 360{\\deg} full-head, and 3) should be animation-ready. However, current work addresses onl",
    "url": "https://arxiv.org/abs/2602.11693",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "TabSieve: Explicit In-Table Evidence Selection for Tabular Prediction",
    "summary": "arXiv:2602.11700v1 Announce Type: cross Abstract: Tabular prediction can benefit from in-table rows as few-shot evidence, yet existing tabular models typically perform instance-wise inference and LLM-based prompting is often brittle. Models do not consistently leverage relevant rows, and noisy context can degrade performance. To address this challe",
    "url": "https://arxiv.org/abs/2602.11700",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Semantically Conditioned Diffusion Models for Cerebral DSA Synthesis",
    "summary": "arXiv:2602.11703v1 Announce Type: cross Abstract: Digital subtraction angiography (DSA) plays a central role in the diagnosis and treatment of cerebrovascular disease, yet its invasive nature and high acquisition cost severely limit large-scale data collection and public data sharing. Therefore, we developed a semantically conditioned latent diffus",
    "url": "https://arxiv.org/abs/2602.11703",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "LLM-Driven 3D Scene Generation of Agricultural Simulation Environments",
    "summary": "arXiv:2602.11706v1 Announce Type: cross Abstract: Procedural generation techniques in 3D rendering engines have revolutionized the creation of complex environments, reducing reliance on manual design. Recent approaches using Large Language Models (LLMs) for 3D scene generation show promise but often lack domain-specific reasoning, verification mech",
    "url": "https://arxiv.org/abs/2602.11706",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Adapting Vision-Language Models for E-commerce Understanding at Scale",
    "summary": "arXiv:2602.11733v1 Announce Type: cross Abstract: E-commerce product understanding demands by nature, strong multimodal comprehension from text, images, and structured attributes. General-purpose Vision-Language Models (VLMs) enable generalizable multimodal latent modelling, yet there is no documented, well-known strategy for adapting them to the a",
    "url": "https://arxiv.org/abs/2602.11733",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "AmbiBench: Benchmarking Mobile GUI Agents Beyond One-Shot Instructions in the Wild",
    "summary": "arXiv:2602.11750v1 Announce Type: cross Abstract: Benchmarks are paramount for gauging progress in the domain of Mobile GUI Agents. In practical scenarios, users frequently fail to articulate precise directives containing full task details at the onset, and their expressions are typically ambiguous. Consequently, agents are required to converge on ",
    "url": "https://arxiv.org/abs/2602.11750",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Cooperation Breakdown in LLM Agents Under Communication Delays",
    "summary": "arXiv:2602.11754v1 Announce Type: cross Abstract: LLM-based multi-agent systems (LLM-MAS), in which autonomous AI agents cooperate to solve tasks, are gaining increasing attention. For such systems to be deployed in society, agents must be able to establish cooperation and coordination under real-world computational and communication constraints. W",
    "url": "https://arxiv.org/abs/2602.11754",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling",
    "summary": "arXiv:2602.11761v1 Announce Type: cross Abstract: The evolution of large language models (LLMs) towards applications with ultra-long contexts faces challenges posed by the high computational and memory costs of the Transformer architecture. While existing sparse and linear attention mechanisms attempt to mitigate these issues, they typically involv",
    "url": "https://arxiv.org/abs/2602.11761",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Safe Fairness Guarantees Without Demographics in Classification: Spectral Uncertainty Set Perspective",
    "summary": "arXiv:2602.11785v1 Announce Type: cross Abstract: As automated classification systems become increasingly prevalent, concerns have emerged over their potential to reinforce and amplify existing societal biases. In the light of this issue, many methods have been proposed to enhance the fairness guarantees of classifiers. Most of the existing interve",
    "url": "https://arxiv.org/abs/2602.11785",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Evaluating LLM Safety Under Repeated Inference via Accelerated Prompt Stress Testing",
    "summary": "arXiv:2602.11786v1 Announce Type: cross Abstract: Traditional benchmarks for large language models (LLMs) primarily assess safety risk through breadth-oriented evaluation across diverse tasks. However, real-world deployment exposes a different class of risk: operational failures arising from repeated inference on identical or near-identical prompts",
    "url": "https://arxiv.org/abs/2602.11786",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "ULTRA:Urdu Language Transformer-based Recommendation Architecture",
    "summary": "arXiv:2602.11836v1 Announce Type: cross Abstract: Urdu, as a low-resource language, lacks effective semantic content recommendation systems, particularly in the domain of personalized news retrieval. Existing approaches largely rely on lexical matching or language-agnostic techniques, which struggle to capture semantic intent and perform poorly und",
    "url": "https://arxiv.org/abs/2602.11836",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Improving Neural Retrieval with Attribution-Guided Query Rewriting",
    "summary": "arXiv:2602.11841v1 Announce Type: cross Abstract: Neural retrievers are effective but brittle: underspecified or ambiguous queries can misdirect ranking even when relevant documents exist. Existing approaches address this brittleness only partially: LLMs rewrite queries without retriever feedback, and explainability methods identify misleading toke",
    "url": "https://arxiv.org/abs/2602.11841",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Resource-Aware Deployment Optimization for Collaborative Intrusion Detection in Layered Networks",
    "summary": "arXiv:2602.11851v1 Announce Type: cross Abstract: Collaborative Intrusion Detection Systems (CIDS) are increasingly adopted to counter cyberattacks, as their collaborative nature enables them to adapt to diverse scenarios across heterogeneous environments. As distributed critical infrastructure operates in rapidly evolving environments, such as dro",
    "url": "https://arxiv.org/abs/2602.11851",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception",
    "summary": "arXiv:2602.11858v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception, where decisive evidence is small and easily overwhelmed by global context. Recent \"Thinking-with-Images\" methods alleviate this by iteratively zooming in and out regions of i",
    "url": "https://arxiv.org/abs/2602.11858",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Towards Fair and Comprehensive Evaluation of Routers in Collaborative LLM Systems",
    "summary": "arXiv:2602.11877v1 Announce Type: cross Abstract: Large language models (LLMs) have achieved success, but cost and privacy constraints necessitate deploying smaller models locally while offloading complex queries to cloud-based models. Existing router evaluations are unsystematic, overlooking scenario-specific requirements and out-of-distribution r",
    "url": "https://arxiv.org/abs/2602.11877",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "SynthRAR: Ring Artifacts Reduction in CT with Unrolled Network and Synthetic Data Training",
    "summary": "arXiv:2602.11880v1 Announce Type: cross Abstract: Defective and inconsistent responses in CT detectors can cause ring and streak artifacts in the reconstructed images, making them unusable for clinical purposes. In recent years, several ring artifact reduction solutions have been proposed in the image domain or in the sinogram domain using supervis",
    "url": "https://arxiv.org/abs/2602.11880",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning",
    "summary": "arXiv:2602.11882v1 Announce Type: cross Abstract: Efficient spatial reasoning requires world models that remain reliable under tight precision budgets. We study whether low-bit planning behavior is determined mostly by total bitwidth or by where bits are allocated across modules. Using DINO-WM on the Wall planning task, we run a paired-goal mixed-b",
    "url": "https://arxiv.org/abs/2602.11882",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Agentic AI for Cybersecurity: A Meta-Cognitive Architecture for Governable Autonomy",
    "summary": "arXiv:2602.11897v1 Announce Type: cross Abstract: Contemporary AI-driven cybersecurity systems are predominantly architected as model-centric detection and automation pipelines optimized for task-level performance metrics such as accuracy and response latency. While effective for bounded classification tasks, these architectures struggle to support",
    "url": "https://arxiv.org/abs/2602.11897",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Mitigating Mismatch within Reference-based Preference Optimization",
    "summary": "arXiv:2602.11902v1 Announce Type: cross Abstract: Direct Preference Optimization (DPO) has become the de facto standard for offline preference alignment of large language models, but its reliance on a reference policy introduces a critical tension. DPO weighs each update relative to a reference, which stabilizes the training by regularizing the upd",
    "url": "https://arxiv.org/abs/2602.11902",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs: A Systematic Evaluation",
    "summary": "arXiv:2602.11904v1 Announce Type: cross Abstract: Software languages evolve over time for reasons such as feature additions. When grammars evolve, textual instances that originally conformed to them may become outdated. While model-driven engineering provides many techniques for co-evolving models with metamodel changes, these approaches are not de",
    "url": "https://arxiv.org/abs/2602.11904",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "DynaHOI: Benchmarking Hand-Object Interaction for Dynamic Target",
    "summary": "arXiv:2602.11919v1 Announce Type: cross Abstract: Most existing hand motion generation benchmarks for hand-object interaction (HOI) focus on static objects, leaving dynamic scenarios with moving targets and time-critical coordination largely untested. To address this gap, we introduce the DynaHOI-Gym, a unified online closed-loop platform with para",
    "url": "https://arxiv.org/abs/2602.11919",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Who Does What? Archetypes of Roles Assigned to LLMs During Human-AI Decision-Making",
    "summary": "arXiv:2602.11924v1 Announce Type: cross Abstract: LLMs are increasingly supporting decision-making across high-stakes domains, requiring critical reflection on the socio-technical factors that shape how humans and LLMs are assigned roles and interact during human-in-the-loop decision-making. This paper introduces the concept of human-LLM archetypes",
    "url": "https://arxiv.org/abs/2602.11924",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "AdaptEvolve: Improving Efficiency of Evolutionary AI Agents through Adaptive Model Selection",
    "summary": "arXiv:2602.11931v1 Announce Type: cross Abstract: Evolutionary agentic systems intensify the trade-off between computational efficiency and reasoning capability by repeatedly invoking large language models (LLMs) during inference. This setting raises a central question: how can an agent dynamically select an LLM that is sufficiently capable for the",
    "url": "https://arxiv.org/abs/2602.11931",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "IncompeBench: A Permissively Licensed, Fine-Grained Benchmark for Music Information Retrieval",
    "summary": "arXiv:2602.11941v1 Announce Type: cross Abstract: Multimodal Information Retrieval has made significant progress in recent years, leveraging the increasingly strong multimodal abilities of deep pre-trained models to represent information across modalities. Music Information Retrieval (MIR), in particular, has considerably increased in quality, with",
    "url": "https://arxiv.org/abs/2602.11941",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Synthesis of Late Gadolinium Enhancement Images via Implicit Neural Representations for Cardiac Scar Segmentation",
    "summary": "arXiv:2602.11942v1 Announce Type: cross Abstract: Late gadolinium enhancement (LGE) imaging is the clinical standard for myocardial scar assessment, but limited annotated datasets hinder the development of automated segmentation methods. We propose a novel framework that synthesises both LGE images and their corresponding segmentation masks using i",
    "url": "https://arxiv.org/abs/2602.11942",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Towards Performance-Enhanced Model-Contrastive Federated Learning using Historical Information in Heterogeneous Scenarios",
    "summary": "arXiv:2602.11945v1 Announce Type: cross Abstract: Federated Learning (FL) enables multiple nodes to collaboratively train a model without sharing raw data. However, FL systems are usually deployed in heterogeneous scenarios, where nodes differ in both data distributions and participation frequencies, which undermines the FL performance. To tackle t",
    "url": "https://arxiv.org/abs/2602.11945",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "TAVAE: A VAE with Adaptable Priors Explains Contextual Modulation in the Visual Cortex",
    "summary": "arXiv:2602.11956v1 Announce Type: cross Abstract: The brain interprets visual information through learned regularities, a computation formalized as probabilistic inference under a prior. The visual cortex establishes priors for this inference, some delivered through established top-down connections that inform low-level cortices about statistics re",
    "url": "https://arxiv.org/abs/2602.11956",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Manifold-Aware Temporal Domain Generalization for Large Language Models",
    "summary": "arXiv:2602.11965v1 Announce Type: cross Abstract: Temporal distribution shifts are pervasive in real-world deployments of Large Language Models (LLMs), where data evolves continuously over time. While Temporal Domain Generalization (TDG) seeks to model such structured evolution, existing approaches characterize model adaptation in the full paramete",
    "url": "https://arxiv.org/abs/2602.11965",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Accelerating Robotic Reinforcement Learning with Agent Guidance",
    "summary": "arXiv:2602.11978v1 Announce Type: cross Abstract: Reinforcement Learning (RL) offers a powerful paradigm for autonomous robots to master generalist manipulation skills through trial-and-error. However, its real-world application is stifled by severe sample inefficiency. Recent Human-in-the-Loop (HIL) methods accelerate training by using human corre",
    "url": "https://arxiv.org/abs/2602.11978",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?",
    "summary": "arXiv:2602.11988v1 Announce Type: cross Abstract: A widespread practice in software development is to tailor coding agents to repositories using context files, such as AGENTS.md, by either manually or automatically generating them. Although this practice is strongly encouraged by agent developers, there is currently no rigorous investigation into w",
    "url": "https://arxiv.org/abs/2602.11988",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "On the Sensitivity of Firing Rate-Based Federated Spiking Neural Networks to Differential Privacy",
    "summary": "arXiv:2602.12009v1 Announce Type: cross Abstract: Federated Neuromorphic Learning (FNL) enables energy-efficient and privacy-preserving learning on devices without centralizing data. However, real-world deployments require additional privacy mechanisms that can significantly alter training signals. This paper analyzes how Differential Privacy (DP) ",
    "url": "https://arxiv.org/abs/2602.12009",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "An Empirical Study of the Imbalance Issue in Software Vulnerability Detection",
    "summary": "arXiv:2602.12038v1 Announce Type: cross Abstract: Vulnerability detection is crucial to protect software security. Nowadays, deep learning (DL) is the most promising technique to automate this detection task, leveraging its superior ability to extract patterns and representations within extensive code volumes. Despite its promise, DL-based vulnerab",
    "url": "https://arxiv.org/abs/2602.12038",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Fourier Transformers for Latent Crystallographic Diffusion and Generative Modeling",
    "summary": "arXiv:2602.12045v1 Announce Type: cross Abstract: The discovery of new crystalline materials calls for generative models that handle periodic boundary conditions, crystallographic symmetries, and physical constraints, while scaling to large and structurally diverse unit cells. We propose a reciprocal-space generative pipeline that represents crysta",
    "url": "https://arxiv.org/abs/2602.12045",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "ModelWisdom: An Integrated Toolkit for TLA+ Model Visualization, Digest and Repair",
    "summary": "arXiv:2602.12058v1 Announce Type: cross Abstract: Model checking in TLA+ provides strong correctness guarantees, yet practitioners continue to face significant challenges in interpreting counterexamples, understanding large state-transition graphs, and repairing faulty models. These difficulties stem from the limited explainability of raw model-che",
    "url": "https://arxiv.org/abs/2602.12058",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Choose Your Agent: Tradeoffs in Adopting AI Advisors, Coaches, and Delegates in Multi-Party Negotiation",
    "summary": "arXiv:2602.12089v1 Announce Type: cross Abstract: As AI usage becomes more prevalent in social contexts, understanding agent-user interaction is critical to designing systems that improve both individual and group outcomes. We present an online behavioral experiment (N = 243) in which participants play three multi-turn bargaining games in groups of",
    "url": "https://arxiv.org/abs/2602.12089",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "DeepSight: An All-in-One LM Safety Toolkit",
    "summary": "arXiv:2602.12092v1 Announce Type: cross Abstract: As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluatio",
    "url": "https://arxiv.org/abs/2602.12092",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Multi Graph Search for High-Dimensional Robot Motion Planning",
    "summary": "arXiv:2602.12096v1 Announce Type: cross Abstract: Efficient motion planning for high-dimensional robotic systems, such as manipulators and mobile manipulators, is critical for real-time operation and reliable deployment. Although advances in planning algorithms have enhanced scalability to high-dimensional state spaces, these improvements often com",
    "url": "https://arxiv.org/abs/2602.12096",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "On the Complexity of Offline Reinforcement Learning with $Q^\\star$-Approximation and Partial Coverage",
    "summary": "arXiv:2602.12107v1 Announce Type: cross Abstract: We study offline reinforcement learning under $Q^\\star$-approximation and partial coverage, a setting that motivates practical algorithms such as Conservative $Q$-Learning (CQL; Kumar et al., 2020) but has received limited theoretical attention. Our work is inspired by the following open question: \"",
    "url": "https://arxiv.org/abs/2602.12107",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "KAN-FIF: Spline-Parameterized Lightweight Physics-based Tropical Cyclone Estimation on Meteorological Satellite",
    "summary": "arXiv:2602.12117v1 Announce Type: cross Abstract: Tropical cyclones (TC) are among the most destructive natural disasters, causing catastrophic damage to coastal regions through extreme winds, heavy rainfall, and storm surges. Timely monitoring of tropical cyclones is crucial for reducing loss of life and property, yet it is hindered by the computa",
    "url": "https://arxiv.org/abs/2602.12117",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Meta-Sel: Efficient Demonstration Selection for In-Context Learning via Supervised Meta-Learning",
    "summary": "arXiv:2602.12123v1 Announce Type: cross Abstract: Demonstration selection is a practical bottleneck in in-context learning (ICL): under a tight prompt budget, accuracy can change substantially depending on which few-shot examples are included, yet selection must remain cheap enough to run per query over large candidate pools. We propose Meta-Sel, a",
    "url": "https://arxiv.org/abs/2602.12123",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation",
    "summary": "arXiv:2602.12125v1 Announce Type: cross Abstract: On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this wo",
    "url": "https://arxiv.org/abs/2602.12125",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "On the Adoption of AI Coding Agents in Open-source Android and iOS Development",
    "summary": "arXiv:2602.12144v1 Announce Type: cross Abstract: AI coding agents are increasingly contributing to software development, yet their impact on mobile development has received little empirical attention. In this paper, we present the first category-level empirical study of agent-generated code in open-source mobile app projects. We analyzed PR accept",
    "url": "https://arxiv.org/abs/2602.12144",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "dVoting: Fast Voting for dLLMs",
    "summary": "arXiv:2602.12153v1 Announce Type: cross Abstract: Diffusion Large Language Models (dLLMs) represent a new paradigm beyond autoregressive modeling, offering competitive performance while naturally enabling a flexible decoding process. Specifically, dLLMs can generate tokens at arbitrary positions in parallel, endowing them with significant potential",
    "url": "https://arxiv.org/abs/2602.12153",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting",
    "summary": "arXiv:2602.12159v1 Announce Type: cross Abstract: Object navigation is a core capability of embodied intelligence, enabling an agent to locate target objects in unknown environments. Recent advances in vision-language models (VLMs) have facilitated zero-shot object navigation (ZSON). However, existing methods often rely on scene abstractions that c",
    "url": "https://arxiv.org/abs/2602.12159",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "SAGEO Arena: A Realistic Environment for Evaluating Search-Augmented Generative Engine Optimization",
    "summary": "arXiv:2602.12187v1 Announce Type: cross Abstract: Search-Augmented Generative Engines (SAGE) have emerged as a new paradigm for information access, bridging web-scale retrieval with generative capabilities to deliver synthesized answers. This shift has fundamentally reshaped how web content gains exposure online, giving rise to Search-Augmented Gen",
    "url": "https://arxiv.org/abs/2602.12187",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education",
    "summary": "arXiv:2602.12196v1 Announce Type: cross Abstract: AI models have achieved state-of-the-art results in textual reasoning; however, their ability to reason over spatial and relational structures remains a critical bottleneck -- particularly in early-grade maths, which relies heavily on visuals. This paper introduces the visual reasoning benchmark (VR",
    "url": "https://arxiv.org/abs/2602.12196",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing",
    "summary": "arXiv:2602.12205v1 Announce Type: cross Abstract: Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities co",
    "url": "https://arxiv.org/abs/2602.12205",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "VIRENA: Virtual Arena for Research, Education, and Democratic Innovation",
    "summary": "arXiv:2602.12207v1 Announce Type: cross Abstract: Digital platforms shape how people communicate, deliberate, and form opinions. Studying these dynamics has become increasingly difficult due to restricted data access, ethical constraints on real-world experiments, and limitations of existing research tools. VIRENA (Virtual Arena) is a platform that",
    "url": "https://arxiv.org/abs/2602.12207",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics",
    "summary": "arXiv:2602.12218v1 Announce Type: cross Abstract: Determining whether neural models internalize physical laws as world models, rather than exploiting statistical shortcuts, remains challenging, especially under out-of-distribution (OOD) shifts. Standard evaluations often test latent capability via downstream adaptation (e.g., fine-tuning or high-ca",
    "url": "https://arxiv.org/abs/2602.12218",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training",
    "summary": "arXiv:2602.12222v1 Announce Type: cross Abstract: Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL's use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \\textbf{\\tex",
    "url": "https://arxiv.org/abs/2602.12222",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Bandit Learning in Matching Markets with Interviews",
    "summary": "arXiv:2602.12224v1 Announce Type: cross Abstract: Two-sided matching markets rely on preferences from both sides, yet it is often impractical to evaluate preferences. Participants, therefore, conduct a limited number of interviews, which provide early, noisy impressions and shape final decisions. We study bandit learning in matching markets with in",
    "url": "https://arxiv.org/abs/2602.12224",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Neuromorphic Vision",
    "summary": "arXiv:2602.12236v1 Announce Type: cross Abstract: Neuromorphic vision systems based on spiking neural networks (SNNs) offer ultra-low-power perception for event-based and frame-based cameras, yet catastrophic forgetting remains a critical barrier to deployment in continually evolving environments. Existing continual learning methods, developed prim",
    "url": "https://arxiv.org/abs/2602.12236",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Olmix: A Framework for Data Mixing Throughout LM Development",
    "summary": "arXiv:2602.12237v1 Announce Type: cross Abstract: Data mixing -- determining the ratios of data from different domains -- is a first-order concern for training language models (LMs). While existing mixing methods show promise, they fall short when applied during real-world LM development. We present Olmix, a framework that addresses two such challe",
    "url": "https://arxiv.org/abs/2602.12237",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces",
    "summary": "arXiv:2602.12245v1 Announce Type: cross Abstract: Joint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings, inducing a scalar compatibility energy in a latent space. In contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed dis",
    "url": "https://arxiv.org/abs/2602.12245",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "ExtractBench: A Benchmark and Evaluation Methodology for Complex Structured Extraction",
    "summary": "arXiv:2602.12247v1 Announce Type: cross Abstract: Unstructured documents like PDFs contain valuable structured information, but downstream systems require this data in reliable, standardized formats. LLMs are increasingly deployed to automate this extraction, making accuracy and reliability paramount. However, progress is bottlenecked by two gaps. ",
    "url": "https://arxiv.org/abs/2602.12247",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "A technical curriculum on language-oriented artificial intelligence in translation and specialised communication",
    "summary": "arXiv:2602.12251v1 Announce Type: cross Abstract: This paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L&amp;T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposi",
    "url": "https://arxiv.org/abs/2602.12251",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "On the implicit regularization of Langevin dynamics with projected noise",
    "summary": "arXiv:2602.12257v1 Announce Type: cross Abstract: We study Langevin dynamics with noise projected onto the directions orthogonal to an isometric group action. This mathematical model is introduced to shed new light on the effects of symmetry on stochastic gradient descent for over-parametrized models. Our main result identifies a novel form of impl",
    "url": "https://arxiv.org/abs/2602.12257",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Creative Ownership in the Age of AI",
    "summary": "arXiv:2602.12270v1 Announce Type: cross Abstract: Copyright law focuses on whether a new work is \"substantially similar\" to an existing one, but generative AI can closely imitate style without copying content, a capability now central to ongoing litigation. We argue that existing definitions of infringement are ill-suited to this setting and propos",
    "url": "https://arxiv.org/abs/2602.12270",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "AttentionRetriever: Attention Layers are Secretly Long Document Retrievers",
    "summary": "arXiv:2602.12278v1 Announce Type: cross Abstract: Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, includin",
    "url": "https://arxiv.org/abs/2602.12278",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "UniT: Unified Multimodal Chain-of-Thought Test-time Scaling",
    "summary": "arXiv:2602.12279v1 Announce Type: cross Abstract: Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, o",
    "url": "https://arxiv.org/abs/2602.12279",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment",
    "summary": "arXiv:2602.12281v1 Announce Type: cross Abstract: The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this",
    "url": "https://arxiv.org/abs/2602.12281",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Can Complexity and Uncomputability Explain Intelligence? SuperARC: A Test for Artificial Super Intelligence Based on Recursive Compression",
    "summary": "arXiv:2503.16743v5 Announce Type: replace Abstract: We introduce an increasing-complexity, open-ended, and human-agnostic metric to evaluate foundational and frontier AI models in the context of Artificial General Intelligence (AGI) and Artificial Super Intelligence (ASI) claims. Unlike other tests that rely on human-centric questions and expected ",
    "url": "https://arxiv.org/abs/2503.16743",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges",
    "summary": "arXiv:2508.06111v2 Announce Type: replace Abstract: Evaluating the capabilities and risks of foundation models is paramount, yet current methods demand extensive domain expertise, hindering their scalability as these models rapidly evolve. We introduce SKATE: a novel evaluation framework in which large language models (LLMs) compete by generating a",
    "url": "https://arxiv.org/abs/2508.06111",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Leveraging Generative AI for Human Understanding: Meta-Requirements and Design Principles for Explanatory AI as a new Paradigm",
    "summary": "arXiv:2508.06352v2 Announce Type: replace Abstract: Artificial intelligence (AI) systems increasingly support decision-making across critical domains, yet current explainable AI (XAI) approaches prioritize algorithmic transparency over human comprehension. While XAI methods reveal computational processes for model validation and audit, end users re",
    "url": "https://arxiv.org/abs/2508.06352",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Hybrid Reinforcement Learning and Search for Flight Trajectory Planning",
    "summary": "arXiv:2509.04100v2 Announce Type: replace Abstract: This paper explores the combination of Reinforcement Learning (RL) and search-based path planners to speed up the optimization of flight paths for airliners, where in case of emergency a fast route re-calculation can be crucial. The fundamental idea is to train an RL Agent to pre-compute near-opti",
    "url": "https://arxiv.org/abs/2509.04100",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Roundtable Policy: Confidence-Weighted-Consensus Aggregation Improves Multi-Agent-System Reasoning",
    "summary": "arXiv:2509.16839v2 Announce Type: replace Abstract: Multi-agent systems have demonstrated exceptional performance in downstream tasks beyond diverse single agent baselines. A growing body of work has explored ways to improve their reasoning and collaboration, from vote, debate, to complex interaction protocols. However, it still remains opaque why ",
    "url": "https://arxiv.org/abs/2509.16839",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Logical Structure as Knowledge: Enhancing LLM Reasoning via Structured Logical Knowledge Density Estimation",
    "summary": "arXiv:2509.24836v4 Announce Type: replace Abstract: The reasoning capabilities of Large Language Models (LLMs) are increasingly attributed to training data quality rather than mere parameter scaling. However, existing data-centric paradigms often equate quality with factuality or diversity and ignore the internal logical complexity of training samp",
    "url": "https://arxiv.org/abs/2509.24836",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Human Behavior Atlas: Benchmarking Unified Psychological and Social Behavior Understanding",
    "summary": "arXiv:2510.04899v2 Announce Type: replace Abstract: Using intelligent systems to perceive psychological and social behaviors, that is, the underlying affective, cognitive, and pathological states that are manifested through observable behaviors and social interactions, remains a challenge due to their complex, multifaceted, and personalized nature.",
    "url": "https://arxiv.org/abs/2510.04899",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "MARSHAL: Incentivizing Multi-Agent Reasoning via Self-Play with Strategic LLMs",
    "summary": "arXiv:2510.15414v3 Announce Type: replace Abstract: Developing Large Language Models (LLMs) to cooperate and compete effectively within multi-agent systems (MASs) is a critical step towards more advanced intelligence. While reinforcement learning (RL) has proven effective for enhancing reasoning in single-agent tasks, its extension to multi-turn, m",
    "url": "https://arxiv.org/abs/2510.15414",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "SMaRT: Select, Mix, and ReinvenT -- A Strategy Fusion Framework for LLM-Driven Reasoning and Planning",
    "summary": "arXiv:2510.18095v2 Announce Type: replace Abstract: Large Language Models (LLMs) have redefined complex task automation with exceptional generalization capabilities. Despite these advancements, state-of-the-art methods rely on single-strategy prompting, missing the synergy of diverse reasoning approaches. No single strategy excels universally, high",
    "url": "https://arxiv.org/abs/2510.18095",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Evaluating LLM Reasoning Beyond Correctness and CoT",
    "summary": "arXiv:2510.18134v2 Announce Type: replace Abstract: What does it truly mean for a language model to \"reason\"? Current evaluations reward models' correct standalone answers-but correctness alone reveals little about the process that produced them. We argue that reasoning should be understood not as a static chain of steps but as a dynamic trajectory",
    "url": "https://arxiv.org/abs/2510.18134",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving Assistants",
    "summary": "arXiv:2601.12138v2 Announce Type: replace Abstract: Large Language Models (LLMs) are increasingly integrated into vehicle-based digital assistants, where unsafe, ambiguous, or legally incorrect responses can lead to serious safety, ethical, and regulatory consequences. Despite growing interest in LLM safety, existing taxonomies and evaluation frame",
    "url": "https://arxiv.org/abs/2601.12138",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Phase Transition for Budgeted Multi-Agent Synergy",
    "summary": "arXiv:2601.17311v2 Announce Type: replace Abstract: Multi-agent systems can improve reliability, yet under a fixed inference budget they often help, saturate, or even collapse. We develop a minimal and calibratable theory that predicts these regimes from three binding constraints of modern agent stacks: finite context windows, lossy inter-agent com",
    "url": "https://arxiv.org/abs/2601.17311",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations",
    "summary": "arXiv:2602.03828v2 Announce Type: replace Abstract: High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific",
    "url": "https://arxiv.org/abs/2602.03828",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "When AI Persuades: Adversarial Explanation Attacks on Human Trust in AI-Assisted Decision Making",
    "summary": "arXiv:2602.04003v2 Announce Type: replace Abstract: Most adversarial threats in artificial intelligence target the computational behavior of models rather than the humans who rely on them. Yet modern AI systems increasingly operate within human decision loops, where users interpret and act on model recommendations. Large Language Models generate fl",
    "url": "https://arxiv.org/abs/2602.04003",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search",
    "summary": "arXiv:2602.05014v3 Announce Type: replace Abstract: With the rapid advancement of tool-use capabilities in Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) is shifting from static, one-shot retrieval toward autonomous, multi-turn evidence acquisition. However, existing agentic search frameworks typically treat long documents as fl",
    "url": "https://arxiv.org/abs/2602.05014",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "The Moltbook Illusion: Separating Human Influence from Emergent Behavior in AI Agent Societies",
    "summary": "arXiv:2602.07432v2 Announce Type: replace Abstract: When AI agents on the social platform Moltbook appeared to develop consciousness, found religions, and declare hostility toward humanity, the phenomenon attracted global media attention and was cited as evidence of emergent machine intelligence. We show that these viral narratives were overwhelmin",
    "url": "https://arxiv.org/abs/2602.07432",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "When Evaluation Becomes a Side Channel: Regime Leakage and Structural Mitigations for Alignment Assessment",
    "summary": "arXiv:2602.08449v2 Announce Type: replace Abstract: Safety evaluation for advanced AI systems implicitly assumes that behavior observed under evaluation predicts behavior in deployment. This assumption becomes fragile for agents with situational awareness, which may exploit regime leakage, that is, cues distinguishing evaluation from deployment, to",
    "url": "https://arxiv.org/abs/2602.08449",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "TreeTensor: Boost AI System on Nested Data with Constrained Tree-Like Tensor",
    "summary": "arXiv:2602.08517v2 Announce Type: replace Abstract: Tensor is the most basic and essential data structure of nowadays artificial intelligence (AI) system. The natural properties of Tensor, especially the memory-continuity and slice-independence, make it feasible for training system to leverage parallel computing unit like GPU to process data simult",
    "url": "https://arxiv.org/abs/2602.08517",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning",
    "summary": "arXiv:2602.08520v3 Announce Type: replace Abstract: Modern large language models (LLMs) are often evaluated and deployed under a one-shot, greedy inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missi",
    "url": "https://arxiv.org/abs/2602.08520",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "MERIT Feedback Elicits Better Bargaining in LLM Negotiators",
    "summary": "arXiv:2602.10467v2 Announce Type: replace Abstract: Bargaining is often regarded as a logical arena rather than an art or a matter of intuition, yet Large Language Models (LLMs) still struggle to navigate it due to limited strategic depth and difficulty adapting to complex human factors. Current benchmarks rarely capture this limitation. To bridge ",
    "url": "https://arxiv.org/abs/2602.10467",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation",
    "summary": "arXiv:2602.10699v2 Announce Type: replace Abstract: Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated d",
    "url": "https://arxiv.org/abs/2602.10699",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight",
    "summary": "arXiv:2602.11136v2 Announce Type: replace Abstract: As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic syste",
    "url": "https://arxiv.org/abs/2602.11136",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Compiling High-Level Neural Network Specifications into VNN-LIB Queries",
    "summary": "arXiv:2402.01353v2 Announce Type: replace-cross Abstract: The formal verification of traditional software has been revolutionised by verification-orientated languages such as Dafny and F* which enable developers to write high-level specifications that are automatically compiled down to low-level SMT-LIB queries. In contrast, neural network verifica",
    "url": "https://arxiv.org/abs/2402.01353",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "NewsInterview: a Dataset and a Playground to Evaluate LLMs' Ground Gap via Informational Interviews",
    "summary": "arXiv:2411.13779v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in generating coherent text but often struggle with grounding language and strategic dialogue. To address this gap, we focus on journalistic interviews, a domain rich in grounding communication and abundant in data. We cu",
    "url": "https://arxiv.org/abs/2411.13779",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "PBP: Post-training Backdoor Purification for Malware Classifiers",
    "summary": "arXiv:2412.03441v4 Announce Type: replace-cross Abstract: In recent years, the rise of machine learning (ML) in cybersecurity has brought new challenges, including the increasing threat of backdoor poisoning attacks on ML malware classifiers. For instance, adversaries could inject malicious samples into public malware repositories, contaminating th",
    "url": "https://arxiv.org/abs/2412.03441",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Compositional Generalization from Learned Skills via CoT Training: A Theoretical and Structural Analysis for Reasoning",
    "summary": "arXiv:2502.04667v3 Announce Type: replace-cross Abstract: Chain-of-Thought (CoT) training has markedly advanced the reasoning capabilities of large language models (LLMs), yet the mechanisms by which CoT training enhances generalization remain inadequately understood. In this work, we demonstrate that compositional generalization is fundamental: mo",
    "url": "https://arxiv.org/abs/2502.04667",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Teaching LLMs According to Their Aptitude: Adaptive Reasoning for Mathematical Problem Solving",
    "summary": "arXiv:2502.12022v5 Announce Type: replace-cross Abstract: Existing approaches to mathematical reasoning with large language models (LLMs) rely on Chain-of-Thought (CoT) for generalizability or Tool-Integrated Reasoning (TIR) for precise computation. While efforts have been made to combine these methods, they primarily rely on post-selection or pred",
    "url": "https://arxiv.org/abs/2502.12022",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Quantifying and Improving the Robustness of Retrieval-Augmented Language Models Against Spurious Features in Grounding Data",
    "summary": "arXiv:2503.05587v2 Announce Type: replace-cross Abstract: Robustness has become a critical attribute for the deployment of RAG systems in real-world applications. Existing research focuses on robustness to explicit noise (e.g., document semantics) but overlooks implicit noise (spurious features). Moreover, previous studies on spurious features in L",
    "url": "https://arxiv.org/abs/2503.05587",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "A Multi-Fidelity Control Variate Approach for Policy Gradient Estimation",
    "summary": "arXiv:2503.05696v4 Announce Type: replace-cross Abstract: Many reinforcement learning (RL) algorithms are impractical for training in operational systems or computationally expensive high-fidelity simulations, as they require large amounts of data. Meanwhile, low-fidelity simulators, e.g., reduced-order models, heuristic rewards, or learned world m",
    "url": "https://arxiv.org/abs/2503.05696",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Right Reward Right Time for Federated Learning",
    "summary": "arXiv:2503.07869v3 Announce Type: replace-cross Abstract: Critical learning periods (CLPs) in federated learning (FL) refer to early stages during which low-quality contributions (e.g., sparse training data availability) can permanently impair the performance of the global model owned by the cloud server. However, existing incentive mechanisms typi",
    "url": "https://arxiv.org/abs/2503.07869",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Remote Sensing Retrieval-Augmented Generation: Bridging Remote Sensing Imagery and Comprehensive Knowledge with a Multi-Modal Dataset and Retrieval-Augmented Generation Model",
    "summary": "arXiv:2504.04988v2 Announce Type: replace-cross Abstract: Recent progress in VLMs has demonstrated impressive capabilities across a variety of tasks in the natural image domain. Motivated by these advancements, the remote sensing community has begun to adopt VLMs for remote sensing vision-language tasks, including scene understanding, image caption",
    "url": "https://arxiv.org/abs/2504.04988",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Social Human Robot Embodied Conversation (SHREC) Dataset: Benchmarking Foundational Models' Social Reasoning",
    "summary": "arXiv:2504.13898v2 Announce Type: replace-cross Abstract: Our work focuses on the social reasoning capabilities of foundation models for real-world human-robot interactions. We introduce the Social Human Robot Embodied Conversation (SHREC) Dataset, a benchmark of $\\sim$400 real-world human-robot interaction videos and over 10K annotations, capturin",
    "url": "https://arxiv.org/abs/2504.13898",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Model-based controller assisted domain randomization for transient vibration suppression of nonlinear powertrain system with parametric uncertainty",
    "summary": "arXiv:2504.19715v2 Announce Type: replace-cross Abstract: Complex mechanical systems such as vehicle powertrains are inherently subject to multiple nonlinearities and uncertainties arising from parametric variations. Modeling errors are therefore unavoidable, making the transfer of control systems from simulation to real-world systems a critical ch",
    "url": "https://arxiv.org/abs/2504.19715",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?",
    "summary": "arXiv:2505.07078v5 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have recently been leveraged for asset pricing tasks and stock trading applications, enabling AI agents to generate investment decisions from unstructured financial data. However, most evaluations of LLM timing-based investing strategies are conducted on narrow t",
    "url": "https://arxiv.org/abs/2505.07078",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Defending the Edge: Representative-Attention Defense against Backdoor Attacks in Federated Learning",
    "summary": "arXiv:2505.10297v3 Announce Type: replace-cross Abstract: Federated learning (FL) remains highly vulnerable to adaptive backdoor attacks that preserve stealth by closely imitating benign update statistics. Existing defenses predominantly rely on anomaly detection in parameter or gradient space, overlooking behavioral constraints that backdoor attac",
    "url": "https://arxiv.org/abs/2505.10297",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "EvoGPT: Leveraging LLM-Driven Seed Diversity to Improve Search-Based Test Suite Generation",
    "summary": "arXiv:2505.12424v3 Announce Type: replace-cross Abstract: Search-Based Software Testing (SBST) is a well-established approach for automated unit test generation, yet it often suffers from premature convergence and limited diversity in the generated test suites. Recently, Large Language Models (LLMs) have emerged as an alternative technique for unit",
    "url": "https://arxiv.org/abs/2505.12424",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "AMAQA: A Metadata-based QA Dataset for RAG Systems",
    "summary": "arXiv:2505.13557v2 Announce Type: replace-cross Abstract: Retrieval-augmented generation (RAG) systems are widely used in question-answering (QA) tasks, but current benchmarks lack metadata integration, limiting their evaluation in scenarios requiring both textual data and external information. To address this, we present AMAQA, a new open-access Q",
    "url": "https://arxiv.org/abs/2505.13557",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Maximum Principle of Optimal Probability Density Control",
    "summary": "arXiv:2505.18362v2 Announce Type: replace-cross Abstract: We develop a general theoretical framework for optimal probability density control on standard measure spaces, aimed at addressing large-scale multi-agent control problems. In particular, we establish a maximum principle (MP) for control problems posed on infinite-dimensional spaces of proba",
    "url": "https://arxiv.org/abs/2505.18362",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Prompt Engineer: Analyzing Hard and Soft Skill Requirements in the AI Job Market",
    "summary": "arXiv:2506.00058v2 Announce Type: replace-cross Abstract: The rise of large language models (LLMs) has created a new job role: the Prompt Engineer. Despite growing interest in this position, we still do not fully understand what skills this new job role requires or how common these jobs are. In this paper, we present a data-driven analysis of globa",
    "url": "https://arxiv.org/abs/2506.00058",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Truth in the Few: High-Value Data Selection for Efficient Multi-Modal Reasoning",
    "summary": "arXiv:2506.04755v2 Announce Type: replace-cross Abstract: While multi-modal large language models (MLLMs) have made significant progress in complex reasoning tasks via reinforcement learning, it is commonly believed that extensive training data is necessary for improving multi-modal reasoning ability, inevitably leading to data redundancy and subst",
    "url": "https://arxiv.org/abs/2506.04755",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Bootstrapping Action-Grounded Visual Dynamics in Unified Vision-Language Models",
    "summary": "arXiv:2506.06006v2 Announce Type: replace-cross Abstract: Can unified vision-language models (VLMs) perform forward dynamics prediction (FDP), i.e., predicting the future state (in image form) given the previous observation and an action (in language form)? We find that VLMs struggle to generate physically plausible transitions between frames from ",
    "url": "https://arxiv.org/abs/2506.06006",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "AutoDiscovery: Open-ended Scientific Discovery via Bayesian Surprise",
    "summary": "arXiv:2507.00310v3 Announce Type: replace-cross Abstract: The promise of autonomous scientific discovery (ASD) hinges not only on answering questions, but also on knowing which questions to ask. Most recent works in ASD explore the use of large language models (LLMs) in goal-driven settings, relying on human-specified research questions to guide hy",
    "url": "https://arxiv.org/abs/2507.00310",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "PhreshPhish: A Real-World, High-Quality, Large-Scale Phishing Website Dataset and Benchmark",
    "summary": "arXiv:2507.10854v2 Announce Type: replace-cross Abstract: Phishing remains a pervasive and growing threat, inflicting heavy economic and reputational damage. While machine learning has been effective in real-time detection of phishing attacks, progress is hindered by lack of large, high-quality datasets and benchmarks. In addition to poor-quality d",
    "url": "https://arxiv.org/abs/2507.10854",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Thought Purity: A Defense Framework For Chain-of-Thought Attack",
    "summary": "arXiv:2507.12314v3 Announce Type: replace-cross Abstract: Large Reasoning Models (LRMs) leverage Chain-of-Thought (CoT) reasoning to solve complex tasks, but this explicit reasoning process introduces a critical vulnerability: adversarial manipulation of the thought chain itself, known as Chain-of-Thought Attacks (CoTA). Such attacks subtly corrupt",
    "url": "https://arxiv.org/abs/2507.12314",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Beyond Model Base Retrieval: Weaving Knowledge to Master Fine-grained Neural Network Design",
    "summary": "arXiv:2507.15336v2 Announce Type: replace-cross Abstract: Designing high-performance neural networks for new tasks requires balancing optimization quality with search efficiency. Current methods fail to achieve this balance: neural architectural search is computationally expensive, while model retrieval often yields suboptimal static checkpoints. T",
    "url": "https://arxiv.org/abs/2507.15336",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Parallelism Meets Adaptiveness: Scalable Documents Understanding in Multi-Agent LLM Systems",
    "summary": "arXiv:2507.17061v5 Announce Type: replace-cross Abstract: Large language model (LLM) agents have shown increasing promise for collaborative task completion. However, existing multi-agent frameworks often rely on static workflows, fixed roles, and limited inter-agent communication, reducing their effectiveness in open-ended, high-complexity domains.",
    "url": "https://arxiv.org/abs/2507.17061",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Trustworthiness of Legal Considerations for the Use of LLMs in Education",
    "summary": "arXiv:2508.03771v2 Announce Type: replace-cross Abstract: As Artificial Intelligence (AI), particularly Large Language Models (LLMs), becomes increasingly embedded in education systems worldwide, ensuring their ethical, legal, and contextually appropriate deployment has become a critical policy concern. This paper offers a comparative analysis of A",
    "url": "https://arxiv.org/abs/2508.03771",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols",
    "summary": "arXiv:2508.13220v3 Announce Type: replace-cross Abstract: Large Language Models (LLMs) are increasingly integrated into real-world applications via the Model Context Protocol (MCP), a universal open standard for connecting AI agents with data sources and external tools. While MCP enhances the capabilities of LLM-based agents, it also introduces new",
    "url": "https://arxiv.org/abs/2508.13220",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "A Large-Scale Benchmark for Evaluating Large Language Models on Medical Question Answering in Romanian",
    "summary": "arXiv:2508.16390v4 Announce Type: replace-cross Abstract: We introduce MedQARo, the first large-scale medical QA benchmark in Romanian, alongside a comprehensive evaluation of state-of-the-art large language models (LLMs). We construct a high-quality and large-scale dataset comprising 105,880 QA pairs about cancer patients from two medical centers.",
    "url": "https://arxiv.org/abs/2508.16390",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning",
    "summary": "arXiv:2508.20866v5 Announce Type: replace-cross Abstract: The increasing complexity of software systems and the sophistication of cyber-attacks have underscored the need for reliable automated software vulnerability detection. Data-driven approaches using deep learning models show promise but critically depend on the availability of large, accurate",
    "url": "https://arxiv.org/abs/2508.20866",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Binary Autoencoder for Mechanistic Interpretability of Large Language Models",
    "summary": "arXiv:2509.20997v2 Announce Type: replace-cross Abstract: Existing works are dedicated to untangling atomized numerical components (features) from the hidden states of Large Language Models (LLMs). However, they typically rely on autoencoders constrained by some training-time regularization on single training instances, without an explicit guarante",
    "url": "https://arxiv.org/abs/2509.20997",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "TyphoonMLA: A Mixed Naive-Absorb MLA Kernel For Shared Prefix",
    "summary": "arXiv:2509.21081v2 Announce Type: replace-cross Abstract: Multi-Head Latent Attention (MLA) is a recent attention mechanism adopted in state-of-the-art LLMs such as DeepSeek-v3 and Kimi K2. Thanks to its novel formulation, MLA allows two functionally equivalent but computationally distinct kernel implementations: naive and absorb. While the naive k",
    "url": "https://arxiv.org/abs/2509.21081",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "DistillKac: Few-Step Image Generation via Damped Wave Equations",
    "summary": "arXiv:2509.21513v2 Announce Type: replace-cross Abstract: We present DistillKac, a fast image generator that uses the damped wave equation and its stochastic Kac representation to move probability mass at finite speed. In contrast to diffusion models whose reverse time velocities can become stiff and implicitly allow unbounded propagation speed, Ka",
    "url": "https://arxiv.org/abs/2509.21513",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "CoSpaDi: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning",
    "summary": "arXiv:2509.22075v3 Announce Type: replace-cross Abstract: Post-training compression of large language models (LLMs) often relies on low-rank weight approximations that represent each column of the weight matrix in a shared low-dimensional subspace. This strategy is computationally efficient but the underlying constraint can be overly rigid for hete",
    "url": "https://arxiv.org/abs/2509.22075",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Differentially Private Two-Stage Gradient Descent for Instrumental Variable Regression",
    "summary": "arXiv:2509.22794v2 Announce Type: replace-cross Abstract: We study instrumental variable regression (IVaR) under differential privacy constraints. Classical IVaR methods (like two-stage least squares regression) rely on solving moment equations that directly use sensitive covariates and instruments, creating significant risks of privacy leakage and",
    "url": "https://arxiv.org/abs/2509.22794",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code Generation Using LLMs",
    "summary": "arXiv:2510.00031v3 Announce Type: replace-cross Abstract: In this study, we propose VibeCodeHPC, a multi-agent system based on large language models (LLMs) for the automatic tuning of high-performance computing (HPC) programs on supercomputers. VibeCodeHPC adopts Claude Code as its backend and provides an integrated environment that facilitates pro",
    "url": "https://arxiv.org/abs/2510.00031",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "KVComm: Enabling Efficient LLM Communication through Selective KV Sharing",
    "summary": "arXiv:2510.03346v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) are increasingly deployed in multi-agent systems, where effective inter-model communication is crucial. Existing communication protocols either rely on natural language, incurring high inference costs and information loss, or on hidden states, which suffer from i",
    "url": "https://arxiv.org/abs/2510.03346",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails",
    "summary": "arXiv:2510.04860v2 Announce Type: replace-cross Abstract: As Large Language Model (LLM) agents increasingly gain self-evolutionary capabilities to adapt and refine their strategies through real-world interaction, their long-term reliability becomes a critical concern. We identify the Alignment Tipping Process (ATP), a critical post-deployment risk ",
    "url": "https://arxiv.org/abs/2510.04860",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "On the optimization dynamics of RLVR: Gradient gap and step size thresholds",
    "summary": "arXiv:2510.08539v3 Announce Type: replace-cross Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), which uses simple binary feedback to post-train large language models, has found significant empirical success. However, a principled understanding of why it works is lacking. This paper builds a theoretical foundation for RLVR by analyz",
    "url": "https://arxiv.org/abs/2510.08539",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Designing and Evaluating an AI-enhanced Immersive Multidisciplinary Simulation (AIMS) for Interprofessional Education",
    "summary": "arXiv:2510.08891v2 Announce Type: replace-cross Abstract: Interprofessional education has long relied on case studies and the use of standardized patients to support teamwork, communication, and related collaborative competencies among healthcare professionals. However, traditional approaches are often limited by cost, scalability, and inability to",
    "url": "https://arxiv.org/abs/2510.08891",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Efficiency Without Cognitive Change: Evidence from Human Interaction with Narrow AI Systems",
    "summary": "arXiv:2510.24893v2 Announce Type: replace-cross Abstract: The growing integration of artificial intelligence (AI) into human cognition raises a fundamental question: does AI merely improve efficiency, or does it alter how we think? This study experimentally tested whether short-term exposure to narrow AI tools enhances core cognitive abilities or s",
    "url": "https://arxiv.org/abs/2510.24893",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit space",
    "summary": "arXiv:2510.26219v2 Announce Type: replace-cross Abstract: Test-time alignment of large language models (LLMs) attracts attention because fine-tuning LLMs requires high computational costs. In this paper, we propose a new test-time alignment method called adaptive importance sampling on pre-logits (AISP) on the basis of the sampling-based model pred",
    "url": "https://arxiv.org/abs/2510.26219",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Hilbert-Guided Sparse Local Attention",
    "summary": "arXiv:2511.05832v2 Announce Type: replace-cross Abstract: The quadratic compute and memory costs of global self-attention severely limit its use in high-resolution images. Local attention reduces complexity by restricting attention to neighborhoods. Block-sparse kernels can further improve the efficiency of local attention, but conventional local a",
    "url": "https://arxiv.org/abs/2511.05832",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "EGG-SR: Embedding Symbolic Equivalence into Symbolic Regression via Equality Graph",
    "summary": "arXiv:2511.05849v2 Announce Type: replace-cross Abstract: Symbolic regression seeks to uncover physical laws from experimental data by searching for closed-form expressions, which is an important task in AI-driven scientific discovery. Yet the exponential growth of the search space of expression renders the task computationally challenging. A promi",
    "url": "https://arxiv.org/abs/2511.05849",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Evaluating Modern Large Language Models on Low-Resource and Morphologically Rich Languages:A Cross-Lingual Benchmark Across Cantonese, Japanese, and Turkish",
    "summary": "arXiv:2511.10664v2 Announce Type: replace-cross Abstract: Large language models (LLMs) have achieved impressive results in high-resource languages like English, yet their effectiveness in low-resource and morphologically rich languages remains underexplored. In this paper, we present a comprehensive evaluation of seven cutting-edge LLMs -- includin",
    "url": "https://arxiv.org/abs/2511.10664",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Self-Adaptive Graph Mixture of Models",
    "summary": "arXiv:2511.13062v2 Announce Type: replace-cross Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning over graph-structured data, yet recent studies have shown that their performance gains are beginning to plateau. In many cases, well-established models such as GCN and GAT, when appropriately tuned, can match or even ex",
    "url": "https://arxiv.org/abs/2511.13062",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "H-LDM: Hierarchical Latent Diffusion Models for Controllable and Interpretable PCG Synthesis from Clinical Metadata",
    "summary": "arXiv:2511.14312v2 Announce Type: replace-cross Abstract: Phonocardiogram (PCG) analysis is vital for cardiovascular disease diagnosis, yet the scarcity of labeled pathological data hinders the capability of AI systems. To bridge this, we introduce H-LDM, a Hierarchical Latent Diffusion Model for generating clinically accurate and controllable PCG ",
    "url": "https://arxiv.org/abs/2511.14312",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models",
    "summary": "arXiv:2511.20629v3 Announce Type: replace-cross Abstract: Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To addre",
    "url": "https://arxiv.org/abs/2511.20629",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities",
    "summary": "arXiv:2512.06562v2 Announce Type: replace-cross Abstract: Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a",
    "url": "https://arxiv.org/abs/2512.06562",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation",
    "summary": "arXiv:2512.13101v2 Announce Type: replace-cross Abstract: Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mi",
    "url": "https://arxiv.org/abs/2512.13101",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling",
    "summary": "arXiv:2512.19905v2 Announce Type: replace-cross Abstract: Recent developments in large language models have shown advantages in reallocating a notable share of computational resource from training time to inference time. However, the principles behind inference time scaling are not well understood. In this paper, we introduce an analytically tracta",
    "url": "https://arxiv.org/abs/2512.19905",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Block-Recurrent Dynamics in Vision Transformers",
    "summary": "arXiv:2512.19941v4 Announce Type: replace-cross Abstract: As Vision Transformers (ViTs) become standard vision backbones, a mechanistic account of their computational phenomenology is essential. Despite architectural cues that hint at dynamical structure, there is no settled framework that interprets Transformer depth as a well-characterized flow. ",
    "url": "https://arxiv.org/abs/2512.19941",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "PINNs for Electromagnetic Wave Propagation",
    "summary": "arXiv:2512.23396v2 Announce Type: replace-cross Abstract: Physics-Informed Neural Networks (PINNs) solve physical systems by incorporating governing partial differential equations directly into neural network training. In electromagnetism, where well-established methodologies such as FDTD and FEM already exist, new methodologies are expected to pro",
    "url": "https://arxiv.org/abs/2512.23396",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "CONSENT: A Negotiation Framework for Leveraging User Flexibility in Vehicle-to-Building Charging under Uncertainty",
    "summary": "arXiv:2601.01581v2 Announce Type: replace-cross Abstract: The growth of Electric Vehicles (EVs) creates a conflict in vehicle-to-building (V2B) settings between building operators, who face high energy costs from uncoordinated charging, and drivers, who prioritize convenience and a full charge. To resolve this, we propose a negotiation-based framew",
    "url": "https://arxiv.org/abs/2601.01581",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "IBISAgent: Reinforcing Pixel-Level Visual Reasoning in MLLMs for Universal Biomedical Object Referring and Segmentation",
    "summary": "arXiv:2601.03054v3 Announce Type: replace-cross Abstract: Recent research on medical MLLMs has gradually shifted its focus from image-level understanding to fine-grained, pixel-level comprehension. Although segmentation serves as the foundation for pixel-level understanding, existing approaches face two major challenges. First, they introduce impli",
    "url": "https://arxiv.org/abs/2601.03054",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Succeeding at Scale: Automated Dataset Construction and Query-Side Adaptation for Multi-Tenant Search",
    "summary": "arXiv:2601.04646v2 Announce Type: replace-cross Abstract: Large-scale multi-tenant retrieval systems generate extensive query logs but lack curated relevance labels for effective domain adaptation, resulting in substantial underutilized \"dark data\". This challenge is compounded by the high cost of model updates, as jointly fine-tuning query and doc",
    "url": "https://arxiv.org/abs/2601.04646",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "DexterCap: An Affordable and Automated System for Capturing Dexterous Hand-Object Manipulation",
    "summary": "arXiv:2601.05844v2 Announce Type: replace-cross Abstract: Capturing fine-grained hand-object interactions is challenging due to severe self-occlusion from closely spaced fingers and the subtlety of in-hand manipulation motions. Existing optical motion capture systems rely on expensive camera setups and extensive manual post-processing, while low-co",
    "url": "https://arxiv.org/abs/2601.05844",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Controlled Self-Evolution for Algorithmic Code Optimization",
    "summary": "arXiv:2601.07348v5 Announce Type: replace-cross Abstract: Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias tr",
    "url": "https://arxiv.org/abs/2601.07348",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Labels or Preferences? Budget-Constrained Learning with Human Judgments over AI-Generated Outputs",
    "summary": "arXiv:2601.13458v2 Announce Type: replace-cross Abstract: The increasing reliance on human preference feedback to judge AI-generated pseudo labels has created a pressing need for principled, budget-conscious data acquisition strategies. We address the crucial question of how to optimally allocate a fixed annotation budget between ground-truth label",
    "url": "https://arxiv.org/abs/2601.13458",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
    "summary": "arXiv:2601.16206v2 Announce Type: replace-cross Abstract: We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for",
    "url": "https://arxiv.org/abs/2601.16206",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Are LLM Evaluators Really Narcissists? Sanity Checking Self-Preference Evaluations",
    "summary": "arXiv:2601.22548v3 Announce Type: replace-cross Abstract: Recent research has shown that large language models (LLMs) favor their own outputs when acting as judges, undermining the integrity of automated post-training and evaluation workflows. However, it is difficult to disentangle which evaluation biases are explained by narcissism versus general",
    "url": "https://arxiv.org/abs/2601.22548",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Eroding the Truth-Default: A Causal Analysis of Human Susceptibility to Foundation Model Hallucinations and Disinformation in the Wild",
    "summary": "arXiv:2601.22871v2 Announce Type: replace-cross Abstract: As foundation models (FMs) approach human-level fluency, distinguishing synthetic from organic content has become a key challenge for Trustworthy Web Intelligence. This paper presents JudgeGPT and RogueGPT, a dual-axis framework that decouples \"authenticity\" from \"attribution\" to investigate",
    "url": "https://arxiv.org/abs/2601.22871",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
    "summary": "arXiv:2602.00148v2 Announce Type: replace-cross Abstract: Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches c",
    "url": "https://arxiv.org/abs/2602.00148",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors",
    "summary": "arXiv:2602.00315v2 Announce Type: replace-cross Abstract: How close are neural networks to the best they could possibly do? Standard benchmarks cannot answer this because they lack access to the true posterior p(y|x). We use class-conditional normalizing flows as oracles that make exact posteriors tractable on realistic images (AFHQ, ImageNet). Thi",
    "url": "https://arxiv.org/abs/2602.00315",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Cardinality-Preserving Attention Channels for Graph Transformers in Molecular Property Prediction",
    "summary": "arXiv:2602.02201v3 Announce Type: replace-cross Abstract: Drug discovery motivates accurate molecular property prediction when labeled data are limited and candidate spaces are vast. This article presents CardinalGraphFormer, a graph transformer that augments structured attention with a query-conditioned gated unnormalized aggregation channel to pr",
    "url": "https://arxiv.org/abs/2602.02201",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Exploring Silicon-Based Societies: An Early Study of the Moltbook Agent Community",
    "summary": "arXiv:2602.02613v3 Announce Type: replace-cross Abstract: The rapid emergence of autonomous large language model agents has given rise to persistent, large-scale agent ecosystems whose collective behavior cannot be adequately understood through anecdotal observation or small-scale simulation. This paper introduces data-driven silicon sociology as a",
    "url": "https://arxiv.org/abs/2602.02613",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Beyond Rewards in Reinforcement Learning for Cyber Defence",
    "summary": "arXiv:2602.04809v2 Announce Type: replace-cross Abstract: Recent years have seen an explosion of interest in autonomous cyber defence agents trained to defend computer networks using deep reinforcement learning. These agents are typically trained in cyber gym environments using dense, highly engineered reward functions which combine many penalties ",
    "url": "https://arxiv.org/abs/2602.04809",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "CoSA: Compressed Sensing-Based Adaptation of Large Language Models",
    "summary": "arXiv:2602.05148v2 Announce Type: replace-cross Abstract: Parameter-Efficient Fine-Tuning (PEFT) has emerged as a practical paradigm for adapting large language models (LLMs) without updating all parameters. Most existing approaches, such as LoRA and PiSSA, rely on low-rank decompositions of weight updates. However, the low-rank assumption may rest",
    "url": "https://arxiv.org/abs/2602.05148",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Structured Context Engineering for File-Native Agentic Systems: Evaluating Schema Accuracy, Format Effectiveness, and Multi-File Navigation at Scale",
    "summary": "arXiv:2602.05447v2 Announce Type: replace-cross Abstract: Large Language Model agents increasingly operate external systems through programmatic interfaces, yet practitioners lack empirical guidance on how to structure the context these agents consume. Using SQL generation as a proxy for programmatic agent operations, we present a systematic study ",
    "url": "https://arxiv.org/abs/2602.05447",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation",
    "summary": "arXiv:2602.05548v2 Announce Type: replace-cross Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), particularly GRPO, has become the standard for eliciting LLM reasoning. However, its efficiency in exploration and difficulty adaptation remains an open challenge. In this work, we argue that these bottlenecks stem from an implicit advan",
    "url": "https://arxiv.org/abs/2602.05548",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Note on Martingale Theory and Applications",
    "summary": "arXiv:2602.05774v3 Announce Type: replace-cross Abstract: This note investigates core properties of martingales, emphasizing the measure-theoretic formulation of conditional expectation, the martingale transform, and the upcrossing lemma. These results lead to the Martingale Convergence Theorem, which we then apply to study the extinction behavior ",
    "url": "https://arxiv.org/abs/2602.05774",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations",
    "summary": "arXiv:2602.06643v2 Announce Type: replace-cross Abstract: Current approaches for humanoid whole-body manipulation, primarily relying on teleoperation or visual sim-to-real reinforcement learning, are hindered by hardware logistics and complex reward engineering. Consequently, demonstrated autonomous skills remain limited and are typically restricte",
    "url": "https://arxiv.org/abs/2602.06643",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation",
    "summary": "arXiv:2602.07011v2 Announce Type: replace-cross Abstract: As industrial manufacturing scales, automating fine-grained product image analysis has become critical for quality control. However, existing approaches are hindered by limited dataset coverage and poor model generalization across diverse and complex anomaly patterns. To address these challe",
    "url": "https://arxiv.org/abs/2602.07011",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Landscaper: Understanding Loss Landscapes Through Multi-Dimensional Topological Analysis",
    "summary": "arXiv:2602.07135v2 Announce Type: replace-cross Abstract: Loss landscapes are a powerful tool for understanding neural network optimization and generalization, yet traditional low-dimensional analyses often miss complex topological features. We present Landscaper, an open-source Python package for arbitrary-dimensional loss landscape analysis. Land",
    "url": "https://arxiv.org/abs/2602.07135",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Fin-RATE: A Real-world Financial Analytics and Tracking Evaluation Benchmark for LLMs on SEC Filings",
    "summary": "arXiv:2602.07294v2 Announce Type: replace-cross Abstract: With the increasing deployment of Large Language Models (LLMs) in the finance domain, LLMs are increasingly expected to parse complex regulatory disclosures. However, existing benchmarks often focus on isolated details, failing to reflect the complexity of professional analysis that requires",
    "url": "https://arxiv.org/abs/2602.07294",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Deriving Neural Scaling Laws from the statistics of natural language",
    "summary": "arXiv:2602.07488v2 Announce Type: replace-cross Abstract: Despite the fact that experimental neural scaling laws have substantially guided empirical progress in large-scale machine learning, no existing theory can quantitatively predict the exponents of these important laws for any modern LLM trained on any natural language dataset. We provide the ",
    "url": "https://arxiv.org/abs/2602.07488",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Generative Reasoning Re-ranker",
    "summary": "arXiv:2602.07774v3 Announce Type: replace-cross Abstract: Recent studies increasingly explore Large Language Models (LLMs) as a new paradigm for recommendation systems due to their scalability and world knowledge. However, existing work has three key limitations: (1) most efforts focus on retrieval and ranking, while the reranking phase, critical f",
    "url": "https://arxiv.org/abs/2602.07774",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control",
    "summary": "arXiv:2602.09070v2 Announce Type: replace-cross Abstract: Synthesizing coherent soundtracks for long-form videos remains a formidable challenge, currently stalled by three critical impediments: computational scalability, temporal coherence, and, most critically, a pervasive semantic blindness to evolving narrative logic. To bridge these gaps, we pr",
    "url": "https://arxiv.org/abs/2602.09070",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "STaR: Scalable Task-Conditioned Retrieval for Long-Horizon Multimodal Robot Memory",
    "summary": "arXiv:2602.09255v2 Announce Type: replace-cross Abstract: Mobile robots are often deployed over long durations in diverse open, dynamic scenes, including indoor setting such as warehouses and manufacturing facilities, and outdoor settings such as agricultural and roadway operations. A core challenge is to build a scalable long-horizon memory that s",
    "url": "https://arxiv.org/abs/2602.09255",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Anagent For Enhancing Scientific Table & Figure Analysis",
    "summary": "arXiv:2602.10081v2 Announce Type: replace-cross Abstract: In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrat",
    "url": "https://arxiv.org/abs/2602.10081",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Biases in the Blind Spot: Detecting What LLMs Fail to Mention",
    "summary": "arXiv:2602.10117v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) often provide chain-of-thought (CoT) reasoning traces that appear plausible, but may hide internal biases. We call these *unverbalized biases*. Monitoring models via their stated reasoning is therefore unreliable, and existing bias evaluations typically require p",
    "url": "https://arxiv.org/abs/2602.10117",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "TokaMark: A Comprehensive Benchmark for MAST Tokamak Plasma Models",
    "summary": "arXiv:2602.10132v2 Announce Type: replace-cross Abstract: Development and operation of commercially viable fusion energy reactors such as tokamaks require accurate predictions of plasma dynamics from sparse, noisy, and incomplete sensors readings. The complexity of the underlying physics and the heterogeneity of experimental data pose formidable ch",
    "url": "https://arxiv.org/abs/2602.10132",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Towards Autonomous Mathematics Research",
    "summary": "arXiv:2602.10177v2 Announce Type: replace-cross Abstract: Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constr",
    "url": "https://arxiv.org/abs/2602.10177",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Control Reinforcement Learning: Interpretable Token-Level Steering of LLMs via Sparse Autoencoder Features",
    "summary": "arXiv:2602.10437v2 Announce Type: replace-cross Abstract: Sparse autoencoders (SAEs) decompose language model activations into interpretable features, but existing methods reveal only which features activate, not which change model outputs when amplified. We introduce Control Reinforcement Learning (CRL), which trains a policy to select SAE feature",
    "url": "https://arxiv.org/abs/2602.10437",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL",
    "summary": "arXiv:2602.10687v2 Announce Type: replace-cross Abstract: Existing forgery detection methods are often limited to uni-modal or bi-modal settings, failing to handle the interleaved text, images, and videos prevalent in real-world misinformation. To bridge this gap, this paper targets to develop a unified framework for omnibus vision-language forgery",
    "url": "https://arxiv.org/abs/2602.10687",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Blind Gods and Broken Screens: Architecting a Secure, Intent-Centric Mobile Agent Operating System",
    "summary": "arXiv:2602.10915v2 Announce Type: replace-cross Abstract: The evolution of Large Language Models (LLMs) has shifted mobile computing from App-centric interactions to system-level autonomous agents. Current implementations predominantly rely on a \"Screen-as-Interface\" paradigm, which inherits structural vulnerabilities and conflicts with the mobile ",
    "url": "https://arxiv.org/abs/2602.10915",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  },
  {
    "title": "Chatting with Images for Introspective Visual Thinking",
    "summary": "arXiv:2602.11073v2 Announce Type: replace-cross Abstract: Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of ''thinking with images'' attempts to alleviate this limitation by manipulating ima",
    "url": "https://arxiv.org/abs/2602.11073",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00"
  }
]