[
  {
    "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
    "summary": "arXiv:2602.13226v1 Announce Type: new Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and pra",
    "url": "https://arxiv.org/abs/2602.13226",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
    "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-for",
    "url": "https://arxiv.org/abs/2602.13232",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
    "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a un",
    "url": "https://arxiv.org/abs/2602.13258",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol",
    "summary": "arXiv:2602.13320v1 Announce Type: new Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context P",
    "url": "https://arxiv.org/abs/2602.13320",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization",
    "summary": "arXiv:2602.13653v1 Announce Type: new Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for dat",
    "url": "https://arxiv.org/abs/2602.13653",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
    "summary": "arXiv:2602.13680v1 Announce Type: new Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that ",
    "url": "https://arxiv.org/abs/2602.13680",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models",
    "summary": "arXiv:2602.13904v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three disti",
    "url": "https://arxiv.org/abs/2602.13904",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v1 Announce Type: new Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision ma",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
    "summary": "arXiv:2602.13933v1 Announce Type: new Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical det",
    "url": "https://arxiv.org/abs/2602.13933",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
    "summary": "arXiv:2602.14130v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provi",
    "url": "https://arxiv.org/abs/2602.14130",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
    "summary": "arXiv:2602.14457v1 Announce Type: new Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the ",
    "url": "https://arxiv.org/abs/2602.14457",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
    "summary": "arXiv:2602.14518v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conf",
    "url": "https://arxiv.org/abs/2602.14518",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization",
    "summary": "arXiv:2602.13210v1 Announce Type: cross Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex enviro",
    "url": "https://arxiv.org/abs/2602.13210",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Global AI Bias Audit for Technical Governance",
    "summary": "arXiv:2602.13246v1 Announce Type: cross Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical ",
    "url": "https://arxiv.org/abs/2602.13246",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs",
    "summary": "arXiv:2602.13289v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessi",
    "url": "https://arxiv.org/abs/2602.13289",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
    "summary": "arXiv:2602.13226v1 Announce Type: new Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and pra",
    "url": "https://arxiv.org/abs/2602.13226",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
    "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-for",
    "url": "https://arxiv.org/abs/2602.13232",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
    "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a un",
    "url": "https://arxiv.org/abs/2602.13258",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol",
    "summary": "arXiv:2602.13320v1 Announce Type: new Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context P",
    "url": "https://arxiv.org/abs/2602.13320",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization",
    "summary": "arXiv:2602.13653v1 Announce Type: new Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for dat",
    "url": "https://arxiv.org/abs/2602.13653",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
    "summary": "arXiv:2602.13680v1 Announce Type: new Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that ",
    "url": "https://arxiv.org/abs/2602.13680",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models",
    "summary": "arXiv:2602.13904v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three disti",
    "url": "https://arxiv.org/abs/2602.13904",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v1 Announce Type: new Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision ma",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
    "summary": "arXiv:2602.13933v1 Announce Type: new Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical det",
    "url": "https://arxiv.org/abs/2602.13933",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
    "summary": "arXiv:2602.14130v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provi",
    "url": "https://arxiv.org/abs/2602.14130",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
    "summary": "arXiv:2602.14457v1 Announce Type: new Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the ",
    "url": "https://arxiv.org/abs/2602.14457",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
    "summary": "arXiv:2602.14518v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conf",
    "url": "https://arxiv.org/abs/2602.14518",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization",
    "summary": "arXiv:2602.13210v1 Announce Type: cross Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex enviro",
    "url": "https://arxiv.org/abs/2602.13210",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Global AI Bias Audit for Technical Governance",
    "summary": "arXiv:2602.13246v1 Announce Type: cross Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical ",
    "url": "https://arxiv.org/abs/2602.13246",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs",
    "summary": "arXiv:2602.13289v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessi",
    "url": "https://arxiv.org/abs/2602.13289",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
    "summary": "arXiv:2602.13226v1 Announce Type: new Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and pra",
    "url": "https://arxiv.org/abs/2602.13226",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
    "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-for",
    "url": "https://arxiv.org/abs/2602.13232",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
    "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a un",
    "url": "https://arxiv.org/abs/2602.13258",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol",
    "summary": "arXiv:2602.13320v1 Announce Type: new Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context P",
    "url": "https://arxiv.org/abs/2602.13320",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization",
    "summary": "arXiv:2602.13653v1 Announce Type: new Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for dat",
    "url": "https://arxiv.org/abs/2602.13653",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
    "summary": "arXiv:2602.13680v1 Announce Type: new Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that ",
    "url": "https://arxiv.org/abs/2602.13680",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models",
    "summary": "arXiv:2602.13904v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three disti",
    "url": "https://arxiv.org/abs/2602.13904",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v1 Announce Type: new Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision ma",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
    "summary": "arXiv:2602.13933v1 Announce Type: new Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical det",
    "url": "https://arxiv.org/abs/2602.13933",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
    "summary": "arXiv:2602.14130v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provi",
    "url": "https://arxiv.org/abs/2602.14130",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
    "summary": "arXiv:2602.14457v1 Announce Type: new Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the ",
    "url": "https://arxiv.org/abs/2602.14457",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
    "summary": "arXiv:2602.14518v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conf",
    "url": "https://arxiv.org/abs/2602.14518",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization",
    "summary": "arXiv:2602.13210v1 Announce Type: cross Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex enviro",
    "url": "https://arxiv.org/abs/2602.13210",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Global AI Bias Audit for Technical Governance",
    "summary": "arXiv:2602.13246v1 Announce Type: cross Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical ",
    "url": "https://arxiv.org/abs/2602.13246",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs",
    "summary": "arXiv:2602.13289v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessi",
    "url": "https://arxiv.org/abs/2602.13289",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
    "summary": "arXiv:2602.13226v1 Announce Type: new Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and pra",
    "url": "https://arxiv.org/abs/2602.13226",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
    "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-for",
    "url": "https://arxiv.org/abs/2602.13232",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
    "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a un",
    "url": "https://arxiv.org/abs/2602.13258",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol",
    "summary": "arXiv:2602.13320v1 Announce Type: new Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context P",
    "url": "https://arxiv.org/abs/2602.13320",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization",
    "summary": "arXiv:2602.13653v1 Announce Type: new Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for dat",
    "url": "https://arxiv.org/abs/2602.13653",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
    "summary": "arXiv:2602.13680v1 Announce Type: new Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that ",
    "url": "https://arxiv.org/abs/2602.13680",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models",
    "summary": "arXiv:2602.13904v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three disti",
    "url": "https://arxiv.org/abs/2602.13904",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v1 Announce Type: new Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision ma",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
    "summary": "arXiv:2602.13933v1 Announce Type: new Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical det",
    "url": "https://arxiv.org/abs/2602.13933",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
    "summary": "arXiv:2602.14130v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provi",
    "url": "https://arxiv.org/abs/2602.14130",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
    "summary": "arXiv:2602.14457v1 Announce Type: new Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the ",
    "url": "https://arxiv.org/abs/2602.14457",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
    "summary": "arXiv:2602.14518v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conf",
    "url": "https://arxiv.org/abs/2602.14518",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization",
    "summary": "arXiv:2602.13210v1 Announce Type: cross Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex enviro",
    "url": "https://arxiv.org/abs/2602.13210",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Global AI Bias Audit for Technical Governance",
    "summary": "arXiv:2602.13246v1 Announce Type: cross Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical ",
    "url": "https://arxiv.org/abs/2602.13246",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs",
    "summary": "arXiv:2602.13289v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessi",
    "url": "https://arxiv.org/abs/2602.13289",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
    "summary": "arXiv:2602.13226v1 Announce Type: new Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and pra",
    "url": "https://arxiv.org/abs/2602.13226",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
    "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-for",
    "url": "https://arxiv.org/abs/2602.13232",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
    "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a un",
    "url": "https://arxiv.org/abs/2602.13258",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol",
    "summary": "arXiv:2602.13320v1 Announce Type: new Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context P",
    "url": "https://arxiv.org/abs/2602.13320",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization",
    "summary": "arXiv:2602.13653v1 Announce Type: new Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for dat",
    "url": "https://arxiv.org/abs/2602.13653",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
    "summary": "arXiv:2602.13680v1 Announce Type: new Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that ",
    "url": "https://arxiv.org/abs/2602.13680",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models",
    "summary": "arXiv:2602.13904v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three disti",
    "url": "https://arxiv.org/abs/2602.13904",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v1 Announce Type: new Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision ma",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
    "summary": "arXiv:2602.13933v1 Announce Type: new Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical det",
    "url": "https://arxiv.org/abs/2602.13933",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
    "summary": "arXiv:2602.14130v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provi",
    "url": "https://arxiv.org/abs/2602.14130",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
    "summary": "arXiv:2602.14457v1 Announce Type: new Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the ",
    "url": "https://arxiv.org/abs/2602.14457",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
    "summary": "arXiv:2602.14518v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conf",
    "url": "https://arxiv.org/abs/2602.14518",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization",
    "summary": "arXiv:2602.13210v1 Announce Type: cross Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex enviro",
    "url": "https://arxiv.org/abs/2602.13210",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Global AI Bias Audit for Technical Governance",
    "summary": "arXiv:2602.13246v1 Announce Type: cross Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical ",
    "url": "https://arxiv.org/abs/2602.13246",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs",
    "summary": "arXiv:2602.13289v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessi",
    "url": "https://arxiv.org/abs/2602.13289",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
    "summary": "arXiv:2602.13226v1 Announce Type: new Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and pra",
    "url": "https://arxiv.org/abs/2602.13226",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
    "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-for",
    "url": "https://arxiv.org/abs/2602.13232",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
    "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a un",
    "url": "https://arxiv.org/abs/2602.13258",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol",
    "summary": "arXiv:2602.13320v1 Announce Type: new Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context P",
    "url": "https://arxiv.org/abs/2602.13320",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization",
    "summary": "arXiv:2602.13653v1 Announce Type: new Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for dat",
    "url": "https://arxiv.org/abs/2602.13653",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
    "summary": "arXiv:2602.13680v1 Announce Type: new Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that ",
    "url": "https://arxiv.org/abs/2602.13680",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models",
    "summary": "arXiv:2602.13904v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three disti",
    "url": "https://arxiv.org/abs/2602.13904",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v1 Announce Type: new Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision ma",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
    "summary": "arXiv:2602.13933v1 Announce Type: new Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical det",
    "url": "https://arxiv.org/abs/2602.13933",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
    "summary": "arXiv:2602.14130v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provi",
    "url": "https://arxiv.org/abs/2602.14130",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
    "summary": "arXiv:2602.14457v1 Announce Type: new Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the ",
    "url": "https://arxiv.org/abs/2602.14457",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
    "summary": "arXiv:2602.14518v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conf",
    "url": "https://arxiv.org/abs/2602.14518",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization",
    "summary": "arXiv:2602.13210v1 Announce Type: cross Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex enviro",
    "url": "https://arxiv.org/abs/2602.13210",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Global AI Bias Audit for Technical Governance",
    "summary": "arXiv:2602.13246v1 Announce Type: cross Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical ",
    "url": "https://arxiv.org/abs/2602.13246",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs",
    "summary": "arXiv:2602.13289v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessi",
    "url": "https://arxiv.org/abs/2602.13289",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "TriGen: NPU Architecture for End-to-End Acceleration of Large Language Models based on SW-HW Co-Design",
    "summary": "arXiv:2602.12962v1 Announce Type: cross Abstract: Recent studies have extensively explored NPU architectures for accelerating AI inference in on-device environments, which are inherently resource-constrained. Meanwhile, transformer-based large language models (LLMs) have become dominant, with rapidly increasing model sizes but low degree of paramet",
    "url": "https://arxiv.org/abs/2602.12962",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Intent-Driven Smart Manufacturing Integrating Knowledge Graphs and Large Language Models",
    "summary": "arXiv:2602.12419v1 Announce Type: new Abstract: The increasing complexity of smart manufacturing environments demands interfaces that can translate high-level human intents into machine-executable actions. This paper presents a unified framework that integrates instruction-tuned Large Language Models (LLMs) with ontology-aligned Knowledge Graphs (K",
    "url": "https://arxiv.org/abs/2602.12419",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents",
    "summary": "arXiv:2602.12876v1 Announce Type: new Abstract: Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain li",
    "url": "https://arxiv.org/abs/2602.12876",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "A Lightweight LLM Framework for Disaster Humanitarian Information Classification",
    "summary": "arXiv:2602.12284v1 Announce Type: cross Abstract: Timely classification of humanitarian information from social media is critical for effective disaster response. However, deploying large language models (LLMs) for this task faces challenges in resource-constrained emergency settings. This paper develops a lightweight, cost-effective framework for ",
    "url": "https://arxiv.org/abs/2602.12284",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "RankLLM: Weighted Ranking of LLMs by Quantifying Question Difficulty",
    "summary": "arXiv:2602.12424v1 Announce Type: cross Abstract: Benchmarks establish a standardized evaluation framework to systematically assess the performance of large language models (LLMs), facilitating objective comparisons and driving advancements in the field. However, existing benchmarks fail to differentiate question difficulty, limiting their ability ",
    "url": "https://arxiv.org/abs/2602.12424",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Self-EvolveRec: Self-Evolving Recommender Systems with LLM-based Directional Feedback",
    "summary": "arXiv:2602.12612v1 Announce Type: cross Abstract: Traditional methods for automating recommender system design, such as Neural Architecture Search (NAS), are often constrained by a fixed search space defined by human priors, limiting innovation to pre-defined operators. While recent LLM-driven code evolution frameworks shift fixed search space targ",
    "url": "https://arxiv.org/abs/2602.12612",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Vision Token Reduction via Attention-Driven Self-Compression for Efficient Multimodal Large Language Models",
    "summary": "arXiv:2602.12618v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) incur significant computational cost from processing numerous vision tokens through all LLM layers. Prior pruning methods operate either before the LLM, limiting generality due to diverse encoder-projector designs or within the LLM using heuristics that are i",
    "url": "https://arxiv.org/abs/2602.12618",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Artic: AI-oriented Real-time Communication for MLLM Video Assistant",
    "summary": "arXiv:2602.12641v1 Announce Type: cross Abstract: AI Video Assistant emerges as a new paradigm for Real-time Communication (RTC), where one peer is a Multimodal Large Language Model (MLLM) deployed in the cloud. This makes interaction between humans and AI more intuitive, akin to chatting with a real person. However, a fundamental mismatch exists b",
    "url": "https://arxiv.org/abs/2602.12641",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "RADAR: Revealing Asymmetric Development of Abilities in MLLM Pre-training",
    "summary": "arXiv:2602.12892v1 Announce Type: cross Abstract: Pre-trained Multi-modal Large Language Models (MLLMs) provide a knowledge-rich foundation for post-training by leveraging their inherent perception and reasoning capabilities to solve complex tasks. However, the lack of an efficient evaluation framework impedes the diagnosis of their performance bot",
    "url": "https://arxiv.org/abs/2602.12892",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Know More, Know Clearer: A Meta-Cognitive Framework for Knowledge Augmentation in Large Language Models",
    "summary": "arXiv:2602.12996v1 Announce Type: cross Abstract: Knowledge augmentation has significantly enhanced the performance of Large Language Models (LLMs) in knowledge-intensive tasks. However, existing methods typically operate on the simplistic premise that model performance equates with internal knowledge, overlooking the knowledge-confidence gaps that",
    "url": "https://arxiv.org/abs/2602.12996",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Buy versus Build an LLM: A Decision Framework for Governments",
    "summary": "arXiv:2602.13033v1 Announce Type: cross Abstract: Large Language Models (LLMs) represent a new frontier of digital infrastructure that can support a wide range of public-sector applications, from general purpose citizen services to specialized and sensitive state functions. When expanding AI access, governments face a set of strategic choices over ",
    "url": "https://arxiv.org/abs/2602.13033",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Asynchronous Verified Semantic Caching for Tiered LLM Architectures",
    "summary": "arXiv:2602.13165v1 Announce Type: cross Abstract: Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential for reducing inference cost and latency. Production deployments typically use a tiered static-dynamic design: a static cache of curated, offline vetted responses ",
    "url": "https://arxiv.org/abs/2602.13165",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Difficulty-Aware Agentic Orchestration for Query-Specific Multi-Agent Workflows",
    "summary": "arXiv:2509.11079v5 Announce Type: replace Abstract: Large Language Model (LLM)-based agentic systems have shown strong capabilities across various tasks. However, existing multi-agent frameworks often rely on static or task-level workflows, which either over-process simple queries or underperform on complex ones, while also neglecting the efficienc",
    "url": "https://arxiv.org/abs/2509.11079",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "LTSM-Bundle: A Toolbox and Benchmark on Large Language Models for Time Series Forecasting",
    "summary": "arXiv:2406.14045v3 Announce Type: replace-cross Abstract: Time Series Forecasting (TSF) has long been a challenge in time series analysis. Inspired by the success of Large Language Models (LLMs), researchers are now developing Large Time Series Models (LTSMs)-universal transformer-based models that use autoregressive prediction-to improve TSF. Howe",
    "url": "https://arxiv.org/abs/2406.14045",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders",
    "summary": "arXiv:2507.03262v4 Announce Type: replace-cross Abstract: Recent multimodal large language models (MLLMs) increasingly integrate multiple vision encoders to improve performance on various benchmarks, assuming that diverse pretraining objectives yield complementary visual signals. However, we show this assumption often fails in practice. Through sys",
    "url": "https://arxiv.org/abs/2507.03262",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling",
    "summary": "arXiv:2602.11761v1 Announce Type: cross Abstract: The evolution of large language models (LLMs) towards applications with ultra-long contexts faces challenges posed by the high computational and memory costs of the Transformer architecture. While existing sparse and linear attention mechanisms attempt to mitigate these issues, they typically involv",
    "url": "https://arxiv.org/abs/2602.11761",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Bi-Level Prompt Optimization for Multimodal LLM-as-a-Judge",
    "summary": "arXiv:2602.11340v1 Announce Type: new Abstract: Large language models (LLMs) have become widely adopted as automated judges for evaluating AI-generated content. Despite their success, aligning LLM-based evaluations with human judgments remains challenging. While supervised fine-tuning on human-labeled data can improve alignment, it is costly and in",
    "url": "https://arxiv.org/abs/2602.11340",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Credit Where It is Due: Cross-Modality Connectivity Drives Precise Reinforcement Learning for MLLM Reasoning",
    "summary": "arXiv:2602.11455v1 Announce Type: new Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), yet how visual evidence is integrated during reasoning remains poorly understood. We explore multimodal RLVR through the lens of cross-modal attentio",
    "url": "https://arxiv.org/abs/2602.11455",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery",
    "summary": "arXiv:2602.11609v1 Announce Type: new Abstract: We present scPilot, the first systematic framework to practice omics-native reasoning: a large language model (LLM) converses in natural language while directly inspecting single-cell RNA-seq data and on-demand bioinformatics tools. scPilot converts core single-cell analyses, i.e., cell-type annotatio",
    "url": "https://arxiv.org/abs/2602.11609",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Do MLLMs Really Understand Space? A Mathematical Reasoning Evaluation",
    "summary": "arXiv:2602.11635v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) have achieved strong performance on perception-oriented tasks, yet their ability to perform mathematical spatial reasoning, defined as the capacity to parse and manipulate two- and three-dimensional relations, remains unclear. Humans easily solve textbook-style",
    "url": "https://arxiv.org/abs/2602.11635",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "PhyNiKCE: A Neurosymbolic Agentic Framework for Autonomous Computational Fluid Dynamics",
    "summary": "arXiv:2602.11666v1 Announce Type: new Abstract: The deployment of autonomous agents for Computational Fluid Dynamics (CFD), is critically limited by the probabilistic nature of Large Language Models (LLMs), which struggle to enforce the strict conservation laws and numerical stability required for physics-based simulations. Reliance on purely seman",
    "url": "https://arxiv.org/abs/2602.11666",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Benchmark Health Index: A Systematic Framework for Benchmarking the Benchmarks of LLMs",
    "summary": "arXiv:2602.11674v1 Announce Type: new Abstract: Large Language Models (LLMs) are advancing rapidly, yet the benchmarks used to measure this progress are becoming increasingly unreliable. Score inflation and selective reporting have eroded the authority of standard benchmarks, leaving the community uncertain about which evaluation results remain tru",
    "url": "https://arxiv.org/abs/2602.11674",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Beyond Pixels: Vector-to-Graph Transformation for Reliable Schematic Auditing",
    "summary": "arXiv:2602.11678v1 Announce Type: new Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable progress in visual understanding, yet they suffer from a critical limitation: structural blindness. Even state-of-the-art models fail to capture topology and symbolic logic in engineering schematics, as their pixel-driven paradigm discards",
    "url": "https://arxiv.org/abs/2602.11678",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs",
    "summary": "arXiv:2602.11729v1 Announce Type: new Abstract: Model diffing, the process of comparing models' internal representations to identify their differences, is a promising approach for uncovering safety-critical behaviors in new models. However, its application has so far been primarily focused on comparing a base model with its finetune. Since new LLM ",
    "url": "https://arxiv.org/abs/2602.11729",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Prototype Transformer: Towards Language Model Architectures Interpretable by Design",
    "summary": "arXiv:2602.11852v1 Announce Type: new Abstract: While state-of-the-art language models (LMs) surpass the vast majority of humans in certain domains, their reasoning remains largely opaque, undermining trust in their output. Furthermore, while autoregressive LMs can output explicit reasoning, their true reasoning process is opaque, which introduces ",
    "url": "https://arxiv.org/abs/2602.11852",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "HybridRAG: A Practical LLM-based ChatBot Framework based on Pre-Generated Q&A over Raw Unstructured Documents",
    "summary": "arXiv:2602.11156v1 Announce Type: cross Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for grounding Large Language Model (LLM)-based chatbot responses on external knowledge. However, existing RAG studies typically assume well-structured textual sources (e.g. Wikipedia or curated datasets) and perform retrieval an",
    "url": "https://arxiv.org/abs/2602.11156",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "The Energy of Falsehood: Detecting Hallucinations via Diffusion Model Likelihoods",
    "summary": "arXiv:2602.11364v1 Announce Type: cross Abstract: Large Language Models (LLMs) frequently hallucinate plausible but incorrect assertions, a vulnerability often missed by uncertainty metrics when models are confidently wrong. We propose DiffuTruth, an unsupervised framework that reconceptualizes fact verification via non equilibrium thermodynamics, ",
    "url": "https://arxiv.org/abs/2602.11364",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Multimodal Fact-Level Attribution for Verifiable Reasoning",
    "summary": "arXiv:2602.11509v1 Announce Type: cross Abstract: Multimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual factual claims. However, existing multimodal groundi",
    "url": "https://arxiv.org/abs/2602.11509",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception",
    "summary": "arXiv:2602.11858v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception, where decisive evidence is small and easily overwhelmed by global context. Recent \"Thinking-with-Images\" methods alleviate this by iteratively zooming in and out regions of i",
    "url": "https://arxiv.org/abs/2602.11858",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "DeepSight: An All-in-One LM Safety Toolkit",
    "summary": "arXiv:2602.12092v1 Announce Type: cross Abstract: As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluatio",
    "url": "https://arxiv.org/abs/2602.12092",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle",
    "summary": "arXiv:2508.05612v4 Announce Type: replace-cross Abstract: Reinforcement learning (RL) has emerged as an effective post-training paradigm for enhancing the reasoning capabilities of multimodal large language model (MLLM). However, current RL pipelines often suffer from training inefficiencies caused by two underexplored issues: Advantage Collapsing,",
    "url": "https://arxiv.org/abs/2508.05612",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Discovering Differences in Strategic Behavior Between Humans and LLMs",
    "summary": "arXiv:2602.10324v1 Announce Type: new Abstract: As Large Language Models (LLMs) are increasingly deployed in social and strategic scenarios, it becomes critical to understand where and why their behavior diverges from that of humans. While behavioral game theory (BGT) provides a framework for analyzing behavior, existing models do not fully capture",
    "url": "https://arxiv.org/abs/2602.10324",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "AgentTrace: A Structured Logging Framework for Agent System Observability",
    "summary": "arXiv:2602.10133v1 Announce Type: cross Abstract: Despite the growing capabilities of autonomous agents powered by large language models (LLMs), their adoption in high-stakes domains remains limited. A key barrier is security: the inherently nondeterministic behavior of LLM agents defies static auditing approaches that have historically underpinned",
    "url": "https://arxiv.org/abs/2602.10133",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Anonymization-Enhanced Privacy Protection for Mobile GUI Agents: Available but Invisible",
    "summary": "arXiv:2602.10139v1 Announce Type: cross Abstract: Mobile Graphical User Interface (GUI) agents have demonstrated strong capabilities in automating complex smartphone tasks by leveraging multimodal large language models (MLLMs) and system-level control interfaces. However, this paradigm introduces significant privacy risks, as agents typically captu",
    "url": "https://arxiv.org/abs/2602.10139",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "PRISM-XR: Empowering Privacy-Aware XR Collaboration with Multimodal Large Language Models",
    "summary": "arXiv:2602.10154v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) enhance collaboration in Extended Reality (XR) environments by enabling flexible object and animation creation through the combination of natural language and visual inputs. However, visual data captured by XR headsets includes real-world backgrounds that may",
    "url": "https://arxiv.org/abs/2602.10154",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Omni-Safety under Cross-Modality Conflict: Vulnerabilities, Dynamics Mechanisms and Efficient Alignment",
    "summary": "arXiv:2602.10161v1 Announce Type: cross Abstract: Omni-modal Large Language Models (OLLMs) greatly expand LLMs' multimodal capabilities but also introduce cross-modal safety risks. However, a systematic understanding of vulnerabilities in omni-modal interactions remains lacking. To bridge this gap, we establish a modality-semantics decoupling princ",
    "url": "https://arxiv.org/abs/2602.10161",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Self-Evolving Recommendation System: End-To-End Autonomous Model Optimization With LLM Agents",
    "summary": "arXiv:2602.10226v1 Announce Type: cross Abstract: Optimizing large-scale machine learning systems, such as recommendation models for global video platforms, requires navigating a massive hyperparameter search space and, more critically, designing sophisticated optimizers, architectures, and reward functions to capture nuanced user behaviors. Achiev",
    "url": "https://arxiv.org/abs/2602.10226",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "C^2ROPE: Causal Continuous Rotary Positional Encoding for 3D Large Multimodal-Models Reasoning",
    "summary": "arXiv:2602.10551v1 Announce Type: cross Abstract: Recent advances in 3D Large Multimodal Models (LMMs) built on Large Language Models (LLMs) have established the alignment of 3D visual features with LLM representations as the dominant paradigm. However, the inherited Rotary Position Embedding (RoPE) introduces limitations for multimodal processing.",
    "url": "https://arxiv.org/abs/2602.10551",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Contrastive Learning for Multi Label ECG Classification with Jaccard Score Based Sigmoid Loss",
    "summary": "arXiv:2602.10553v1 Announce Type: cross Abstract: Recent advances in large language models (LLMs) have enabled the development of multimodal medical AI. While models such as MedGemini achieve high accuracy on VQA tasks like USMLE MM, their performance on ECG based tasks remains limited, and some models, such as MedGemma, do not support ECG data at ",
    "url": "https://arxiv.org/abs/2602.10553",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning",
    "summary": "arXiv:2602.10575v1 Announce Type: cross Abstract: Metaphorical comprehension in images remains a critical challenge for Nowadays AI systems. While Multimodal Large Language Models (MLLMs) excel at basic Visual Question Answering (VQA), they consistently struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visua",
    "url": "https://arxiv.org/abs/2602.10575",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization",
    "summary": "arXiv:2602.10576v1 Announce Type: cross Abstract: Symbolic regression aims to distill mathematical equations from observational data. Recent approaches have successfully leveraged Large Language Models (LLMs) to generate equation hypotheses, capitalizing on their vast pre-trained scientific priors. However, existing frameworks predominantly treat t",
    "url": "https://arxiv.org/abs/2602.10576",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Flow caching for autoregressive video generation",
    "summary": "arXiv:2602.10825v1 Announce Type: cross Abstract: Autoregressive models, often built on Transformer architectures, represent a powerful paradigm for generating ultra-long videos by synthesizing content in sequential chunks. However, this sequential generation process is notoriously slow. While caching strategies have proven effective for accelerati",
    "url": "https://arxiv.org/abs/2602.10825",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Fine-Tuning GPT-5 for GPU Kernel Generation",
    "summary": "arXiv:2602.11000v1 Announce Type: cross Abstract: Developing efficient GPU kernels is essential for scaling modern AI systems, yet it remains a complex task due to intricate hardware architectures and the need for specialized optimization expertise. Although Large Language Models (LLMs) demonstrate strong capabilities in general sequential code gen",
    "url": "https://arxiv.org/abs/2602.11000",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Synthetic Homes: An Accessible Multimodal Pipeline for Producing Residential Building Data with Generative AI",
    "summary": "arXiv:2509.09794v2 Announce Type: replace Abstract: Computational models have emerged as powerful tools for energy modeling research, touting scalability and quantitative results. However, these models require a plethora of data, some of which can be inaccessible, expensive, or can raise privacy concerns. We introduce a modular multimodal framework",
    "url": "https://arxiv.org/abs/2509.09794",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility",
    "summary": "arXiv:2602.03402v2 Announce Type: replace Abstract: Vision language models (VLMs) extend the reasoning capabilities of large language models (LLMs) to cross-modal settings, yet remain highly vulnerable to multimodal jailbreak attacks. Existing defenses predominantly rely on safety fine-tuning or aggressive token manipulations, incurring substantial",
    "url": "https://arxiv.org/abs/2602.03402",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "TernaryLM: Memory-Efficient Language Modeling via Native 1-Bit Quantization with Adaptive Layer-wise Scaling",
    "summary": "arXiv:2602.07374v1 Announce Type: cross Abstract: Large language models (LLMs) achieve remarkable performance but demand substantial computational resources, limiting deployment on edge devices and resource-constrained environments. We present TernaryLM, a 132M parameter transformer architecture that employs native 1-bit ternary quantization {-1, 0",
    "url": "https://arxiv.org/abs/2602.07374",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs",
    "summary": "arXiv:2602.08621v1 Announce Type: cross Abstract: By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior w",
    "url": "https://arxiv.org/abs/2602.08621",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning",
    "summary": "arXiv:2602.07543v2 Announce Type: new Abstract: Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based ",
    "url": "https://arxiv.org/abs/2602.07543",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Towards Adaptive, Scalable, and Robust Coordination of LLM Agents: A Dynamic Ad-Hoc Networking Perspective",
    "summary": "arXiv:2602.08009v1 Announce Type: new Abstract: Multi-agent architectures built on large language models (LLMs) have demonstrated the potential to realize swarm intelligence through well-crafted collaboration. However, the substantial burden of manual orchestration inherently raises an imperative to automate the design of agentic workflows. We fram",
    "url": "https://arxiv.org/abs/2602.08009",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation",
    "summary": "arXiv:2602.08229v1 Announce Type: new Abstract: The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation ",
    "url": "https://arxiv.org/abs/2602.08229",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs",
    "summary": "arXiv:2602.08241v1 Announce Type: new Abstract: While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis show",
    "url": "https://arxiv.org/abs/2602.08241",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis",
    "summary": "arXiv:2602.08276v1 Announce Type: new Abstract: Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concep",
    "url": "https://arxiv.org/abs/2602.08276",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI",
    "summary": "arXiv:2602.08373v1 Announce Type: new Abstract: Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repa",
    "url": "https://arxiv.org/abs/2602.08373",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "PRISM: A Principled Framework for Multi-Agent Reasoning via Gain Decomposition",
    "summary": "arXiv:2602.08586v2 Announce Type: new Abstract: Multi-agent collaboration has emerged as a promising paradigm for enhancing reasoning capabilities of Large Language Models (LLMs). However, existing approaches remain largely heuristic, lacking principled guidance on what drives performance gains and how to systematically optimize multi-agent reasoni",
    "url": "https://arxiv.org/abs/2602.08586",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture",
    "summary": "arXiv:2602.08597v1 Announce Type: new Abstract: Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal i",
    "url": "https://arxiv.org/abs/2602.08597",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "iGRPO: Self-Feedback-Driven LLM Reasoning",
    "summary": "arXiv:2602.09000v1 Announce Type: new Abstract: Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability",
    "url": "https://arxiv.org/abs/2602.09000",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing",
    "summary": "arXiv:2602.07045v1 Announce Type: cross Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for co",
    "url": "https://arxiv.org/abs/2602.07045",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Extended to Reality: Prompt Injection in 3D Environments",
    "summary": "arXiv:2602.07104v1 Announce Type: cross Abstract: Multimodal large language models (MLLMs) have advanced the capabilities to interpret and act on visual input in 3D environments, empowering diverse applications such as robotics and situated conversational agents. When MLLMs reason over camera-captured views of the physical world, a new attack surfa",
    "url": "https://arxiv.org/abs/2602.07104",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models",
    "summary": "arXiv:2602.07106v1 Announce Type: cross Abstract: Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-",
    "url": "https://arxiv.org/abs/2602.07106",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Multimodal Enhancement of Sequential Recommendation",
    "summary": "arXiv:2602.07207v1 Announce Type: cross Abstract: We propose a novel recommender framework, MuSTRec (Multimodal and Sequential Transformer-based Recommendation), that unifies multimodal and sequential recommendation paradigms. MuSTRec captures cross-item similarities and collaborative filtering signals, by building item-item graphs from extracted t",
    "url": "https://arxiv.org/abs/2602.07207",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "TernaryLM: Memory-Efficient Language Modeling via Native 1-Bit Quantization with Adaptive Layer-wise Scaling",
    "summary": "arXiv:2602.07374v1 Announce Type: cross Abstract: Large language models (LLMs) achieve remarkable performance but demand substantial computational resources, limiting deployment on edge devices and resource-constrained environments. We present TernaryLM, a 132M parameter transformer architecture that employs native 1-bit ternary quantization {-1, 0",
    "url": "https://arxiv.org/abs/2602.07374",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs",
    "summary": "arXiv:2602.08621v1 Announce Type: cross Abstract: By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior w",
    "url": "https://arxiv.org/abs/2602.08621",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning",
    "summary": "arXiv:2602.07543v1 Announce Type: new Abstract: Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based ",
    "url": "https://arxiv.org/abs/2602.07543",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "Towards Adaptive, Scalable, and Robust Coordination of LLM Agents: A Dynamic Ad-Hoc Networking Perspective",
    "summary": "arXiv:2602.08009v1 Announce Type: new Abstract: Multi-agent architectures built on large language models (LLMs) have demonstrated the potential to realize swarm intelligence through well-crafted collaboration. However, the substantial burden of manual orchestration inherently raises an imperative to automate the design of agentic workflows. We fram",
    "url": "https://arxiv.org/abs/2602.08009",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation",
    "summary": "arXiv:2602.08229v1 Announce Type: new Abstract: The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation ",
    "url": "https://arxiv.org/abs/2602.08229",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs",
    "summary": "arXiv:2602.08241v1 Announce Type: new Abstract: While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis show",
    "url": "https://arxiv.org/abs/2602.08241",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis",
    "summary": "arXiv:2602.08276v1 Announce Type: new Abstract: Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concep",
    "url": "https://arxiv.org/abs/2602.08276",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI",
    "summary": "arXiv:2602.08373v1 Announce Type: new Abstract: Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repa",
    "url": "https://arxiv.org/abs/2602.08373",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "PRISM: A Principled Framework for Multi-Agent Reasoning via Gain Decomposition",
    "summary": "arXiv:2602.08586v1 Announce Type: new Abstract: Multi-agent collaboration has emerged as a promising paradigm for enhancing reasoning capabilities of Large Language Models (LLMs). However, existing approaches remain largely heuristic, lacking principled guidance on what drives performance gains and how to systematically optimize multi-agent reasoni",
    "url": "https://arxiv.org/abs/2602.08586",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture",
    "summary": "arXiv:2602.08597v1 Announce Type: new Abstract: Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal i",
    "url": "https://arxiv.org/abs/2602.08597",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "iGRPO: Self-Feedback-Driven LLM Reasoning",
    "summary": "arXiv:2602.09000v1 Announce Type: new Abstract: Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability",
    "url": "https://arxiv.org/abs/2602.09000",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing",
    "summary": "arXiv:2602.07045v1 Announce Type: cross Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for co",
    "url": "https://arxiv.org/abs/2602.07045",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "Extended to Reality: Prompt Injection in 3D Environments",
    "summary": "arXiv:2602.07104v1 Announce Type: cross Abstract: Multimodal large language models (MLLMs) have advanced the capabilities to interpret and act on visual input in 3D environments, empowering diverse applications such as robotics and situated conversational agents. When MLLMs reason over camera-captured views of the physical world, a new attack surfa",
    "url": "https://arxiv.org/abs/2602.07104",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models",
    "summary": "arXiv:2602.07106v1 Announce Type: cross Abstract: Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-",
    "url": "https://arxiv.org/abs/2602.07106",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "Multimodal Enhancement of Sequential Recommendation",
    "summary": "arXiv:2602.07207v1 Announce Type: cross Abstract: We propose a novel recommender framework, MuSTRec (Multimodal and Sequential Transformer-based Recommendation), that unifies multimodal and sequential recommendation paradigms. MuSTRec captures cross-item similarities and collaborative filtering signals, by building item-item graphs from extracted t",
    "url": "https://arxiv.org/abs/2602.07207",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "Trifuse: Enhancing Attention-Based GUI Grounding via Multimodal Fusion",
    "summary": "arXiv:2602.06351v1 Announce Type: new Abstract: GUI grounding maps natural language instructions to the correct interface elements, serving as the perception foundation for GUI agents. Existing approaches predominantly rely on fine-tuning multimodal large language models (MLLMs) using large-scale GUI datasets to predict target element coordinates, ",
    "url": "https://arxiv.org/abs/2602.06351",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "Towards Understanding What State Space Models Learn About Code",
    "summary": "arXiv:2602.06774v1 Announce Type: new Abstract: State Space Models (SSMs) have emerged as an efficient alternative to the transformer architecture. Recent studies show that SSMs can match or surpass Transformers on code understanding tasks, such as code retrieval, when trained under similar conditions. However, their internal mechanisms remain a bl",
    "url": "https://arxiv.org/abs/2602.06774",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "LLM Active Alignment: A Nash Equilibrium Perspective",
    "summary": "arXiv:2602.06836v1 Announce Type: new Abstract: We develop a game-theoretic framework for predicting and steering the behavior of populations of large language models (LLMs) through Nash equilibrium (NE) analysis. To avoid the intractability of equilibrium computation in open-ended text spaces, we model each agent's action as a mixture over human s",
    "url": "https://arxiv.org/abs/2602.06836",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "Analyzing Diffusion and Autoregressive Vision Language Models in Multimodal Embedding Space",
    "summary": "arXiv:2602.06056v1 Announce Type: cross Abstract: Embedding models are a fundamental component of modern AI systems such as semantic search and retrieval-augmented generation. Recent advances in large foundation models have substantially accelerated the development of embedding models, including those based on Large Language Models (LLMs), Vision L",
    "url": "https://arxiv.org/abs/2602.06056",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "DAVE: Distribution-aware Attribution via ViT Gradient Decomposition",
    "summary": "arXiv:2602.06613v1 Announce Type: cross Abstract: Vision Transformers (ViTs) have become a dominant architecture in computer vision, yet producing stable and high-resolution attribution maps for these models remains challenging. Architectural components such as patch embeddings and attention routing often introduce structured artifacts in pixel-lev",
    "url": "https://arxiv.org/abs/2602.06613",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "Multimodal Generative Retrieval Model with Staged Pretraining for Food Delivery on Meituan",
    "summary": "arXiv:2602.06654v1 Announce Type: cross Abstract: Multimodal retrieval models are becoming increasingly important in scenarios such as food delivery, where rich multimodal features can meet diverse user needs and enable precise retrieval. Mainstream approaches typically employ a dual-tower architecture between queries and items, and perform joint o",
    "url": "https://arxiv.org/abs/2602.06654",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "Bridging 6G IoT and AI: LLM-Based Efficient Approach for Physical Layer's Optimization Tasks",
    "summary": "arXiv:2602.06819v1 Announce Type: cross Abstract: This paper investigates the role of large language models (LLMs) in sixth-generation (6G) Internet of Things (IoT) networks and proposes a prompt-engineering-based real-time feedback and verification (PE-RTFV) framework that perform physical-layer's optimization tasks through an iteratively process.",
    "url": "https://arxiv.org/abs/2602.06819",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "The Quantum Sieve Tracer: A Hybrid Framework for Layer-Wise Activation Tracing in Large Language Models",
    "summary": "arXiv:2602.06852v1 Announce Type: cross Abstract: Mechanistic interpretability aims to reverse-engineer the internal computations of Large Language Models (LLMs), yet separating sparse semantic signals from high-dimensional polysemantic noise remains a significant challenge. This paper introduces the Quantum Sieve Tracer, a hybrid quantum-classical",
    "url": "https://arxiv.org/abs/2602.06852",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code",
    "summary": "arXiv:2602.06875v1 Announce Type: cross Abstract: Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, wi",
    "url": "https://arxiv.org/abs/2602.06875",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "From Kepler to Newton: Inductive Biases Guide Learned World Models in Transformers",
    "summary": "arXiv:2602.06923v1 Announce Type: cross Abstract: Can general-purpose AI architectures go beyond prediction to discover the physical laws governing the universe? True intelligence relies on \"world models\" -- causal abstractions that allow an agent to not only predict future states but understand the underlying governing dynamics. While previous \"AI",
    "url": "https://arxiv.org/abs/2602.06923",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search",
    "summary": "arXiv:2602.05014v2 Announce Type: replace Abstract: With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search framework",
    "url": "https://arxiv.org/abs/2602.05014",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "AgentXRay: White-Boxing Agentic Systems via Workflow Reconstruction",
    "summary": "arXiv:2602.05353v2 Announce Type: replace Abstract: Large Language Models have shown strong capabilities in complex problem solving, yet many agentic systems remain difficult to interpret and control due to opaque internal workflows. While some frameworks offer explicit architectures for collaboration, many deployed agentic systems operate as black",
    "url": "https://arxiv.org/abs/2602.05353",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "Echo State Transformer: Attention Over Finite Memories",
    "summary": "arXiv:2507.02917v3 Announce Type: replace-cross Abstract: While Large Language Models and their underlying Transformer architecture are remarkably efficient, they do not reflect how our brain processes and learns a diversity of cognitive tasks such as language, nor how it leverages working memory. Furthermore, Transformers encounters a computationa",
    "url": "https://arxiv.org/abs/2507.02917",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation",
    "summary": "arXiv:2508.01309v2 Announce Type: replace-cross Abstract: The scarcity and high cost of high-quality domain-specific question-answering (QA) datasets limit supervised fine-tuning of large language models (LLMs). We introduce $\\textbf{D-SCoRE}$, a training-free framework that leverages LLMs and prompt engineering to automatically generate diverse, r",
    "url": "https://arxiv.org/abs/2508.01309",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "An item is worth one token in Multimodal Large Language Models-based Sequential Recommendation",
    "summary": "arXiv:2511.05885v3 Announce Type: replace-cross Abstract: Sequential recommendations (SR) predict users' future interactions based on their historical behavior. The rise of Large Language Models (LLMs) has brought powerful generative and reasoning capabilities, significantly enhancing SR performance, while Multimodal LLMs (MLLMs) further extend thi",
    "url": "https://arxiv.org/abs/2511.05885",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "Trifuse: Enhancing Attention-Based GUI Grounding via Multimodal Fusion",
    "summary": "arXiv:2602.06351v1 Announce Type: new Abstract: GUI grounding maps natural language instructions to the correct interface elements, serving as the perception foundation for GUI agents. Existing approaches predominantly rely on fine-tuning multimodal large language models (MLLMs) using large-scale GUI datasets to predict target element coordinates, ",
    "url": "https://arxiv.org/abs/2602.06351",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "Towards Understanding What State Space Models Learn About Code",
    "summary": "arXiv:2602.06774v1 Announce Type: new Abstract: State Space Models (SSMs) have emerged as an efficient alternative to the transformer architecture. Recent studies show that SSMs can match or surpass Transformers on code understanding tasks, such as code retrieval, when trained under similar conditions. However, their internal mechanisms remain a bl",
    "url": "https://arxiv.org/abs/2602.06774",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "LLM Active Alignment: A Nash Equilibrium Perspective",
    "summary": "arXiv:2602.06836v1 Announce Type: new Abstract: We develop a game-theoretic framework for predicting and steering the behavior of populations of large language models (LLMs) through Nash equilibrium (NE) analysis. To avoid the intractability of equilibrium computation in open-ended text spaces, we model each agent's action as a mixture over human s",
    "url": "https://arxiv.org/abs/2602.06836",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "Analyzing Diffusion and Autoregressive Vision Language Models in Multimodal Embedding Space",
    "summary": "arXiv:2602.06056v1 Announce Type: cross Abstract: Embedding models are a fundamental component of modern AI systems such as semantic search and retrieval-augmented generation. Recent advances in large foundation models have substantially accelerated the development of embedding models, including those based on Large Language Models (LLMs), Vision L",
    "url": "https://arxiv.org/abs/2602.06056",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "DAVE: Distribution-aware Attribution via ViT Gradient Decomposition",
    "summary": "arXiv:2602.06613v1 Announce Type: cross Abstract: Vision Transformers (ViTs) have become a dominant architecture in computer vision, yet producing stable and high-resolution attribution maps for these models remains challenging. Architectural components such as patch embeddings and attention routing often introduce structured artifacts in pixel-lev",
    "url": "https://arxiv.org/abs/2602.06613",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "Multimodal Generative Retrieval Model with Staged Pretraining for Food Delivery on Meituan",
    "summary": "arXiv:2602.06654v1 Announce Type: cross Abstract: Multimodal retrieval models are becoming increasingly important in scenarios such as food delivery, where rich multimodal features can meet diverse user needs and enable precise retrieval. Mainstream approaches typically employ a dual-tower architecture between queries and items, and perform joint o",
    "url": "https://arxiv.org/abs/2602.06654",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "Bridging 6G IoT and AI: LLM-Based Efficient Approach for Physical Layer's Optimization Tasks",
    "summary": "arXiv:2602.06819v1 Announce Type: cross Abstract: This paper investigates the role of large language models (LLMs) in sixth-generation (6G) Internet of Things (IoT) networks and proposes a prompt-engineering-based real-time feedback and verification (PE-RTFV) framework that perform physical-layer's optimization tasks through an iteratively process.",
    "url": "https://arxiv.org/abs/2602.06819",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "The Quantum Sieve Tracer: A Hybrid Framework for Layer-Wise Activation Tracing in Large Language Models",
    "summary": "arXiv:2602.06852v1 Announce Type: cross Abstract: Mechanistic interpretability aims to reverse-engineer the internal computations of Large Language Models (LLMs), yet separating sparse semantic signals from high-dimensional polysemantic noise remains a significant challenge. This paper introduces the Quantum Sieve Tracer, a hybrid quantum-classical",
    "url": "https://arxiv.org/abs/2602.06852",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code",
    "summary": "arXiv:2602.06875v1 Announce Type: cross Abstract: Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, wi",
    "url": "https://arxiv.org/abs/2602.06875",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "From Kepler to Newton: Inductive Biases Guide Learned World Models in Transformers",
    "summary": "arXiv:2602.06923v1 Announce Type: cross Abstract: Can general-purpose AI architectures go beyond prediction to discover the physical laws governing the universe? True intelligence relies on \"world models\" -- causal abstractions that allow an agent to not only predict future states but understand the underlying governing dynamics. While previous \"AI",
    "url": "https://arxiv.org/abs/2602.06923",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search",
    "summary": "arXiv:2602.05014v2 Announce Type: replace Abstract: With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search framework",
    "url": "https://arxiv.org/abs/2602.05014",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "AgentXRay: White-Boxing Agentic Systems via Workflow Reconstruction",
    "summary": "arXiv:2602.05353v2 Announce Type: replace Abstract: Large Language Models have shown strong capabilities in complex problem solving, yet many agentic systems remain difficult to interpret and control due to opaque internal workflows. While some frameworks offer explicit architectures for collaboration, many deployed agentic systems operate as black",
    "url": "https://arxiv.org/abs/2602.05353",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "Echo State Transformer: Attention Over Finite Memories",
    "summary": "arXiv:2507.02917v3 Announce Type: replace-cross Abstract: While Large Language Models and their underlying Transformer architecture are remarkably efficient, they do not reflect how our brain processes and learns a diversity of cognitive tasks such as language, nor how it leverages working memory. Furthermore, Transformers encounters a computationa",
    "url": "https://arxiv.org/abs/2507.02917",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation",
    "summary": "arXiv:2508.01309v2 Announce Type: replace-cross Abstract: The scarcity and high cost of high-quality domain-specific question-answering (QA) datasets limit supervised fine-tuning of large language models (LLMs). We introduce $\\textbf{D-SCoRE}$, a training-free framework that leverages LLMs and prompt engineering to automatically generate diverse, r",
    "url": "https://arxiv.org/abs/2508.01309",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "An item is worth one token in Multimodal Large Language Models-based Sequential Recommendation",
    "summary": "arXiv:2511.05885v3 Announce Type: replace-cross Abstract: Sequential recommendations (SR) predict users' future interactions based on their historical behavior. The rise of Large Language Models (LLMs) has brought powerful generative and reasoning capabilities, significantly enhancing SR performance, while Multimodal LLMs (MLLMs) further extend thi",
    "url": "https://arxiv.org/abs/2511.05885",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "Understanding LLM Evaluator Behavior: A Structured Multi-Evaluator Framework for Merchant Risk Assessment",
    "summary": "arXiv:2602.05110v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly used as evaluators of reasoning quality, yet their reliability and bias in payments-risk settings remain poorly understood. We introduce a structured multi-evaluator framework for assessing LLM reasoning in Merchant Category Code (MCC)-based merchant risk ",
    "url": "https://arxiv.org/abs/2602.05110",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "Hallucination-Resistant Security Planning with a Large Language Model",
    "summary": "arXiv:2602.05279v1 Announce Type: new Abstract: Large language models (LLMs) are promising tools for supporting security management tasks, such as incident response planning. However, their unreliability and tendency to hallucinate remain significant challenges. In this paper, we address these challenges by introducing a principled framework for us",
    "url": "https://arxiv.org/abs/2602.05279",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "ProAct: Agentic Lookahead in Interactive Environments",
    "summary": "arXiv:2602.05327v1 Announce Type: new Abstract: Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a ",
    "url": "https://arxiv.org/abs/2602.05327",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "AgentXRay: White-Boxing Agentic Systems via Workflow Reconstruction",
    "summary": "arXiv:2602.05353v1 Announce Type: new Abstract: Large Language Models have shown strong capabilities in complex problem solving, yet many agentic systems remain difficult to interpret and control due to opaque internal workflows. While some frameworks offer explicit architectures for collaboration, many deployed agentic systems operate as black box",
    "url": "https://arxiv.org/abs/2602.05353",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "A Unified Multimodal Framework for Dataset Construction and Model-Based Diagnosis of Ameloblastoma",
    "summary": "arXiv:2602.05515v1 Announce Type: new Abstract: Artificial intelligence (AI)-enabled diagnostics in maxillofacial pathology require structured, high-quality multimodal datasets. However, existing resources provide limited ameloblastoma coverage and lack the format consistency needed for direct model training. We present a newly curated multimodal d",
    "url": "https://arxiv.org/abs/2602.05515",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "Reasoning-guided Collaborative Filtering with Language Models for Explainable Recommendation",
    "summary": "arXiv:2602.05544v1 Announce Type: new Abstract: Large Language Models (LLMs) exhibit potential for explainable recommendation systems but overlook collaborative signals, while prevailing methods treat recommendation and explanation as separate tasks, resulting in a memory footprint. We present RGCF-XRec, a hybrid framework that introduces reasoning",
    "url": "https://arxiv.org/abs/2602.05544",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "A$^2$-LLM: An End-to-end Conversational Audio Avatar Large Language Model",
    "summary": "arXiv:2602.04913v1 Announce Type: cross Abstract: Developing expressive and responsive conversational digital humans is a cornerstone of next-generation human-computer interaction. While large language models (LLMs) have significantly enhanced dialogue capabilities, most current systems still rely on cascaded architectures that connect independent ",
    "url": "https://arxiv.org/abs/2602.04913",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "PriMod4AI: Lifecycle-Aware Privacy Threat Modeling for AI Systems using LLM",
    "summary": "arXiv:2602.04927v1 Announce Type: cross Abstract: Artificial intelligence systems introduce complex privacy risks throughout their lifecycle, especially when processing sensitive or high-dimensional data. Beyond the seven traditional privacy threat categories defined by the LINDDUN framework, AI systems are also exposed to model-centric privacy att",
    "url": "https://arxiv.org/abs/2602.04927",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "Linear Model Merging Unlocks Simple and Scalable Multimodal Data Mixture Optimization",
    "summary": "arXiv:2602.04937v1 Announce Type: cross Abstract: Selecting the best data mixture is critical for successful Supervised Fine-Tuning (SFT) of Multimodal Large Language Models. However, determining the optimal mixture weights across multiple domain-specific datasets remains a significant bottleneck due to the combinatorial search space and the high c",
    "url": "https://arxiv.org/abs/2602.04937",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "LMMRec: LLM-driven Motivation-aware Multimodal Recommendation",
    "summary": "arXiv:2602.05474v1 Announce Type: cross Abstract: Motivation-based recommendation systems uncover user behavior drivers. Motivation modeling, crucial for decision-making and content preference, explains recommendation generation. Existing methods often treat motivation as latent variables from interaction data, neglecting heterogeneous information ",
    "url": "https://arxiv.org/abs/2602.05474",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "A Unified Framework for Rethinking Policy Divergence Measures in GRPO",
    "summary": "arXiv:2602.05494v1 Announce Type: cross Abstract: Reinforcement Learning with Verified Reward (RLVR) has emerged as a critical paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). Most existing RLVR methods, such as GRPO and its variants, ensure stable updates by constraining policy divergence through clipping likeliho",
    "url": "https://arxiv.org/abs/2602.05494",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "Transport and Merge: Cross-Architecture Merging for Large Language Models",
    "summary": "arXiv:2602.05495v1 Announce Type: cross Abstract: Large language models (LLMs) achieve strong capabilities by scaling model capacity and training data, yet many real-world deployments rely on smaller models trained or adapted from low-resource data. This gap motivates the need for mechanisms to transfer knowledge from large, high-resource models to",
    "url": "https://arxiv.org/abs/2602.05495",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "In-context Time Series Predictor",
    "summary": "arXiv:2405.14982v2 Announce Type: replace-cross Abstract: Recent Transformer-based large language models (LLMs) demonstrate in-context learning ability to perform various functions based solely on the provided context, without updating model parameters. To fully utilize the in-context capabilities in time series forecasting (TSF) problems, unlike p",
    "url": "https://arxiv.org/abs/2405.14982",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models",
    "summary": "arXiv:2503.06749v3 Announce Type: replace-cross Abstract: DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles",
    "url": "https://arxiv.org/abs/2503.06749",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "Generalizable Trajectory Prediction via Inverse Reinforcement Learning with Mamba-Graph Architecture",
    "summary": "arXiv:2506.12474v2 Announce Type: replace-cross Abstract: Accurate driving behavior modeling is fundamental to safe and efficient trajectory prediction, yet remains challenging in complex traffic scenarios. This paper presents a novel Inverse Reinforcement Learning (IRL) framework that captures human-like decision-making by inferring diverse reward",
    "url": "https://arxiv.org/abs/2506.12474",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "Interfaze: The Future of AI is built on Task-Specific Small Models",
    "summary": "arXiv:2602.04101v1 Announce Type: new Abstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OC",
    "url": "https://arxiv.org/abs/2602.04101",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-07T03:10:55.111214+00:00"
  },
  {
    "title": "OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows",
    "summary": "arXiv:2602.04144v1 Announce Type: new Abstract: Data incompleteness severely impedes the reliability of multimodal systems. Existing reconstruction methods face distinct bottlenecks: conventional parametric/generative models are prone to hallucinations due to over-reliance on internal memory, while retrieval-augmented frameworks struggle with retri",
    "url": "https://arxiv.org/abs/2602.04144",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-07T03:10:55.111214+00:00"
  },
  {
    "title": "ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas",
    "summary": "arXiv:2602.04296v1 Announce Type: cross Abstract: Large language models (LLMs) have revolutionized automated code generation, yet the evaluation of their real-world effectiveness remains limited by static benchmarks and simplistic metrics. We present ProxyWar, a novel framework that systematically assesses code generation quality by embedding LLM-g",
    "url": "https://arxiv.org/abs/2602.04296",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-07T03:10:55.111214+00:00"
  },
  {
    "title": "History-Guided Iterative Visual Reasoning with Self-Correction",
    "summary": "arXiv:2602.04413v1 Announce Type: cross Abstract: Self-consistency methods are the core technique for improving the reasoning reliability of multimodal large language models (MLLMs). By generating multiple reasoning results through repeated sampling and selecting the best answer via voting, they play an important role in cross-modal tasks. However,",
    "url": "https://arxiv.org/abs/2602.04413",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-07T03:10:55.111214+00:00"
  },
  {
    "title": "Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases",
    "summary": "arXiv:2602.04739v1 Announce Type: cross Abstract: Multimodal large language models (MLLMs) are increasingly deployed in real-world systems, yet their safety under adversarial prompting remains underexplored. We present a two-phase evaluation of MLLM harmlessness using a fixed benchmark of 726 adversarial prompts authored by 26 professional red team",
    "url": "https://arxiv.org/abs/2602.04739",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-07T03:10:55.111214+00:00"
  },
  {
    "title": "Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing",
    "summary": "arXiv:2510.04670v3 Announce Type: replace Abstract: Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic Framework for Multimodal fMRI Response Encoding), an agnostic interface that standardizes time-aligned post-fusion tokens from varied encoders, a",
    "url": "https://arxiv.org/abs/2510.04670",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-07T03:10:55.111214+00:00"
  },
  {
    "title": "M^3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark",
    "summary": "arXiv:2511.17729v4 Announce Type: replace Abstract: We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resou",
    "url": "https://arxiv.org/abs/2511.17729",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-07T03:10:55.111214+00:00"
  },
  {
    "title": "Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving",
    "summary": "arXiv:2601.21164v2 Announce Type: replace Abstract: Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by th",
    "url": "https://arxiv.org/abs/2601.21164",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-07T03:10:55.111214+00:00"
  },
  {
    "title": "Building Interpretable Models for Moral Decision-Making",
    "summary": "arXiv:2602.03351v2 Announce Type: replace Abstract: We build a custom transformer model to study how neural networks make moral decisions on trolley-style dilemmas. The model processes structured scenarios using embeddings that encode who is affected, how many people, and which outcome they belong to. Our 2-layer architecture achieves 77% accuracy ",
    "url": "https://arxiv.org/abs/2602.03351",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-07T03:10:55.111214+00:00"
  },
  {
    "title": "Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders",
    "summary": "arXiv:2507.03262v3 Announce Type: replace-cross Abstract: Recent multimodal large language models (MLLMs) increasingly integrate multiple vision encoders to improve performance on various benchmarks, assuming that diverse pretraining objectives yield complementary visual signals. However, we show this assumption often fails in practice. Through sys",
    "url": "https://arxiv.org/abs/2507.03262",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-07T03:10:55.111214+00:00"
  },
  {
    "title": "Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy",
    "summary": "arXiv:2509.24385v2 Announce Type: replace-cross Abstract: Recent developments in Multimodal Large Language Models (MLLMs) have significantly improved Vision-Language (VL) reasoning in 2D domains. However, extending these capabilities to 3D scene understanding remains a major challenge. Existing 3D Multimodal Large Language Models (3D-MLLMs) often d",
    "url": "https://arxiv.org/abs/2509.24385",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-07T03:10:55.111214+00:00"
  },
  {
    "title": "Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression",
    "summary": "arXiv:2510.02345v3 Announce Type: replace-cross Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load imbalance, parameter redundancy, and communication overhead. We introduce a unified framework based on dynamic expert clustering and structured compression to address these issues cohesively. Our method employs an ",
    "url": "https://arxiv.org/abs/2510.02345",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-07T03:10:55.111214+00:00"
  },
  {
    "title": "Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks",
    "summary": "arXiv:2510.02712v4 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their robustness in extended multi-turn dialogues remains poorly understood. Existing evaluation frameworks focus on static benchmarks and single-turn assessments, failing to capture the temporal dynamics of conversation",
    "url": "https://arxiv.org/abs/2510.02712",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-07T03:10:55.111214+00:00"
  },
  {
    "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
    "summary": "arXiv:2602.02437v2 Announce Type: replace-cross Abstract: Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework tha",
    "url": "https://arxiv.org/abs/2602.02437",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-07T03:10:55.111214+00:00"
  },
  {
    "title": "Knowledge Model Prompting Increases LLM Performance on Planning Tasks",
    "summary": "arXiv:2602.03900v1 Announce Type: new Abstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into quest",
    "url": "https://arxiv.org/abs/2602.03900",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-07T03:10:55.111214+00:00"
  },
  {
    "title": "Interfaze: The Future of AI is built on Task-Specific Small Models",
    "summary": "arXiv:2602.04101v1 Announce Type: new Abstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OC",
    "url": "https://arxiv.org/abs/2602.04101",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:43:59.740086+00:00"
  },
  {
    "title": "OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows",
    "summary": "arXiv:2602.04144v1 Announce Type: new Abstract: Data incompleteness severely impedes the reliability of multimodal systems. Existing reconstruction methods face distinct bottlenecks: conventional parametric/generative models are prone to hallucinations due to over-reliance on internal memory, while retrieval-augmented frameworks struggle with retri",
    "url": "https://arxiv.org/abs/2602.04144",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:43:59.740086+00:00"
  },
  {
    "title": "ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas",
    "summary": "arXiv:2602.04296v1 Announce Type: cross Abstract: Large language models (LLMs) have revolutionized automated code generation, yet the evaluation of their real-world effectiveness remains limited by static benchmarks and simplistic metrics. We present ProxyWar, a novel framework that systematically assesses code generation quality by embedding LLM-g",
    "url": "https://arxiv.org/abs/2602.04296",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:43:59.740086+00:00"
  },
  {
    "title": "History-Guided Iterative Visual Reasoning with Self-Correction",
    "summary": "arXiv:2602.04413v1 Announce Type: cross Abstract: Self-consistency methods are the core technique for improving the reasoning reliability of multimodal large language models (MLLMs). By generating multiple reasoning results through repeated sampling and selecting the best answer via voting, they play an important role in cross-modal tasks. However,",
    "url": "https://arxiv.org/abs/2602.04413",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:43:59.740086+00:00"
  },
  {
    "title": "Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases",
    "summary": "arXiv:2602.04739v1 Announce Type: cross Abstract: Multimodal large language models (MLLMs) are increasingly deployed in real-world systems, yet their safety under adversarial prompting remains underexplored. We present a two-phase evaluation of MLLM harmlessness using a fixed benchmark of 726 adversarial prompts authored by 26 professional red team",
    "url": "https://arxiv.org/abs/2602.04739",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:43:59.740086+00:00"
  },
  {
    "title": "Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing",
    "summary": "arXiv:2510.04670v3 Announce Type: replace Abstract: Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic Framework for Multimodal fMRI Response Encoding), an agnostic interface that standardizes time-aligned post-fusion tokens from varied encoders, a",
    "url": "https://arxiv.org/abs/2510.04670",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:43:59.740086+00:00"
  },
  {
    "title": "M^3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark",
    "summary": "arXiv:2511.17729v4 Announce Type: replace Abstract: We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resou",
    "url": "https://arxiv.org/abs/2511.17729",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:43:59.740086+00:00"
  },
  {
    "title": "Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving",
    "summary": "arXiv:2601.21164v2 Announce Type: replace Abstract: Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by th",
    "url": "https://arxiv.org/abs/2601.21164",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:43:59.740086+00:00"
  },
  {
    "title": "Building Interpretable Models for Moral Decision-Making",
    "summary": "arXiv:2602.03351v2 Announce Type: replace Abstract: We build a custom transformer model to study how neural networks make moral decisions on trolley-style dilemmas. The model processes structured scenarios using embeddings that encode who is affected, how many people, and which outcome they belong to. Our 2-layer architecture achieves 77% accuracy ",
    "url": "https://arxiv.org/abs/2602.03351",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:43:59.740086+00:00"
  },
  {
    "title": "Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders",
    "summary": "arXiv:2507.03262v3 Announce Type: replace-cross Abstract: Recent multimodal large language models (MLLMs) increasingly integrate multiple vision encoders to improve performance on various benchmarks, assuming that diverse pretraining objectives yield complementary visual signals. However, we show this assumption often fails in practice. Through sys",
    "url": "https://arxiv.org/abs/2507.03262",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:43:59.740086+00:00"
  },
  {
    "title": "Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy",
    "summary": "arXiv:2509.24385v2 Announce Type: replace-cross Abstract: Recent developments in Multimodal Large Language Models (MLLMs) have significantly improved Vision-Language (VL) reasoning in 2D domains. However, extending these capabilities to 3D scene understanding remains a major challenge. Existing 3D Multimodal Large Language Models (3D-MLLMs) often d",
    "url": "https://arxiv.org/abs/2509.24385",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:43:59.740086+00:00"
  },
  {
    "title": "Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression",
    "summary": "arXiv:2510.02345v3 Announce Type: replace-cross Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load imbalance, parameter redundancy, and communication overhead. We introduce a unified framework based on dynamic expert clustering and structured compression to address these issues cohesively. Our method employs an ",
    "url": "https://arxiv.org/abs/2510.02345",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:43:59.740086+00:00"
  },
  {
    "title": "Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks",
    "summary": "arXiv:2510.02712v4 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their robustness in extended multi-turn dialogues remains poorly understood. Existing evaluation frameworks focus on static benchmarks and single-turn assessments, failing to capture the temporal dynamics of conversation",
    "url": "https://arxiv.org/abs/2510.02712",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:43:59.740086+00:00"
  },
  {
    "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
    "summary": "arXiv:2602.02437v2 Announce Type: replace-cross Abstract: Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework tha",
    "url": "https://arxiv.org/abs/2602.02437",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:43:59.740086+00:00"
  },
  {
    "title": "Knowledge Model Prompting Increases LLM Performance on Planning Tasks",
    "summary": "arXiv:2602.03900v1 Announce Type: new Abstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into quest",
    "url": "https://arxiv.org/abs/2602.03900",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-06T12:43:59.740086+00:00"
  },
  {
    "title": "Interfaze: The Future of AI is built on Task-Specific Small Models",
    "summary": "arXiv:2602.04101v1 Announce Type: new Abstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OC",
    "url": "https://arxiv.org/abs/2602.04101",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:38:59.092933+00:00"
  },
  {
    "title": "OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows",
    "summary": "arXiv:2602.04144v1 Announce Type: new Abstract: Data incompleteness severely impedes the reliability of multimodal systems. Existing reconstruction methods face distinct bottlenecks: conventional parametric/generative models are prone to hallucinations due to over-reliance on internal memory, while retrieval-augmented frameworks struggle with retri",
    "url": "https://arxiv.org/abs/2602.04144",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:38:59.092933+00:00"
  },
  {
    "title": "ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas",
    "summary": "arXiv:2602.04296v1 Announce Type: cross Abstract: Large language models (LLMs) have revolutionized automated code generation, yet the evaluation of their real-world effectiveness remains limited by static benchmarks and simplistic metrics. We present ProxyWar, a novel framework that systematically assesses code generation quality by embedding LLM-g",
    "url": "https://arxiv.org/abs/2602.04296",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:38:59.092933+00:00"
  },
  {
    "title": "History-Guided Iterative Visual Reasoning with Self-Correction",
    "summary": "arXiv:2602.04413v1 Announce Type: cross Abstract: Self-consistency methods are the core technique for improving the reasoning reliability of multimodal large language models (MLLMs). By generating multiple reasoning results through repeated sampling and selecting the best answer via voting, they play an important role in cross-modal tasks. However,",
    "url": "https://arxiv.org/abs/2602.04413",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:38:59.092933+00:00"
  },
  {
    "title": "Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases",
    "summary": "arXiv:2602.04739v1 Announce Type: cross Abstract: Multimodal large language models (MLLMs) are increasingly deployed in real-world systems, yet their safety under adversarial prompting remains underexplored. We present a two-phase evaluation of MLLM harmlessness using a fixed benchmark of 726 adversarial prompts authored by 26 professional red team",
    "url": "https://arxiv.org/abs/2602.04739",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:38:59.092933+00:00"
  },
  {
    "title": "Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing",
    "summary": "arXiv:2510.04670v3 Announce Type: replace Abstract: Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic Framework for Multimodal fMRI Response Encoding), an agnostic interface that standardizes time-aligned post-fusion tokens from varied encoders, a",
    "url": "https://arxiv.org/abs/2510.04670",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:38:59.092933+00:00"
  },
  {
    "title": "M^3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark",
    "summary": "arXiv:2511.17729v4 Announce Type: replace Abstract: We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resou",
    "url": "https://arxiv.org/abs/2511.17729",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:38:59.092933+00:00"
  },
  {
    "title": "Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving",
    "summary": "arXiv:2601.21164v2 Announce Type: replace Abstract: Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by th",
    "url": "https://arxiv.org/abs/2601.21164",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:38:59.092933+00:00"
  },
  {
    "title": "Building Interpretable Models for Moral Decision-Making",
    "summary": "arXiv:2602.03351v2 Announce Type: replace Abstract: We build a custom transformer model to study how neural networks make moral decisions on trolley-style dilemmas. The model processes structured scenarios using embeddings that encode who is affected, how many people, and which outcome they belong to. Our 2-layer architecture achieves 77% accuracy ",
    "url": "https://arxiv.org/abs/2602.03351",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:38:59.092933+00:00"
  },
  {
    "title": "Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders",
    "summary": "arXiv:2507.03262v3 Announce Type: replace-cross Abstract: Recent multimodal large language models (MLLMs) increasingly integrate multiple vision encoders to improve performance on various benchmarks, assuming that diverse pretraining objectives yield complementary visual signals. However, we show this assumption often fails in practice. Through sys",
    "url": "https://arxiv.org/abs/2507.03262",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:38:59.092933+00:00"
  },
  {
    "title": "Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy",
    "summary": "arXiv:2509.24385v2 Announce Type: replace-cross Abstract: Recent developments in Multimodal Large Language Models (MLLMs) have significantly improved Vision-Language (VL) reasoning in 2D domains. However, extending these capabilities to 3D scene understanding remains a major challenge. Existing 3D Multimodal Large Language Models (3D-MLLMs) often d",
    "url": "https://arxiv.org/abs/2509.24385",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:38:59.092933+00:00"
  },
  {
    "title": "Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression",
    "summary": "arXiv:2510.02345v3 Announce Type: replace-cross Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load imbalance, parameter redundancy, and communication overhead. We introduce a unified framework based on dynamic expert clustering and structured compression to address these issues cohesively. Our method employs an ",
    "url": "https://arxiv.org/abs/2510.02345",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:38:59.092933+00:00"
  },
  {
    "title": "Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks",
    "summary": "arXiv:2510.02712v4 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their robustness in extended multi-turn dialogues remains poorly understood. Existing evaluation frameworks focus on static benchmarks and single-turn assessments, failing to capture the temporal dynamics of conversation",
    "url": "https://arxiv.org/abs/2510.02712",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:38:59.092933+00:00"
  },
  {
    "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
    "summary": "arXiv:2602.02437v2 Announce Type: replace-cross Abstract: Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework tha",
    "url": "https://arxiv.org/abs/2602.02437",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:38:59.092933+00:00"
  },
  {
    "title": "Knowledge Model Prompting Increases LLM Performance on Planning Tasks",
    "summary": "arXiv:2602.03900v1 Announce Type: new Abstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into quest",
    "url": "https://arxiv.org/abs/2602.03900",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-06T12:38:59.092933+00:00"
  },
  {
    "title": "Interfaze: The Future of AI is built on Task-Specific Small Models",
    "summary": "arXiv:2602.04101v1 Announce Type: new Abstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OC",
    "url": "https://arxiv.org/abs/2602.04101",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:20:47.126404+00:00"
  },
  {
    "title": "OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows",
    "summary": "arXiv:2602.04144v1 Announce Type: new Abstract: Data incompleteness severely impedes the reliability of multimodal systems. Existing reconstruction methods face distinct bottlenecks: conventional parametric/generative models are prone to hallucinations due to over-reliance on internal memory, while retrieval-augmented frameworks struggle with retri",
    "url": "https://arxiv.org/abs/2602.04144",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:20:47.126404+00:00"
  },
  {
    "title": "ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas",
    "summary": "arXiv:2602.04296v1 Announce Type: cross Abstract: Large language models (LLMs) have revolutionized automated code generation, yet the evaluation of their real-world effectiveness remains limited by static benchmarks and simplistic metrics. We present ProxyWar, a novel framework that systematically assesses code generation quality by embedding LLM-g",
    "url": "https://arxiv.org/abs/2602.04296",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:20:47.126404+00:00"
  },
  {
    "title": "History-Guided Iterative Visual Reasoning with Self-Correction",
    "summary": "arXiv:2602.04413v1 Announce Type: cross Abstract: Self-consistency methods are the core technique for improving the reasoning reliability of multimodal large language models (MLLMs). By generating multiple reasoning results through repeated sampling and selecting the best answer via voting, they play an important role in cross-modal tasks. However,",
    "url": "https://arxiv.org/abs/2602.04413",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:20:47.126404+00:00"
  },
  {
    "title": "Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases",
    "summary": "arXiv:2602.04739v1 Announce Type: cross Abstract: Multimodal large language models (MLLMs) are increasingly deployed in real-world systems, yet their safety under adversarial prompting remains underexplored. We present a two-phase evaluation of MLLM harmlessness using a fixed benchmark of 726 adversarial prompts authored by 26 professional red team",
    "url": "https://arxiv.org/abs/2602.04739",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:20:47.126404+00:00"
  },
  {
    "title": "Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing",
    "summary": "arXiv:2510.04670v3 Announce Type: replace Abstract: Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic Framework for Multimodal fMRI Response Encoding), an agnostic interface that standardizes time-aligned post-fusion tokens from varied encoders, a",
    "url": "https://arxiv.org/abs/2510.04670",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:20:47.126404+00:00"
  },
  {
    "title": "M^3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark",
    "summary": "arXiv:2511.17729v4 Announce Type: replace Abstract: We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resou",
    "url": "https://arxiv.org/abs/2511.17729",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:20:47.126404+00:00"
  },
  {
    "title": "Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving",
    "summary": "arXiv:2601.21164v2 Announce Type: replace Abstract: Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by th",
    "url": "https://arxiv.org/abs/2601.21164",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:20:47.126404+00:00"
  },
  {
    "title": "Building Interpretable Models for Moral Decision-Making",
    "summary": "arXiv:2602.03351v2 Announce Type: replace Abstract: We build a custom transformer model to study how neural networks make moral decisions on trolley-style dilemmas. The model processes structured scenarios using embeddings that encode who is affected, how many people, and which outcome they belong to. Our 2-layer architecture achieves 77% accuracy ",
    "url": "https://arxiv.org/abs/2602.03351",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:20:47.126404+00:00"
  },
  {
    "title": "Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders",
    "summary": "arXiv:2507.03262v3 Announce Type: replace-cross Abstract: Recent multimodal large language models (MLLMs) increasingly integrate multiple vision encoders to improve performance on various benchmarks, assuming that diverse pretraining objectives yield complementary visual signals. However, we show this assumption often fails in practice. Through sys",
    "url": "https://arxiv.org/abs/2507.03262",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:20:47.126404+00:00"
  },
  {
    "title": "Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy",
    "summary": "arXiv:2509.24385v2 Announce Type: replace-cross Abstract: Recent developments in Multimodal Large Language Models (MLLMs) have significantly improved Vision-Language (VL) reasoning in 2D domains. However, extending these capabilities to 3D scene understanding remains a major challenge. Existing 3D Multimodal Large Language Models (3D-MLLMs) often d",
    "url": "https://arxiv.org/abs/2509.24385",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:20:47.126404+00:00"
  },
  {
    "title": "Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression",
    "summary": "arXiv:2510.02345v3 Announce Type: replace-cross Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load imbalance, parameter redundancy, and communication overhead. We introduce a unified framework based on dynamic expert clustering and structured compression to address these issues cohesively. Our method employs an ",
    "url": "https://arxiv.org/abs/2510.02345",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:20:47.126404+00:00"
  },
  {
    "title": "Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks",
    "summary": "arXiv:2510.02712v4 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their robustness in extended multi-turn dialogues remains poorly understood. Existing evaluation frameworks focus on static benchmarks and single-turn assessments, failing to capture the temporal dynamics of conversation",
    "url": "https://arxiv.org/abs/2510.02712",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:20:47.126404+00:00"
  },
  {
    "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
    "summary": "arXiv:2602.02437v2 Announce Type: replace-cross Abstract: Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework tha",
    "url": "https://arxiv.org/abs/2602.02437",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T12:20:47.126404+00:00"
  },
  {
    "title": "Knowledge Model Prompting Increases LLM Performance on Planning Tasks",
    "summary": "arXiv:2602.03900v1 Announce Type: new Abstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into quest",
    "url": "https://arxiv.org/abs/2602.03900",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-06T12:20:47.126404+00:00"
  },
  {
    "title": "Interfaze: The Future of AI is built on Task-Specific Small Models",
    "summary": "arXiv:2602.04101v1 Announce Type: new Abstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OC",
    "url": "https://arxiv.org/abs/2602.04101",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T11:49:35.333557+00:00"
  },
  {
    "title": "OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows",
    "summary": "arXiv:2602.04144v1 Announce Type: new Abstract: Data incompleteness severely impedes the reliability of multimodal systems. Existing reconstruction methods face distinct bottlenecks: conventional parametric/generative models are prone to hallucinations due to over-reliance on internal memory, while retrieval-augmented frameworks struggle with retri",
    "url": "https://arxiv.org/abs/2602.04144",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T11:49:35.333557+00:00"
  },
  {
    "title": "ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas",
    "summary": "arXiv:2602.04296v1 Announce Type: cross Abstract: Large language models (LLMs) have revolutionized automated code generation, yet the evaluation of their real-world effectiveness remains limited by static benchmarks and simplistic metrics. We present ProxyWar, a novel framework that systematically assesses code generation quality by embedding LLM-g",
    "url": "https://arxiv.org/abs/2602.04296",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T11:49:35.333557+00:00"
  },
  {
    "title": "History-Guided Iterative Visual Reasoning with Self-Correction",
    "summary": "arXiv:2602.04413v1 Announce Type: cross Abstract: Self-consistency methods are the core technique for improving the reasoning reliability of multimodal large language models (MLLMs). By generating multiple reasoning results through repeated sampling and selecting the best answer via voting, they play an important role in cross-modal tasks. However,",
    "url": "https://arxiv.org/abs/2602.04413",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T11:49:35.333557+00:00"
  },
  {
    "title": "Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases",
    "summary": "arXiv:2602.04739v1 Announce Type: cross Abstract: Multimodal large language models (MLLMs) are increasingly deployed in real-world systems, yet their safety under adversarial prompting remains underexplored. We present a two-phase evaluation of MLLM harmlessness using a fixed benchmark of 726 adversarial prompts authored by 26 professional red team",
    "url": "https://arxiv.org/abs/2602.04739",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T11:49:35.333557+00:00"
  },
  {
    "title": "Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing",
    "summary": "arXiv:2510.04670v3 Announce Type: replace Abstract: Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic Framework for Multimodal fMRI Response Encoding), an agnostic interface that standardizes time-aligned post-fusion tokens from varied encoders, a",
    "url": "https://arxiv.org/abs/2510.04670",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T11:49:35.333557+00:00"
  },
  {
    "title": "M^3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark",
    "summary": "arXiv:2511.17729v4 Announce Type: replace Abstract: We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resou",
    "url": "https://arxiv.org/abs/2511.17729",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T11:49:35.333557+00:00"
  },
  {
    "title": "Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving",
    "summary": "arXiv:2601.21164v2 Announce Type: replace Abstract: Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by th",
    "url": "https://arxiv.org/abs/2601.21164",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T11:49:35.333557+00:00"
  },
  {
    "title": "Building Interpretable Models for Moral Decision-Making",
    "summary": "arXiv:2602.03351v2 Announce Type: replace Abstract: We build a custom transformer model to study how neural networks make moral decisions on trolley-style dilemmas. The model processes structured scenarios using embeddings that encode who is affected, how many people, and which outcome they belong to. Our 2-layer architecture achieves 77% accuracy ",
    "url": "https://arxiv.org/abs/2602.03351",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T11:49:35.333557+00:00"
  },
  {
    "title": "Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders",
    "summary": "arXiv:2507.03262v3 Announce Type: replace-cross Abstract: Recent multimodal large language models (MLLMs) increasingly integrate multiple vision encoders to improve performance on various benchmarks, assuming that diverse pretraining objectives yield complementary visual signals. However, we show this assumption often fails in practice. Through sys",
    "url": "https://arxiv.org/abs/2507.03262",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T11:49:35.333557+00:00"
  },
  {
    "title": "Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy",
    "summary": "arXiv:2509.24385v2 Announce Type: replace-cross Abstract: Recent developments in Multimodal Large Language Models (MLLMs) have significantly improved Vision-Language (VL) reasoning in 2D domains. However, extending these capabilities to 3D scene understanding remains a major challenge. Existing 3D Multimodal Large Language Models (3D-MLLMs) often d",
    "url": "https://arxiv.org/abs/2509.24385",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T11:49:35.333557+00:00"
  },
  {
    "title": "Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression",
    "summary": "arXiv:2510.02345v3 Announce Type: replace-cross Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load imbalance, parameter redundancy, and communication overhead. We introduce a unified framework based on dynamic expert clustering and structured compression to address these issues cohesively. Our method employs an ",
    "url": "https://arxiv.org/abs/2510.02345",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T11:49:35.333557+00:00"
  },
  {
    "title": "Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks",
    "summary": "arXiv:2510.02712v4 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their robustness in extended multi-turn dialogues remains poorly understood. Existing evaluation frameworks focus on static benchmarks and single-turn assessments, failing to capture the temporal dynamics of conversation",
    "url": "https://arxiv.org/abs/2510.02712",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T11:49:35.333557+00:00"
  },
  {
    "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
    "summary": "arXiv:2602.02437v2 Announce Type: replace-cross Abstract: Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework tha",
    "url": "https://arxiv.org/abs/2602.02437",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-06T11:49:35.333557+00:00"
  },
  {
    "title": "Knowledge Model Prompting Increases LLM Performance on Planning Tasks",
    "summary": "arXiv:2602.03900v1 Announce Type: new Abstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into quest",
    "url": "https://arxiv.org/abs/2602.03900",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-06T11:49:35.333557+00:00"
  }
]