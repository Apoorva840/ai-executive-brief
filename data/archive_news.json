[
  {
    "title": "DMCD: Semantic-Statistical Framework for Causal Discovery",
    "summary": "arXiv:2602.20333v1 Announce Type: new Abstract: We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed pr",
    "url": "https://arxiv.org/abs/2602.20333",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "PyVision-RL: Forging Open Agentic Vision Models via RL",
    "summary": "arXiv:2602.20739v1 Announce Type: new Abstract: Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that",
    "url": "https://arxiv.org/abs/2602.20739",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "Evaluating the Reliability of Digital Forensic Evidence Discovered by Large Language Model: A Case Study",
    "summary": "arXiv:2602.20202v1 Announce Type: cross Abstract: The growing reliance on AI-identified digital evidence raises significant concerns about its reliability, particularly as large language models (LLMs) are increasingly integrated into forensic investigations. This paper proposes a structured framework that automates forensic artifact extraction, ref",
    "url": "https://arxiv.org/abs/2602.20202",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "KnapSpec: Self-Speculative Decoding via Adaptive Layer Selection as a Knapsack Problem",
    "summary": "arXiv:2602.20217v1 Announce Type: cross Abstract: Self-speculative decoding (SSD) accelerates LLM inference by skipping layers to create an efficient draft model, yet existing methods often rely on static heuristics that ignore the dynamic computational overhead of attention in long-context scenarios. We propose KnapSpec, a training-free framework ",
    "url": "https://arxiv.org/abs/2602.20217",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "An Approach to Combining Video and Speech with Large Language Models in Human-Robot Interaction",
    "summary": "arXiv:2602.20219v1 Announce Type: cross Abstract: Interpreting human intent accurately is a central challenge in human-robot interaction (HRI) and a key requirement for achieving more natural and intuitive collaboration between humans and machines. This work presents a novel multimodal HRI framework that combines advanced vision-language models, sp",
    "url": "https://arxiv.org/abs/2602.20219",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "Circuit Tracing in Vision-Language Models: Understanding the Internal Mechanisms of Multimodal Thinking",
    "summary": "arXiv:2602.20330v1 Announce Type: cross Abstract: Vision-language models (VLMs) are powerful but remain opaque black boxes. We introduce the first framework for transparent circuit tracing in VLMs to systematically analyze multimodal reasoning. By utilizing transcoders, attribution graphs, and attention-based methods, we uncover how VLMs hierarchic",
    "url": "https://arxiv.org/abs/2602.20330",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "Protein Language Models Diverge from Natural Language: Comparative Analysis and Improved Inference",
    "summary": "arXiv:2602.20449v1 Announce Type: cross Abstract: Modern Protein Language Models (PLMs) apply transformer-based model architectures from natural language processing to biological sequences, predicting a variety of protein functions and properties. However, protein language has key differences from natural language, such as a rich functional space d",
    "url": "https://arxiv.org/abs/2602.20449",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "Communication-Inspired Tokenization for Structured Image Representations",
    "summary": "arXiv:2602.20731v1 Announce Type: cross Abstract: Discrete image tokenizers have emerged as a key component of modern vision and multimodal systems, providing a sequential interface for transformer-based architectures. However, most existing approaches remain primarily optimized for reconstruction and compression, often yielding tokens that capture",
    "url": "https://arxiv.org/abs/2602.20731",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "CrystaL: Spontaneous Emergence of Visual Latents in MLLMs",
    "summary": "arXiv:2602.20980v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable performance by integrating powerful language backbones with large-scale visual encoders. Among these, latent Chain-of-Thought (CoT) methods enable implicit reasoning in continuous hidden states, facilitating seamless vision-language i",
    "url": "https://arxiv.org/abs/2602.20980",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents",
    "summary": "arXiv:2602.12876v2 Announce Type: replace Abstract: Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remai",
    "url": "https://arxiv.org/abs/2602.12876",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models",
    "summary": "arXiv:2506.03922v2 Announce Type: replace-cross Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant potential to advance a broad range of domains. However, current benchmarks for evaluating MLLMs primarily emphasize general knowledge and vertical step-by-step reasoning typical of STEM disciplines, while overlooking the ",
    "url": "https://arxiv.org/abs/2506.03922",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "K-Function: Joint Pronunciation Transcription and Feedback for Evaluating Kids Language Function",
    "summary": "arXiv:2507.03043v3 Announce Type: replace-cross Abstract: Evaluating young children's language is challenging for automatic speech recognizers due to high-pitched voices, prolonged sounds, and limited data. We introduce K-Function, a framework that combines accurate sub-word transcription with objective, Large Language Model (LLM)-driven scoring. I",
    "url": "https://arxiv.org/abs/2507.03043",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "Characterizing State Space Model and Hybrid Language Model Performance with Long Context",
    "summary": "arXiv:2507.12442v3 Announce Type: replace-cross Abstract: Emerging applications such as AR are driving demands for machine intelligence capable of processing continuous and/or long-context inputs on local devices. However, currently dominant models based on Transformer architecture suffers from the quadratic computational and memory overhead, which",
    "url": "https://arxiv.org/abs/2507.12442",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "Silent Inconsistency in Data-Parallel Full Fine-Tuning: Diagnosing Worker-Level Optimization Misalignment",
    "summary": "arXiv:2602.14462v2 Announce Type: replace-cross Abstract: Data-parallel (DP) training with synchronous all-reduce is a dominant paradigm for full-parameter fine-tuning of large language models (LLMs). While parameter synchronization guarantees numerical equivalence of model weights after each iteration, it does not necessarily imply alignment of wo",
    "url": "https://arxiv.org/abs/2602.14462",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production",
    "summary": "arXiv:2602.20558v1 Announce Type: new Abstract: Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate ",
    "url": "https://arxiv.org/abs/2602.20558",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "DMCD: Semantic-Statistical Framework for Causal Discovery",
    "summary": "arXiv:2602.20333v1 Announce Type: new Abstract: We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed pr",
    "url": "https://arxiv.org/abs/2602.20333",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "PyVision-RL: Forging Open Agentic Vision Models via RL",
    "summary": "arXiv:2602.20739v1 Announce Type: new Abstract: Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that",
    "url": "https://arxiv.org/abs/2602.20739",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "Evaluating the Reliability of Digital Forensic Evidence Discovered by Large Language Model: A Case Study",
    "summary": "arXiv:2602.20202v1 Announce Type: cross Abstract: The growing reliance on AI-identified digital evidence raises significant concerns about its reliability, particularly as large language models (LLMs) are increasingly integrated into forensic investigations. This paper proposes a structured framework that automates forensic artifact extraction, ref",
    "url": "https://arxiv.org/abs/2602.20202",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "KnapSpec: Self-Speculative Decoding via Adaptive Layer Selection as a Knapsack Problem",
    "summary": "arXiv:2602.20217v1 Announce Type: cross Abstract: Self-speculative decoding (SSD) accelerates LLM inference by skipping layers to create an efficient draft model, yet existing methods often rely on static heuristics that ignore the dynamic computational overhead of attention in long-context scenarios. We propose KnapSpec, a training-free framework ",
    "url": "https://arxiv.org/abs/2602.20217",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "An Approach to Combining Video and Speech with Large Language Models in Human-Robot Interaction",
    "summary": "arXiv:2602.20219v1 Announce Type: cross Abstract: Interpreting human intent accurately is a central challenge in human-robot interaction (HRI) and a key requirement for achieving more natural and intuitive collaboration between humans and machines. This work presents a novel multimodal HRI framework that combines advanced vision-language models, sp",
    "url": "https://arxiv.org/abs/2602.20219",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "Circuit Tracing in Vision-Language Models: Understanding the Internal Mechanisms of Multimodal Thinking",
    "summary": "arXiv:2602.20330v1 Announce Type: cross Abstract: Vision-language models (VLMs) are powerful but remain opaque black boxes. We introduce the first framework for transparent circuit tracing in VLMs to systematically analyze multimodal reasoning. By utilizing transcoders, attribution graphs, and attention-based methods, we uncover how VLMs hierarchic",
    "url": "https://arxiv.org/abs/2602.20330",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "Protein Language Models Diverge from Natural Language: Comparative Analysis and Improved Inference",
    "summary": "arXiv:2602.20449v1 Announce Type: cross Abstract: Modern Protein Language Models (PLMs) apply transformer-based model architectures from natural language processing to biological sequences, predicting a variety of protein functions and properties. However, protein language has key differences from natural language, such as a rich functional space d",
    "url": "https://arxiv.org/abs/2602.20449",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "Communication-Inspired Tokenization for Structured Image Representations",
    "summary": "arXiv:2602.20731v1 Announce Type: cross Abstract: Discrete image tokenizers have emerged as a key component of modern vision and multimodal systems, providing a sequential interface for transformer-based architectures. However, most existing approaches remain primarily optimized for reconstruction and compression, often yielding tokens that capture",
    "url": "https://arxiv.org/abs/2602.20731",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "CrystaL: Spontaneous Emergence of Visual Latents in MLLMs",
    "summary": "arXiv:2602.20980v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable performance by integrating powerful language backbones with large-scale visual encoders. Among these, latent Chain-of-Thought (CoT) methods enable implicit reasoning in continuous hidden states, facilitating seamless vision-language i",
    "url": "https://arxiv.org/abs/2602.20980",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents",
    "summary": "arXiv:2602.12876v2 Announce Type: replace Abstract: Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remai",
    "url": "https://arxiv.org/abs/2602.12876",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models",
    "summary": "arXiv:2506.03922v2 Announce Type: replace-cross Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant potential to advance a broad range of domains. However, current benchmarks for evaluating MLLMs primarily emphasize general knowledge and vertical step-by-step reasoning typical of STEM disciplines, while overlooking the ",
    "url": "https://arxiv.org/abs/2506.03922",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "K-Function: Joint Pronunciation Transcription and Feedback for Evaluating Kids Language Function",
    "summary": "arXiv:2507.03043v3 Announce Type: replace-cross Abstract: Evaluating young children's language is challenging for automatic speech recognizers due to high-pitched voices, prolonged sounds, and limited data. We introduce K-Function, a framework that combines accurate sub-word transcription with objective, Large Language Model (LLM)-driven scoring. I",
    "url": "https://arxiv.org/abs/2507.03043",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "Characterizing State Space Model and Hybrid Language Model Performance with Long Context",
    "summary": "arXiv:2507.12442v3 Announce Type: replace-cross Abstract: Emerging applications such as AR are driving demands for machine intelligence capable of processing continuous and/or long-context inputs on local devices. However, currently dominant models based on Transformer architecture suffers from the quadratic computational and memory overhead, which",
    "url": "https://arxiv.org/abs/2507.12442",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "Silent Inconsistency in Data-Parallel Full Fine-Tuning: Diagnosing Worker-Level Optimization Misalignment",
    "summary": "arXiv:2602.14462v2 Announce Type: replace-cross Abstract: Data-parallel (DP) training with synchronous all-reduce is a dominant paradigm for full-parameter fine-tuning of large language models (LLMs). While parameter synchronization guarantees numerical equivalence of model weights after each iteration, it does not necessarily imply alignment of wo",
    "url": "https://arxiv.org/abs/2602.14462",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production",
    "summary": "arXiv:2602.20558v1 Announce Type: new Abstract: Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate ",
    "url": "https://arxiv.org/abs/2602.20558",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "Could AI Data Centers Be Moved to Outer Space?",
    "summary": "Massive data centers for generative AI are bad for the Earth. How about launching them into orbit?",
    "url": "https://www.wired.com/story/could-we-put-ai-data-centers-in-space/",
    "source": "Wired AI",
    "published_at": "2026-02-20T12:00:00+00:00",
    "score": 5,
    "archived_at": "2026-02-21T05:04:55.318982+00:00"
  },
  {
    "title": "Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation",
    "summary": "arXiv:2602.16727v1 Announce Type: new Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost l",
    "url": "https://arxiv.org/abs/2602.16727",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy",
    "summary": "arXiv:2602.17229v1 Announce Type: new Abstract: The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional acti",
    "url": "https://arxiv.org/abs/2602.17229",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
    "summary": "arXiv:2602.17560v1 Announce Type: new Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \\textit{(i)} the lack of a unified theoretical framework for g",
    "url": "https://arxiv.org/abs/2602.17560",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "MALLVI: a multi agent framework for integrated generalized robotics manipulation",
    "summary": "arXiv:2602.16898v1 Announce Type: cross Abstract: Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.We present",
    "url": "https://arxiv.org/abs/2602.16898",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "Beyond Pipelines: A Fundamental Study on the Rise of Generative-Retrieval Architectures in Web Research",
    "summary": "arXiv:2602.17450v1 Announce Type: cross Abstract: Web research and practices have evolved significantly over time, offering users diverse and accessible solutions across a wide range of tasks. While advanced concepts such as Web 4.0 have emerged from mature technologies, the introduction of large language models (LLMs) has profoundly influenced bot",
    "url": "https://arxiv.org/abs/2602.17450",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "A Scalable Framework for Evaluating Health Language Models",
    "summary": "arXiv:2503.23339v3 Announce Type: replace Abstract: Large language models (LLMs) have emerged as powerful tools for analyzing complex datasets. Recent studies demonstrate their potential to generate useful, personalized responses when provided with patient-specific health information that encompasses lifestyle, biomarkers, and context. As LLM-drive",
    "url": "https://arxiv.org/abs/2503.23339",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs",
    "summary": "arXiv:2510.09201v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have shown remarkable success, and their multimodal expansions (MLLMs) further unlock capabilities spanning images, videos, and other modalities beyond text. However, despite this shift, prompt optimization approaches, designed to reduce the burden of manual prom",
    "url": "https://arxiv.org/abs/2510.09201",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "SoK: DARPA's AI Cyber Challenge (AIxCC): Competition Design, Architectures, and Lessons Learned",
    "summary": "arXiv:2602.07666v2 Announce Type: replace-cross Abstract: DARPA's AI Cyber Challenge (AIxCC, 2023--2025) is the largest competition to date for building fully autonomous cyber reasoning systems (CRSs) that leverage recent advances in AI -- particularly large language models (LLMs) -- to discover and remediate vulnerabilities in real-world open-sour",
    "url": "https://arxiv.org/abs/2602.07666",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs",
    "summary": "arXiv:2602.08351v2 Announce Type: replace-cross Abstract: Co-optimizing data and model configurations for training LLMs presents a classic chicken-and-egg dilemma: The best training data configuration (e.g., data mixture) for a downstream task depends on the chosen model configuration (e.g., model architecture), and vice versa. However, jointly opt",
    "url": "https://arxiv.org/abs/2602.08351",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction",
    "summary": "arXiv:2602.13294v2 Announce Type: replace-cross Abstract: Evaluating whether Multimodal Large Language Models (MLLMs) genuinely reason about physical dynamics remains challenging. Most existing benchmarks rely on recognition-style protocols such as Visual Question Answering (VQA) and Violation of Expectation (VoE), which can often be answered witho",
    "url": "https://arxiv.org/abs/2602.13294",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages",
    "summary": "arXiv:2602.16832v1 Announce Type: new Abstract: Safety alignment of large language models (LLMs) is mostly evaluated in English and contract-bound, leaving multilingual vulnerabilities understudied. We introduce \\textbf{Indic Jailbreak Robustness (IJR)}, a judge-free benchmark for adversarial safety across 12 Indic and South Asian languages (2.1 Bi",
    "url": "https://arxiv.org/abs/2602.16832",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs",
    "summary": "arXiv:2602.16902v1 Announce Type: new Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability ",
    "url": "https://arxiv.org/abs/2602.16902",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "Narrow fine-tuning erodes safety alignment in vision-language agents",
    "summary": "arXiv:2602.16931v1 Announce Type: new Abstract: Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe em",
    "url": "https://arxiv.org/abs/2602.16931",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs",
    "summary": "arXiv:2602.16935v1 Announce Type: new Abstract: While Large Language Model (LLM) capabilities have scaled, safety guardrails remain largely stateless, treating multi-turn dialogues as a series of disconnected events. This lack of temporal awareness facilitates a \"Safety Gap\" where adversarial tactics, like Crescendo and ActorAttack, slowly bleed ma",
    "url": "https://arxiv.org/abs/2602.16935",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "SourceBench: Can AI Answers Reference Quality Web Sources?",
    "summary": "arXiv:2602.16942v1 Announce Type: new Abstract: Large language models (LLMs) increasingly answer queries by citing web sources, but existing evaluations emphasize answer correctness rather than evidence quality. We introduce SourceBench, a benchmark for measuring the quality of cited web sources across 100 real-world queries spanning informational,",
    "url": "https://arxiv.org/abs/2602.16942",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "Fly0: Decoupling Semantic Grounding from Geometric Planning for Zero-Shot Aerial Navigation",
    "summary": "arXiv:2602.15875v1 Announce Type: cross Abstract: Current Visual-Language Navigation (VLN) methodologies face a trade-off between semantic understanding and control precision. While Multimodal Large Language Models (MLLMs) offer superior reasoning, deploying them as low-level controllers leads to high latency, trajectory oscillations, and poor gene",
    "url": "https://arxiv.org/abs/2602.15875",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "Surrogate Modeling for Neutron Transport: A Neural Operator Approach",
    "summary": "arXiv:2602.15890v1 Announce Type: cross Abstract: This work introduces a neural operator based surrogate modeling framework for neutron transport computation. Two architectures, the Deep Operator Network (DeepONet) and the Fourier Neural Operator (FNO), were trained for fixed source problems to learn the mapping from anisotropic neutron sources, Q(",
    "url": "https://arxiv.org/abs/2602.15890",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "Doc-to-LoRA: Learning to Instantly Internalize Contexts",
    "summary": "arXiv:2602.15902v1 Announce Type: cross Abstract: Long input sequences are central to in-context learning, document understanding, and multi-step reasoning of Large Language Models (LLMs). However, the quadratic attention cost of Transformers makes inference memory-intensive and slow. While context distillation (CD) can transfer information into mo",
    "url": "https://arxiv.org/abs/2602.15902",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "EarthSpatialBench: Benchmarking Spatial Reasoning Capabilities of Multimodal LLMs on Earth Imagery",
    "summary": "arXiv:2602.15918v1 Announce Type: cross Abstract: Benchmarking spatial reasoning in multimodal large language models (MLLMs) has attracted growing interest in computer vision due to its importance for embodied AI and other agentic systems that require precise interaction with the physical world. However, spatial reasoning on Earth imagery has lagge",
    "url": "https://arxiv.org/abs/2602.15918",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models",
    "summary": "arXiv:2602.16467v1 Announce Type: cross Abstract: The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination qu",
    "url": "https://arxiv.org/abs/2602.16467",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v2 Announce Type: replace Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decisio",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "Prompt When the Animal is: Temporal Animal Behavior Grounding with Positional Recovery Training",
    "summary": "arXiv:2405.05523v2 Announce Type: replace-cross Abstract: Temporal grounding is crucial in multimodal learning, but it poses challenges when applied to animal behavior data due to the sparsity and uniform distribution of moments. To address these challenges, we propose a novel Positional Recovery Training framework (Port), which prompts the model w",
    "url": "https://arxiv.org/abs/2405.05523",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision",
    "summary": "arXiv:2508.08177v3 Announce Type: replace-cross Abstract: Accurately grounding regions of interest (ROIs) is critical for diagnosis and treatment planning in medical imaging. While multimodal large language models (MLLMs) combine visual perception with natural language, current medical-grounding pipelines still rely on supervised fine-tuning with e",
    "url": "https://arxiv.org/abs/2508.08177",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "Predicting Training Re-evaluation Curves Enables Effective Data Curriculums for LLMs",
    "summary": "arXiv:2509.25380v2 Announce Type: replace-cross Abstract: Data curriculums have become central to successful LLM training, yet principles governing optimal data placement remain unclear. We introduce the *training re-evaluation curve (TREC)*, a diagnostic that retrospectively evaluates training batches *using the final model weights*. The TREC char",
    "url": "https://arxiv.org/abs/2509.25380",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "CreativityPrism: A Holistic Evaluation Framework for Large Language Model Creativity",
    "summary": "arXiv:2510.20091v2 Announce Type: replace-cross Abstract: Creativity is often seen as a hallmark of human intelligence. While large language models (LLMs) are increasingly perceived as generating creative text, there is still no holistic and scalable framework to evaluate their creativity across diverse scenarios. Existing methods of LLM creativity",
    "url": "https://arxiv.org/abs/2510.20091",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models",
    "summary": "arXiv:2602.15689v2 Announce Type: replace-cross Abstract: Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. ",
    "url": "https://arxiv.org/abs/2602.15689",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "How Uncertain Is the Grade? A Benchmark of Uncertainty Metrics for LLM-Based Automatic Assessment",
    "summary": "arXiv:2602.16039v1 Announce Type: new Abstract: The rapid rise of large language models (LLMs) is reshaping the landscape of automatic assessment in education. While these systems demonstrate substantial advantages in adaptability to diverse question types and flexibility in output formats, they also introduce new challenges related to output uncer",
    "url": "https://arxiv.org/abs/2602.16039",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "GPSBench: Do Large Language Models Understand GPS Coordinates?",
    "summary": "arXiv:2602.16105v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly deployed in applications that interact with the physical world, such as navigation, robotics, or mapping, making robust geospatial reasoning a critical capability. Despite that, LLMs' ability to reason about GPS coordinates and real-world geography remains",
    "url": "https://arxiv.org/abs/2602.16105",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "Toward Scalable Verifiable Reward: Proxy State-Based Evaluation for Multi-turn Tool-Calling LLM Agents",
    "summary": "arXiv:2602.16246v1 Announce Type: new Abstract: Interactive large language model (LLM) agents operating via multi-turn dialogue and multi-step tool calling are increasingly used in production. Benchmarks for these agents must both reliably compare models and yield on-policy training data. Prior agentic benchmarks (e.g., tau-bench, tau2-bench, AppWo",
    "url": "https://arxiv.org/abs/2602.16246",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
    "summary": "arXiv:2602.16653v1 Announce Type: new Abstract: Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigati",
    "url": "https://arxiv.org/abs/2602.16653",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "How Vision Becomes Language: A Layer-wise Information-Theoretic Analysis of Multimodal Reasoning",
    "summary": "arXiv:2602.15580v1 Announce Type: new Abstract: When a multimodal Transformer answers a visual question, is the prediction driven by visual evidence, linguistic reasoning, or genuinely fused cross-modal computation -- and how does this structure evolve across layers? We address this question with a layer-wise framework based on Partial Information ",
    "url": "https://arxiv.org/abs/2602.15580",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "TokaMind: A Multi-Modal Transformer Foundation Model for Tokamak Plasma Dynamics",
    "summary": "arXiv:2602.15084v1 Announce Type: cross Abstract: We present TokaMind, an open-source foundation model framework for fusion plasma modeling, based on a Multi-Modal Transformer (MMT) and trained on heterogeneous tokamak diagnostics from the publicly available MAST dataset. TokaMind supports multiple data modalities (time-series, 2D profiles, and vid",
    "url": "https://arxiv.org/abs/2602.15084",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "Improving MLLMs in Embodied Exploration and Question Answering with Human-Inspired Memory Modeling",
    "summary": "arXiv:2602.15513v1 Announce Type: cross Abstract: Deploying Multimodal Large Language Models as the brain of embodied agents remains challenging, particularly under long-horizon observations and limited context budgets. Existing memory assisted methods often rely on textual summaries, which discard rich visual and spatial details and remain brittle",
    "url": "https://arxiv.org/abs/2602.15513",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models",
    "summary": "arXiv:2602.15689v1 Announce Type: cross Abstract: Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. As a res",
    "url": "https://arxiv.org/abs/2602.15689",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models",
    "summary": "arXiv:2602.15758v1 Announce Type: cross Abstract: While Multimodal Large Language Models (MLLMs) perform strongly on single-turn chart generation, their ability to support real-world exploratory data analysis remains underexplored. In practice, users iteratively refine visualizations through multi-turn interactions that require maintaining common g",
    "url": "https://arxiv.org/abs/2602.15758",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "Decision Quality Evaluation Framework at Pinterest",
    "summary": "arXiv:2602.15809v1 Announce Type: cross Abstract: Online platforms require robust systems to enforce content safety policies at scale. A critical component of these systems is the ability to evaluate the quality of moderation decisions made by both human agents and Large Language Models (LLMs). However, this evaluation is challenging due to the inh",
    "url": "https://arxiv.org/abs/2602.15809",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "Prover Agent: An Agent-Based Framework for Formal Mathematical Proofs",
    "summary": "arXiv:2506.19923v5 Announce Type: replace Abstract: We present Prover Agent, a novel AI agent for automated theorem proving that integrates large language models (LLMs) with a formal proof assistant, Lean. Prover Agent coordinates an informal reasoning LLM, a formal prover model, and feedback from Lean while also generating auxiliary lemmas. These ",
    "url": "https://arxiv.org/abs/2506.19923",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models",
    "summary": "arXiv:2508.00576v2 Announce Type: replace Abstract: Multimodal AI models have achieved impressive performance in tasks that require integrating information from multiple modalities, such as vision and language. However, their \"black-box\" nature poses a major barrier to deployment in high-stakes applications where interpretability and trustworthines",
    "url": "https://arxiv.org/abs/2508.00576",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces",
    "summary": "arXiv:2511.07587v2 Announce Type: replace Abstract: Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions, which h",
    "url": "https://arxiv.org/abs/2511.07587",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents",
    "summary": "arXiv:2601.15311v3 Announce Type: replace Abstract: Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the \"Lost in the Middle\" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily \"Flat RAG\" architectures relying on vector databas",
    "url": "https://arxiv.org/abs/2601.15311",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "Sparse Autoencoders for Sequential Recommendation Models: Interpretation and Flexible Control",
    "summary": "arXiv:2507.12202v2 Announce Type: replace-cross Abstract: Many current state-of-the-art models for sequential recommendations are based on transformer architectures. Interpretation and explanation of such black box models is an important research question, as a better understanding of their internals can help understand, influence, and control thei",
    "url": "https://arxiv.org/abs/2507.12202",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "LogiPart: Local Large Language Models for Data Exploration at Scale with Logical Partitioning",
    "summary": "arXiv:2509.22211v3 Announce Type: replace-cross Abstract: The discovery of deep, steerable taxonomies in large text corpora is currently restricted by a trade-off between the surface-level efficiency of topic models and the prohibitive, non-scalable assignment costs of LLM-integrated frameworks. We introduce \\textbf{LogiPart}, a scalable, hypothesi",
    "url": "https://arxiv.org/abs/2509.22211",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "Text-Guided Layer Fusion Mitigates Hallucination in Multimodal LLMs",
    "summary": "arXiv:2601.03100v2 Announce Type: replace-cross Abstract: Multimodal large language models (MLLMs) typically rely on a single late-layer feature from a frozen vision encoder, leaving the encoder's rich hierarchy of visual cues under-utilized. MLLMs still suffer from visually ungrounded hallucinations, often relying on language priors rather than im",
    "url": "https://arxiv.org/abs/2601.03100",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "Annotation-Efficient Vision-Language Model Adaptation to the Polish Language Using the LLaVA Framework",
    "summary": "arXiv:2602.14073v2 Announce Type: replace-cross Abstract: Most vision-language models (VLMs) are trained on English-centric data, limiting their performance in other languages and cultural contexts. This restricts their usability for non-English-speaking users and hinders the development of multimodal systems that reflect diverse linguistic and cul",
    "url": "https://arxiv.org/abs/2602.14073",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "Protecting Language Models Against Unauthorized Distillation through Trace Rewriting",
    "summary": "arXiv:2602.15143v1 Announce Type: new Abstract: Knowledge distillation is a widely adopted technique for transferring capabilities from LLMs to smaller, more efficient student models. However, unauthorized use of knowledge distillation takes unfair advantage of the considerable effort and cost put into developing frontier models. We investigate met",
    "url": "https://arxiv.org/abs/2602.15143",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
    "summary": "arXiv:2602.13226v1 Announce Type: new Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and pra",
    "url": "https://arxiv.org/abs/2602.13226",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
    "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-for",
    "url": "https://arxiv.org/abs/2602.13232",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
    "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a un",
    "url": "https://arxiv.org/abs/2602.13258",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol",
    "summary": "arXiv:2602.13320v1 Announce Type: new Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context P",
    "url": "https://arxiv.org/abs/2602.13320",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization",
    "summary": "arXiv:2602.13653v1 Announce Type: new Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for dat",
    "url": "https://arxiv.org/abs/2602.13653",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
    "summary": "arXiv:2602.13680v1 Announce Type: new Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that ",
    "url": "https://arxiv.org/abs/2602.13680",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models",
    "summary": "arXiv:2602.13904v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three disti",
    "url": "https://arxiv.org/abs/2602.13904",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v1 Announce Type: new Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision ma",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
    "summary": "arXiv:2602.13933v1 Announce Type: new Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical det",
    "url": "https://arxiv.org/abs/2602.13933",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
    "summary": "arXiv:2602.14130v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provi",
    "url": "https://arxiv.org/abs/2602.14130",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
    "summary": "arXiv:2602.14457v1 Announce Type: new Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the ",
    "url": "https://arxiv.org/abs/2602.14457",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
    "summary": "arXiv:2602.14518v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conf",
    "url": "https://arxiv.org/abs/2602.14518",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization",
    "summary": "arXiv:2602.13210v1 Announce Type: cross Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex enviro",
    "url": "https://arxiv.org/abs/2602.13210",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "Global AI Bias Audit for Technical Governance",
    "summary": "arXiv:2602.13246v1 Announce Type: cross Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical ",
    "url": "https://arxiv.org/abs/2602.13246",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs",
    "summary": "arXiv:2602.13289v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessi",
    "url": "https://arxiv.org/abs/2602.13289",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
    "summary": "arXiv:2602.13226v1 Announce Type: new Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and pra",
    "url": "https://arxiv.org/abs/2602.13226",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
    "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-for",
    "url": "https://arxiv.org/abs/2602.13232",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
    "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a un",
    "url": "https://arxiv.org/abs/2602.13258",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol",
    "summary": "arXiv:2602.13320v1 Announce Type: new Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context P",
    "url": "https://arxiv.org/abs/2602.13320",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization",
    "summary": "arXiv:2602.13653v1 Announce Type: new Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for dat",
    "url": "https://arxiv.org/abs/2602.13653",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
    "summary": "arXiv:2602.13680v1 Announce Type: new Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that ",
    "url": "https://arxiv.org/abs/2602.13680",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models",
    "summary": "arXiv:2602.13904v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three disti",
    "url": "https://arxiv.org/abs/2602.13904",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v1 Announce Type: new Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision ma",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
    "summary": "arXiv:2602.13933v1 Announce Type: new Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical det",
    "url": "https://arxiv.org/abs/2602.13933",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
    "summary": "arXiv:2602.14130v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provi",
    "url": "https://arxiv.org/abs/2602.14130",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
    "summary": "arXiv:2602.14457v1 Announce Type: new Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the ",
    "url": "https://arxiv.org/abs/2602.14457",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
    "summary": "arXiv:2602.14518v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conf",
    "url": "https://arxiv.org/abs/2602.14518",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization",
    "summary": "arXiv:2602.13210v1 Announce Type: cross Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex enviro",
    "url": "https://arxiv.org/abs/2602.13210",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Global AI Bias Audit for Technical Governance",
    "summary": "arXiv:2602.13246v1 Announce Type: cross Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical ",
    "url": "https://arxiv.org/abs/2602.13246",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs",
    "summary": "arXiv:2602.13289v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessi",
    "url": "https://arxiv.org/abs/2602.13289",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
    "summary": "arXiv:2602.13226v1 Announce Type: new Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and pra",
    "url": "https://arxiv.org/abs/2602.13226",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
    "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-for",
    "url": "https://arxiv.org/abs/2602.13232",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
    "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a un",
    "url": "https://arxiv.org/abs/2602.13258",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol",
    "summary": "arXiv:2602.13320v1 Announce Type: new Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context P",
    "url": "https://arxiv.org/abs/2602.13320",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization",
    "summary": "arXiv:2602.13653v1 Announce Type: new Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for dat",
    "url": "https://arxiv.org/abs/2602.13653",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
    "summary": "arXiv:2602.13680v1 Announce Type: new Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that ",
    "url": "https://arxiv.org/abs/2602.13680",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models",
    "summary": "arXiv:2602.13904v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three disti",
    "url": "https://arxiv.org/abs/2602.13904",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v1 Announce Type: new Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision ma",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
    "summary": "arXiv:2602.13933v1 Announce Type: new Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical det",
    "url": "https://arxiv.org/abs/2602.13933",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
    "summary": "arXiv:2602.14130v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provi",
    "url": "https://arxiv.org/abs/2602.14130",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
    "summary": "arXiv:2602.14457v1 Announce Type: new Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the ",
    "url": "https://arxiv.org/abs/2602.14457",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
    "summary": "arXiv:2602.14518v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conf",
    "url": "https://arxiv.org/abs/2602.14518",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization",
    "summary": "arXiv:2602.13210v1 Announce Type: cross Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex enviro",
    "url": "https://arxiv.org/abs/2602.13210",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Global AI Bias Audit for Technical Governance",
    "summary": "arXiv:2602.13246v1 Announce Type: cross Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical ",
    "url": "https://arxiv.org/abs/2602.13246",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs",
    "summary": "arXiv:2602.13289v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessi",
    "url": "https://arxiv.org/abs/2602.13289",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
    "summary": "arXiv:2602.13226v1 Announce Type: new Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and pra",
    "url": "https://arxiv.org/abs/2602.13226",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
    "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-for",
    "url": "https://arxiv.org/abs/2602.13232",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
    "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a un",
    "url": "https://arxiv.org/abs/2602.13258",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol",
    "summary": "arXiv:2602.13320v1 Announce Type: new Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context P",
    "url": "https://arxiv.org/abs/2602.13320",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization",
    "summary": "arXiv:2602.13653v1 Announce Type: new Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for dat",
    "url": "https://arxiv.org/abs/2602.13653",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
    "summary": "arXiv:2602.13680v1 Announce Type: new Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that ",
    "url": "https://arxiv.org/abs/2602.13680",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models",
    "summary": "arXiv:2602.13904v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three disti",
    "url": "https://arxiv.org/abs/2602.13904",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v1 Announce Type: new Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision ma",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
    "summary": "arXiv:2602.13933v1 Announce Type: new Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical det",
    "url": "https://arxiv.org/abs/2602.13933",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
    "summary": "arXiv:2602.14130v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provi",
    "url": "https://arxiv.org/abs/2602.14130",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
    "summary": "arXiv:2602.14457v1 Announce Type: new Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the ",
    "url": "https://arxiv.org/abs/2602.14457",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
    "summary": "arXiv:2602.14518v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conf",
    "url": "https://arxiv.org/abs/2602.14518",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization",
    "summary": "arXiv:2602.13210v1 Announce Type: cross Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex enviro",
    "url": "https://arxiv.org/abs/2602.13210",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Global AI Bias Audit for Technical Governance",
    "summary": "arXiv:2602.13246v1 Announce Type: cross Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical ",
    "url": "https://arxiv.org/abs/2602.13246",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs",
    "summary": "arXiv:2602.13289v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessi",
    "url": "https://arxiv.org/abs/2602.13289",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
    "summary": "arXiv:2602.13226v1 Announce Type: new Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and pra",
    "url": "https://arxiv.org/abs/2602.13226",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
    "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-for",
    "url": "https://arxiv.org/abs/2602.13232",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
    "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a un",
    "url": "https://arxiv.org/abs/2602.13258",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol",
    "summary": "arXiv:2602.13320v1 Announce Type: new Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context P",
    "url": "https://arxiv.org/abs/2602.13320",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization",
    "summary": "arXiv:2602.13653v1 Announce Type: new Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for dat",
    "url": "https://arxiv.org/abs/2602.13653",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
    "summary": "arXiv:2602.13680v1 Announce Type: new Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that ",
    "url": "https://arxiv.org/abs/2602.13680",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models",
    "summary": "arXiv:2602.13904v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three disti",
    "url": "https://arxiv.org/abs/2602.13904",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v1 Announce Type: new Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision ma",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
    "summary": "arXiv:2602.13933v1 Announce Type: new Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical det",
    "url": "https://arxiv.org/abs/2602.13933",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
    "summary": "arXiv:2602.14130v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provi",
    "url": "https://arxiv.org/abs/2602.14130",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
    "summary": "arXiv:2602.14457v1 Announce Type: new Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the ",
    "url": "https://arxiv.org/abs/2602.14457",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
    "summary": "arXiv:2602.14518v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conf",
    "url": "https://arxiv.org/abs/2602.14518",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization",
    "summary": "arXiv:2602.13210v1 Announce Type: cross Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex enviro",
    "url": "https://arxiv.org/abs/2602.13210",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Global AI Bias Audit for Technical Governance",
    "summary": "arXiv:2602.13246v1 Announce Type: cross Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical ",
    "url": "https://arxiv.org/abs/2602.13246",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs",
    "summary": "arXiv:2602.13289v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessi",
    "url": "https://arxiv.org/abs/2602.13289",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
    "summary": "arXiv:2602.13226v1 Announce Type: new Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and pra",
    "url": "https://arxiv.org/abs/2602.13226",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
    "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-for",
    "url": "https://arxiv.org/abs/2602.13232",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
    "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a un",
    "url": "https://arxiv.org/abs/2602.13258",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol",
    "summary": "arXiv:2602.13320v1 Announce Type: new Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context P",
    "url": "https://arxiv.org/abs/2602.13320",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization",
    "summary": "arXiv:2602.13653v1 Announce Type: new Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for dat",
    "url": "https://arxiv.org/abs/2602.13653",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
    "summary": "arXiv:2602.13680v1 Announce Type: new Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that ",
    "url": "https://arxiv.org/abs/2602.13680",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models",
    "summary": "arXiv:2602.13904v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three disti",
    "url": "https://arxiv.org/abs/2602.13904",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v1 Announce Type: new Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision ma",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
    "summary": "arXiv:2602.13933v1 Announce Type: new Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical det",
    "url": "https://arxiv.org/abs/2602.13933",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
    "summary": "arXiv:2602.14130v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provi",
    "url": "https://arxiv.org/abs/2602.14130",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
    "summary": "arXiv:2602.14457v1 Announce Type: new Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the ",
    "url": "https://arxiv.org/abs/2602.14457",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
    "summary": "arXiv:2602.14518v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conf",
    "url": "https://arxiv.org/abs/2602.14518",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization",
    "summary": "arXiv:2602.13210v1 Announce Type: cross Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex enviro",
    "url": "https://arxiv.org/abs/2602.13210",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Global AI Bias Audit for Technical Governance",
    "summary": "arXiv:2602.13246v1 Announce Type: cross Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical ",
    "url": "https://arxiv.org/abs/2602.13246",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs",
    "summary": "arXiv:2602.13289v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessi",
    "url": "https://arxiv.org/abs/2602.13289",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
    "summary": "arXiv:2602.13226v1 Announce Type: new Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and pra",
    "url": "https://arxiv.org/abs/2602.13226",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
    "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-for",
    "url": "https://arxiv.org/abs/2602.13232",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
    "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a un",
    "url": "https://arxiv.org/abs/2602.13258",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol",
    "summary": "arXiv:2602.13320v1 Announce Type: new Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context P",
    "url": "https://arxiv.org/abs/2602.13320",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization",
    "summary": "arXiv:2602.13653v1 Announce Type: new Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for dat",
    "url": "https://arxiv.org/abs/2602.13653",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
    "summary": "arXiv:2602.13680v1 Announce Type: new Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that ",
    "url": "https://arxiv.org/abs/2602.13680",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models",
    "summary": "arXiv:2602.13904v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three disti",
    "url": "https://arxiv.org/abs/2602.13904",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v1 Announce Type: new Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision ma",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
    "summary": "arXiv:2602.13933v1 Announce Type: new Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical det",
    "url": "https://arxiv.org/abs/2602.13933",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
    "summary": "arXiv:2602.14130v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provi",
    "url": "https://arxiv.org/abs/2602.14130",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
    "summary": "arXiv:2602.14457v1 Announce Type: new Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the ",
    "url": "https://arxiv.org/abs/2602.14457",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
    "summary": "arXiv:2602.14518v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conf",
    "url": "https://arxiv.org/abs/2602.14518",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization",
    "summary": "arXiv:2602.13210v1 Announce Type: cross Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex enviro",
    "url": "https://arxiv.org/abs/2602.13210",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Global AI Bias Audit for Technical Governance",
    "summary": "arXiv:2602.13246v1 Announce Type: cross Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical ",
    "url": "https://arxiv.org/abs/2602.13246",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs",
    "summary": "arXiv:2602.13289v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessi",
    "url": "https://arxiv.org/abs/2602.13289",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "TriGen: NPU Architecture for End-to-End Acceleration of Large Language Models based on SW-HW Co-Design",
    "summary": "arXiv:2602.12962v1 Announce Type: cross Abstract: Recent studies have extensively explored NPU architectures for accelerating AI inference in on-device environments, which are inherently resource-constrained. Meanwhile, transformer-based large language models (LLMs) have become dominant, with rapidly increasing model sizes but low degree of paramet",
    "url": "https://arxiv.org/abs/2602.12962",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Intent-Driven Smart Manufacturing Integrating Knowledge Graphs and Large Language Models",
    "summary": "arXiv:2602.12419v1 Announce Type: new Abstract: The increasing complexity of smart manufacturing environments demands interfaces that can translate high-level human intents into machine-executable actions. This paper presents a unified framework that integrates instruction-tuned Large Language Models (LLMs) with ontology-aligned Knowledge Graphs (K",
    "url": "https://arxiv.org/abs/2602.12419",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents",
    "summary": "arXiv:2602.12876v1 Announce Type: new Abstract: Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain li",
    "url": "https://arxiv.org/abs/2602.12876",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "A Lightweight LLM Framework for Disaster Humanitarian Information Classification",
    "summary": "arXiv:2602.12284v1 Announce Type: cross Abstract: Timely classification of humanitarian information from social media is critical for effective disaster response. However, deploying large language models (LLMs) for this task faces challenges in resource-constrained emergency settings. This paper develops a lightweight, cost-effective framework for ",
    "url": "https://arxiv.org/abs/2602.12284",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "RankLLM: Weighted Ranking of LLMs by Quantifying Question Difficulty",
    "summary": "arXiv:2602.12424v1 Announce Type: cross Abstract: Benchmarks establish a standardized evaluation framework to systematically assess the performance of large language models (LLMs), facilitating objective comparisons and driving advancements in the field. However, existing benchmarks fail to differentiate question difficulty, limiting their ability ",
    "url": "https://arxiv.org/abs/2602.12424",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Self-EvolveRec: Self-Evolving Recommender Systems with LLM-based Directional Feedback",
    "summary": "arXiv:2602.12612v1 Announce Type: cross Abstract: Traditional methods for automating recommender system design, such as Neural Architecture Search (NAS), are often constrained by a fixed search space defined by human priors, limiting innovation to pre-defined operators. While recent LLM-driven code evolution frameworks shift fixed search space targ",
    "url": "https://arxiv.org/abs/2602.12612",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Vision Token Reduction via Attention-Driven Self-Compression for Efficient Multimodal Large Language Models",
    "summary": "arXiv:2602.12618v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) incur significant computational cost from processing numerous vision tokens through all LLM layers. Prior pruning methods operate either before the LLM, limiting generality due to diverse encoder-projector designs or within the LLM using heuristics that are i",
    "url": "https://arxiv.org/abs/2602.12618",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Artic: AI-oriented Real-time Communication for MLLM Video Assistant",
    "summary": "arXiv:2602.12641v1 Announce Type: cross Abstract: AI Video Assistant emerges as a new paradigm for Real-time Communication (RTC), where one peer is a Multimodal Large Language Model (MLLM) deployed in the cloud. This makes interaction between humans and AI more intuitive, akin to chatting with a real person. However, a fundamental mismatch exists b",
    "url": "https://arxiv.org/abs/2602.12641",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "RADAR: Revealing Asymmetric Development of Abilities in MLLM Pre-training",
    "summary": "arXiv:2602.12892v1 Announce Type: cross Abstract: Pre-trained Multi-modal Large Language Models (MLLMs) provide a knowledge-rich foundation for post-training by leveraging their inherent perception and reasoning capabilities to solve complex tasks. However, the lack of an efficient evaluation framework impedes the diagnosis of their performance bot",
    "url": "https://arxiv.org/abs/2602.12892",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Know More, Know Clearer: A Meta-Cognitive Framework for Knowledge Augmentation in Large Language Models",
    "summary": "arXiv:2602.12996v1 Announce Type: cross Abstract: Knowledge augmentation has significantly enhanced the performance of Large Language Models (LLMs) in knowledge-intensive tasks. However, existing methods typically operate on the simplistic premise that model performance equates with internal knowledge, overlooking the knowledge-confidence gaps that",
    "url": "https://arxiv.org/abs/2602.12996",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Buy versus Build an LLM: A Decision Framework for Governments",
    "summary": "arXiv:2602.13033v1 Announce Type: cross Abstract: Large Language Models (LLMs) represent a new frontier of digital infrastructure that can support a wide range of public-sector applications, from general purpose citizen services to specialized and sensitive state functions. When expanding AI access, governments face a set of strategic choices over ",
    "url": "https://arxiv.org/abs/2602.13033",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Asynchronous Verified Semantic Caching for Tiered LLM Architectures",
    "summary": "arXiv:2602.13165v1 Announce Type: cross Abstract: Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential for reducing inference cost and latency. Production deployments typically use a tiered static-dynamic design: a static cache of curated, offline vetted responses ",
    "url": "https://arxiv.org/abs/2602.13165",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Difficulty-Aware Agentic Orchestration for Query-Specific Multi-Agent Workflows",
    "summary": "arXiv:2509.11079v5 Announce Type: replace Abstract: Large Language Model (LLM)-based agentic systems have shown strong capabilities across various tasks. However, existing multi-agent frameworks often rely on static or task-level workflows, which either over-process simple queries or underperform on complex ones, while also neglecting the efficienc",
    "url": "https://arxiv.org/abs/2509.11079",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "LTSM-Bundle: A Toolbox and Benchmark on Large Language Models for Time Series Forecasting",
    "summary": "arXiv:2406.14045v3 Announce Type: replace-cross Abstract: Time Series Forecasting (TSF) has long been a challenge in time series analysis. Inspired by the success of Large Language Models (LLMs), researchers are now developing Large Time Series Models (LTSMs)-universal transformer-based models that use autoregressive prediction-to improve TSF. Howe",
    "url": "https://arxiv.org/abs/2406.14045",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders",
    "summary": "arXiv:2507.03262v4 Announce Type: replace-cross Abstract: Recent multimodal large language models (MLLMs) increasingly integrate multiple vision encoders to improve performance on various benchmarks, assuming that diverse pretraining objectives yield complementary visual signals. However, we show this assumption often fails in practice. Through sys",
    "url": "https://arxiv.org/abs/2507.03262",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling",
    "summary": "arXiv:2602.11761v1 Announce Type: cross Abstract: The evolution of large language models (LLMs) towards applications with ultra-long contexts faces challenges posed by the high computational and memory costs of the Transformer architecture. While existing sparse and linear attention mechanisms attempt to mitigate these issues, they typically involv",
    "url": "https://arxiv.org/abs/2602.11761",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Bi-Level Prompt Optimization for Multimodal LLM-as-a-Judge",
    "summary": "arXiv:2602.11340v1 Announce Type: new Abstract: Large language models (LLMs) have become widely adopted as automated judges for evaluating AI-generated content. Despite their success, aligning LLM-based evaluations with human judgments remains challenging. While supervised fine-tuning on human-labeled data can improve alignment, it is costly and in",
    "url": "https://arxiv.org/abs/2602.11340",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Credit Where It is Due: Cross-Modality Connectivity Drives Precise Reinforcement Learning for MLLM Reasoning",
    "summary": "arXiv:2602.11455v1 Announce Type: new Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), yet how visual evidence is integrated during reasoning remains poorly understood. We explore multimodal RLVR through the lens of cross-modal attentio",
    "url": "https://arxiv.org/abs/2602.11455",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery",
    "summary": "arXiv:2602.11609v1 Announce Type: new Abstract: We present scPilot, the first systematic framework to practice omics-native reasoning: a large language model (LLM) converses in natural language while directly inspecting single-cell RNA-seq data and on-demand bioinformatics tools. scPilot converts core single-cell analyses, i.e., cell-type annotatio",
    "url": "https://arxiv.org/abs/2602.11609",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Do MLLMs Really Understand Space? A Mathematical Reasoning Evaluation",
    "summary": "arXiv:2602.11635v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) have achieved strong performance on perception-oriented tasks, yet their ability to perform mathematical spatial reasoning, defined as the capacity to parse and manipulate two- and three-dimensional relations, remains unclear. Humans easily solve textbook-style",
    "url": "https://arxiv.org/abs/2602.11635",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "PhyNiKCE: A Neurosymbolic Agentic Framework for Autonomous Computational Fluid Dynamics",
    "summary": "arXiv:2602.11666v1 Announce Type: new Abstract: The deployment of autonomous agents for Computational Fluid Dynamics (CFD), is critically limited by the probabilistic nature of Large Language Models (LLMs), which struggle to enforce the strict conservation laws and numerical stability required for physics-based simulations. Reliance on purely seman",
    "url": "https://arxiv.org/abs/2602.11666",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Benchmark Health Index: A Systematic Framework for Benchmarking the Benchmarks of LLMs",
    "summary": "arXiv:2602.11674v1 Announce Type: new Abstract: Large Language Models (LLMs) are advancing rapidly, yet the benchmarks used to measure this progress are becoming increasingly unreliable. Score inflation and selective reporting have eroded the authority of standard benchmarks, leaving the community uncertain about which evaluation results remain tru",
    "url": "https://arxiv.org/abs/2602.11674",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Beyond Pixels: Vector-to-Graph Transformation for Reliable Schematic Auditing",
    "summary": "arXiv:2602.11678v1 Announce Type: new Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable progress in visual understanding, yet they suffer from a critical limitation: structural blindness. Even state-of-the-art models fail to capture topology and symbolic logic in engineering schematics, as their pixel-driven paradigm discards",
    "url": "https://arxiv.org/abs/2602.11678",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs",
    "summary": "arXiv:2602.11729v1 Announce Type: new Abstract: Model diffing, the process of comparing models' internal representations to identify their differences, is a promising approach for uncovering safety-critical behaviors in new models. However, its application has so far been primarily focused on comparing a base model with its finetune. Since new LLM ",
    "url": "https://arxiv.org/abs/2602.11729",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Prototype Transformer: Towards Language Model Architectures Interpretable by Design",
    "summary": "arXiv:2602.11852v1 Announce Type: new Abstract: While state-of-the-art language models (LMs) surpass the vast majority of humans in certain domains, their reasoning remains largely opaque, undermining trust in their output. Furthermore, while autoregressive LMs can output explicit reasoning, their true reasoning process is opaque, which introduces ",
    "url": "https://arxiv.org/abs/2602.11852",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "HybridRAG: A Practical LLM-based ChatBot Framework based on Pre-Generated Q&A over Raw Unstructured Documents",
    "summary": "arXiv:2602.11156v1 Announce Type: cross Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for grounding Large Language Model (LLM)-based chatbot responses on external knowledge. However, existing RAG studies typically assume well-structured textual sources (e.g. Wikipedia or curated datasets) and perform retrieval an",
    "url": "https://arxiv.org/abs/2602.11156",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "The Energy of Falsehood: Detecting Hallucinations via Diffusion Model Likelihoods",
    "summary": "arXiv:2602.11364v1 Announce Type: cross Abstract: Large Language Models (LLMs) frequently hallucinate plausible but incorrect assertions, a vulnerability often missed by uncertainty metrics when models are confidently wrong. We propose DiffuTruth, an unsupervised framework that reconceptualizes fact verification via non equilibrium thermodynamics, ",
    "url": "https://arxiv.org/abs/2602.11364",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Multimodal Fact-Level Attribution for Verifiable Reasoning",
    "summary": "arXiv:2602.11509v1 Announce Type: cross Abstract: Multimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual factual claims. However, existing multimodal groundi",
    "url": "https://arxiv.org/abs/2602.11509",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception",
    "summary": "arXiv:2602.11858v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception, where decisive evidence is small and easily overwhelmed by global context. Recent \"Thinking-with-Images\" methods alleviate this by iteratively zooming in and out regions of i",
    "url": "https://arxiv.org/abs/2602.11858",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "DeepSight: An All-in-One LM Safety Toolkit",
    "summary": "arXiv:2602.12092v1 Announce Type: cross Abstract: As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluatio",
    "url": "https://arxiv.org/abs/2602.12092",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle",
    "summary": "arXiv:2508.05612v4 Announce Type: replace-cross Abstract: Reinforcement learning (RL) has emerged as an effective post-training paradigm for enhancing the reasoning capabilities of multimodal large language model (MLLM). However, current RL pipelines often suffer from training inefficiencies caused by two underexplored issues: Advantage Collapsing,",
    "url": "https://arxiv.org/abs/2508.05612",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Discovering Differences in Strategic Behavior Between Humans and LLMs",
    "summary": "arXiv:2602.10324v1 Announce Type: new Abstract: As Large Language Models (LLMs) are increasingly deployed in social and strategic scenarios, it becomes critical to understand where and why their behavior diverges from that of humans. While behavioral game theory (BGT) provides a framework for analyzing behavior, existing models do not fully capture",
    "url": "https://arxiv.org/abs/2602.10324",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "AgentTrace: A Structured Logging Framework for Agent System Observability",
    "summary": "arXiv:2602.10133v1 Announce Type: cross Abstract: Despite the growing capabilities of autonomous agents powered by large language models (LLMs), their adoption in high-stakes domains remains limited. A key barrier is security: the inherently nondeterministic behavior of LLM agents defies static auditing approaches that have historically underpinned",
    "url": "https://arxiv.org/abs/2602.10133",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Anonymization-Enhanced Privacy Protection for Mobile GUI Agents: Available but Invisible",
    "summary": "arXiv:2602.10139v1 Announce Type: cross Abstract: Mobile Graphical User Interface (GUI) agents have demonstrated strong capabilities in automating complex smartphone tasks by leveraging multimodal large language models (MLLMs) and system-level control interfaces. However, this paradigm introduces significant privacy risks, as agents typically captu",
    "url": "https://arxiv.org/abs/2602.10139",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "PRISM-XR: Empowering Privacy-Aware XR Collaboration with Multimodal Large Language Models",
    "summary": "arXiv:2602.10154v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) enhance collaboration in Extended Reality (XR) environments by enabling flexible object and animation creation through the combination of natural language and visual inputs. However, visual data captured by XR headsets includes real-world backgrounds that may",
    "url": "https://arxiv.org/abs/2602.10154",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Omni-Safety under Cross-Modality Conflict: Vulnerabilities, Dynamics Mechanisms and Efficient Alignment",
    "summary": "arXiv:2602.10161v1 Announce Type: cross Abstract: Omni-modal Large Language Models (OLLMs) greatly expand LLMs' multimodal capabilities but also introduce cross-modal safety risks. However, a systematic understanding of vulnerabilities in omni-modal interactions remains lacking. To bridge this gap, we establish a modality-semantics decoupling princ",
    "url": "https://arxiv.org/abs/2602.10161",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Self-Evolving Recommendation System: End-To-End Autonomous Model Optimization With LLM Agents",
    "summary": "arXiv:2602.10226v1 Announce Type: cross Abstract: Optimizing large-scale machine learning systems, such as recommendation models for global video platforms, requires navigating a massive hyperparameter search space and, more critically, designing sophisticated optimizers, architectures, and reward functions to capture nuanced user behaviors. Achiev",
    "url": "https://arxiv.org/abs/2602.10226",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "C^2ROPE: Causal Continuous Rotary Positional Encoding for 3D Large Multimodal-Models Reasoning",
    "summary": "arXiv:2602.10551v1 Announce Type: cross Abstract: Recent advances in 3D Large Multimodal Models (LMMs) built on Large Language Models (LLMs) have established the alignment of 3D visual features with LLM representations as the dominant paradigm. However, the inherited Rotary Position Embedding (RoPE) introduces limitations for multimodal processing.",
    "url": "https://arxiv.org/abs/2602.10551",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Contrastive Learning for Multi Label ECG Classification with Jaccard Score Based Sigmoid Loss",
    "summary": "arXiv:2602.10553v1 Announce Type: cross Abstract: Recent advances in large language models (LLMs) have enabled the development of multimodal medical AI. While models such as MedGemini achieve high accuracy on VQA tasks like USMLE MM, their performance on ECG based tasks remains limited, and some models, such as MedGemma, do not support ECG data at ",
    "url": "https://arxiv.org/abs/2602.10553",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning",
    "summary": "arXiv:2602.10575v1 Announce Type: cross Abstract: Metaphorical comprehension in images remains a critical challenge for Nowadays AI systems. While Multimodal Large Language Models (MLLMs) excel at basic Visual Question Answering (VQA), they consistently struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visua",
    "url": "https://arxiv.org/abs/2602.10575",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization",
    "summary": "arXiv:2602.10576v1 Announce Type: cross Abstract: Symbolic regression aims to distill mathematical equations from observational data. Recent approaches have successfully leveraged Large Language Models (LLMs) to generate equation hypotheses, capitalizing on their vast pre-trained scientific priors. However, existing frameworks predominantly treat t",
    "url": "https://arxiv.org/abs/2602.10576",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Flow caching for autoregressive video generation",
    "summary": "arXiv:2602.10825v1 Announce Type: cross Abstract: Autoregressive models, often built on Transformer architectures, represent a powerful paradigm for generating ultra-long videos by synthesizing content in sequential chunks. However, this sequential generation process is notoriously slow. While caching strategies have proven effective for accelerati",
    "url": "https://arxiv.org/abs/2602.10825",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Fine-Tuning GPT-5 for GPU Kernel Generation",
    "summary": "arXiv:2602.11000v1 Announce Type: cross Abstract: Developing efficient GPU kernels is essential for scaling modern AI systems, yet it remains a complex task due to intricate hardware architectures and the need for specialized optimization expertise. Although Large Language Models (LLMs) demonstrate strong capabilities in general sequential code gen",
    "url": "https://arxiv.org/abs/2602.11000",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Synthetic Homes: An Accessible Multimodal Pipeline for Producing Residential Building Data with Generative AI",
    "summary": "arXiv:2509.09794v2 Announce Type: replace Abstract: Computational models have emerged as powerful tools for energy modeling research, touting scalability and quantitative results. However, these models require a plethora of data, some of which can be inaccessible, expensive, or can raise privacy concerns. We introduce a modular multimodal framework",
    "url": "https://arxiv.org/abs/2509.09794",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility",
    "summary": "arXiv:2602.03402v2 Announce Type: replace Abstract: Vision language models (VLMs) extend the reasoning capabilities of large language models (LLMs) to cross-modal settings, yet remain highly vulnerable to multimodal jailbreak attacks. Existing defenses predominantly rely on safety fine-tuning or aggressive token manipulations, incurring substantial",
    "url": "https://arxiv.org/abs/2602.03402",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "TernaryLM: Memory-Efficient Language Modeling via Native 1-Bit Quantization with Adaptive Layer-wise Scaling",
    "summary": "arXiv:2602.07374v1 Announce Type: cross Abstract: Large language models (LLMs) achieve remarkable performance but demand substantial computational resources, limiting deployment on edge devices and resource-constrained environments. We present TernaryLM, a 132M parameter transformer architecture that employs native 1-bit ternary quantization {-1, 0",
    "url": "https://arxiv.org/abs/2602.07374",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs",
    "summary": "arXiv:2602.08621v1 Announce Type: cross Abstract: By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior w",
    "url": "https://arxiv.org/abs/2602.08621",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning",
    "summary": "arXiv:2602.07543v2 Announce Type: new Abstract: Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based ",
    "url": "https://arxiv.org/abs/2602.07543",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Towards Adaptive, Scalable, and Robust Coordination of LLM Agents: A Dynamic Ad-Hoc Networking Perspective",
    "summary": "arXiv:2602.08009v1 Announce Type: new Abstract: Multi-agent architectures built on large language models (LLMs) have demonstrated the potential to realize swarm intelligence through well-crafted collaboration. However, the substantial burden of manual orchestration inherently raises an imperative to automate the design of agentic workflows. We fram",
    "url": "https://arxiv.org/abs/2602.08009",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation",
    "summary": "arXiv:2602.08229v1 Announce Type: new Abstract: The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation ",
    "url": "https://arxiv.org/abs/2602.08229",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs",
    "summary": "arXiv:2602.08241v1 Announce Type: new Abstract: While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis show",
    "url": "https://arxiv.org/abs/2602.08241",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis",
    "summary": "arXiv:2602.08276v1 Announce Type: new Abstract: Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concep",
    "url": "https://arxiv.org/abs/2602.08276",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI",
    "summary": "arXiv:2602.08373v1 Announce Type: new Abstract: Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repa",
    "url": "https://arxiv.org/abs/2602.08373",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "PRISM: A Principled Framework for Multi-Agent Reasoning via Gain Decomposition",
    "summary": "arXiv:2602.08586v2 Announce Type: new Abstract: Multi-agent collaboration has emerged as a promising paradigm for enhancing reasoning capabilities of Large Language Models (LLMs). However, existing approaches remain largely heuristic, lacking principled guidance on what drives performance gains and how to systematically optimize multi-agent reasoni",
    "url": "https://arxiv.org/abs/2602.08586",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture",
    "summary": "arXiv:2602.08597v1 Announce Type: new Abstract: Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal i",
    "url": "https://arxiv.org/abs/2602.08597",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "iGRPO: Self-Feedback-Driven LLM Reasoning",
    "summary": "arXiv:2602.09000v1 Announce Type: new Abstract: Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability",
    "url": "https://arxiv.org/abs/2602.09000",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing",
    "summary": "arXiv:2602.07045v1 Announce Type: cross Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for co",
    "url": "https://arxiv.org/abs/2602.07045",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Extended to Reality: Prompt Injection in 3D Environments",
    "summary": "arXiv:2602.07104v1 Announce Type: cross Abstract: Multimodal large language models (MLLMs) have advanced the capabilities to interpret and act on visual input in 3D environments, empowering diverse applications such as robotics and situated conversational agents. When MLLMs reason over camera-captured views of the physical world, a new attack surfa",
    "url": "https://arxiv.org/abs/2602.07104",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models",
    "summary": "arXiv:2602.07106v1 Announce Type: cross Abstract: Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-",
    "url": "https://arxiv.org/abs/2602.07106",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Multimodal Enhancement of Sequential Recommendation",
    "summary": "arXiv:2602.07207v1 Announce Type: cross Abstract: We propose a novel recommender framework, MuSTRec (Multimodal and Sequential Transformer-based Recommendation), that unifies multimodal and sequential recommendation paradigms. MuSTRec captures cross-item similarities and collaborative filtering signals, by building item-item graphs from extracted t",
    "url": "https://arxiv.org/abs/2602.07207",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "TernaryLM: Memory-Efficient Language Modeling via Native 1-Bit Quantization with Adaptive Layer-wise Scaling",
    "summary": "arXiv:2602.07374v1 Announce Type: cross Abstract: Large language models (LLMs) achieve remarkable performance but demand substantial computational resources, limiting deployment on edge devices and resource-constrained environments. We present TernaryLM, a 132M parameter transformer architecture that employs native 1-bit ternary quantization {-1, 0",
    "url": "https://arxiv.org/abs/2602.07374",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs",
    "summary": "arXiv:2602.08621v1 Announce Type: cross Abstract: By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior w",
    "url": "https://arxiv.org/abs/2602.08621",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning",
    "summary": "arXiv:2602.07543v1 Announce Type: new Abstract: Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based ",
    "url": "https://arxiv.org/abs/2602.07543",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "Towards Adaptive, Scalable, and Robust Coordination of LLM Agents: A Dynamic Ad-Hoc Networking Perspective",
    "summary": "arXiv:2602.08009v1 Announce Type: new Abstract: Multi-agent architectures built on large language models (LLMs) have demonstrated the potential to realize swarm intelligence through well-crafted collaboration. However, the substantial burden of manual orchestration inherently raises an imperative to automate the design of agentic workflows. We fram",
    "url": "https://arxiv.org/abs/2602.08009",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation",
    "summary": "arXiv:2602.08229v1 Announce Type: new Abstract: The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation ",
    "url": "https://arxiv.org/abs/2602.08229",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs",
    "summary": "arXiv:2602.08241v1 Announce Type: new Abstract: While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis show",
    "url": "https://arxiv.org/abs/2602.08241",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis",
    "summary": "arXiv:2602.08276v1 Announce Type: new Abstract: Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concep",
    "url": "https://arxiv.org/abs/2602.08276",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI",
    "summary": "arXiv:2602.08373v1 Announce Type: new Abstract: Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repa",
    "url": "https://arxiv.org/abs/2602.08373",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "PRISM: A Principled Framework for Multi-Agent Reasoning via Gain Decomposition",
    "summary": "arXiv:2602.08586v1 Announce Type: new Abstract: Multi-agent collaboration has emerged as a promising paradigm for enhancing reasoning capabilities of Large Language Models (LLMs). However, existing approaches remain largely heuristic, lacking principled guidance on what drives performance gains and how to systematically optimize multi-agent reasoni",
    "url": "https://arxiv.org/abs/2602.08586",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture",
    "summary": "arXiv:2602.08597v1 Announce Type: new Abstract: Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal i",
    "url": "https://arxiv.org/abs/2602.08597",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "iGRPO: Self-Feedback-Driven LLM Reasoning",
    "summary": "arXiv:2602.09000v1 Announce Type: new Abstract: Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability",
    "url": "https://arxiv.org/abs/2602.09000",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing",
    "summary": "arXiv:2602.07045v1 Announce Type: cross Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for co",
    "url": "https://arxiv.org/abs/2602.07045",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "Extended to Reality: Prompt Injection in 3D Environments",
    "summary": "arXiv:2602.07104v1 Announce Type: cross Abstract: Multimodal large language models (MLLMs) have advanced the capabilities to interpret and act on visual input in 3D environments, empowering diverse applications such as robotics and situated conversational agents. When MLLMs reason over camera-captured views of the physical world, a new attack surfa",
    "url": "https://arxiv.org/abs/2602.07104",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models",
    "summary": "arXiv:2602.07106v1 Announce Type: cross Abstract: Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-",
    "url": "https://arxiv.org/abs/2602.07106",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "Multimodal Enhancement of Sequential Recommendation",
    "summary": "arXiv:2602.07207v1 Announce Type: cross Abstract: We propose a novel recommender framework, MuSTRec (Multimodal and Sequential Transformer-based Recommendation), that unifies multimodal and sequential recommendation paradigms. MuSTRec captures cross-item similarities and collaborative filtering signals, by building item-item graphs from extracted t",
    "url": "https://arxiv.org/abs/2602.07207",
    "source": "Arxiv AI",
    "published_at": "2026-02-10T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-11T03:33:27.528076+00:00"
  },
  {
    "title": "Trifuse: Enhancing Attention-Based GUI Grounding via Multimodal Fusion",
    "summary": "arXiv:2602.06351v1 Announce Type: new Abstract: GUI grounding maps natural language instructions to the correct interface elements, serving as the perception foundation for GUI agents. Existing approaches predominantly rely on fine-tuning multimodal large language models (MLLMs) using large-scale GUI datasets to predict target element coordinates, ",
    "url": "https://arxiv.org/abs/2602.06351",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "Towards Understanding What State Space Models Learn About Code",
    "summary": "arXiv:2602.06774v1 Announce Type: new Abstract: State Space Models (SSMs) have emerged as an efficient alternative to the transformer architecture. Recent studies show that SSMs can match or surpass Transformers on code understanding tasks, such as code retrieval, when trained under similar conditions. However, their internal mechanisms remain a bl",
    "url": "https://arxiv.org/abs/2602.06774",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "LLM Active Alignment: A Nash Equilibrium Perspective",
    "summary": "arXiv:2602.06836v1 Announce Type: new Abstract: We develop a game-theoretic framework for predicting and steering the behavior of populations of large language models (LLMs) through Nash equilibrium (NE) analysis. To avoid the intractability of equilibrium computation in open-ended text spaces, we model each agent's action as a mixture over human s",
    "url": "https://arxiv.org/abs/2602.06836",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "Analyzing Diffusion and Autoregressive Vision Language Models in Multimodal Embedding Space",
    "summary": "arXiv:2602.06056v1 Announce Type: cross Abstract: Embedding models are a fundamental component of modern AI systems such as semantic search and retrieval-augmented generation. Recent advances in large foundation models have substantially accelerated the development of embedding models, including those based on Large Language Models (LLMs), Vision L",
    "url": "https://arxiv.org/abs/2602.06056",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "DAVE: Distribution-aware Attribution via ViT Gradient Decomposition",
    "summary": "arXiv:2602.06613v1 Announce Type: cross Abstract: Vision Transformers (ViTs) have become a dominant architecture in computer vision, yet producing stable and high-resolution attribution maps for these models remains challenging. Architectural components such as patch embeddings and attention routing often introduce structured artifacts in pixel-lev",
    "url": "https://arxiv.org/abs/2602.06613",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "Multimodal Generative Retrieval Model with Staged Pretraining for Food Delivery on Meituan",
    "summary": "arXiv:2602.06654v1 Announce Type: cross Abstract: Multimodal retrieval models are becoming increasingly important in scenarios such as food delivery, where rich multimodal features can meet diverse user needs and enable precise retrieval. Mainstream approaches typically employ a dual-tower architecture between queries and items, and perform joint o",
    "url": "https://arxiv.org/abs/2602.06654",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "Bridging 6G IoT and AI: LLM-Based Efficient Approach for Physical Layer's Optimization Tasks",
    "summary": "arXiv:2602.06819v1 Announce Type: cross Abstract: This paper investigates the role of large language models (LLMs) in sixth-generation (6G) Internet of Things (IoT) networks and proposes a prompt-engineering-based real-time feedback and verification (PE-RTFV) framework that perform physical-layer's optimization tasks through an iteratively process.",
    "url": "https://arxiv.org/abs/2602.06819",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "The Quantum Sieve Tracer: A Hybrid Framework for Layer-Wise Activation Tracing in Large Language Models",
    "summary": "arXiv:2602.06852v1 Announce Type: cross Abstract: Mechanistic interpretability aims to reverse-engineer the internal computations of Large Language Models (LLMs), yet separating sparse semantic signals from high-dimensional polysemantic noise remains a significant challenge. This paper introduces the Quantum Sieve Tracer, a hybrid quantum-classical",
    "url": "https://arxiv.org/abs/2602.06852",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code",
    "summary": "arXiv:2602.06875v1 Announce Type: cross Abstract: Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, wi",
    "url": "https://arxiv.org/abs/2602.06875",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "From Kepler to Newton: Inductive Biases Guide Learned World Models in Transformers",
    "summary": "arXiv:2602.06923v1 Announce Type: cross Abstract: Can general-purpose AI architectures go beyond prediction to discover the physical laws governing the universe? True intelligence relies on \"world models\" -- causal abstractions that allow an agent to not only predict future states but understand the underlying governing dynamics. While previous \"AI",
    "url": "https://arxiv.org/abs/2602.06923",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search",
    "summary": "arXiv:2602.05014v2 Announce Type: replace Abstract: With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search framework",
    "url": "https://arxiv.org/abs/2602.05014",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "AgentXRay: White-Boxing Agentic Systems via Workflow Reconstruction",
    "summary": "arXiv:2602.05353v2 Announce Type: replace Abstract: Large Language Models have shown strong capabilities in complex problem solving, yet many agentic systems remain difficult to interpret and control due to opaque internal workflows. While some frameworks offer explicit architectures for collaboration, many deployed agentic systems operate as black",
    "url": "https://arxiv.org/abs/2602.05353",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "Echo State Transformer: Attention Over Finite Memories",
    "summary": "arXiv:2507.02917v3 Announce Type: replace-cross Abstract: While Large Language Models and their underlying Transformer architecture are remarkably efficient, they do not reflect how our brain processes and learns a diversity of cognitive tasks such as language, nor how it leverages working memory. Furthermore, Transformers encounters a computationa",
    "url": "https://arxiv.org/abs/2507.02917",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation",
    "summary": "arXiv:2508.01309v2 Announce Type: replace-cross Abstract: The scarcity and high cost of high-quality domain-specific question-answering (QA) datasets limit supervised fine-tuning of large language models (LLMs). We introduce $\\textbf{D-SCoRE}$, a training-free framework that leverages LLMs and prompt engineering to automatically generate diverse, r",
    "url": "https://arxiv.org/abs/2508.01309",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "An item is worth one token in Multimodal Large Language Models-based Sequential Recommendation",
    "summary": "arXiv:2511.05885v3 Announce Type: replace-cross Abstract: Sequential recommendations (SR) predict users' future interactions based on their historical behavior. The rise of Large Language Models (LLMs) has brought powerful generative and reasoning capabilities, significantly enhancing SR performance, while Multimodal LLMs (MLLMs) further extend thi",
    "url": "https://arxiv.org/abs/2511.05885",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-10T03:40:14.793398+00:00"
  },
  {
    "title": "Trifuse: Enhancing Attention-Based GUI Grounding via Multimodal Fusion",
    "summary": "arXiv:2602.06351v1 Announce Type: new Abstract: GUI grounding maps natural language instructions to the correct interface elements, serving as the perception foundation for GUI agents. Existing approaches predominantly rely on fine-tuning multimodal large language models (MLLMs) using large-scale GUI datasets to predict target element coordinates, ",
    "url": "https://arxiv.org/abs/2602.06351",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "Towards Understanding What State Space Models Learn About Code",
    "summary": "arXiv:2602.06774v1 Announce Type: new Abstract: State Space Models (SSMs) have emerged as an efficient alternative to the transformer architecture. Recent studies show that SSMs can match or surpass Transformers on code understanding tasks, such as code retrieval, when trained under similar conditions. However, their internal mechanisms remain a bl",
    "url": "https://arxiv.org/abs/2602.06774",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "LLM Active Alignment: A Nash Equilibrium Perspective",
    "summary": "arXiv:2602.06836v1 Announce Type: new Abstract: We develop a game-theoretic framework for predicting and steering the behavior of populations of large language models (LLMs) through Nash equilibrium (NE) analysis. To avoid the intractability of equilibrium computation in open-ended text spaces, we model each agent's action as a mixture over human s",
    "url": "https://arxiv.org/abs/2602.06836",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "Analyzing Diffusion and Autoregressive Vision Language Models in Multimodal Embedding Space",
    "summary": "arXiv:2602.06056v1 Announce Type: cross Abstract: Embedding models are a fundamental component of modern AI systems such as semantic search and retrieval-augmented generation. Recent advances in large foundation models have substantially accelerated the development of embedding models, including those based on Large Language Models (LLMs), Vision L",
    "url": "https://arxiv.org/abs/2602.06056",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "DAVE: Distribution-aware Attribution via ViT Gradient Decomposition",
    "summary": "arXiv:2602.06613v1 Announce Type: cross Abstract: Vision Transformers (ViTs) have become a dominant architecture in computer vision, yet producing stable and high-resolution attribution maps for these models remains challenging. Architectural components such as patch embeddings and attention routing often introduce structured artifacts in pixel-lev",
    "url": "https://arxiv.org/abs/2602.06613",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "Multimodal Generative Retrieval Model with Staged Pretraining for Food Delivery on Meituan",
    "summary": "arXiv:2602.06654v1 Announce Type: cross Abstract: Multimodal retrieval models are becoming increasingly important in scenarios such as food delivery, where rich multimodal features can meet diverse user needs and enable precise retrieval. Mainstream approaches typically employ a dual-tower architecture between queries and items, and perform joint o",
    "url": "https://arxiv.org/abs/2602.06654",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "Bridging 6G IoT and AI: LLM-Based Efficient Approach for Physical Layer's Optimization Tasks",
    "summary": "arXiv:2602.06819v1 Announce Type: cross Abstract: This paper investigates the role of large language models (LLMs) in sixth-generation (6G) Internet of Things (IoT) networks and proposes a prompt-engineering-based real-time feedback and verification (PE-RTFV) framework that perform physical-layer's optimization tasks through an iteratively process.",
    "url": "https://arxiv.org/abs/2602.06819",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "The Quantum Sieve Tracer: A Hybrid Framework for Layer-Wise Activation Tracing in Large Language Models",
    "summary": "arXiv:2602.06852v1 Announce Type: cross Abstract: Mechanistic interpretability aims to reverse-engineer the internal computations of Large Language Models (LLMs), yet separating sparse semantic signals from high-dimensional polysemantic noise remains a significant challenge. This paper introduces the Quantum Sieve Tracer, a hybrid quantum-classical",
    "url": "https://arxiv.org/abs/2602.06852",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code",
    "summary": "arXiv:2602.06875v1 Announce Type: cross Abstract: Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, wi",
    "url": "https://arxiv.org/abs/2602.06875",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "From Kepler to Newton: Inductive Biases Guide Learned World Models in Transformers",
    "summary": "arXiv:2602.06923v1 Announce Type: cross Abstract: Can general-purpose AI architectures go beyond prediction to discover the physical laws governing the universe? True intelligence relies on \"world models\" -- causal abstractions that allow an agent to not only predict future states but understand the underlying governing dynamics. While previous \"AI",
    "url": "https://arxiv.org/abs/2602.06923",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search",
    "summary": "arXiv:2602.05014v2 Announce Type: replace Abstract: With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search framework",
    "url": "https://arxiv.org/abs/2602.05014",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "AgentXRay: White-Boxing Agentic Systems via Workflow Reconstruction",
    "summary": "arXiv:2602.05353v2 Announce Type: replace Abstract: Large Language Models have shown strong capabilities in complex problem solving, yet many agentic systems remain difficult to interpret and control due to opaque internal workflows. While some frameworks offer explicit architectures for collaboration, many deployed agentic systems operate as black",
    "url": "https://arxiv.org/abs/2602.05353",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "Echo State Transformer: Attention Over Finite Memories",
    "summary": "arXiv:2507.02917v3 Announce Type: replace-cross Abstract: While Large Language Models and their underlying Transformer architecture are remarkably efficient, they do not reflect how our brain processes and learns a diversity of cognitive tasks such as language, nor how it leverages working memory. Furthermore, Transformers encounters a computationa",
    "url": "https://arxiv.org/abs/2507.02917",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation",
    "summary": "arXiv:2508.01309v2 Announce Type: replace-cross Abstract: The scarcity and high cost of high-quality domain-specific question-answering (QA) datasets limit supervised fine-tuning of large language models (LLMs). We introduce $\\textbf{D-SCoRE}$, a training-free framework that leverages LLMs and prompt engineering to automatically generate diverse, r",
    "url": "https://arxiv.org/abs/2508.01309",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "An item is worth one token in Multimodal Large Language Models-based Sequential Recommendation",
    "summary": "arXiv:2511.05885v3 Announce Type: replace-cross Abstract: Sequential recommendations (SR) predict users' future interactions based on their historical behavior. The rise of Large Language Models (LLMs) has brought powerful generative and reasoning capabilities, significantly enhancing SR performance, while Multimodal LLMs (MLLMs) further extend thi",
    "url": "https://arxiv.org/abs/2511.05885",
    "source": "Arxiv AI",
    "published_at": "2026-02-09T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-09T08:15:08.878734+00:00"
  },
  {
    "title": "Understanding LLM Evaluator Behavior: A Structured Multi-Evaluator Framework for Merchant Risk Assessment",
    "summary": "arXiv:2602.05110v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly used as evaluators of reasoning quality, yet their reliability and bias in payments-risk settings remain poorly understood. We introduce a structured multi-evaluator framework for assessing LLM reasoning in Merchant Category Code (MCC)-based merchant risk ",
    "url": "https://arxiv.org/abs/2602.05110",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "Hallucination-Resistant Security Planning with a Large Language Model",
    "summary": "arXiv:2602.05279v1 Announce Type: new Abstract: Large language models (LLMs) are promising tools for supporting security management tasks, such as incident response planning. However, their unreliability and tendency to hallucinate remain significant challenges. In this paper, we address these challenges by introducing a principled framework for us",
    "url": "https://arxiv.org/abs/2602.05279",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "ProAct: Agentic Lookahead in Interactive Environments",
    "summary": "arXiv:2602.05327v1 Announce Type: new Abstract: Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a ",
    "url": "https://arxiv.org/abs/2602.05327",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "AgentXRay: White-Boxing Agentic Systems via Workflow Reconstruction",
    "summary": "arXiv:2602.05353v1 Announce Type: new Abstract: Large Language Models have shown strong capabilities in complex problem solving, yet many agentic systems remain difficult to interpret and control due to opaque internal workflows. While some frameworks offer explicit architectures for collaboration, many deployed agentic systems operate as black box",
    "url": "https://arxiv.org/abs/2602.05353",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "A Unified Multimodal Framework for Dataset Construction and Model-Based Diagnosis of Ameloblastoma",
    "summary": "arXiv:2602.05515v1 Announce Type: new Abstract: Artificial intelligence (AI)-enabled diagnostics in maxillofacial pathology require structured, high-quality multimodal datasets. However, existing resources provide limited ameloblastoma coverage and lack the format consistency needed for direct model training. We present a newly curated multimodal d",
    "url": "https://arxiv.org/abs/2602.05515",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "Reasoning-guided Collaborative Filtering with Language Models for Explainable Recommendation",
    "summary": "arXiv:2602.05544v1 Announce Type: new Abstract: Large Language Models (LLMs) exhibit potential for explainable recommendation systems but overlook collaborative signals, while prevailing methods treat recommendation and explanation as separate tasks, resulting in a memory footprint. We present RGCF-XRec, a hybrid framework that introduces reasoning",
    "url": "https://arxiv.org/abs/2602.05544",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "A$^2$-LLM: An End-to-end Conversational Audio Avatar Large Language Model",
    "summary": "arXiv:2602.04913v1 Announce Type: cross Abstract: Developing expressive and responsive conversational digital humans is a cornerstone of next-generation human-computer interaction. While large language models (LLMs) have significantly enhanced dialogue capabilities, most current systems still rely on cascaded architectures that connect independent ",
    "url": "https://arxiv.org/abs/2602.04913",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "PriMod4AI: Lifecycle-Aware Privacy Threat Modeling for AI Systems using LLM",
    "summary": "arXiv:2602.04927v1 Announce Type: cross Abstract: Artificial intelligence systems introduce complex privacy risks throughout their lifecycle, especially when processing sensitive or high-dimensional data. Beyond the seven traditional privacy threat categories defined by the LINDDUN framework, AI systems are also exposed to model-centric privacy att",
    "url": "https://arxiv.org/abs/2602.04927",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "Linear Model Merging Unlocks Simple and Scalable Multimodal Data Mixture Optimization",
    "summary": "arXiv:2602.04937v1 Announce Type: cross Abstract: Selecting the best data mixture is critical for successful Supervised Fine-Tuning (SFT) of Multimodal Large Language Models. However, determining the optimal mixture weights across multiple domain-specific datasets remains a significant bottleneck due to the combinatorial search space and the high c",
    "url": "https://arxiv.org/abs/2602.04937",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "LMMRec: LLM-driven Motivation-aware Multimodal Recommendation",
    "summary": "arXiv:2602.05474v1 Announce Type: cross Abstract: Motivation-based recommendation systems uncover user behavior drivers. Motivation modeling, crucial for decision-making and content preference, explains recommendation generation. Existing methods often treat motivation as latent variables from interaction data, neglecting heterogeneous information ",
    "url": "https://arxiv.org/abs/2602.05474",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "A Unified Framework for Rethinking Policy Divergence Measures in GRPO",
    "summary": "arXiv:2602.05494v1 Announce Type: cross Abstract: Reinforcement Learning with Verified Reward (RLVR) has emerged as a critical paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). Most existing RLVR methods, such as GRPO and its variants, ensure stable updates by constraining policy divergence through clipping likeliho",
    "url": "https://arxiv.org/abs/2602.05494",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "Transport and Merge: Cross-Architecture Merging for Large Language Models",
    "summary": "arXiv:2602.05495v1 Announce Type: cross Abstract: Large language models (LLMs) achieve strong capabilities by scaling model capacity and training data, yet many real-world deployments rely on smaller models trained or adapted from low-resource data. This gap motivates the need for mechanisms to transfer knowledge from large, high-resource models to",
    "url": "https://arxiv.org/abs/2602.05495",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "In-context Time Series Predictor",
    "summary": "arXiv:2405.14982v2 Announce Type: replace-cross Abstract: Recent Transformer-based large language models (LLMs) demonstrate in-context learning ability to perform various functions based solely on the provided context, without updating model parameters. To fully utilize the in-context capabilities in time series forecasting (TSF) problems, unlike p",
    "url": "https://arxiv.org/abs/2405.14982",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  },
  {
    "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models",
    "summary": "arXiv:2503.06749v3 Announce Type: replace-cross Abstract: DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles",
    "url": "https://arxiv.org/abs/2503.06749",
    "source": "Arxiv AI",
    "published_at": "2026-02-07T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-08T03:45:11.802210+00:00"
  }
]