[
  {
    "title": "ViT-Linearizer: Distilling Quadratic Knowledge into Linear-Time Vision Models",
    "summary": "arXiv:2504.00037v2 Announce Type: replace-cross Abstract: Vision Transformers (ViTs) have delivered remarkable progress through global self-attention, yet their quadratic complexity can become prohibitive for high-resolution inputs. In this work, we present ViT-Linearizer, a cross-architecture distillation framework that transfers rich ViT represen",
    "url": "https://arxiv.org/abs/2504.00037",
    "source": "Arxiv AI",
    "published_at": "2026-02-28T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-03-01T03:29:33.372316+00:00"
  },
  {
    "title": "Agentic AI for Intent-driven Optimization in Cell-free O-RAN",
    "summary": "arXiv:2602.22539v1 Announce Type: new Abstract: Agentic artificial intelligence (AI) is emerging as a key enabler for autonomous radio access networks (RANs), where multiple large language model (LLM)-based agents reason and collaborate to achieve operator-defined intents. The open RAN (O-RAN) architecture enables the deployment and coordination of",
    "url": "https://arxiv.org/abs/2602.22539",
    "source": "Arxiv AI",
    "published_at": "2026-02-28T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-03-01T03:29:33.372316+00:00"
  },
  {
    "title": "CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety",
    "summary": "arXiv:2602.22557v1 Announce Type: new Abstract: Current safety mechanisms for Large Language Models (LLMs) rely heavily on static, fine-tuned classifiers that suffer from adaptation rigidity, the inability to enforce new governance rules without expensive retraining. To address this, we introduce CourtGuard, a retrieval-augmented multi-agent framew",
    "url": "https://arxiv.org/abs/2602.22557",
    "source": "Arxiv AI",
    "published_at": "2026-02-28T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-03-01T03:29:33.372316+00:00"
  },
  {
    "title": "MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks",
    "summary": "arXiv:2602.22808v1 Announce Type: new Abstract: Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy throu",
    "url": "https://arxiv.org/abs/2602.22808",
    "source": "Arxiv AI",
    "published_at": "2026-02-28T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-03-01T03:29:33.372316+00:00"
  },
  {
    "title": "FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning",
    "summary": "arXiv:2602.22963v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is spa",
    "url": "https://arxiv.org/abs/2602.22963",
    "source": "Arxiv AI",
    "published_at": "2026-02-28T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-03-01T03:29:33.372316+00:00"
  },
  {
    "title": "SPM-Bench: Benchmarking Large Language Models for Scanning Probe Microscopy",
    "summary": "arXiv:2602.22971v1 Announce Type: new Abstract: As LLMs achieved breakthroughs in general reasoning, their proficiency in specialized scientific domains reveals pronounced gaps in existing benchmarks due to data contamination, insufficient complexity, and prohibitive human labor costs. Here we present SPM-Bench, an original, PhD-level multimodal be",
    "url": "https://arxiv.org/abs/2602.22971",
    "source": "Arxiv AI",
    "published_at": "2026-02-28T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-03-01T03:29:33.372316+00:00"
  },
  {
    "title": "To Deceive is to Teach? Forging Perceptual Robustness via Adversarial Reinforcement Learning",
    "summary": "arXiv:2602.22227v1 Announce Type: cross Abstract: Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) exhibit perceptual fragility when confronted with visually complex scenes. This weakness stems from a reliance on finite training datasets, which are prohibitively expensive to scale and impose a ceiling on model robustn",
    "url": "https://arxiv.org/abs/2602.22227",
    "source": "Arxiv AI",
    "published_at": "2026-02-28T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-03-01T03:29:33.372316+00:00"
  },
  {
    "title": "Deep Sequence Modeling with Quantum Dynamics: Language as a Wave Function",
    "summary": "arXiv:2602.22255v1 Announce Type: cross Abstract: We introduce a sequence modeling framework in which the latent state is a complex-valued wave function evolving on a finite-dimensional Hilbert space under a learned, time-dependent Hamiltonian. Unlike standard recurrent architectures that rely on gating mechanisms to suppress competing hypotheses, ",
    "url": "https://arxiv.org/abs/2602.22255",
    "source": "Arxiv AI",
    "published_at": "2026-02-28T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-03-01T03:29:33.372316+00:00"
  },
  {
    "title": "Decoding the Hook: A Multimodal LLM Framework for Analyzing the Hooking Period of Video Ads",
    "summary": "arXiv:2602.22299v1 Announce Type: cross Abstract: Video-based ads are a vital medium for brands to engage consumers, with social media platforms leveraging user data to optimize ad delivery and boost engagement. A crucial but under-explored aspect is the 'hooking period', the first three seconds that capture viewer attention and influence engagemen",
    "url": "https://arxiv.org/abs/2602.22299",
    "source": "Arxiv AI",
    "published_at": "2026-02-28T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-03-01T03:29:33.372316+00:00"
  },
  {
    "title": "Ruyi2 Technical Report",
    "summary": "arXiv:2602.22543v1 Announce Type: cross Abstract: Large Language Models (LLMs) face significant challenges regarding deployment costs and latency, necessitating adaptive computing strategies. Building upon the AI Flow framework, we introduce Ruyi2 as an evolution of our adaptive model series designed for efficient variable-depth computation. While ",
    "url": "https://arxiv.org/abs/2602.22543",
    "source": "Arxiv AI",
    "published_at": "2026-02-28T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-03-01T03:29:33.372316+00:00"
  },
  {
    "title": "ContextRL: Enhancing MLLM's Knowledge Discovery Efficiency with Context-Augmented RL",
    "summary": "arXiv:2602.22623v1 Announce Type: cross Abstract: We propose ContextRL, a novel framework that leverages context augmentation to overcome these bottlenecks. Specifically, to enhance Identifiability, we provide the reward model with full reference solutions as context, enabling fine-grained process verification to filter out false positives (samples",
    "url": "https://arxiv.org/abs/2602.22623",
    "source": "Arxiv AI",
    "published_at": "2026-02-28T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-03-01T03:29:33.372316+00:00"
  },
  {
    "title": "IMMACULATE: A Practical LLM Auditing Framework via Verifiable Computation",
    "summary": "arXiv:2602.22700v1 Announce Type: cross Abstract: Commercial large language models are typically deployed as black-box API services, requiring users to trust providers to execute inference correctly and report token usage honestly. We present IMMACULATE, a practical auditing framework that detects economically motivated deviations-such as model sub",
    "url": "https://arxiv.org/abs/2602.22700",
    "source": "Arxiv AI",
    "published_at": "2026-02-28T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-03-01T03:29:33.372316+00:00"
  },
  {
    "title": "SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs",
    "summary": "arXiv:2602.22716v1 Announce Type: cross Abstract: 3D Large Vision-Language Models (3D LVLMs) built upon Large Language Models (LLMs) have achieved remarkable progress across various multimodal tasks. However, their inherited position-dependent modeling mechanism, Rotary Position Embedding (RoPE), remains suboptimal for 3D multimodal understanding. ",
    "url": "https://arxiv.org/abs/2602.22716",
    "source": "Arxiv AI",
    "published_at": "2026-02-28T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-03-01T03:29:33.372316+00:00"
  },
  {
    "title": "LLM4AD: A Platform for Algorithm Design with Large Language Model",
    "summary": "arXiv:2412.17287v2 Announce Type: replace Abstract: We introduce LLM4AD, a unified Python platform for algorithm design (AD) with large language models (LLMs). LLM4AD is a generic framework with modularized blocks for search methods, algorithm design tasks, and LLM interface. The platform integrates numerous key methods and supports a wide range of",
    "url": "https://arxiv.org/abs/2412.17287",
    "source": "Arxiv AI",
    "published_at": "2026-02-28T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-03-01T03:29:33.372316+00:00"
  },
  {
    "title": "ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices",
    "summary": "arXiv:2602.21858v2 Announce Type: replace Abstract: Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomousl",
    "url": "https://arxiv.org/abs/2602.21858",
    "source": "Arxiv AI",
    "published_at": "2026-02-28T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-03-01T03:29:33.372316+00:00"
  },
  {
    "title": "ViT-Linearizer: Distilling Quadratic Knowledge into Linear-Time Vision Models",
    "summary": "arXiv:2504.00037v2 Announce Type: replace-cross Abstract: Vision Transformers (ViTs) have delivered remarkable progress through global self-attention, yet their quadratic complexity can become prohibitive for high-resolution inputs. In this work, we present ViT-Linearizer, a cross-architecture distillation framework that transfers rich ViT represen",
    "url": "https://arxiv.org/abs/2504.00037",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "Agentic AI for Intent-driven Optimization in Cell-free O-RAN",
    "summary": "arXiv:2602.22539v1 Announce Type: new Abstract: Agentic artificial intelligence (AI) is emerging as a key enabler for autonomous radio access networks (RANs), where multiple large language model (LLM)-based agents reason and collaborate to achieve operator-defined intents. The open RAN (O-RAN) architecture enables the deployment and coordination of",
    "url": "https://arxiv.org/abs/2602.22539",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety",
    "summary": "arXiv:2602.22557v1 Announce Type: new Abstract: Current safety mechanisms for Large Language Models (LLMs) rely heavily on static, fine-tuned classifiers that suffer from adaptation rigidity, the inability to enforce new governance rules without expensive retraining. To address this, we introduce CourtGuard, a retrieval-augmented multi-agent framew",
    "url": "https://arxiv.org/abs/2602.22557",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks",
    "summary": "arXiv:2602.22808v1 Announce Type: new Abstract: Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy throu",
    "url": "https://arxiv.org/abs/2602.22808",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning",
    "summary": "arXiv:2602.22963v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is spa",
    "url": "https://arxiv.org/abs/2602.22963",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "SPM-Bench: Benchmarking Large Language Models for Scanning Probe Microscopy",
    "summary": "arXiv:2602.22971v1 Announce Type: new Abstract: As LLMs achieved breakthroughs in general reasoning, their proficiency in specialized scientific domains reveals pronounced gaps in existing benchmarks due to data contamination, insufficient complexity, and prohibitive human labor costs. Here we present SPM-Bench, an original, PhD-level multimodal be",
    "url": "https://arxiv.org/abs/2602.22971",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "To Deceive is to Teach? Forging Perceptual Robustness via Adversarial Reinforcement Learning",
    "summary": "arXiv:2602.22227v1 Announce Type: cross Abstract: Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) exhibit perceptual fragility when confronted with visually complex scenes. This weakness stems from a reliance on finite training datasets, which are prohibitively expensive to scale and impose a ceiling on model robustn",
    "url": "https://arxiv.org/abs/2602.22227",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "Deep Sequence Modeling with Quantum Dynamics: Language as a Wave Function",
    "summary": "arXiv:2602.22255v1 Announce Type: cross Abstract: We introduce a sequence modeling framework in which the latent state is a complex-valued wave function evolving on a finite-dimensional Hilbert space under a learned, time-dependent Hamiltonian. Unlike standard recurrent architectures that rely on gating mechanisms to suppress competing hypotheses, ",
    "url": "https://arxiv.org/abs/2602.22255",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "Decoding the Hook: A Multimodal LLM Framework for Analyzing the Hooking Period of Video Ads",
    "summary": "arXiv:2602.22299v1 Announce Type: cross Abstract: Video-based ads are a vital medium for brands to engage consumers, with social media platforms leveraging user data to optimize ad delivery and boost engagement. A crucial but under-explored aspect is the 'hooking period', the first three seconds that capture viewer attention and influence engagemen",
    "url": "https://arxiv.org/abs/2602.22299",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "Ruyi2 Technical Report",
    "summary": "arXiv:2602.22543v1 Announce Type: cross Abstract: Large Language Models (LLMs) face significant challenges regarding deployment costs and latency, necessitating adaptive computing strategies. Building upon the AI Flow framework, we introduce Ruyi2 as an evolution of our adaptive model series designed for efficient variable-depth computation. While ",
    "url": "https://arxiv.org/abs/2602.22543",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "ContextRL: Enhancing MLLM's Knowledge Discovery Efficiency with Context-Augmented RL",
    "summary": "arXiv:2602.22623v1 Announce Type: cross Abstract: We propose ContextRL, a novel framework that leverages context augmentation to overcome these bottlenecks. Specifically, to enhance Identifiability, we provide the reward model with full reference solutions as context, enabling fine-grained process verification to filter out false positives (samples",
    "url": "https://arxiv.org/abs/2602.22623",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "IMMACULATE: A Practical LLM Auditing Framework via Verifiable Computation",
    "summary": "arXiv:2602.22700v1 Announce Type: cross Abstract: Commercial large language models are typically deployed as black-box API services, requiring users to trust providers to execute inference correctly and report token usage honestly. We present IMMACULATE, a practical auditing framework that detects economically motivated deviations-such as model sub",
    "url": "https://arxiv.org/abs/2602.22700",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs",
    "summary": "arXiv:2602.22716v1 Announce Type: cross Abstract: 3D Large Vision-Language Models (3D LVLMs) built upon Large Language Models (LLMs) have achieved remarkable progress across various multimodal tasks. However, their inherited position-dependent modeling mechanism, Rotary Position Embedding (RoPE), remains suboptimal for 3D multimodal understanding. ",
    "url": "https://arxiv.org/abs/2602.22716",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "LLM4AD: A Platform for Algorithm Design with Large Language Model",
    "summary": "arXiv:2412.17287v2 Announce Type: replace Abstract: We introduce LLM4AD, a unified Python platform for algorithm design (AD) with large language models (LLMs). LLM4AD is a generic framework with modularized blocks for search methods, algorithm design tasks, and LLM interface. The platform integrates numerous key methods and supports a wide range of",
    "url": "https://arxiv.org/abs/2412.17287",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices",
    "summary": "arXiv:2602.21858v2 Announce Type: replace Abstract: Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomousl",
    "url": "https://arxiv.org/abs/2602.21858",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-28T02:45:35.207047+00:00"
  },
  {
    "title": "Latent Context Compilation: Distilling Long Context into Compact Portable Memory",
    "summary": "arXiv:2602.21221v1 Announce Type: cross Abstract: Efficient long-context LLM deployment is stalled by a dichotomy between amortized compression, which struggles with out-of-distribution generalization, and Test-Time Training, which incurs prohibitive synthetic data costs and requires modifying model weights, creating stateful parameters that compli",
    "url": "https://arxiv.org/abs/2602.21221",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-27T03:16:01.376058+00:00"
  },
  {
    "title": "A General Equilibrium Theory of Orchestrated AI Agent Systems",
    "summary": "arXiv:2602.21255v1 Announce Type: cross Abstract: We establish a general equilibrium theory for systems of large language model (LLM) agents operating under centralized orchestration. The framework is a production economy in the sense of Arrow-Debreu (1954), extended to infinite-dimensional commodity spaces following Bewley (1972). Each LLM agent i",
    "url": "https://arxiv.org/abs/2602.21255",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-27T03:16:01.376058+00:00"
  },
  {
    "title": "Causal Decoding for Hallucination-Resistant Multimodal Large Language Models",
    "summary": "arXiv:2602.21441v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) deliver detailed responses on vision-language tasks, yet remain susceptible to object hallucination (introducing objects not present in the image), undermining reliability in practice. Prior efforts often rely on heuristic penalties, post-hoc correction, or g",
    "url": "https://arxiv.org/abs/2602.21441",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-27T03:16:01.376058+00:00"
  },
  {
    "title": "1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning",
    "summary": "arXiv:2508.07667v3 Announce Type: replace Abstract: Addressing contextual privacy concerns remains challenging in interactive settings where large language models (LLMs) process information from multiple sources (e.g., summarizing meetings with private and public information). We introduce a multi-agent framework that decomposes privacy reasoning i",
    "url": "https://arxiv.org/abs/2508.07667",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-27T03:16:01.376058+00:00"
  },
  {
    "title": "When Can Transformers Count to n?",
    "summary": "arXiv:2407.15160v3 Announce Type: replace-cross Abstract: Large language models based on the transformer architecture can solve highly complex tasks, yet their fundamental limitations on simple algorithmic problems remain poorly understood. In this work, we focus on basic counting tasks and investigate how the difficulty of these tasks scales with ",
    "url": "https://arxiv.org/abs/2407.15160",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-27T03:16:01.376058+00:00"
  },
  {
    "title": "Compose and Fuse: Revisiting the Foundational Bottlenecks in Multimodal Reasoning",
    "summary": "arXiv:2509.23744v2 Announce Type: replace-cross Abstract: Multimodal large language models (MLLMs) promise enhanced reasoning by integrating diverse inputs such as text, vision, and audio. Yet cross-modal reasoning remains underexplored, with conflicting reports on whether added modalities help or harm performance. These inconsistencies stem from a",
    "url": "https://arxiv.org/abs/2509.23744",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-27T03:16:01.376058+00:00"
  },
  {
    "title": "SciTS: Scientific Time Series Understanding and Generation with LLMs",
    "summary": "arXiv:2510.03255v2 Announce Type: replace-cross Abstract: The scientific reasoning ability of large language models (LLMs) has recently attracted significant attention. Time series, as a fundamental modality in scientific data, presents unique challenges that are often overlooked in current multimodal LLMs, which either encode numerical sequences a",
    "url": "https://arxiv.org/abs/2510.03255",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-27T03:16:01.376058+00:00"
  },
  {
    "title": "TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs",
    "summary": "arXiv:2602.00288v3 Announce Type: replace-cross Abstract: Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-te",
    "url": "https://arxiv.org/abs/2602.00288",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-27T03:16:01.376058+00:00"
  },
  {
    "title": "MALLVI: A Multi-Agent Framework for Integrated Generalized Robotics Manipulation",
    "summary": "arXiv:2602.16898v3 Announce Type: replace-cross Abstract: Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings. M",
    "url": "https://arxiv.org/abs/2602.16898",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-27T03:16:01.376058+00:00"
  },
  {
    "title": "Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers",
    "summary": "arXiv:2602.18022v2 Announce Type: replace-cross Abstract: Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value s",
    "url": "https://arxiv.org/abs/2602.18022",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-27T03:16:01.376058+00:00"
  },
  {
    "title": "Google's Nano Banana 2 brings Pro-level image generation to Flash speeds at up to 40% lower API cost",
    "summary": "Google's new Nano Banana 2 image generation model pairs the capabilities of the pricier Pro model with Gemini Flash speed at nearly half the cost. It's now the default in the Gemini app. The article Google&#039;s Nano Banana 2 brings Pro-level image generation to Flash speeds at up to 40% lower API cost appeared first on The Decoder.",
    "url": "https://the-decoder.com/googles-nano-banana-2-brings-pro-level-image-generation-to-flash-speeds-at-up-to-40-lower-api-cost/",
    "source": "The Decoder",
    "published_at": "2026-02-26T18:45:03+00:00",
    "score": 7,
    "archived_at": "2026-02-27T03:16:01.376058+00:00"
  },
  {
    "title": "Alibaba's open Qwen 3.5 takes aim at GPT-5 mini and Claude Sonnet 4.5 at a fraction of the cost",
    "summary": "Alibaba has introduced the new Qwen 3.5 model series. It comprises four models: Qwen3.5-Flash, Qwen3.5-35B-A3B, Qwen3.5-122B-A10B and Qwen3.5-27B. The article Alibaba&#039;s open Qwen 3.5 takes aim at GPT-5 mini and Claude Sonnet 4.5 at a fraction of the cost appeared first on The Decoder.",
    "url": "https://the-decoder.com/alibabas-open-qwen-3-5-takes-aim-at-gpt-5-mini-and-claude-sonnet-4-5-at-a-fraction-of-the-cost/",
    "source": "The Decoder",
    "published_at": "2026-02-26T09:14:52+00:00",
    "score": 7,
    "archived_at": "2026-02-27T03:16:01.376058+00:00"
  },
  {
    "title": "A Dynamic Survey of Soft Set Theory and Its Extensions",
    "summary": "arXiv:2602.21268v1 Announce Type: new Abstract: Soft set theory provides a direct framework for parameterized decision modeling by assigning to each attribute (parameter) a subset of a given universe, thereby representing uncertainty in a structured way [1, 2]. Over the past decades, the theory has expanded into numerous variants-including hypersof",
    "url": "https://arxiv.org/abs/2602.21268",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-27T03:16:01.376058+00:00"
  },
  {
    "title": "Beyond Refusal: Probing the Limits of Agentic Self-Correction for Semantic Sensitive Information",
    "summary": "arXiv:2602.21496v1 Announce Type: new Abstract: While defenses for structured PII are mature, Large Language Models (LLMs) pose a new threat: Semantic Sensitive Information (SemSI), where models infer sensitive identity attributes, generate reputation-harmful content, or hallucinate potentially wrong information. The capacity of LLMs to self-regula",
    "url": "https://arxiv.org/abs/2602.21496",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-27T03:16:01.376058+00:00"
  },
  {
    "title": "The ASIR Courage Model: A Phase-Dynamic Framework for Truth Transitions in Human and AI Systems",
    "summary": "arXiv:2602.21745v1 Announce Type: new Abstract: We introduce the ASIR (Awakened Shared Intelligence Relationship) Courage Model, a phase-dynamic framework that formalizes truth-disclosure as a state transition rather than a personality trait. The mode characterizes the shift from suppression (S0) to expression (S1) as occurring when facilitative fo",
    "url": "https://arxiv.org/abs/2602.21745",
    "source": "Arxiv AI",
    "published_at": "2026-02-26T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-27T03:16:01.376058+00:00"
  },
  {
    "title": "DMCD: Semantic-Statistical Framework for Causal Discovery",
    "summary": "arXiv:2602.20333v1 Announce Type: new Abstract: We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed pr",
    "url": "https://arxiv.org/abs/2602.20333",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-26T03:19:00.977217+00:00"
  },
  {
    "title": "PyVision-RL: Forging Open Agentic Vision Models via RL",
    "summary": "arXiv:2602.20739v1 Announce Type: new Abstract: Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that",
    "url": "https://arxiv.org/abs/2602.20739",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-26T03:19:00.977217+00:00"
  },
  {
    "title": "Evaluating the Reliability of Digital Forensic Evidence Discovered by Large Language Model: A Case Study",
    "summary": "arXiv:2602.20202v1 Announce Type: cross Abstract: The growing reliance on AI-identified digital evidence raises significant concerns about its reliability, particularly as large language models (LLMs) are increasingly integrated into forensic investigations. This paper proposes a structured framework that automates forensic artifact extraction, ref",
    "url": "https://arxiv.org/abs/2602.20202",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-26T03:19:00.977217+00:00"
  },
  {
    "title": "KnapSpec: Self-Speculative Decoding via Adaptive Layer Selection as a Knapsack Problem",
    "summary": "arXiv:2602.20217v1 Announce Type: cross Abstract: Self-speculative decoding (SSD) accelerates LLM inference by skipping layers to create an efficient draft model, yet existing methods often rely on static heuristics that ignore the dynamic computational overhead of attention in long-context scenarios. We propose KnapSpec, a training-free framework ",
    "url": "https://arxiv.org/abs/2602.20217",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-26T03:19:00.977217+00:00"
  },
  {
    "title": "An Approach to Combining Video and Speech with Large Language Models in Human-Robot Interaction",
    "summary": "arXiv:2602.20219v1 Announce Type: cross Abstract: Interpreting human intent accurately is a central challenge in human-robot interaction (HRI) and a key requirement for achieving more natural and intuitive collaboration between humans and machines. This work presents a novel multimodal HRI framework that combines advanced vision-language models, sp",
    "url": "https://arxiv.org/abs/2602.20219",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-26T03:19:00.977217+00:00"
  },
  {
    "title": "Circuit Tracing in Vision-Language Models: Understanding the Internal Mechanisms of Multimodal Thinking",
    "summary": "arXiv:2602.20330v1 Announce Type: cross Abstract: Vision-language models (VLMs) are powerful but remain opaque black boxes. We introduce the first framework for transparent circuit tracing in VLMs to systematically analyze multimodal reasoning. By utilizing transcoders, attribution graphs, and attention-based methods, we uncover how VLMs hierarchic",
    "url": "https://arxiv.org/abs/2602.20330",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-26T03:19:00.977217+00:00"
  },
  {
    "title": "Protein Language Models Diverge from Natural Language: Comparative Analysis and Improved Inference",
    "summary": "arXiv:2602.20449v1 Announce Type: cross Abstract: Modern Protein Language Models (PLMs) apply transformer-based model architectures from natural language processing to biological sequences, predicting a variety of protein functions and properties. However, protein language has key differences from natural language, such as a rich functional space d",
    "url": "https://arxiv.org/abs/2602.20449",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-26T03:19:00.977217+00:00"
  },
  {
    "title": "Communication-Inspired Tokenization for Structured Image Representations",
    "summary": "arXiv:2602.20731v1 Announce Type: cross Abstract: Discrete image tokenizers have emerged as a key component of modern vision and multimodal systems, providing a sequential interface for transformer-based architectures. However, most existing approaches remain primarily optimized for reconstruction and compression, often yielding tokens that capture",
    "url": "https://arxiv.org/abs/2602.20731",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-26T03:19:00.977217+00:00"
  },
  {
    "title": "CrystaL: Spontaneous Emergence of Visual Latents in MLLMs",
    "summary": "arXiv:2602.20980v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable performance by integrating powerful language backbones with large-scale visual encoders. Among these, latent Chain-of-Thought (CoT) methods enable implicit reasoning in continuous hidden states, facilitating seamless vision-language i",
    "url": "https://arxiv.org/abs/2602.20980",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-26T03:19:00.977217+00:00"
  },
  {
    "title": "BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents",
    "summary": "arXiv:2602.12876v2 Announce Type: replace Abstract: Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remai",
    "url": "https://arxiv.org/abs/2602.12876",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-26T03:19:00.977217+00:00"
  },
  {
    "title": "HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models",
    "summary": "arXiv:2506.03922v2 Announce Type: replace-cross Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant potential to advance a broad range of domains. However, current benchmarks for evaluating MLLMs primarily emphasize general knowledge and vertical step-by-step reasoning typical of STEM disciplines, while overlooking the ",
    "url": "https://arxiv.org/abs/2506.03922",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-26T03:19:00.977217+00:00"
  },
  {
    "title": "K-Function: Joint Pronunciation Transcription and Feedback for Evaluating Kids Language Function",
    "summary": "arXiv:2507.03043v3 Announce Type: replace-cross Abstract: Evaluating young children's language is challenging for automatic speech recognizers due to high-pitched voices, prolonged sounds, and limited data. We introduce K-Function, a framework that combines accurate sub-word transcription with objective, Large Language Model (LLM)-driven scoring. I",
    "url": "https://arxiv.org/abs/2507.03043",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-26T03:19:00.977217+00:00"
  },
  {
    "title": "Characterizing State Space Model and Hybrid Language Model Performance with Long Context",
    "summary": "arXiv:2507.12442v3 Announce Type: replace-cross Abstract: Emerging applications such as AR are driving demands for machine intelligence capable of processing continuous and/or long-context inputs on local devices. However, currently dominant models based on Transformer architecture suffers from the quadratic computational and memory overhead, which",
    "url": "https://arxiv.org/abs/2507.12442",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-26T03:19:00.977217+00:00"
  },
  {
    "title": "Silent Inconsistency in Data-Parallel Full Fine-Tuning: Diagnosing Worker-Level Optimization Misalignment",
    "summary": "arXiv:2602.14462v2 Announce Type: replace-cross Abstract: Data-parallel (DP) training with synchronous all-reduce is a dominant paradigm for full-parameter fine-tuning of large language models (LLMs). While parameter synchronization guarantees numerical equivalence of model weights after each iteration, it does not necessarily imply alignment of wo",
    "url": "https://arxiv.org/abs/2602.14462",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-26T03:19:00.977217+00:00"
  },
  {
    "title": "From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production",
    "summary": "arXiv:2602.20558v1 Announce Type: new Abstract: Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate ",
    "url": "https://arxiv.org/abs/2602.20558",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-26T03:19:00.977217+00:00"
  },
  {
    "title": "DMCD: Semantic-Statistical Framework for Causal Discovery",
    "summary": "arXiv:2602.20333v1 Announce Type: new Abstract: We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed pr",
    "url": "https://arxiv.org/abs/2602.20333",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "PyVision-RL: Forging Open Agentic Vision Models via RL",
    "summary": "arXiv:2602.20739v1 Announce Type: new Abstract: Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that",
    "url": "https://arxiv.org/abs/2602.20739",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "Evaluating the Reliability of Digital Forensic Evidence Discovered by Large Language Model: A Case Study",
    "summary": "arXiv:2602.20202v1 Announce Type: cross Abstract: The growing reliance on AI-identified digital evidence raises significant concerns about its reliability, particularly as large language models (LLMs) are increasingly integrated into forensic investigations. This paper proposes a structured framework that automates forensic artifact extraction, ref",
    "url": "https://arxiv.org/abs/2602.20202",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "KnapSpec: Self-Speculative Decoding via Adaptive Layer Selection as a Knapsack Problem",
    "summary": "arXiv:2602.20217v1 Announce Type: cross Abstract: Self-speculative decoding (SSD) accelerates LLM inference by skipping layers to create an efficient draft model, yet existing methods often rely on static heuristics that ignore the dynamic computational overhead of attention in long-context scenarios. We propose KnapSpec, a training-free framework ",
    "url": "https://arxiv.org/abs/2602.20217",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "An Approach to Combining Video and Speech with Large Language Models in Human-Robot Interaction",
    "summary": "arXiv:2602.20219v1 Announce Type: cross Abstract: Interpreting human intent accurately is a central challenge in human-robot interaction (HRI) and a key requirement for achieving more natural and intuitive collaboration between humans and machines. This work presents a novel multimodal HRI framework that combines advanced vision-language models, sp",
    "url": "https://arxiv.org/abs/2602.20219",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "Circuit Tracing in Vision-Language Models: Understanding the Internal Mechanisms of Multimodal Thinking",
    "summary": "arXiv:2602.20330v1 Announce Type: cross Abstract: Vision-language models (VLMs) are powerful but remain opaque black boxes. We introduce the first framework for transparent circuit tracing in VLMs to systematically analyze multimodal reasoning. By utilizing transcoders, attribution graphs, and attention-based methods, we uncover how VLMs hierarchic",
    "url": "https://arxiv.org/abs/2602.20330",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "Protein Language Models Diverge from Natural Language: Comparative Analysis and Improved Inference",
    "summary": "arXiv:2602.20449v1 Announce Type: cross Abstract: Modern Protein Language Models (PLMs) apply transformer-based model architectures from natural language processing to biological sequences, predicting a variety of protein functions and properties. However, protein language has key differences from natural language, such as a rich functional space d",
    "url": "https://arxiv.org/abs/2602.20449",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "Communication-Inspired Tokenization for Structured Image Representations",
    "summary": "arXiv:2602.20731v1 Announce Type: cross Abstract: Discrete image tokenizers have emerged as a key component of modern vision and multimodal systems, providing a sequential interface for transformer-based architectures. However, most existing approaches remain primarily optimized for reconstruction and compression, often yielding tokens that capture",
    "url": "https://arxiv.org/abs/2602.20731",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "CrystaL: Spontaneous Emergence of Visual Latents in MLLMs",
    "summary": "arXiv:2602.20980v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable performance by integrating powerful language backbones with large-scale visual encoders. Among these, latent Chain-of-Thought (CoT) methods enable implicit reasoning in continuous hidden states, facilitating seamless vision-language i",
    "url": "https://arxiv.org/abs/2602.20980",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents",
    "summary": "arXiv:2602.12876v2 Announce Type: replace Abstract: Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remai",
    "url": "https://arxiv.org/abs/2602.12876",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models",
    "summary": "arXiv:2506.03922v2 Announce Type: replace-cross Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant potential to advance a broad range of domains. However, current benchmarks for evaluating MLLMs primarily emphasize general knowledge and vertical step-by-step reasoning typical of STEM disciplines, while overlooking the ",
    "url": "https://arxiv.org/abs/2506.03922",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "K-Function: Joint Pronunciation Transcription and Feedback for Evaluating Kids Language Function",
    "summary": "arXiv:2507.03043v3 Announce Type: replace-cross Abstract: Evaluating young children's language is challenging for automatic speech recognizers due to high-pitched voices, prolonged sounds, and limited data. We introduce K-Function, a framework that combines accurate sub-word transcription with objective, Large Language Model (LLM)-driven scoring. I",
    "url": "https://arxiv.org/abs/2507.03043",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "Characterizing State Space Model and Hybrid Language Model Performance with Long Context",
    "summary": "arXiv:2507.12442v3 Announce Type: replace-cross Abstract: Emerging applications such as AR are driving demands for machine intelligence capable of processing continuous and/or long-context inputs on local devices. However, currently dominant models based on Transformer architecture suffers from the quadratic computational and memory overhead, which",
    "url": "https://arxiv.org/abs/2507.12442",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "Silent Inconsistency in Data-Parallel Full Fine-Tuning: Diagnosing Worker-Level Optimization Misalignment",
    "summary": "arXiv:2602.14462v2 Announce Type: replace-cross Abstract: Data-parallel (DP) training with synchronous all-reduce is a dominant paradigm for full-parameter fine-tuning of large language models (LLMs). While parameter synchronization guarantees numerical equivalence of model weights after each iteration, it does not necessarily imply alignment of wo",
    "url": "https://arxiv.org/abs/2602.14462",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production",
    "summary": "arXiv:2602.20558v1 Announce Type: new Abstract: Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate ",
    "url": "https://arxiv.org/abs/2602.20558",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-25T10:46:39.680616+00:00"
  },
  {
    "title": "DMCD: Semantic-Statistical Framework for Causal Discovery",
    "summary": "arXiv:2602.20333v1 Announce Type: new Abstract: We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed pr",
    "url": "https://arxiv.org/abs/2602.20333",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "PyVision-RL: Forging Open Agentic Vision Models via RL",
    "summary": "arXiv:2602.20739v1 Announce Type: new Abstract: Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that",
    "url": "https://arxiv.org/abs/2602.20739",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "Evaluating the Reliability of Digital Forensic Evidence Discovered by Large Language Model: A Case Study",
    "summary": "arXiv:2602.20202v1 Announce Type: cross Abstract: The growing reliance on AI-identified digital evidence raises significant concerns about its reliability, particularly as large language models (LLMs) are increasingly integrated into forensic investigations. This paper proposes a structured framework that automates forensic artifact extraction, ref",
    "url": "https://arxiv.org/abs/2602.20202",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "KnapSpec: Self-Speculative Decoding via Adaptive Layer Selection as a Knapsack Problem",
    "summary": "arXiv:2602.20217v1 Announce Type: cross Abstract: Self-speculative decoding (SSD) accelerates LLM inference by skipping layers to create an efficient draft model, yet existing methods often rely on static heuristics that ignore the dynamic computational overhead of attention in long-context scenarios. We propose KnapSpec, a training-free framework ",
    "url": "https://arxiv.org/abs/2602.20217",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "An Approach to Combining Video and Speech with Large Language Models in Human-Robot Interaction",
    "summary": "arXiv:2602.20219v1 Announce Type: cross Abstract: Interpreting human intent accurately is a central challenge in human-robot interaction (HRI) and a key requirement for achieving more natural and intuitive collaboration between humans and machines. This work presents a novel multimodal HRI framework that combines advanced vision-language models, sp",
    "url": "https://arxiv.org/abs/2602.20219",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "Circuit Tracing in Vision-Language Models: Understanding the Internal Mechanisms of Multimodal Thinking",
    "summary": "arXiv:2602.20330v1 Announce Type: cross Abstract: Vision-language models (VLMs) are powerful but remain opaque black boxes. We introduce the first framework for transparent circuit tracing in VLMs to systematically analyze multimodal reasoning. By utilizing transcoders, attribution graphs, and attention-based methods, we uncover how VLMs hierarchic",
    "url": "https://arxiv.org/abs/2602.20330",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "Protein Language Models Diverge from Natural Language: Comparative Analysis and Improved Inference",
    "summary": "arXiv:2602.20449v1 Announce Type: cross Abstract: Modern Protein Language Models (PLMs) apply transformer-based model architectures from natural language processing to biological sequences, predicting a variety of protein functions and properties. However, protein language has key differences from natural language, such as a rich functional space d",
    "url": "https://arxiv.org/abs/2602.20449",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "Communication-Inspired Tokenization for Structured Image Representations",
    "summary": "arXiv:2602.20731v1 Announce Type: cross Abstract: Discrete image tokenizers have emerged as a key component of modern vision and multimodal systems, providing a sequential interface for transformer-based architectures. However, most existing approaches remain primarily optimized for reconstruction and compression, often yielding tokens that capture",
    "url": "https://arxiv.org/abs/2602.20731",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "CrystaL: Spontaneous Emergence of Visual Latents in MLLMs",
    "summary": "arXiv:2602.20980v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable performance by integrating powerful language backbones with large-scale visual encoders. Among these, latent Chain-of-Thought (CoT) methods enable implicit reasoning in continuous hidden states, facilitating seamless vision-language i",
    "url": "https://arxiv.org/abs/2602.20980",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents",
    "summary": "arXiv:2602.12876v2 Announce Type: replace Abstract: Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remai",
    "url": "https://arxiv.org/abs/2602.12876",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models",
    "summary": "arXiv:2506.03922v2 Announce Type: replace-cross Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant potential to advance a broad range of domains. However, current benchmarks for evaluating MLLMs primarily emphasize general knowledge and vertical step-by-step reasoning typical of STEM disciplines, while overlooking the ",
    "url": "https://arxiv.org/abs/2506.03922",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "K-Function: Joint Pronunciation Transcription and Feedback for Evaluating Kids Language Function",
    "summary": "arXiv:2507.03043v3 Announce Type: replace-cross Abstract: Evaluating young children's language is challenging for automatic speech recognizers due to high-pitched voices, prolonged sounds, and limited data. We introduce K-Function, a framework that combines accurate sub-word transcription with objective, Large Language Model (LLM)-driven scoring. I",
    "url": "https://arxiv.org/abs/2507.03043",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "Characterizing State Space Model and Hybrid Language Model Performance with Long Context",
    "summary": "arXiv:2507.12442v3 Announce Type: replace-cross Abstract: Emerging applications such as AR are driving demands for machine intelligence capable of processing continuous and/or long-context inputs on local devices. However, currently dominant models based on Transformer architecture suffers from the quadratic computational and memory overhead, which",
    "url": "https://arxiv.org/abs/2507.12442",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "Silent Inconsistency in Data-Parallel Full Fine-Tuning: Diagnosing Worker-Level Optimization Misalignment",
    "summary": "arXiv:2602.14462v2 Announce Type: replace-cross Abstract: Data-parallel (DP) training with synchronous all-reduce is a dominant paradigm for full-parameter fine-tuning of large language models (LLMs). While parameter synchronization guarantees numerical equivalence of model weights after each iteration, it does not necessarily imply alignment of wo",
    "url": "https://arxiv.org/abs/2602.14462",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production",
    "summary": "arXiv:2602.20558v1 Announce Type: new Abstract: Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate ",
    "url": "https://arxiv.org/abs/2602.20558",
    "source": "Arxiv AI",
    "published_at": "2026-02-25T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-25T08:34:37.673672+00:00"
  },
  {
    "title": "Could AI Data Centers Be Moved to Outer Space?",
    "summary": "Massive data centers for generative AI are bad for the Earth. How about launching them into orbit?",
    "url": "https://www.wired.com/story/could-we-put-ai-data-centers-in-space/",
    "source": "Wired AI",
    "published_at": "2026-02-20T12:00:00+00:00",
    "score": 5,
    "archived_at": "2026-02-21T05:04:55.318982+00:00"
  },
  {
    "title": "Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation",
    "summary": "arXiv:2602.16727v1 Announce Type: new Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost l",
    "url": "https://arxiv.org/abs/2602.16727",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy",
    "summary": "arXiv:2602.17229v1 Announce Type: new Abstract: The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional acti",
    "url": "https://arxiv.org/abs/2602.17229",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
    "summary": "arXiv:2602.17560v1 Announce Type: new Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \\textit{(i)} the lack of a unified theoretical framework for g",
    "url": "https://arxiv.org/abs/2602.17560",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "MALLVI: a multi agent framework for integrated generalized robotics manipulation",
    "summary": "arXiv:2602.16898v1 Announce Type: cross Abstract: Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.We present",
    "url": "https://arxiv.org/abs/2602.16898",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "Beyond Pipelines: A Fundamental Study on the Rise of Generative-Retrieval Architectures in Web Research",
    "summary": "arXiv:2602.17450v1 Announce Type: cross Abstract: Web research and practices have evolved significantly over time, offering users diverse and accessible solutions across a wide range of tasks. While advanced concepts such as Web 4.0 have emerged from mature technologies, the introduction of large language models (LLMs) has profoundly influenced bot",
    "url": "https://arxiv.org/abs/2602.17450",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "A Scalable Framework for Evaluating Health Language Models",
    "summary": "arXiv:2503.23339v3 Announce Type: replace Abstract: Large language models (LLMs) have emerged as powerful tools for analyzing complex datasets. Recent studies demonstrate their potential to generate useful, personalized responses when provided with patient-specific health information that encompasses lifestyle, biomarkers, and context. As LLM-drive",
    "url": "https://arxiv.org/abs/2503.23339",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs",
    "summary": "arXiv:2510.09201v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have shown remarkable success, and their multimodal expansions (MLLMs) further unlock capabilities spanning images, videos, and other modalities beyond text. However, despite this shift, prompt optimization approaches, designed to reduce the burden of manual prom",
    "url": "https://arxiv.org/abs/2510.09201",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "SoK: DARPA's AI Cyber Challenge (AIxCC): Competition Design, Architectures, and Lessons Learned",
    "summary": "arXiv:2602.07666v2 Announce Type: replace-cross Abstract: DARPA's AI Cyber Challenge (AIxCC, 2023--2025) is the largest competition to date for building fully autonomous cyber reasoning systems (CRSs) that leverage recent advances in AI -- particularly large language models (LLMs) -- to discover and remediate vulnerabilities in real-world open-sour",
    "url": "https://arxiv.org/abs/2602.07666",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs",
    "summary": "arXiv:2602.08351v2 Announce Type: replace-cross Abstract: Co-optimizing data and model configurations for training LLMs presents a classic chicken-and-egg dilemma: The best training data configuration (e.g., data mixture) for a downstream task depends on the chosen model configuration (e.g., model architecture), and vice versa. However, jointly opt",
    "url": "https://arxiv.org/abs/2602.08351",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction",
    "summary": "arXiv:2602.13294v2 Announce Type: replace-cross Abstract: Evaluating whether Multimodal Large Language Models (MLLMs) genuinely reason about physical dynamics remains challenging. Most existing benchmarks rely on recognition-style protocols such as Visual Question Answering (VQA) and Violation of Expectation (VoE), which can often be answered witho",
    "url": "https://arxiv.org/abs/2602.13294",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages",
    "summary": "arXiv:2602.16832v1 Announce Type: new Abstract: Safety alignment of large language models (LLMs) is mostly evaluated in English and contract-bound, leaving multilingual vulnerabilities understudied. We introduce \\textbf{Indic Jailbreak Robustness (IJR)}, a judge-free benchmark for adversarial safety across 12 Indic and South Asian languages (2.1 Bi",
    "url": "https://arxiv.org/abs/2602.16832",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs",
    "summary": "arXiv:2602.16902v1 Announce Type: new Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability ",
    "url": "https://arxiv.org/abs/2602.16902",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "Narrow fine-tuning erodes safety alignment in vision-language agents",
    "summary": "arXiv:2602.16931v1 Announce Type: new Abstract: Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe em",
    "url": "https://arxiv.org/abs/2602.16931",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs",
    "summary": "arXiv:2602.16935v1 Announce Type: new Abstract: While Large Language Model (LLM) capabilities have scaled, safety guardrails remain largely stateless, treating multi-turn dialogues as a series of disconnected events. This lack of temporal awareness facilitates a \"Safety Gap\" where adversarial tactics, like Crescendo and ActorAttack, slowly bleed ma",
    "url": "https://arxiv.org/abs/2602.16935",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "SourceBench: Can AI Answers Reference Quality Web Sources?",
    "summary": "arXiv:2602.16942v1 Announce Type: new Abstract: Large language models (LLMs) increasingly answer queries by citing web sources, but existing evaluations emphasize answer correctness rather than evidence quality. We introduce SourceBench, a benchmark for measuring the quality of cited web sources across 100 real-world queries spanning informational,",
    "url": "https://arxiv.org/abs/2602.16942",
    "source": "Arxiv AI",
    "published_at": "2026-02-20T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-21T03:09:31.599259+00:00"
  },
  {
    "title": "Fly0: Decoupling Semantic Grounding from Geometric Planning for Zero-Shot Aerial Navigation",
    "summary": "arXiv:2602.15875v1 Announce Type: cross Abstract: Current Visual-Language Navigation (VLN) methodologies face a trade-off between semantic understanding and control precision. While Multimodal Large Language Models (MLLMs) offer superior reasoning, deploying them as low-level controllers leads to high latency, trajectory oscillations, and poor gene",
    "url": "https://arxiv.org/abs/2602.15875",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "Surrogate Modeling for Neutron Transport: A Neural Operator Approach",
    "summary": "arXiv:2602.15890v1 Announce Type: cross Abstract: This work introduces a neural operator based surrogate modeling framework for neutron transport computation. Two architectures, the Deep Operator Network (DeepONet) and the Fourier Neural Operator (FNO), were trained for fixed source problems to learn the mapping from anisotropic neutron sources, Q(",
    "url": "https://arxiv.org/abs/2602.15890",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "Doc-to-LoRA: Learning to Instantly Internalize Contexts",
    "summary": "arXiv:2602.15902v1 Announce Type: cross Abstract: Long input sequences are central to in-context learning, document understanding, and multi-step reasoning of Large Language Models (LLMs). However, the quadratic attention cost of Transformers makes inference memory-intensive and slow. While context distillation (CD) can transfer information into mo",
    "url": "https://arxiv.org/abs/2602.15902",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "EarthSpatialBench: Benchmarking Spatial Reasoning Capabilities of Multimodal LLMs on Earth Imagery",
    "summary": "arXiv:2602.15918v1 Announce Type: cross Abstract: Benchmarking spatial reasoning in multimodal large language models (MLLMs) has attracted growing interest in computer vision due to its importance for embodied AI and other agentic systems that require precise interaction with the physical world. However, spatial reasoning on Earth imagery has lagge",
    "url": "https://arxiv.org/abs/2602.15918",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models",
    "summary": "arXiv:2602.16467v1 Announce Type: cross Abstract: The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination qu",
    "url": "https://arxiv.org/abs/2602.16467",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v2 Announce Type: replace Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decisio",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "Prompt When the Animal is: Temporal Animal Behavior Grounding with Positional Recovery Training",
    "summary": "arXiv:2405.05523v2 Announce Type: replace-cross Abstract: Temporal grounding is crucial in multimodal learning, but it poses challenges when applied to animal behavior data due to the sparsity and uniform distribution of moments. To address these challenges, we propose a novel Positional Recovery Training framework (Port), which prompts the model w",
    "url": "https://arxiv.org/abs/2405.05523",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision",
    "summary": "arXiv:2508.08177v3 Announce Type: replace-cross Abstract: Accurately grounding regions of interest (ROIs) is critical for diagnosis and treatment planning in medical imaging. While multimodal large language models (MLLMs) combine visual perception with natural language, current medical-grounding pipelines still rely on supervised fine-tuning with e",
    "url": "https://arxiv.org/abs/2508.08177",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "Predicting Training Re-evaluation Curves Enables Effective Data Curriculums for LLMs",
    "summary": "arXiv:2509.25380v2 Announce Type: replace-cross Abstract: Data curriculums have become central to successful LLM training, yet principles governing optimal data placement remain unclear. We introduce the *training re-evaluation curve (TREC)*, a diagnostic that retrospectively evaluates training batches *using the final model weights*. The TREC char",
    "url": "https://arxiv.org/abs/2509.25380",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "CreativityPrism: A Holistic Evaluation Framework for Large Language Model Creativity",
    "summary": "arXiv:2510.20091v2 Announce Type: replace-cross Abstract: Creativity is often seen as a hallmark of human intelligence. While large language models (LLMs) are increasingly perceived as generating creative text, there is still no holistic and scalable framework to evaluate their creativity across diverse scenarios. Existing methods of LLM creativity",
    "url": "https://arxiv.org/abs/2510.20091",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models",
    "summary": "arXiv:2602.15689v2 Announce Type: replace-cross Abstract: Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. ",
    "url": "https://arxiv.org/abs/2602.15689",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "How Uncertain Is the Grade? A Benchmark of Uncertainty Metrics for LLM-Based Automatic Assessment",
    "summary": "arXiv:2602.16039v1 Announce Type: new Abstract: The rapid rise of large language models (LLMs) is reshaping the landscape of automatic assessment in education. While these systems demonstrate substantial advantages in adaptability to diverse question types and flexibility in output formats, they also introduce new challenges related to output uncer",
    "url": "https://arxiv.org/abs/2602.16039",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "GPSBench: Do Large Language Models Understand GPS Coordinates?",
    "summary": "arXiv:2602.16105v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly deployed in applications that interact with the physical world, such as navigation, robotics, or mapping, making robust geospatial reasoning a critical capability. Despite that, LLMs' ability to reason about GPS coordinates and real-world geography remains",
    "url": "https://arxiv.org/abs/2602.16105",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "Toward Scalable Verifiable Reward: Proxy State-Based Evaluation for Multi-turn Tool-Calling LLM Agents",
    "summary": "arXiv:2602.16246v1 Announce Type: new Abstract: Interactive large language model (LLM) agents operating via multi-turn dialogue and multi-step tool calling are increasingly used in production. Benchmarks for these agents must both reliably compare models and yield on-policy training data. Prior agentic benchmarks (e.g., tau-bench, tau2-bench, AppWo",
    "url": "https://arxiv.org/abs/2602.16246",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
    "summary": "arXiv:2602.16653v1 Announce Type: new Abstract: Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigati",
    "url": "https://arxiv.org/abs/2602.16653",
    "source": "Arxiv AI",
    "published_at": "2026-02-19T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-20T03:18:27.367288+00:00"
  },
  {
    "title": "How Vision Becomes Language: A Layer-wise Information-Theoretic Analysis of Multimodal Reasoning",
    "summary": "arXiv:2602.15580v1 Announce Type: new Abstract: When a multimodal Transformer answers a visual question, is the prediction driven by visual evidence, linguistic reasoning, or genuinely fused cross-modal computation -- and how does this structure evolve across layers? We address this question with a layer-wise framework based on Partial Information ",
    "url": "https://arxiv.org/abs/2602.15580",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "TokaMind: A Multi-Modal Transformer Foundation Model for Tokamak Plasma Dynamics",
    "summary": "arXiv:2602.15084v1 Announce Type: cross Abstract: We present TokaMind, an open-source foundation model framework for fusion plasma modeling, based on a Multi-Modal Transformer (MMT) and trained on heterogeneous tokamak diagnostics from the publicly available MAST dataset. TokaMind supports multiple data modalities (time-series, 2D profiles, and vid",
    "url": "https://arxiv.org/abs/2602.15084",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "Improving MLLMs in Embodied Exploration and Question Answering with Human-Inspired Memory Modeling",
    "summary": "arXiv:2602.15513v1 Announce Type: cross Abstract: Deploying Multimodal Large Language Models as the brain of embodied agents remains challenging, particularly under long-horizon observations and limited context budgets. Existing memory assisted methods often rely on textual summaries, which discard rich visual and spatial details and remain brittle",
    "url": "https://arxiv.org/abs/2602.15513",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models",
    "summary": "arXiv:2602.15689v1 Announce Type: cross Abstract: Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. As a res",
    "url": "https://arxiv.org/abs/2602.15689",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models",
    "summary": "arXiv:2602.15758v1 Announce Type: cross Abstract: While Multimodal Large Language Models (MLLMs) perform strongly on single-turn chart generation, their ability to support real-world exploratory data analysis remains underexplored. In practice, users iteratively refine visualizations through multi-turn interactions that require maintaining common g",
    "url": "https://arxiv.org/abs/2602.15758",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "Decision Quality Evaluation Framework at Pinterest",
    "summary": "arXiv:2602.15809v1 Announce Type: cross Abstract: Online platforms require robust systems to enforce content safety policies at scale. A critical component of these systems is the ability to evaluate the quality of moderation decisions made by both human agents and Large Language Models (LLMs). However, this evaluation is challenging due to the inh",
    "url": "https://arxiv.org/abs/2602.15809",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "Prover Agent: An Agent-Based Framework for Formal Mathematical Proofs",
    "summary": "arXiv:2506.19923v5 Announce Type: replace Abstract: We present Prover Agent, a novel AI agent for automated theorem proving that integrates large language models (LLMs) with a formal proof assistant, Lean. Prover Agent coordinates an informal reasoning LLM, a formal prover model, and feedback from Lean while also generating auxiliary lemmas. These ",
    "url": "https://arxiv.org/abs/2506.19923",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models",
    "summary": "arXiv:2508.00576v2 Announce Type: replace Abstract: Multimodal AI models have achieved impressive performance in tasks that require integrating information from multiple modalities, such as vision and language. However, their \"black-box\" nature poses a major barrier to deployment in high-stakes applications where interpretability and trustworthines",
    "url": "https://arxiv.org/abs/2508.00576",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces",
    "summary": "arXiv:2511.07587v2 Announce Type: replace Abstract: Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions, which h",
    "url": "https://arxiv.org/abs/2511.07587",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents",
    "summary": "arXiv:2601.15311v3 Announce Type: replace Abstract: Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the \"Lost in the Middle\" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily \"Flat RAG\" architectures relying on vector databas",
    "url": "https://arxiv.org/abs/2601.15311",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "Sparse Autoencoders for Sequential Recommendation Models: Interpretation and Flexible Control",
    "summary": "arXiv:2507.12202v2 Announce Type: replace-cross Abstract: Many current state-of-the-art models for sequential recommendations are based on transformer architectures. Interpretation and explanation of such black box models is an important research question, as a better understanding of their internals can help understand, influence, and control thei",
    "url": "https://arxiv.org/abs/2507.12202",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "LogiPart: Local Large Language Models for Data Exploration at Scale with Logical Partitioning",
    "summary": "arXiv:2509.22211v3 Announce Type: replace-cross Abstract: The discovery of deep, steerable taxonomies in large text corpora is currently restricted by a trade-off between the surface-level efficiency of topic models and the prohibitive, non-scalable assignment costs of LLM-integrated frameworks. We introduce \\textbf{LogiPart}, a scalable, hypothesi",
    "url": "https://arxiv.org/abs/2509.22211",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "Text-Guided Layer Fusion Mitigates Hallucination in Multimodal LLMs",
    "summary": "arXiv:2601.03100v2 Announce Type: replace-cross Abstract: Multimodal large language models (MLLMs) typically rely on a single late-layer feature from a frozen vision encoder, leaving the encoder's rich hierarchy of visual cues under-utilized. MLLMs still suffer from visually ungrounded hallucinations, often relying on language priors rather than im",
    "url": "https://arxiv.org/abs/2601.03100",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "Annotation-Efficient Vision-Language Model Adaptation to the Polish Language Using the LLaVA Framework",
    "summary": "arXiv:2602.14073v2 Announce Type: replace-cross Abstract: Most vision-language models (VLMs) are trained on English-centric data, limiting their performance in other languages and cultural contexts. This restricts their usability for non-English-speaking users and hinders the development of multimodal systems that reflect diverse linguistic and cul",
    "url": "https://arxiv.org/abs/2602.14073",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "Protecting Language Models Against Unauthorized Distillation through Trace Rewriting",
    "summary": "arXiv:2602.15143v1 Announce Type: new Abstract: Knowledge distillation is a widely adopted technique for transferring capabilities from LLMs to smaller, more efficient student models. However, unauthorized use of knowledge distillation takes unfair advantage of the considerable effort and cost put into developing frontier models. We investigate met",
    "url": "https://arxiv.org/abs/2602.15143",
    "source": "Arxiv AI",
    "published_at": "2026-02-18T05:00:00+00:00",
    "score": 6,
    "archived_at": "2026-02-19T03:23:14.974735+00:00"
  },
  {
    "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
    "summary": "arXiv:2602.13226v1 Announce Type: new Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and pra",
    "url": "https://arxiv.org/abs/2602.13226",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
    "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-for",
    "url": "https://arxiv.org/abs/2602.13232",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
    "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a un",
    "url": "https://arxiv.org/abs/2602.13258",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol",
    "summary": "arXiv:2602.13320v1 Announce Type: new Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context P",
    "url": "https://arxiv.org/abs/2602.13320",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization",
    "summary": "arXiv:2602.13653v1 Announce Type: new Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for dat",
    "url": "https://arxiv.org/abs/2602.13653",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
    "summary": "arXiv:2602.13680v1 Announce Type: new Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that ",
    "url": "https://arxiv.org/abs/2602.13680",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models",
    "summary": "arXiv:2602.13904v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three disti",
    "url": "https://arxiv.org/abs/2602.13904",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v1 Announce Type: new Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision ma",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
    "summary": "arXiv:2602.13933v1 Announce Type: new Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical det",
    "url": "https://arxiv.org/abs/2602.13933",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
    "summary": "arXiv:2602.14130v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provi",
    "url": "https://arxiv.org/abs/2602.14130",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
    "summary": "arXiv:2602.14457v1 Announce Type: new Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the ",
    "url": "https://arxiv.org/abs/2602.14457",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
    "summary": "arXiv:2602.14518v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conf",
    "url": "https://arxiv.org/abs/2602.14518",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization",
    "summary": "arXiv:2602.13210v1 Announce Type: cross Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex enviro",
    "url": "https://arxiv.org/abs/2602.13210",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "Global AI Bias Audit for Technical Governance",
    "summary": "arXiv:2602.13246v1 Announce Type: cross Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical ",
    "url": "https://arxiv.org/abs/2602.13246",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs",
    "summary": "arXiv:2602.13289v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessi",
    "url": "https://arxiv.org/abs/2602.13289",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-18T03:24:47.532428+00:00"
  },
  {
    "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
    "summary": "arXiv:2602.13226v1 Announce Type: new Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and pra",
    "url": "https://arxiv.org/abs/2602.13226",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
    "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-for",
    "url": "https://arxiv.org/abs/2602.13232",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
    "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a un",
    "url": "https://arxiv.org/abs/2602.13258",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol",
    "summary": "arXiv:2602.13320v1 Announce Type: new Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context P",
    "url": "https://arxiv.org/abs/2602.13320",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization",
    "summary": "arXiv:2602.13653v1 Announce Type: new Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for dat",
    "url": "https://arxiv.org/abs/2602.13653",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
    "summary": "arXiv:2602.13680v1 Announce Type: new Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that ",
    "url": "https://arxiv.org/abs/2602.13680",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models",
    "summary": "arXiv:2602.13904v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three disti",
    "url": "https://arxiv.org/abs/2602.13904",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v1 Announce Type: new Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision ma",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
    "summary": "arXiv:2602.13933v1 Announce Type: new Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical det",
    "url": "https://arxiv.org/abs/2602.13933",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
    "summary": "arXiv:2602.14130v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provi",
    "url": "https://arxiv.org/abs/2602.14130",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
    "summary": "arXiv:2602.14457v1 Announce Type: new Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the ",
    "url": "https://arxiv.org/abs/2602.14457",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
    "summary": "arXiv:2602.14518v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conf",
    "url": "https://arxiv.org/abs/2602.14518",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization",
    "summary": "arXiv:2602.13210v1 Announce Type: cross Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex enviro",
    "url": "https://arxiv.org/abs/2602.13210",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Global AI Bias Audit for Technical Governance",
    "summary": "arXiv:2602.13246v1 Announce Type: cross Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical ",
    "url": "https://arxiv.org/abs/2602.13246",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs",
    "summary": "arXiv:2602.13289v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessi",
    "url": "https://arxiv.org/abs/2602.13289",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T12:05:31.636494+00:00"
  },
  {
    "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
    "summary": "arXiv:2602.13226v1 Announce Type: new Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and pra",
    "url": "https://arxiv.org/abs/2602.13226",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
    "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-for",
    "url": "https://arxiv.org/abs/2602.13232",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
    "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a un",
    "url": "https://arxiv.org/abs/2602.13258",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol",
    "summary": "arXiv:2602.13320v1 Announce Type: new Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context P",
    "url": "https://arxiv.org/abs/2602.13320",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization",
    "summary": "arXiv:2602.13653v1 Announce Type: new Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for dat",
    "url": "https://arxiv.org/abs/2602.13653",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
    "summary": "arXiv:2602.13680v1 Announce Type: new Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that ",
    "url": "https://arxiv.org/abs/2602.13680",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models",
    "summary": "arXiv:2602.13904v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three disti",
    "url": "https://arxiv.org/abs/2602.13904",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v1 Announce Type: new Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision ma",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
    "summary": "arXiv:2602.13933v1 Announce Type: new Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical det",
    "url": "https://arxiv.org/abs/2602.13933",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
    "summary": "arXiv:2602.14130v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provi",
    "url": "https://arxiv.org/abs/2602.14130",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
    "summary": "arXiv:2602.14457v1 Announce Type: new Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the ",
    "url": "https://arxiv.org/abs/2602.14457",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
    "summary": "arXiv:2602.14518v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conf",
    "url": "https://arxiv.org/abs/2602.14518",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization",
    "summary": "arXiv:2602.13210v1 Announce Type: cross Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex enviro",
    "url": "https://arxiv.org/abs/2602.13210",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Global AI Bias Audit for Technical Governance",
    "summary": "arXiv:2602.13246v1 Announce Type: cross Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical ",
    "url": "https://arxiv.org/abs/2602.13246",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs",
    "summary": "arXiv:2602.13289v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessi",
    "url": "https://arxiv.org/abs/2602.13289",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:47:35.904354+00:00"
  },
  {
    "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
    "summary": "arXiv:2602.13226v1 Announce Type: new Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and pra",
    "url": "https://arxiv.org/abs/2602.13226",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
    "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-for",
    "url": "https://arxiv.org/abs/2602.13232",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
    "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a un",
    "url": "https://arxiv.org/abs/2602.13258",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol",
    "summary": "arXiv:2602.13320v1 Announce Type: new Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context P",
    "url": "https://arxiv.org/abs/2602.13320",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization",
    "summary": "arXiv:2602.13653v1 Announce Type: new Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for dat",
    "url": "https://arxiv.org/abs/2602.13653",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
    "summary": "arXiv:2602.13680v1 Announce Type: new Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that ",
    "url": "https://arxiv.org/abs/2602.13680",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models",
    "summary": "arXiv:2602.13904v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three disti",
    "url": "https://arxiv.org/abs/2602.13904",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v1 Announce Type: new Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision ma",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
    "summary": "arXiv:2602.13933v1 Announce Type: new Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical det",
    "url": "https://arxiv.org/abs/2602.13933",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
    "summary": "arXiv:2602.14130v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provi",
    "url": "https://arxiv.org/abs/2602.14130",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
    "summary": "arXiv:2602.14457v1 Announce Type: new Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the ",
    "url": "https://arxiv.org/abs/2602.14457",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
    "summary": "arXiv:2602.14518v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conf",
    "url": "https://arxiv.org/abs/2602.14518",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization",
    "summary": "arXiv:2602.13210v1 Announce Type: cross Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex enviro",
    "url": "https://arxiv.org/abs/2602.13210",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Global AI Bias Audit for Technical Governance",
    "summary": "arXiv:2602.13246v1 Announce Type: cross Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical ",
    "url": "https://arxiv.org/abs/2602.13246",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs",
    "summary": "arXiv:2602.13289v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessi",
    "url": "https://arxiv.org/abs/2602.13289",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:38:27.188850+00:00"
  },
  {
    "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
    "summary": "arXiv:2602.13226v1 Announce Type: new Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and pra",
    "url": "https://arxiv.org/abs/2602.13226",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
    "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-for",
    "url": "https://arxiv.org/abs/2602.13232",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
    "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a un",
    "url": "https://arxiv.org/abs/2602.13258",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol",
    "summary": "arXiv:2602.13320v1 Announce Type: new Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context P",
    "url": "https://arxiv.org/abs/2602.13320",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization",
    "summary": "arXiv:2602.13653v1 Announce Type: new Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for dat",
    "url": "https://arxiv.org/abs/2602.13653",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
    "summary": "arXiv:2602.13680v1 Announce Type: new Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that ",
    "url": "https://arxiv.org/abs/2602.13680",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models",
    "summary": "arXiv:2602.13904v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three disti",
    "url": "https://arxiv.org/abs/2602.13904",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v1 Announce Type: new Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision ma",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
    "summary": "arXiv:2602.13933v1 Announce Type: new Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical det",
    "url": "https://arxiv.org/abs/2602.13933",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
    "summary": "arXiv:2602.14130v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provi",
    "url": "https://arxiv.org/abs/2602.14130",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
    "summary": "arXiv:2602.14457v1 Announce Type: new Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the ",
    "url": "https://arxiv.org/abs/2602.14457",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
    "summary": "arXiv:2602.14518v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conf",
    "url": "https://arxiv.org/abs/2602.14518",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization",
    "summary": "arXiv:2602.13210v1 Announce Type: cross Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex enviro",
    "url": "https://arxiv.org/abs/2602.13210",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Global AI Bias Audit for Technical Governance",
    "summary": "arXiv:2602.13246v1 Announce Type: cross Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical ",
    "url": "https://arxiv.org/abs/2602.13246",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs",
    "summary": "arXiv:2602.13289v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessi",
    "url": "https://arxiv.org/abs/2602.13289",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:30:15.987190+00:00"
  },
  {
    "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
    "summary": "arXiv:2602.13226v1 Announce Type: new Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and pra",
    "url": "https://arxiv.org/abs/2602.13226",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
    "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-for",
    "url": "https://arxiv.org/abs/2602.13232",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
    "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a un",
    "url": "https://arxiv.org/abs/2602.13258",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol",
    "summary": "arXiv:2602.13320v1 Announce Type: new Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context P",
    "url": "https://arxiv.org/abs/2602.13320",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization",
    "summary": "arXiv:2602.13653v1 Announce Type: new Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for dat",
    "url": "https://arxiv.org/abs/2602.13653",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
    "summary": "arXiv:2602.13680v1 Announce Type: new Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that ",
    "url": "https://arxiv.org/abs/2602.13680",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models",
    "summary": "arXiv:2602.13904v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three disti",
    "url": "https://arxiv.org/abs/2602.13904",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v1 Announce Type: new Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision ma",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
    "summary": "arXiv:2602.13933v1 Announce Type: new Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical det",
    "url": "https://arxiv.org/abs/2602.13933",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
    "summary": "arXiv:2602.14130v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provi",
    "url": "https://arxiv.org/abs/2602.14130",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
    "summary": "arXiv:2602.14457v1 Announce Type: new Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the ",
    "url": "https://arxiv.org/abs/2602.14457",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
    "summary": "arXiv:2602.14518v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conf",
    "url": "https://arxiv.org/abs/2602.14518",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization",
    "summary": "arXiv:2602.13210v1 Announce Type: cross Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex enviro",
    "url": "https://arxiv.org/abs/2602.13210",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Global AI Bias Audit for Technical Governance",
    "summary": "arXiv:2602.13246v1 Announce Type: cross Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical ",
    "url": "https://arxiv.org/abs/2602.13246",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs",
    "summary": "arXiv:2602.13289v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessi",
    "url": "https://arxiv.org/abs/2602.13289",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T11:18:45.294314+00:00"
  },
  {
    "title": "Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection",
    "summary": "arXiv:2602.13226v1 Announce Type: new Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and pra",
    "url": "https://arxiv.org/abs/2602.13226",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading",
    "summary": "arXiv:2602.13232v1 Announce Type: new Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-for",
    "url": "https://arxiv.org/abs/2602.13232",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems",
    "summary": "arXiv:2602.13258v1 Announce Type: new Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a un",
    "url": "https://arxiv.org/abs/2602.13258",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol",
    "summary": "arXiv:2602.13320v1 Announce Type: new Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context P",
    "url": "https://arxiv.org/abs/2602.13320",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization",
    "summary": "arXiv:2602.13653v1 Announce Type: new Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for dat",
    "url": "https://arxiv.org/abs/2602.13653",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "AllMem: A Memory-centric Recipe for Efficient Long-context Modeling",
    "summary": "arXiv:2602.13680v1 Announce Type: new Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \\textsc{AllMem}, a novel and efficient hybrid architecture that ",
    "url": "https://arxiv.org/abs/2602.13680",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models",
    "summary": "arXiv:2602.13904v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three disti",
    "url": "https://arxiv.org/abs/2602.13904",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design",
    "summary": "arXiv:2602.13912v1 Announce Type: new Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision ma",
    "url": "https://arxiv.org/abs/2602.13912",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling",
    "summary": "arXiv:2602.13933v1 Announce Type: new Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical det",
    "url": "https://arxiv.org/abs/2602.13933",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity",
    "summary": "arXiv:2602.14130v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provi",
    "url": "https://arxiv.org/abs/2602.14130",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
    "summary": "arXiv:2602.14457v1 Announce Type: new Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the ",
    "url": "https://arxiv.org/abs/2602.14457",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning",
    "summary": "arXiv:2602.14518v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conf",
    "url": "https://arxiv.org/abs/2602.14518",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization",
    "summary": "arXiv:2602.13210v1 Announce Type: cross Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex enviro",
    "url": "https://arxiv.org/abs/2602.13210",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Global AI Bias Audit for Technical Governance",
    "summary": "arXiv:2602.13246v1 Announce Type: cross Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical ",
    "url": "https://arxiv.org/abs/2602.13246",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs",
    "summary": "arXiv:2602.13289v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessi",
    "url": "https://arxiv.org/abs/2602.13289",
    "source": "Arxiv AI",
    "published_at": "2026-02-17T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T10:48:45.936071+00:00"
  },
  {
    "title": "TriGen: NPU Architecture for End-to-End Acceleration of Large Language Models based on SW-HW Co-Design",
    "summary": "arXiv:2602.12962v1 Announce Type: cross Abstract: Recent studies have extensively explored NPU architectures for accelerating AI inference in on-device environments, which are inherently resource-constrained. Meanwhile, transformer-based large language models (LLMs) have become dominant, with rapidly increasing model sizes but low degree of paramet",
    "url": "https://arxiv.org/abs/2602.12962",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Intent-Driven Smart Manufacturing Integrating Knowledge Graphs and Large Language Models",
    "summary": "arXiv:2602.12419v1 Announce Type: new Abstract: The increasing complexity of smart manufacturing environments demands interfaces that can translate high-level human intents into machine-executable actions. This paper presents a unified framework that integrates instruction-tuned Large Language Models (LLMs) with ontology-aligned Knowledge Graphs (K",
    "url": "https://arxiv.org/abs/2602.12419",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents",
    "summary": "arXiv:2602.12876v1 Announce Type: new Abstract: Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain li",
    "url": "https://arxiv.org/abs/2602.12876",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "A Lightweight LLM Framework for Disaster Humanitarian Information Classification",
    "summary": "arXiv:2602.12284v1 Announce Type: cross Abstract: Timely classification of humanitarian information from social media is critical for effective disaster response. However, deploying large language models (LLMs) for this task faces challenges in resource-constrained emergency settings. This paper develops a lightweight, cost-effective framework for ",
    "url": "https://arxiv.org/abs/2602.12284",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "RankLLM: Weighted Ranking of LLMs by Quantifying Question Difficulty",
    "summary": "arXiv:2602.12424v1 Announce Type: cross Abstract: Benchmarks establish a standardized evaluation framework to systematically assess the performance of large language models (LLMs), facilitating objective comparisons and driving advancements in the field. However, existing benchmarks fail to differentiate question difficulty, limiting their ability ",
    "url": "https://arxiv.org/abs/2602.12424",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Self-EvolveRec: Self-Evolving Recommender Systems with LLM-based Directional Feedback",
    "summary": "arXiv:2602.12612v1 Announce Type: cross Abstract: Traditional methods for automating recommender system design, such as Neural Architecture Search (NAS), are often constrained by a fixed search space defined by human priors, limiting innovation to pre-defined operators. While recent LLM-driven code evolution frameworks shift fixed search space targ",
    "url": "https://arxiv.org/abs/2602.12612",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Vision Token Reduction via Attention-Driven Self-Compression for Efficient Multimodal Large Language Models",
    "summary": "arXiv:2602.12618v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) incur significant computational cost from processing numerous vision tokens through all LLM layers. Prior pruning methods operate either before the LLM, limiting generality due to diverse encoder-projector designs or within the LLM using heuristics that are i",
    "url": "https://arxiv.org/abs/2602.12618",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Artic: AI-oriented Real-time Communication for MLLM Video Assistant",
    "summary": "arXiv:2602.12641v1 Announce Type: cross Abstract: AI Video Assistant emerges as a new paradigm for Real-time Communication (RTC), where one peer is a Multimodal Large Language Model (MLLM) deployed in the cloud. This makes interaction between humans and AI more intuitive, akin to chatting with a real person. However, a fundamental mismatch exists b",
    "url": "https://arxiv.org/abs/2602.12641",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "RADAR: Revealing Asymmetric Development of Abilities in MLLM Pre-training",
    "summary": "arXiv:2602.12892v1 Announce Type: cross Abstract: Pre-trained Multi-modal Large Language Models (MLLMs) provide a knowledge-rich foundation for post-training by leveraging their inherent perception and reasoning capabilities to solve complex tasks. However, the lack of an efficient evaluation framework impedes the diagnosis of their performance bot",
    "url": "https://arxiv.org/abs/2602.12892",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Know More, Know Clearer: A Meta-Cognitive Framework for Knowledge Augmentation in Large Language Models",
    "summary": "arXiv:2602.12996v1 Announce Type: cross Abstract: Knowledge augmentation has significantly enhanced the performance of Large Language Models (LLMs) in knowledge-intensive tasks. However, existing methods typically operate on the simplistic premise that model performance equates with internal knowledge, overlooking the knowledge-confidence gaps that",
    "url": "https://arxiv.org/abs/2602.12996",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Buy versus Build an LLM: A Decision Framework for Governments",
    "summary": "arXiv:2602.13033v1 Announce Type: cross Abstract: Large Language Models (LLMs) represent a new frontier of digital infrastructure that can support a wide range of public-sector applications, from general purpose citizen services to specialized and sensitive state functions. When expanding AI access, governments face a set of strategic choices over ",
    "url": "https://arxiv.org/abs/2602.13033",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Asynchronous Verified Semantic Caching for Tiered LLM Architectures",
    "summary": "arXiv:2602.13165v1 Announce Type: cross Abstract: Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential for reducing inference cost and latency. Production deployments typically use a tiered static-dynamic design: a static cache of curated, offline vetted responses ",
    "url": "https://arxiv.org/abs/2602.13165",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Difficulty-Aware Agentic Orchestration for Query-Specific Multi-Agent Workflows",
    "summary": "arXiv:2509.11079v5 Announce Type: replace Abstract: Large Language Model (LLM)-based agentic systems have shown strong capabilities across various tasks. However, existing multi-agent frameworks often rely on static or task-level workflows, which either over-process simple queries or underperform on complex ones, while also neglecting the efficienc",
    "url": "https://arxiv.org/abs/2509.11079",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "LTSM-Bundle: A Toolbox and Benchmark on Large Language Models for Time Series Forecasting",
    "summary": "arXiv:2406.14045v3 Announce Type: replace-cross Abstract: Time Series Forecasting (TSF) has long been a challenge in time series analysis. Inspired by the success of Large Language Models (LLMs), researchers are now developing Large Time Series Models (LTSMs)-universal transformer-based models that use autoregressive prediction-to improve TSF. Howe",
    "url": "https://arxiv.org/abs/2406.14045",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders",
    "summary": "arXiv:2507.03262v4 Announce Type: replace-cross Abstract: Recent multimodal large language models (MLLMs) increasingly integrate multiple vision encoders to improve performance on various benchmarks, assuming that diverse pretraining objectives yield complementary visual signals. However, we show this assumption often fails in practice. Through sys",
    "url": "https://arxiv.org/abs/2507.03262",
    "source": "Arxiv AI",
    "published_at": "2026-02-16T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-17T03:20:57.359668+00:00"
  },
  {
    "title": "MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling",
    "summary": "arXiv:2602.11761v1 Announce Type: cross Abstract: The evolution of large language models (LLMs) towards applications with ultra-long contexts faces challenges posed by the high computational and memory costs of the Transformer architecture. While existing sparse and linear attention mechanisms attempt to mitigate these issues, they typically involv",
    "url": "https://arxiv.org/abs/2602.11761",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Bi-Level Prompt Optimization for Multimodal LLM-as-a-Judge",
    "summary": "arXiv:2602.11340v1 Announce Type: new Abstract: Large language models (LLMs) have become widely adopted as automated judges for evaluating AI-generated content. Despite their success, aligning LLM-based evaluations with human judgments remains challenging. While supervised fine-tuning on human-labeled data can improve alignment, it is costly and in",
    "url": "https://arxiv.org/abs/2602.11340",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Credit Where It is Due: Cross-Modality Connectivity Drives Precise Reinforcement Learning for MLLM Reasoning",
    "summary": "arXiv:2602.11455v1 Announce Type: new Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), yet how visual evidence is integrated during reasoning remains poorly understood. We explore multimodal RLVR through the lens of cross-modal attentio",
    "url": "https://arxiv.org/abs/2602.11455",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery",
    "summary": "arXiv:2602.11609v1 Announce Type: new Abstract: We present scPilot, the first systematic framework to practice omics-native reasoning: a large language model (LLM) converses in natural language while directly inspecting single-cell RNA-seq data and on-demand bioinformatics tools. scPilot converts core single-cell analyses, i.e., cell-type annotatio",
    "url": "https://arxiv.org/abs/2602.11609",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Do MLLMs Really Understand Space? A Mathematical Reasoning Evaluation",
    "summary": "arXiv:2602.11635v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) have achieved strong performance on perception-oriented tasks, yet their ability to perform mathematical spatial reasoning, defined as the capacity to parse and manipulate two- and three-dimensional relations, remains unclear. Humans easily solve textbook-style",
    "url": "https://arxiv.org/abs/2602.11635",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "PhyNiKCE: A Neurosymbolic Agentic Framework for Autonomous Computational Fluid Dynamics",
    "summary": "arXiv:2602.11666v1 Announce Type: new Abstract: The deployment of autonomous agents for Computational Fluid Dynamics (CFD), is critically limited by the probabilistic nature of Large Language Models (LLMs), which struggle to enforce the strict conservation laws and numerical stability required for physics-based simulations. Reliance on purely seman",
    "url": "https://arxiv.org/abs/2602.11666",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Benchmark Health Index: A Systematic Framework for Benchmarking the Benchmarks of LLMs",
    "summary": "arXiv:2602.11674v1 Announce Type: new Abstract: Large Language Models (LLMs) are advancing rapidly, yet the benchmarks used to measure this progress are becoming increasingly unreliable. Score inflation and selective reporting have eroded the authority of standard benchmarks, leaving the community uncertain about which evaluation results remain tru",
    "url": "https://arxiv.org/abs/2602.11674",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Beyond Pixels: Vector-to-Graph Transformation for Reliable Schematic Auditing",
    "summary": "arXiv:2602.11678v1 Announce Type: new Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable progress in visual understanding, yet they suffer from a critical limitation: structural blindness. Even state-of-the-art models fail to capture topology and symbolic logic in engineering schematics, as their pixel-driven paradigm discards",
    "url": "https://arxiv.org/abs/2602.11678",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs",
    "summary": "arXiv:2602.11729v1 Announce Type: new Abstract: Model diffing, the process of comparing models' internal representations to identify their differences, is a promising approach for uncovering safety-critical behaviors in new models. However, its application has so far been primarily focused on comparing a base model with its finetune. Since new LLM ",
    "url": "https://arxiv.org/abs/2602.11729",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Prototype Transformer: Towards Language Model Architectures Interpretable by Design",
    "summary": "arXiv:2602.11852v1 Announce Type: new Abstract: While state-of-the-art language models (LMs) surpass the vast majority of humans in certain domains, their reasoning remains largely opaque, undermining trust in their output. Furthermore, while autoregressive LMs can output explicit reasoning, their true reasoning process is opaque, which introduces ",
    "url": "https://arxiv.org/abs/2602.11852",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "HybridRAG: A Practical LLM-based ChatBot Framework based on Pre-Generated Q&A over Raw Unstructured Documents",
    "summary": "arXiv:2602.11156v1 Announce Type: cross Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for grounding Large Language Model (LLM)-based chatbot responses on external knowledge. However, existing RAG studies typically assume well-structured textual sources (e.g. Wikipedia or curated datasets) and perform retrieval an",
    "url": "https://arxiv.org/abs/2602.11156",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "The Energy of Falsehood: Detecting Hallucinations via Diffusion Model Likelihoods",
    "summary": "arXiv:2602.11364v1 Announce Type: cross Abstract: Large Language Models (LLMs) frequently hallucinate plausible but incorrect assertions, a vulnerability often missed by uncertainty metrics when models are confidently wrong. We propose DiffuTruth, an unsupervised framework that reconceptualizes fact verification via non equilibrium thermodynamics, ",
    "url": "https://arxiv.org/abs/2602.11364",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Multimodal Fact-Level Attribution for Verifiable Reasoning",
    "summary": "arXiv:2602.11509v1 Announce Type: cross Abstract: Multimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual factual claims. However, existing multimodal groundi",
    "url": "https://arxiv.org/abs/2602.11509",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception",
    "summary": "arXiv:2602.11858v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception, where decisive evidence is small and easily overwhelmed by global context. Recent \"Thinking-with-Images\" methods alleviate this by iteratively zooming in and out regions of i",
    "url": "https://arxiv.org/abs/2602.11858",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "DeepSight: An All-in-One LM Safety Toolkit",
    "summary": "arXiv:2602.12092v1 Announce Type: cross Abstract: As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluatio",
    "url": "https://arxiv.org/abs/2602.12092",
    "source": "Arxiv AI",
    "published_at": "2026-02-13T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-14T03:14:31.327080+00:00"
  },
  {
    "title": "Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle",
    "summary": "arXiv:2508.05612v4 Announce Type: replace-cross Abstract: Reinforcement learning (RL) has emerged as an effective post-training paradigm for enhancing the reasoning capabilities of multimodal large language model (MLLM). However, current RL pipelines often suffer from training inefficiencies caused by two underexplored issues: Advantage Collapsing,",
    "url": "https://arxiv.org/abs/2508.05612",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Discovering Differences in Strategic Behavior Between Humans and LLMs",
    "summary": "arXiv:2602.10324v1 Announce Type: new Abstract: As Large Language Models (LLMs) are increasingly deployed in social and strategic scenarios, it becomes critical to understand where and why their behavior diverges from that of humans. While behavioral game theory (BGT) provides a framework for analyzing behavior, existing models do not fully capture",
    "url": "https://arxiv.org/abs/2602.10324",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "AgentTrace: A Structured Logging Framework for Agent System Observability",
    "summary": "arXiv:2602.10133v1 Announce Type: cross Abstract: Despite the growing capabilities of autonomous agents powered by large language models (LLMs), their adoption in high-stakes domains remains limited. A key barrier is security: the inherently nondeterministic behavior of LLM agents defies static auditing approaches that have historically underpinned",
    "url": "https://arxiv.org/abs/2602.10133",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Anonymization-Enhanced Privacy Protection for Mobile GUI Agents: Available but Invisible",
    "summary": "arXiv:2602.10139v1 Announce Type: cross Abstract: Mobile Graphical User Interface (GUI) agents have demonstrated strong capabilities in automating complex smartphone tasks by leveraging multimodal large language models (MLLMs) and system-level control interfaces. However, this paradigm introduces significant privacy risks, as agents typically captu",
    "url": "https://arxiv.org/abs/2602.10139",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "PRISM-XR: Empowering Privacy-Aware XR Collaboration with Multimodal Large Language Models",
    "summary": "arXiv:2602.10154v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) enhance collaboration in Extended Reality (XR) environments by enabling flexible object and animation creation through the combination of natural language and visual inputs. However, visual data captured by XR headsets includes real-world backgrounds that may",
    "url": "https://arxiv.org/abs/2602.10154",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Omni-Safety under Cross-Modality Conflict: Vulnerabilities, Dynamics Mechanisms and Efficient Alignment",
    "summary": "arXiv:2602.10161v1 Announce Type: cross Abstract: Omni-modal Large Language Models (OLLMs) greatly expand LLMs' multimodal capabilities but also introduce cross-modal safety risks. However, a systematic understanding of vulnerabilities in omni-modal interactions remains lacking. To bridge this gap, we establish a modality-semantics decoupling princ",
    "url": "https://arxiv.org/abs/2602.10161",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Self-Evolving Recommendation System: End-To-End Autonomous Model Optimization With LLM Agents",
    "summary": "arXiv:2602.10226v1 Announce Type: cross Abstract: Optimizing large-scale machine learning systems, such as recommendation models for global video platforms, requires navigating a massive hyperparameter search space and, more critically, designing sophisticated optimizers, architectures, and reward functions to capture nuanced user behaviors. Achiev",
    "url": "https://arxiv.org/abs/2602.10226",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "C^2ROPE: Causal Continuous Rotary Positional Encoding for 3D Large Multimodal-Models Reasoning",
    "summary": "arXiv:2602.10551v1 Announce Type: cross Abstract: Recent advances in 3D Large Multimodal Models (LMMs) built on Large Language Models (LLMs) have established the alignment of 3D visual features with LLM representations as the dominant paradigm. However, the inherited Rotary Position Embedding (RoPE) introduces limitations for multimodal processing.",
    "url": "https://arxiv.org/abs/2602.10551",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Contrastive Learning for Multi Label ECG Classification with Jaccard Score Based Sigmoid Loss",
    "summary": "arXiv:2602.10553v1 Announce Type: cross Abstract: Recent advances in large language models (LLMs) have enabled the development of multimodal medical AI. While models such as MedGemini achieve high accuracy on VQA tasks like USMLE MM, their performance on ECG based tasks remains limited, and some models, such as MedGemma, do not support ECG data at ",
    "url": "https://arxiv.org/abs/2602.10553",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning",
    "summary": "arXiv:2602.10575v1 Announce Type: cross Abstract: Metaphorical comprehension in images remains a critical challenge for Nowadays AI systems. While Multimodal Large Language Models (MLLMs) excel at basic Visual Question Answering (VQA), they consistently struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visua",
    "url": "https://arxiv.org/abs/2602.10575",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization",
    "summary": "arXiv:2602.10576v1 Announce Type: cross Abstract: Symbolic regression aims to distill mathematical equations from observational data. Recent approaches have successfully leveraged Large Language Models (LLMs) to generate equation hypotheses, capitalizing on their vast pre-trained scientific priors. However, existing frameworks predominantly treat t",
    "url": "https://arxiv.org/abs/2602.10576",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Flow caching for autoregressive video generation",
    "summary": "arXiv:2602.10825v1 Announce Type: cross Abstract: Autoregressive models, often built on Transformer architectures, represent a powerful paradigm for generating ultra-long videos by synthesizing content in sequential chunks. However, this sequential generation process is notoriously slow. While caching strategies have proven effective for accelerati",
    "url": "https://arxiv.org/abs/2602.10825",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Fine-Tuning GPT-5 for GPU Kernel Generation",
    "summary": "arXiv:2602.11000v1 Announce Type: cross Abstract: Developing efficient GPU kernels is essential for scaling modern AI systems, yet it remains a complex task due to intricate hardware architectures and the need for specialized optimization expertise. Although Large Language Models (LLMs) demonstrate strong capabilities in general sequential code gen",
    "url": "https://arxiv.org/abs/2602.11000",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Synthetic Homes: An Accessible Multimodal Pipeline for Producing Residential Building Data with Generative AI",
    "summary": "arXiv:2509.09794v2 Announce Type: replace Abstract: Computational models have emerged as powerful tools for energy modeling research, touting scalability and quantitative results. However, these models require a plethora of data, some of which can be inaccessible, expensive, or can raise privacy concerns. We introduce a modular multimodal framework",
    "url": "https://arxiv.org/abs/2509.09794",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility",
    "summary": "arXiv:2602.03402v2 Announce Type: replace Abstract: Vision language models (VLMs) extend the reasoning capabilities of large language models (LLMs) to cross-modal settings, yet remain highly vulnerable to multimodal jailbreak attacks. Existing defenses predominantly rely on safety fine-tuning or aggressive token manipulations, incurring substantial",
    "url": "https://arxiv.org/abs/2602.03402",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-13T03:29:04.573309+00:00"
  },
  {
    "title": "TernaryLM: Memory-Efficient Language Modeling via Native 1-Bit Quantization with Adaptive Layer-wise Scaling",
    "summary": "arXiv:2602.07374v1 Announce Type: cross Abstract: Large language models (LLMs) achieve remarkable performance but demand substantial computational resources, limiting deployment on edge devices and resource-constrained environments. We present TernaryLM, a 132M parameter transformer architecture that employs native 1-bit ternary quantization {-1, 0",
    "url": "https://arxiv.org/abs/2602.07374",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs",
    "summary": "arXiv:2602.08621v1 Announce Type: cross Abstract: By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior w",
    "url": "https://arxiv.org/abs/2602.08621",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 8,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning",
    "summary": "arXiv:2602.07543v2 Announce Type: new Abstract: Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based ",
    "url": "https://arxiv.org/abs/2602.07543",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Towards Adaptive, Scalable, and Robust Coordination of LLM Agents: A Dynamic Ad-Hoc Networking Perspective",
    "summary": "arXiv:2602.08009v1 Announce Type: new Abstract: Multi-agent architectures built on large language models (LLMs) have demonstrated the potential to realize swarm intelligence through well-crafted collaboration. However, the substantial burden of manual orchestration inherently raises an imperative to automate the design of agentic workflows. We fram",
    "url": "https://arxiv.org/abs/2602.08009",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation",
    "summary": "arXiv:2602.08229v1 Announce Type: new Abstract: The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation ",
    "url": "https://arxiv.org/abs/2602.08229",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs",
    "summary": "arXiv:2602.08241v1 Announce Type: new Abstract: While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis show",
    "url": "https://arxiv.org/abs/2602.08241",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis",
    "summary": "arXiv:2602.08276v1 Announce Type: new Abstract: Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concep",
    "url": "https://arxiv.org/abs/2602.08276",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI",
    "summary": "arXiv:2602.08373v1 Announce Type: new Abstract: Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repa",
    "url": "https://arxiv.org/abs/2602.08373",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "PRISM: A Principled Framework for Multi-Agent Reasoning via Gain Decomposition",
    "summary": "arXiv:2602.08586v2 Announce Type: new Abstract: Multi-agent collaboration has emerged as a promising paradigm for enhancing reasoning capabilities of Large Language Models (LLMs). However, existing approaches remain largely heuristic, lacking principled guidance on what drives performance gains and how to systematically optimize multi-agent reasoni",
    "url": "https://arxiv.org/abs/2602.08586",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture",
    "summary": "arXiv:2602.08597v1 Announce Type: new Abstract: Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal i",
    "url": "https://arxiv.org/abs/2602.08597",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "iGRPO: Self-Feedback-Driven LLM Reasoning",
    "summary": "arXiv:2602.09000v1 Announce Type: new Abstract: Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability",
    "url": "https://arxiv.org/abs/2602.09000",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing",
    "summary": "arXiv:2602.07045v1 Announce Type: cross Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for co",
    "url": "https://arxiv.org/abs/2602.07045",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Extended to Reality: Prompt Injection in 3D Environments",
    "summary": "arXiv:2602.07104v1 Announce Type: cross Abstract: Multimodal large language models (MLLMs) have advanced the capabilities to interpret and act on visual input in 3D environments, empowering diverse applications such as robotics and situated conversational agents. When MLLMs reason over camera-captured views of the physical world, a new attack surfa",
    "url": "https://arxiv.org/abs/2602.07104",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  },
  {
    "title": "Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models",
    "summary": "arXiv:2602.07106v1 Announce Type: cross Abstract: Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-",
    "url": "https://arxiv.org/abs/2602.07106",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00",
    "score": 7,
    "archived_at": "2026-02-12T03:28:41.934855+00:00"
  }
]