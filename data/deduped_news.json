[
  {
    "title": "Loyalty Is Dead in Silicon Valley",
    "summary": "Founders used to be wedded to their companies. Now, anyone can be lured away for the right price.",
    "url": "https://www.wired.com/story/model-behavior-loyalty-is-dead-in-silicon-valley/",
    "source": "Wired AI",
    "published_at": "2026-02-05T21:33:11+00:00"
  },
  {
    "title": "ICE and CBP’s Face-Recognition App Can’t Actually Verify Who People Are",
    "summary": "ICE has used Mobile Fortify to identify immigrants and citizens alike over 100,000 times, by one estimate. It wasn't built to work like that—and only got approved after DHS abandoned its own privacy rules.",
    "url": "https://www.wired.com/story/cbp-ice-dhs-mobile-fortify-face-recognition-verify-identity/",
    "source": "Wired AI",
    "published_at": "2026-02-05T20:28:34+00:00"
  },
  {
    "title": "Knowledge Model Prompting Increases LLM Performance on Planning Tasks",
    "summary": "arXiv:2602.03900v1 Announce Type: new Abstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into quest",
    "url": "https://arxiv.org/abs/2602.03900",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation",
    "summary": "arXiv:2602.03950v1 Announce Type: new Abstract: Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems hav",
    "url": "https://arxiv.org/abs/2602.03950",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent",
    "summary": "arXiv:2602.03955v1 Announce Type: new Abstract: While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights",
    "url": "https://arxiv.org/abs/2602.03955",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Active Epistemic Control for Query-Efficient Verified Planning",
    "summary": "arXiv:2602.03974v1 Announce Type: new Abstract: Planning in interactive environments is challenging under partial observability: task-critical preconditions (e.g., object locations or container states) may be unknown at decision time, yet grounding them through interaction is costly. Learned world models can cheaply predict missing facts, but predi",
    "url": "https://arxiv.org/abs/2602.03974",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure",
    "summary": "arXiv:2602.03975v1 Announce Type: new Abstract: Test-time computation has become a primary driver of progress in large language model (LLM) reasoning, but it is increasingly bottlenecked by expensive verification. In many reasoning systems, a large fraction of verifier calls are spent on redundant or unpromising intermediate hypotheses. We study re",
    "url": "https://arxiv.org/abs/2602.03975",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning",
    "summary": "arXiv:2602.03978v1 Announce Type: new Abstract: As Large Reasoning Models (LRMs) are increasingly deployed, auditing their chain-of-thought (CoT) traces for safety becomes critical. Recent work has reported that monitorability--the degree to which CoT faithfully and informatively reflects internal computation--can appear as a \"free gift\" during the",
    "url": "https://arxiv.org/abs/2602.03978",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "When AI Persuades: Adversarial Explanation Attacks on Human Trust in AI-Assisted Decision Making",
    "summary": "arXiv:2602.04003v1 Announce Type: new Abstract: Most adversarial threats in artificial intelligence target the computational behavior of models rather than the humans who rely on them. Yet modern AI systems increasingly operate within human decision loops, where users interpret and act on model recommendations. Large Language Models generate fluent",
    "url": "https://arxiv.org/abs/2602.04003",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Axiomatic Foundations of Counterfactual Explanations",
    "summary": "arXiv:2602.04028v1 Announce Type: new Abstract: Explaining autonomous and intelligent systems is critical in order to improve trust in their decisions. Counterfactuals have emerged as one of the most compelling forms of explanation. They address ``why not'' questions by revealing how decisions could be altered. Despite the growing literature, most ",
    "url": "https://arxiv.org/abs/2602.04028",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL",
    "summary": "arXiv:2602.04089v1 Announce Type: new Abstract: Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, f",
    "url": "https://arxiv.org/abs/2602.04089",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Interfaze: The Future of AI is built on Task-Specific Small Models",
    "summary": "arXiv:2602.04101v1 Announce Type: new Abstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OC",
    "url": "https://arxiv.org/abs/2602.04101",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows",
    "summary": "arXiv:2602.04144v1 Announce Type: new Abstract: Data incompleteness severely impedes the reliability of multimodal systems. Existing reconstruction methods face distinct bottlenecks: conventional parametric/generative models are prone to hallucinations due to over-reliance on internal memory, while retrieval-augmented frameworks struggle with retri",
    "url": "https://arxiv.org/abs/2602.04144",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Steering LLMs via Scalable Interactive Oversight",
    "summary": "arXiv:2602.04210v1 Announce Type: new Abstract: As Large Language Models increasingly automate complex, long-horizon tasks such as \\emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and",
    "url": "https://arxiv.org/abs/2602.04210",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons",
    "summary": "arXiv:2602.04213v1 Announce Type: new Abstract: Imitation learning has shown success in many tasks by learning from expert demonstrations. However, most existing work relies on large-scale demonstrations from technical professionals and close monitoring of the training process. These are challenging for a layperson when they want to teach the agent",
    "url": "https://arxiv.org/abs/2602.04213",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search",
    "summary": "arXiv:2602.04248v1 Announce Type: new Abstract: Inference-time scaling strategies, particularly Monte Carlo Tree Search (MCTS), have significantly enhanced the reasoning capabilities of Large Language Models (LLMs). However, current approaches remain predominantly stateless, discarding successful reasoning patterns after each problem instance and f",
    "url": "https://arxiv.org/abs/2602.04248",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning",
    "summary": "arXiv:2602.04284v1 Announce Type: new Abstract: Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To th",
    "url": "https://arxiv.org/abs/2602.04284",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "From Assumptions to Actions: Turning LLM Reasoning into Uncertainty-Aware Planning for Embodied Agents",
    "summary": "arXiv:2602.04326v1 Announce Type: new Abstract: Embodied agents operating in multi-agent, partially observable, and decentralized environments must plan and act despite pervasive uncertainty about hidden objects and collaborators' intentions. Recent advances in applying Large Language Models (LLMs) to embodied agents have addressed many long-standi",
    "url": "https://arxiv.org/abs/2602.04326",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Digital Twins & ZeroConf AI: Structuring Automated Intelligent Pipelines for Industrial Applications",
    "summary": "arXiv:2602.04385v1 Announce Type: new Abstract: The increasing complexity of Cyber-Physical Systems (CPS), particularly in the industrial domain, has amplified the challenges associated with the effective integration of Artificial Intelligence (AI) and Machine Learning (ML) techniques. Fragmentation across IoT and IIoT technologies, manifested thro",
    "url": "https://arxiv.org/abs/2602.04385",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "ReThinker: Scientific Reasoning by Rethinking with Guided Reflection and Confidence Control",
    "summary": "arXiv:2602.04496v1 Announce Type: new Abstract: Expert-level scientific reasoning remains challenging for large language models, particularly on benchmarks such as Humanity's Last Exam (HLE), where rigid tool pipelines, brittle multi-agent coordination, and inefficient test-time scaling often limit performance. We introduce ReThinker, a confidence-",
    "url": "https://arxiv.org/abs/2602.04496",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "From Competition to Collaboration: Designing Sustainable Mechanisms Between LLMs and Online Forums",
    "summary": "arXiv:2602.04572v1 Announce Type: new Abstract: While Generative AI (GenAI) systems draw users away from (Q&amp;A) forums, they also depend on the very data those forums produce to improve their performance. Addressing this paradox, we propose a framework of sequential interaction, in which a GenAI system proposes questions to a forum that can publ",
    "url": "https://arxiv.org/abs/2602.04572",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration",
    "summary": "arXiv:2602.04575v2 Announce Type: new Abstract: For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fu",
    "url": "https://arxiv.org/abs/2602.04575",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning",
    "summary": "arXiv:2602.04634v1 Announce Type: new Abstract: Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In thi",
    "url": "https://arxiv.org/abs/2602.04634",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents",
    "summary": "arXiv:2602.04813v1 Announce Type: new Abstract: Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consis",
    "url": "https://arxiv.org/abs/2602.04813",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Are AI Capabilities Increasing Exponentially? A Competing Hypothesis",
    "summary": "arXiv:2602.04836v1 Announce Type: new Abstract: Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation & Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the data",
    "url": "https://arxiv.org/abs/2602.04836",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing",
    "summary": "arXiv:2602.04837v1 Announce Type: new Abstract: Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improveme",
    "url": "https://arxiv.org/abs/2602.04837",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Fluid Representations in Reasoning Models",
    "summary": "arXiv:2602.04843v1 Announce Type: new Abstract: Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model",
    "url": "https://arxiv.org/abs/2602.04843",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Merged ChemProt-DrugProt for Relation Extraction from Biomedical Literature",
    "summary": "arXiv:2405.18605v2 Announce Type: cross Abstract: The extraction of chemical-gene relations plays a pivotal role in understanding the intricate interactions between chemical compounds and genes, with significant implications for drug discovery, disease understanding, and biomedical research. This paper presents a data set created by merging the Che",
    "url": "https://arxiv.org/abs/2405.18605",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "HybridQuestion: Human-AI Collaboration for Identifying High-Impact Research Questions",
    "summary": "arXiv:2602.03849v1 Announce Type: cross Abstract: The \"AI Scientist\" paradigm is transforming scientific research by automating key stages of the research process, from idea generation to scholarly writing. This shift is expected to accelerate discovery and expand the scope of scientific inquiry. However, a key question remains unclear: can AI scie",
    "url": "https://arxiv.org/abs/2602.03849",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "WebAccessVL: Making an Accessible Web via Violation-Conditioned VLM",
    "summary": "arXiv:2602.03850v1 Announce Type: cross Abstract: We present a vision-language model (VLM) that automatically edits website HTML to address Web Content Accessibility Guidelines 2 (WCAG2) violations. We formulate this as a supervised image-conditioned program synthesis task, where the model learns to correct HTML given the HTML and its rendering. We",
    "url": "https://arxiv.org/abs/2602.03850",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Perceptions of AI-CBT: Trust and Barriers in Chinese Postgrads",
    "summary": "arXiv:2602.03852v1 Announce Type: cross Abstract: The mental well-being of graduate students is an increasing concern, yet the adoption of scalable support remains uneven. Artificial intelligence-powered cognitive behavioral therapy chatbots (AI-CBT) offer low barrier help, but little is known about how Chinese postgraduates perceive and use them. ",
    "url": "https://arxiv.org/abs/2602.03852",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "PaperX: A Unified Framework for Multimodal Academic Presentation Generation with Scholar DAG",
    "summary": "arXiv:2602.03866v2 Announce Type: cross Abstract: Transforming scientific papers into multimodal presentation content is essential for research dissemination but remains labor intensive. Existing automated solutions typically treat each format as an isolated downstream task, leading to redundant processing and semantic inconsistency. We introduce P",
    "url": "https://arxiv.org/abs/2602.03866",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Benchmarking Automatic Speech Recognition for Indian Languages in Agricultural Contexts",
    "summary": "arXiv:2602.03868v1 Announce Type: cross Abstract: The digitization of agricultural advisory services in India requires robust Automatic Speech Recognition (ASR) systems capable of accurately transcribing domain-specific terminology in multiple Indian languages. This paper presents a benchmarking framework for evaluating ASR performance in agricultu",
    "url": "https://arxiv.org/abs/2602.03868",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Understanding the Impact of Differentially Private Training on Memorization of Long-Tailed Data",
    "summary": "arXiv:2602.03872v1 Announce Type: cross Abstract: Recent research shows that modern deep learning models achieve high predictive accuracy partly by memorizing individual training samples. Such memorization raises serious privacy concerns, motivating the widespread adoption of differentially private training algorithms such as DP-SGD. However, a gro",
    "url": "https://arxiv.org/abs/2602.03872",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Decoding Ambiguous Emotions with Test-Time Scaling in Audio-Language Models",
    "summary": "arXiv:2602.03873v1 Announce Type: cross Abstract: Emotion recognition from human speech is a critical enabler for socially aware conversational AI. However, while most prior work frames emotion recognition as a categorical classification problem, real-world affective states are often ambiguous, overlapping, and context-dependent, posing significant",
    "url": "https://arxiv.org/abs/2602.03873",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Reversible Deep Learning for 13C NMR in Chemoinformatics: On Structures and Spectra",
    "summary": "arXiv:2602.03875v2 Announce Type: cross Abstract: We introduce a reversible deep learning model for 13C NMR that uses a single conditional invertible neural network for both directions between molecular structures and spectra. The network is built from i-RevNet style bijective blocks, so the forward map and its inverse are available by construction",
    "url": "https://arxiv.org/abs/2602.03875",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "GOPO: Policy Optimization using Ranked Rewards",
    "summary": "arXiv:2602.03876v1 Announce Type: cross Abstract: Standard reinforcement learning from human feedback (RLHF) trains a reward model on pairwise preference data and then uses it for policy optimization. However, while reward models are optimized to capture relative preferences, existing policy optimization techniques rely on absolute reward magnitude",
    "url": "https://arxiv.org/abs/2602.03876",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "TruKAN: Towards More Efficient Kolmogorov-Arnold Networks Using Truncated Power Functions",
    "summary": "arXiv:2602.03879v1 Announce Type: cross Abstract: To address the trade-off between computational efficiency and adherence to Kolmogorov-Arnold Network (KAN) principles, we propose TruKAN, a new architecture based on the KAN structure and learnable activation functions. TruKAN replaces the B-spline basis in KAN with a family of truncated power funct",
    "url": "https://arxiv.org/abs/2602.03879",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "DiGAN: Diffusion-Guided Attention Network for Early Alzheimer's Disease Detection",
    "summary": "arXiv:2602.03881v1 Announce Type: cross Abstract: Early diagnosis of Alzheimer's disease (AD) remains a major challenge due to the subtle and temporally irregular progression of structural brain changes in the prodromal stages. Existing deep learning approaches require large longitudinal datasets and often fail to model the temporal continuity and ",
    "url": "https://arxiv.org/abs/2602.03881",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "PriorProbe: Recovering Individual-Level Priors for Personalizing Neural Networks in Facial Expression Recognition",
    "summary": "arXiv:2602.03882v1 Announce Type: cross Abstract: Incorporating individual-level cognitive priors offers an important route to personalizing neural networks, yet accurately eliciting such priors remains challenging: existing methods either fail to uniquely identify them or introduce systematic biases. Here, we introduce PriorProbe, a novel elicitat",
    "url": "https://arxiv.org/abs/2602.03882",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Explainable Computer Vision Framework for Automated Pore Detection and Criticality Assessment in Additive Manufacturing",
    "summary": "arXiv:2602.03883v1 Announce Type: cross Abstract: Internal porosity remains a critical defect mode in additively manufactured components, compromising structural performance and limiting industrial adoption. Automated defect detection methods exist but lack interpretability, preventing engineers from understanding the physical basis of criticality ",
    "url": "https://arxiv.org/abs/2602.03883",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Sounding Highlights: Dual-Pathway Audio Encoders for Audio-Visual Video Highlight Detection",
    "summary": "arXiv:2602.03891v2 Announce Type: cross Abstract: Audio-visual video highlight detection aims to automatically identify the most salient moments in videos by leveraging both visual and auditory cues. However, existing models often underutilize the audio modality, focusing on high-level semantic features while failing to fully leverage the rich, dyn",
    "url": "https://arxiv.org/abs/2602.03891",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Audit After Segmentation: Reference-Free Mask Quality Assessment for Language-Referred Audio-Visual Segmentation",
    "summary": "arXiv:2602.03892v1 Announce Type: cross Abstract: Language-referred audio-visual segmentation (Ref-AVS) aims to segment target objects described by natural language by jointly reasoning over video, audio, and text. Beyond generating segmentation masks, providing rich and interpretable diagnoses of mask quality remains largely underexplored. In this",
    "url": "https://arxiv.org/abs/2602.03892",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Vision Transformers for Zero-Shot Clustering of Animal Images: A Comparative Benchmarking Study",
    "summary": "arXiv:2602.03894v1 Announce Type: cross Abstract: Manual labeling of animal images remains a significant bottleneck in ecological research, limiting the scale and efficiency of biodiversity monitoring efforts. This study investigates whether state-of-the-art Vision Transformer (ViT) foundation models can reduce thousands of unlabeled animal images ",
    "url": "https://arxiv.org/abs/2602.03894",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Byzantine Machine Learning: MultiKrum and an optimal notion of robustness",
    "summary": "arXiv:2602.03899v1 Announce Type: cross Abstract: Aggregation rules are the cornerstone of distributed (or federated) learning in the presence of adversaries, under the so-called Byzantine threat model. They are also interesting mathematical objects from the point of view of robust mean estimation. The Krum aggregation rule has been extensively stu",
    "url": "https://arxiv.org/abs/2602.03899",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "All-Atom GPCR-Ligand Simulation via Residual Isometric Latent Flow",
    "summary": "arXiv:2602.03902v1 Announce Type: cross Abstract: G-protein-coupled receptors (GPCRs), primary targets for over one-third of approved therapeutics, rely on intricate conformational transitions to transduce signals. While Molecular Dynamics (MD) is essential for elucidating this transduction process, particularly within ligand-bound complexes, conve",
    "url": "https://arxiv.org/abs/2602.03902",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "GeoIB: Geometry-Aware Information Bottleneck via Statistical-Manifold Compression",
    "summary": "arXiv:2602.03906v1 Announce Type: cross Abstract: Information Bottleneck (IB) is widely used, but in deep learning, it is usually implemented through tractable surrogates, such as variational bounds or neural mutual information (MI) estimators, rather than directly controlling the MI I(X;Z) itself. The looseness and estimator-dependent bias can mak",
    "url": "https://arxiv.org/abs/2602.03906",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "HY3D-Bench: Generation of 3D Assets",
    "summary": "arXiv:2602.03907v1 Announce Type: cross Abstract: While recent advances in neural representations and generative models have revolutionized 3D content creation, the field remains constrained by significant data processing bottlenecks. To address this, we introduce HY3D-Bench, an open-source ecosystem designed to establish a unified, high-quality fo",
    "url": "https://arxiv.org/abs/2602.03907",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Entropy-Aware Structural Alignment for Zero-Shot Handwritten Chinese Character Recognition",
    "summary": "arXiv:2602.03913v1 Announce Type: cross Abstract: Zero-shot Handwritten Chinese Character Recognition (HCCR) aims to recognize unseen characters by leveraging radical-based semantic compositions. However, existing approaches often treat characters as flat radical sequences, neglecting the hierarchical topology and the uneven information density of ",
    "url": "https://arxiv.org/abs/2602.03913",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Phaedra: Learning High-Fidelity Discrete Tokenization for the Physical Science",
    "summary": "arXiv:2602.03915v1 Announce Type: cross Abstract: Tokens are discrete representations that allow modern deep learning to scale by transforming high-dimensional data into sequences that can be efficiently learned, generated, and generalized to new tasks. These have become foundational for image and video generation and, more recently, physical simul",
    "url": "https://arxiv.org/abs/2602.03915",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "SpecMD: A Comprehensive Study On Speculative Expert Prefetching",
    "summary": "arXiv:2602.03921v1 Announce Type: cross Abstract: Mixture-of-Experts (MoE) models enable sparse expert activation, meaning that only a subset of the model's parameters is used during each inference. However, to translate this sparsity into practical performance, an expert caching mechanism is required. Previous works have proposed hardware-centric ",
    "url": "https://arxiv.org/abs/2602.03921",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "WIND: Weather Inverse Diffusion for Zero-Shot Atmospheric Modeling",
    "summary": "arXiv:2602.03924v1 Announce Type: cross Abstract: Deep learning has revolutionized weather and climate modeling, yet the current landscape remains fragmented: highly specialized models are typically trained individually for distinct tasks. To unify this landscape, we introduce WIND, a single pre-trained foundation model capable of replacing special",
    "url": "https://arxiv.org/abs/2602.03924",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "First-Principles AI finds crystallization of fractional quantum Hall liquids",
    "summary": "arXiv:2602.03927v1 Announce Type: cross Abstract: When does a fractional quantum Hall (FQH) liquid crystallize? Addressing this question requires a framework that treats fractionalization and crystallization on equal footing, especially in strong Landau-level mixing regime. Here, we introduce MagNet, a self-attention neural-network variational wave",
    "url": "https://arxiv.org/abs/2602.03927",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Linguistic Blind Spots in Clinical Decision Extraction",
    "summary": "arXiv:2602.03942v1 Announce Type: cross Abstract: Extracting medical decisions from clinical notes is a key step for clinical decision support and patient-facing care summaries. We study how the linguistic characteristics of clinical decisions vary across decision categories and whether these differences explain extraction failures. Using MedDec di",
    "url": "https://arxiv.org/abs/2602.03942",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Semantic Rate Distortion and Posterior Design: Compute Constraints, Multimodality, and Strategic Inference",
    "summary": "arXiv:2602.03949v1 Announce Type: cross Abstract: We study strategic Gaussian semantic compression under rate and compute constraints, where an encoder and decoder optimize distinct quadratic objectives. A latent Gaussian state generates a task dependent semantic variable, and the decoder best responds via MMSE estimation, reducing the encoder's pr",
    "url": "https://arxiv.org/abs/2602.03949",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Structural shifts in institutional participation and collaboration within the AI arXiv preprint research ecosystem",
    "summary": "arXiv:2602.03969v1 Announce Type: cross Abstract: The emergence of large language models (LLMs) represents a significant technological shift within the scientific ecosystem, particularly within the field of artificial intelligence (AI). This paper examines structural changes in the AI research landscape using a dataset of arXiv preprints (cs.AI) fr",
    "url": "https://arxiv.org/abs/2602.03969",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Fixed Budget is No Harder Than Fixed Confidence in Best-Arm Identification up to Logarithmic Factors",
    "summary": "arXiv:2602.03972v1 Announce Type: cross Abstract: The best-arm identification (BAI) problem is one of the most fundamental problems in interactive machine learning, which has two flavors: the fixed-budget setting (FB) and the fixed-confidence setting (FC). For $K$-armed bandits with the unique best arm, the optimal sample complexities for both sett",
    "url": "https://arxiv.org/abs/2602.03972",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Transformers perform adaptive partial pooling",
    "summary": "arXiv:2602.03980v1 Announce Type: cross Abstract: Because language is creative, any reasonable language model must generalize, deciding what to say in novel contexts by using information from similar contexts. But what about contexts that are not novel but merely infrequent? In hierarchical regression, the model's predictions for behavior in a cont",
    "url": "https://arxiv.org/abs/2602.03980",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "DeXposure-FM: A Time-series, Graph Foundation Model for Credit Exposures and Stability on Decentralized Financial Networks",
    "summary": "arXiv:2602.03981v1 Announce Type: cross Abstract: Credit exposure in Decentralized Finance (DeFi) is often implicit and token-mediated, creating a dense web of inter-protocol dependencies. Thus, a shock to one token may result in significant and uncontrolled contagion effects. As the DeFi ecosystem becomes increasingly linked with traditional finan",
    "url": "https://arxiv.org/abs/2602.03981",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "When Chains of Thought Don't Matter: Causal Bypass in Large Language Models",
    "summary": "arXiv:2602.03994v1 Announce Type: cross Abstract: Chain-of-thought (CoT) prompting is widely assumed to expose a model's reasoning process and improve transparency. We attempted to enforce this assumption by penalizing unfaithful reasoning, but found that surface-level compliance does not guarantee causal reliance. Our central finding is negative: ",
    "url": "https://arxiv.org/abs/2602.03994",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Rational ANOVA Networks",
    "summary": "arXiv:2602.04006v1 Announce Type: cross Abstract: Deep neural networks typically treat nonlinearities as fixed primitives (e.g., ReLU), limiting both interpretability and the granularity of control over the induced function class. While recent additive models (like KANs) attempt to address this using splines, they often suffer from computational in",
    "url": "https://arxiv.org/abs/2602.04006",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "PromptSplit: Revealing Prompt-Level Disagreement in Generative Models",
    "summary": "arXiv:2602.04009v1 Announce Type: cross Abstract: Prompt-guided generative AI models have rapidly expanded across vision and language domains, producing realistic and diverse outputs from textual inputs. The growing variety of such models, trained with different data and architectures, calls for principled methods to identify which types of prompts",
    "url": "https://arxiv.org/abs/2602.04009",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Understanding and Guiding Layer Placement in Parameter-Efficient Fine-Tuning of Large Language Models",
    "summary": "arXiv:2602.04019v1 Announce Type: cross Abstract: As large language models (LLMs) continue to grow, the cost of full-parameter fine-tuning has made parameter-efficient fine-tuning (PEFT) the default strategy for downstream adaptation. Constraints from inference latency in scalable serving and fine-tuning cost in edge or rapid-deployment settings ma",
    "url": "https://arxiv.org/abs/2602.04019",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "PluRel: Synthetic Data unlocks Scaling Laws for Relational Foundation Models",
    "summary": "arXiv:2602.04029v1 Announce Type: cross Abstract: Relational Foundation Models (RFMs) facilitate data-driven decision-making by learning from complex multi-table databases. However, the diverse relational databases needed to train such models are rarely public due to privacy constraints. While there are methods to generate synthetic tabular data of",
    "url": "https://arxiv.org/abs/2602.04029",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "On the Credibility of Evaluating LLMs using Survey Questions",
    "summary": "arXiv:2602.04033v1 Announce Type: cross Abstract: Recent studies evaluate the value orientation of large language models (LLMs) using adapted social surveys, typically by prompting models with survey questions and comparing their responses to average human responses. This paper identifies limitations in this methodology that, depending on the exact",
    "url": "https://arxiv.org/abs/2602.04033",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Principles of Lipschitz continuity in neural networks",
    "summary": "arXiv:2602.04078v1 Announce Type: cross Abstract: Deep learning has achieved remarkable success across a wide range of domains, significantly expanding the frontiers of what is achievable in artificial intelligence. Yet, despite these advances, critical challenges remain -- most notably, ensuring robustness to small input perturbations and generali",
    "url": "https://arxiv.org/abs/2602.04078",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Structure-Informed Estimation for Pilot-Limited MIMO Channels via Tensor Decomposition",
    "summary": "arXiv:2602.04083v1 Announce Type: cross Abstract: Channel estimation in wideband multiple-input multiple-output (MIMO) systems faces fundamental pilot overhead limitations in high-dimensional beyond-5G and sixth-generation (6G) scenarios. This paper presents a hybrid tensor-neural architecture that formulates pilot-limited channel estimation as low",
    "url": "https://arxiv.org/abs/2602.04083",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "A computational account of dreaming: learning and memory consolidation",
    "summary": "arXiv:2602.04095v1 Announce Type: cross Abstract: A number of studies have concluded that dreaming is mostly caused by randomly arriving internal signals because \"dream contents are random impulses\", and argued that dream sleep is unlikely to play an important part in our intellectual capacity. On the contrary, numerous functional studies have reve",
    "url": "https://arxiv.org/abs/2602.04095",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "DMS2F-HAD: A Dual-branch Mamba-based Spatial-Spectral Fusion Network for Hyperspectral Anomaly Detection",
    "summary": "arXiv:2602.04102v1 Announce Type: cross Abstract: Hyperspectral anomaly detection (HAD) aims to identify rare and irregular targets in high-dimensional hyperspectral images (HSIs), which are often noisy and unlabelled data. Existing deep learning methods either fail to capture long-range spectral dependencies (e.g., convolutional neural networks) o",
    "url": "https://arxiv.org/abs/2602.04102",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Tinker Tales: Supporting Child-AI Collaboration through Co-Creative Storytelling with Educational Scaffolding",
    "summary": "arXiv:2602.04109v1 Announce Type: cross Abstract: Artificial intelligence (AI) is increasingly framed as a collaborative partner in creative activities, yet children's interactions with AI have largely been studied in AI-led instructional settings rather than co-creative collaboration. This leaves open questions about how children can meaningfully ",
    "url": "https://arxiv.org/abs/2602.04109",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach",
    "summary": "arXiv:2602.04116v1 Announce Type: cross Abstract: Graph Foundation Models (GFMs) have achieved remarkable success in generalizing across diverse domains. However, they mainly focus on Text-Attributed Graphs (TAGs), leaving Multimodal-Attributed Graphs (MAGs) largely untapped. Developing Multimodal Graph Foundation Models (MGFMs) allows for leveragi",
    "url": "https://arxiv.org/abs/2602.04116",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Scalable Explainability-as-a-Service (XaaS) for Edge AI Systems",
    "summary": "arXiv:2602.04120v1 Announce Type: cross Abstract: Though Explainable AI (XAI) has made significant advancements, its inclusion in edge and IoT systems is typically ad-hoc and inefficient. Most current methods are \"coupled\" in such a way that they generate explanations simultaneously with model inferences. As a result, these approaches incur redunda",
    "url": "https://arxiv.org/abs/2602.04120",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "From Lemmas to Dependencies: What Signals Drive Light Verbs Classification?",
    "summary": "arXiv:2602.04127v1 Announce Type: cross Abstract: Light verb constructions (LVCs) are a challenging class of verbal multiword expressions, especially in Turkish, where rich morphology and productive complex predicates create minimal contrasts between idiomatic predicate meanings and literal verb--argument uses. This paper asks what signals drive LV",
    "url": "https://arxiv.org/abs/2602.04127",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "KGLAMP: Knowledge Graph-guided Language model for Adaptive Multi-robot Planning and Replanning",
    "summary": "arXiv:2602.04129v1 Announce Type: cross Abstract: Heterogeneous multi-robot systems are increasingly deployed in long-horizon missions that require coordination among robots with diverse capabilities. However, existing planning approaches struggle to construct accurate symbolic representations and maintain plan consistency in dynamic environments. ",
    "url": "https://arxiv.org/abs/2602.04129",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "JSynFlow: Japanese Synthesised Flowchart Visual Question Answering Dataset built with Large Language Models",
    "summary": "arXiv:2602.04142v2 Announce Type: cross Abstract: Vision and language models (VLMs) are expected to analyse complex documents, such as those containing flowcharts, through a question-answering (QA) interface. The ability to recognise and interpret these flowcharts is in high demand, as they provide valuable insights unavailable in text-only explana",
    "url": "https://arxiv.org/abs/2602.04142",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "MA3DSG: Multi-Agent 3D Scene Graph Generation for Large-Scale Indoor Environments",
    "summary": "arXiv:2602.04152v1 Announce Type: cross Abstract: Current 3D scene graph generation (3DSGG) approaches heavily rely on a single-agent assumption and small-scale environments, exhibiting limited scalability to real-world scenarios. In this work, we introduce Multi-Agent 3D Scene Graph Generation (MA3DSG) model, the first framework designed to tackle",
    "url": "https://arxiv.org/abs/2602.04152",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Pruning for Generalization: A Transfer-Oriented Spatiotemporal Graph Framework",
    "summary": "arXiv:2602.04153v1 Announce Type: cross Abstract: Multivariate time series forecasting in graph-structured domains is critical for real-world applications, yet existing spatiotemporal models often suffer from performance degradation under data scarcity and cross-domain shifts. We address these challenges through the lens of structure-aware context ",
    "url": "https://arxiv.org/abs/2602.04153",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Improving 2D Diffusion Models for 3D Medical Imaging with Inter-Slice Consistent Stochasticity",
    "summary": "arXiv:2602.04162v1 Announce Type: cross Abstract: 3D medical imaging is in high demand and essential for clinical diagnosis and scientific research. Currently, diffusion models (DMs) have become an effective tool for medical imaging reconstruction thanks to their ability to learn rich, high-quality data priors. However, learning the 3D data distrib",
    "url": "https://arxiv.org/abs/2602.04162",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Topology-Aware Revival for Efficient Sparse Training",
    "summary": "arXiv:2602.04166v1 Announce Type: cross Abstract: Static sparse training is a promising route to efficient learning by committing to a fixed mask pattern, yet the constrained structure reduces robustness. Early pruning decisions can lock the network into a brittle structure that is difficult to escape, especially in deep reinforcement learning (RL)",
    "url": "https://arxiv.org/abs/2602.04166",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "HoloEv-Net: Efficient Event-based Action Recognition via Holographic Spatial Embedding and Global Spectral Gating",
    "summary": "arXiv:2602.04182v1 Announce Type: cross Abstract: Event-based Action Recognition (EAR) has attracted significant attention due to the high temporal resolution and high dynamic range of event cameras. However, existing methods typically suffer from (i) the computational redundancy of dense voxel representations, (ii) structural redundancy inherent i",
    "url": "https://arxiv.org/abs/2602.04182",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Natural Language Instructions for Scene-Responsive Human-in-the-Loop Motion Planning in Autonomous Driving using Vision-Language-Action Models",
    "summary": "arXiv:2602.04184v1 Announce Type: cross Abstract: Instruction-grounded driving, where passenger language guides trajectory planning, requires vehicles to understand intent before motion. However, most prior instruction-following planners rely on simulation or fixed command vocabularies, limiting real-world generalization. doScenes, the first real-w",
    "url": "https://arxiv.org/abs/2602.04184",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "From Helpfulness to Toxic Proactivity: Diagnosing Behavioral Misalignment in LLM Agents",
    "summary": "arXiv:2602.04197v1 Announce Type: cross Abstract: The enhanced capabilities of LLM-based agents come with an emergency for model planning and tool-use abilities. Attributing to helpful-harmless trade-off from LLM alignment, agents typically also inherit the flaw of \"over-refusal\", which is a passive failure mode. However, the proactive planning and",
    "url": "https://arxiv.org/abs/2602.04197",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Enforcing Monotonic Progress in Legal Cross-Examination: Preventing Long-Horizon Stagnation in LLM-Based Inquiry",
    "summary": "arXiv:2602.04206v1 Announce Type: cross Abstract: Large language models (LLMs) exhibit impressive linguistic fluency but struggle to reliably complete long-horizon tasks under explicit procedural constraints. In legal cross-examination, purely proba-bilistic generation often maintains behavioral coherence while failing to ensure procedural advancem",
    "url": "https://arxiv.org/abs/2602.04206",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models",
    "summary": "arXiv:2602.04208v1 Announce Type: cross Abstract: Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic control, with test-time scaling (TTS) gaining attention to enhance robustness beyond training. However, existing TTS methods for VLAs require additional training, verifiers, and multiple forward pass",
    "url": "https://arxiv.org/abs/2602.04208",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Language Models Struggle to Use Representations Learned In-Context",
    "summary": "arXiv:2602.04212v1 Announce Type: cross Abstract: Though large language models (LLMs) have enabled great success across a wide variety of tasks, they still appear to fall short of one of the loftier goals of artificial intelligence research: creating an artificial system that can adapt its behavior to radically new contexts upon deployment. One imp",
    "url": "https://arxiv.org/abs/2602.04212",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "OAT: Ordered Action Tokenization",
    "summary": "arXiv:2602.04215v1 Announce Type: cross Abstract: Autoregressive policies offer a compelling foundation for scalable robot learning by enabling discrete abstraction, token-level reasoning, and flexible inference. However, applying autoregressive modeling to continuous robot actions requires an effective action tokenization scheme. Existing approach",
    "url": "https://arxiv.org/abs/2602.04215",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "RAPO: Risk-Aware Preference Optimization for Generalizable Safe Reasoning",
    "summary": "arXiv:2602.04224v1 Announce Type: cross Abstract: Large Reasoning Models (LRMs) have achieved tremendous success with their chain-of-thought (CoT) reasoning, yet also face safety issues similar to those of basic language models. In particular, while algorithms are designed to guide them to deliberately refuse harmful prompts with safe reasoning, th",
    "url": "https://arxiv.org/abs/2602.04224",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "ACIL: Active Class Incremental Learning for Image Classification",
    "summary": "arXiv:2602.04252v1 Announce Type: cross Abstract: Continual learning (or class incremental learning) is a realistic learning scenario for computer vision systems, where deep neural networks are trained on episodic data, and the data from previous episodes are generally inaccessible to the model. Existing research in this domain has primarily focuse",
    "url": "https://arxiv.org/abs/2602.04252",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "AppleVLM: End-to-end Autonomous Driving with Advanced Perception and Planning-Enhanced Vision-Language Models",
    "summary": "arXiv:2602.04256v1 Announce Type: cross Abstract: End-to-end autonomous driving has emerged as a promising paradigm integrating perception, decision-making, and control within a unified learning framework. Recently, Vision-Language Models (VLMs) have gained significant attention for their potential to enhance the robustness and generalization of en",
    "url": "https://arxiv.org/abs/2602.04256",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "From Dead Neurons to Deep Approximators: Deep Bernstein Networks as a Provable Alternative to Residual Layers",
    "summary": "arXiv:2602.04264v1 Announce Type: cross Abstract: Residual connections are the de facto standard for mitigating vanishing gradients, yet they impose structural constraints and fail to address the inherent inefficiencies of piecewise linear activations. We show that Deep Bernstein Networks (which utilizes Bernstein polynomials as activation function",
    "url": "https://arxiv.org/abs/2602.04264",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Thickening-to-Thinning: Reward Shaping via Human-Inspired Learning Dynamics for LLM Reasoning",
    "summary": "arXiv:2602.04265v1 Announce Type: cross Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for enhancing reasoning in Large Language Models (LLMs). However, it frequently encounters challenges such as entropy collapse, excessive verbosity, and insufficient exploration for hard problems. Crucially, ex",
    "url": "https://arxiv.org/abs/2602.04265",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization",
    "summary": "arXiv:2602.04271v1 Announce Type: cross Abstract: 4D generation has made remarkable progress in synthesizing dynamic 3D objects from input text, images, or videos. However, existing methods often represent motion as an implicit deformation field, which limits direct control and editability. To address this issue, we propose SkeletonGaussian, a nove",
    "url": "https://arxiv.org/abs/2602.04271",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Multi Objective Design Optimization of Non Pneumatic Passenger Car Tires Using Finite Element Modeling, Machine Learning, and Particle swarm Optimization and Bayesian Optimization Algorithms",
    "summary": "arXiv:2602.04277v1 Announce Type: cross Abstract: Non Pneumatic tires offer a promising alternative to pneumatic tires. However, their discontinuous spoke structures present challenges in stiffness tuning, durability, and high speed vibration. This study introduces an integrated generative design and machine learning driven framework to optimize UP",
    "url": "https://arxiv.org/abs/2602.04277",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Contextual Drag: How Errors in the Context Affect LLM Reasoning",
    "summary": "arXiv:2602.04288v1 Announce Type: cross Abstract: Central to many self-improvement pipelines for large language models (LLMs) is the assumption that models can improve by reflecting on past mistakes. We study a phenomenon termed contextual drag: the presence of failed attempts in the context biases subsequent generations toward structurally similar",
    "url": "https://arxiv.org/abs/2602.04288",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Disentangling Causal Importance from Emergent Structure in Multi-Expert Orchestration",
    "summary": "arXiv:2602.04291v1 Announce Type: cross Abstract: Multi-expert systems, where multiple Large Language Models (LLMs) collaborate to solve complex tasks, are increasingly adopted for high-performance reasoning and generation. However, the orchestration policies governing expert interaction and sequencing remain largely opaque. We introduce INFORM, an",
    "url": "https://arxiv.org/abs/2602.04291",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "How Few-shot Demonstrations Affect Prompt-based Defenses Against LLM Jailbreak Attacks",
    "summary": "arXiv:2602.04294v1 Announce Type: cross Abstract: Large Language Models (LLMs) face increasing threats from jailbreak attacks that bypass safety alignment. While prompt-based defenses such as Role-Oriented Prompts (RoP) and Task-Oriented Prompts (ToP) have shown effectiveness, the role of few-shot demonstrations in these defense strategies remains ",
    "url": "https://arxiv.org/abs/2602.04294",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas",
    "summary": "arXiv:2602.04296v1 Announce Type: cross Abstract: Large language models (LLMs) have revolutionized automated code generation, yet the evaluation of their real-world effectiveness remains limited by static benchmarks and simplistic metrics. We present ProxyWar, a novel framework that systematically assesses code generation quality by embedding LLM-g",
    "url": "https://arxiv.org/abs/2602.04296",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Revisiting Prompt Sensitivity in Large Language Models for Text Classification: The Role of Prompt Underspecification",
    "summary": "arXiv:2602.04297v1 Announce Type: cross Abstract: Large language models (LLMs) are widely used as zero-shot and few-shot classifiers, where task behaviour is largely controlled through prompting. A growing number of works have observed that LLMs are sensitive to prompt variations, with small changes leading to large changes in performance. However,",
    "url": "https://arxiv.org/abs/2602.04297",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Beyond Static Cropping: Layer-Adaptive Visual Localization and Decoding Enhancement",
    "summary": "arXiv:2602.04304v1 Announce Type: cross Abstract: Large Vision-Language Models (LVLMs) have advanced rapidly by aligning visual patches with the text embedding space, but a fixed visual-token budget forces images to be resized to a uniform pretraining resolution, often erasing fine-grained details and causing hallucinations via over-reliance on lan",
    "url": "https://arxiv.org/abs/2602.04304",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "DeFrame: Debiasing Large Language Models Against Framing Effects",
    "summary": "arXiv:2602.04306v1 Announce Type: cross Abstract: As large language models (LLMs) are increasingly deployed in real-world applications, ensuring their fair responses across demographics has become crucial. Despite many efforts, an ongoing challenge is hidden bias: LLMs appear fair under standard evaluations, but can produce biased responses outside",
    "url": "https://arxiv.org/abs/2602.04306",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Efficient Equivariant High-Order Crystal Tensor Prediction via Cartesian Local-Environment Many-Body Coupling",
    "summary": "arXiv:2602.04323v1 Announce Type: cross Abstract: End-to-end prediction of high-order crystal tensor properties from atomic structures remains challenging: while spherical-harmonic equivariant models are expressive, their Clebsch-Gordan tensor products incur substantial compute and memory costs for higher-order targets. We propose the Cartesian Env",
    "url": "https://arxiv.org/abs/2602.04323",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner",
    "summary": "arXiv:2602.04337v1 Announce Type: cross Abstract: Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, co",
    "url": "https://arxiv.org/abs/2602.04337",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning",
    "summary": "arXiv:2602.04340v1 Announce Type: cross Abstract: Pre-trained vision-language models such as CLIP exhibit strong transferability, yet adapting them to downstream image classification tasks under limited annotation budgets remains challenging. In active learning settings, the model must select the most informative samples for annotation from a large",
    "url": "https://arxiv.org/abs/2602.04340",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "UnMaskFork: Test-Time Scaling for Masked Diffusion via Deterministic Action Branching",
    "summary": "arXiv:2602.04344v1 Announce Type: cross Abstract: Test-time scaling strategies have effectively leveraged inference-time compute to enhance the reasoning abilities of Autoregressive Large Language Models. In this work, we demonstrate that Masked Diffusion Language Models (MDLMs) are inherently amenable to advanced search strategies, owing to their ",
    "url": "https://arxiv.org/abs/2602.04344",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "VecSet-Edit: Unleashing Pre-trained LRM for Mesh Editing from Single Image",
    "summary": "arXiv:2602.04349v1 Announce Type: cross Abstract: 3D editing has emerged as a critical research area to provide users with flexible control over 3D assets. While current editing approaches predominantly focus on 3D Gaussian Splatting or multi-view images, the direct editing of 3D meshes remains underexplored. Prior attempts, such as VoxHammer, rely",
    "url": "https://arxiv.org/abs/2602.04349",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Counterfactual Explanations for Hypergraph Neural Networks",
    "summary": "arXiv:2602.04360v1 Announce Type: cross Abstract: Hypergraph neural networks (HGNNs) effectively model higher-order interactions in many real-world systems but remain difficult to interpret, limiting their deployment in high-stakes settings. We introduce CF-HyperGNNExplainer, a counterfactual explanation method for HGNNs that identifies the minimal",
    "url": "https://arxiv.org/abs/2602.04360",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "SparVAR: Exploring Sparsity in Visual AutoRegressive Modeling for Training-Free Acceleration",
    "summary": "arXiv:2602.04361v1 Announce Type: cross Abstract: Visual AutoRegressive (VAR) modeling has garnered significant attention for its innovative next-scale prediction paradigm. However, mainstream VAR paradigms attend to all tokens across historical scales at each autoregressive step. As the next scale resolution grows, the computational complexity of ",
    "url": "https://arxiv.org/abs/2602.04361",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Beyond KL Divergence: Policy Optimization with Flexible Bregman Divergences for LLM Reasoning",
    "summary": "arXiv:2602.04380v1 Announce Type: cross Abstract: Policy optimization methods like Group Relative Policy Optimization (GRPO) and its variants have achieved strong results on mathematical reasoning and code generation tasks. Despite extensive exploration of reward processing strategies and training dynamics, all existing group-based methods exclusiv",
    "url": "https://arxiv.org/abs/2602.04380",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Enabling Real-Time Colonoscopic Polyp Segmentation on Commodity CPUs via Ultra-Lightweight Architecture",
    "summary": "arXiv:2602.04381v1 Announce Type: cross Abstract: Early detection of colorectal cancer hinges on real-time, accurate polyp identification and resection. Yet current high-precision segmentation models rely on GPUs, making them impractical to deploy in primary hospitals, mobile endoscopy units, or capsule robots. To bridge this gap, we present the Ul",
    "url": "https://arxiv.org/abs/2602.04381",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Blockchain Federated Learning for Sustainable Retail: Reducing Waste through Collaborative Demand Forecasting",
    "summary": "arXiv:2602.04384v1 Announce Type: cross Abstract: Effective demand forecasting is crucial for reducing food waste. However, data privacy concerns often hinder collaboration among retailers, limiting the potential for improved predictive accuracy. In this study, we explore the application of Federated Learning (FL) in Sustainable Supply Chain Manage",
    "url": "https://arxiv.org/abs/2602.04384",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "LoRDO: Distributed Low-Rank Optimization with Infrequent Communication",
    "summary": "arXiv:2602.04396v1 Announce Type: cross Abstract: Distributed training of foundation models via $\\texttt{DDP}$ is limited by interconnect bandwidth. While infrequent communication strategies reduce synchronization frequency, they remain bottlenecked by the memory and communication requirements of optimizer states. Low-rank optimizers can alleviate ",
    "url": "https://arxiv.org/abs/2602.04396",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Bi-directional Bias Attribution: Debiasing Large Language Models without Modifying Prompts",
    "summary": "arXiv:2602.04398v1 Announce Type: cross Abstract: Large language models (LLMs) have demonstrated impressive capabilities across a wide range of natural language processing tasks. However, their outputs often exhibit social biases, raising fairness concerns. Existing debiasing methods, such as fine-tuning on additional datasets or prompt engineering",
    "url": "https://arxiv.org/abs/2602.04398",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Performative Learning Theory",
    "summary": "arXiv:2602.04402v1 Announce Type: cross Abstract: Performative predictions influence the very outcomes they aim to forecast. We study performative predictions that affect a sample (e.g., only existing users of an app) and/or the whole population (e.g., all potential app users). This raises the question of how well models generalize under performati",
    "url": "https://arxiv.org/abs/2602.04402",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "History-Guided Iterative Visual Reasoning with Self-Correction",
    "summary": "arXiv:2602.04413v1 Announce Type: cross Abstract: Self-consistency methods are the core technique for improving the reasoning reliability of multimodal large language models (MLLMs). By generating multiple reasoning results through repeated sampling and selecting the best answer via voting, they play an important role in cross-modal tasks. However,",
    "url": "https://arxiv.org/abs/2602.04413",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Med-MMFL: A Multimodal Federated Learning Benchmark in Healthcare",
    "summary": "arXiv:2602.04416v1 Announce Type: cross Abstract: Federated learning (FL) enables collaborative model training across decentralized medical institutions while preserving data privacy. However, medical FL benchmarks remain scarce, with existing efforts focusing mainly on unimodal or bimodal modalities and a limited range of medical tasks. This gap u",
    "url": "https://arxiv.org/abs/2602.04416",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "EMA Policy Gradient: Taming Reinforcement Learning for LLMs with EMA Anchor and Top-k KL",
    "summary": "arXiv:2602.04417v1 Announce Type: cross Abstract: Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to acquire increasingly complex reasoning and agentic behaviors. In this work, we propose two simple techniques to improve policy gradient algorithms for LLMs. First, we replace the fixed anchor policy during RL with an Exponential",
    "url": "https://arxiv.org/abs/2602.04417",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "SPEAR: An Engineering Case Study of Multi-Agent Coordination for Smart Contract Auditing",
    "summary": "arXiv:2602.04418v1 Announce Type: cross Abstract: We present SPEAR, a multi-agent coordination framework for smart contract auditing that applies established MAS patterns in a realistic security analysis workflow. SPEAR models auditing as a coordinated mission carried out by specialized agents: a Planning Agent prioritizes contracts using risk-awar",
    "url": "https://arxiv.org/abs/2602.04418",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "No One-Size-Fits-All: Building Systems For Translation to Bashkir, Kazakh, Kyrgyz, Tatar and Chuvash Using Synthetic And Original Data",
    "summary": "arXiv:2602.04442v1 Announce Type: cross Abstract: We explore machine translation for five Turkic language pairs: Russian-Bashkir, Russian-Kazakh, Russian-Kyrgyz, English-Tatar, English-Chuvash. Fine-tuning nllb-200-distilled-600M with LoRA on synthetic data achieved chrF++ 49.71 for Kazakh and 46.94 for Bashkir. Prompting DeepSeek-V3.2 with retriev",
    "url": "https://arxiv.org/abs/2602.04442",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Mixture of Masters: Sparse Chess Language Models with Player Routing",
    "summary": "arXiv:2602.04447v1 Announce Type: cross Abstract: Modern chess language models are dense transformers trained on millions of games played by thousands of high-rated individuals. However, these monolithic networks tend to collapse into mode-averaged behavior, where stylistic boundaries are blurred, and rare but effective strategies are suppressed. T",
    "url": "https://arxiv.org/abs/2602.04447",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "RASA: Routing-Aware Safety Alignment for Mixture-of-Experts Models",
    "summary": "arXiv:2602.04448v1 Announce Type: cross Abstract: Mixture-of-Experts (MoE) language models introduce unique challenges for safety alignment due to their sparse routing mechanisms, which can enable degenerate optimization behaviors under standard full-parameter fine-tuning. In our preliminary experiments, we observe that naively applying full-parame",
    "url": "https://arxiv.org/abs/2602.04448",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Growth First, Care Second? Tracing the Landscape of LLM Value Preferences in Everyday Dilemmas",
    "summary": "arXiv:2602.04456v1 Announce Type: cross Abstract: People increasingly seek advice online from both human peers and large language model (LLM)-based chatbots. Such advice rarely involves identifying a single correct answer; instead, it typically requires navigating trade-offs among competing values. We aim to characterize how LLMs navigate value tra",
    "url": "https://arxiv.org/abs/2602.04456",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Is Micro Domain-Adaptive Pre-Training Effective for Real-World Operations? Multi-Step Evaluation Reveals Potential and Bottlenecks",
    "summary": "arXiv:2602.04466v1 Announce Type: cross Abstract: When applying LLMs to real-world enterprise operations, LLMs need to handle proprietary knowledge in small domains of specific operations ($\\textbf{micro domains}$). A previous study shows micro domain-adaptive pre-training ($\\textbf{mDAPT}$) with fewer documents is effective, similarly to DAPT in l",
    "url": "https://arxiv.org/abs/2602.04466",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "LLM-Empowered Cooperative Content Caching in Vehicular Fog Caching-Assisted Platoon Networks",
    "summary": "arXiv:2602.04471v1 Announce Type: cross Abstract: This letter proposes a novel three-tier content caching architecture for Vehicular Fog Caching (VFC)-assisted platoon, where the VFC is formed by the vehicles driving near the platoon. The system strategically coordinates storage across local platoon vehicles, dynamic VFC clusters, and cloud server ",
    "url": "https://arxiv.org/abs/2602.04471",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish",
    "summary": "arXiv:2602.04492v1 Announce Type: cross Abstract: Constructing mechanistic models of neural circuits is a fundamental goal of neuroscience, yet verifying such models is limited by the lack of ground truth. To rigorously test model discovery, we establish an in silico testbed using neuromechanical simulations of a larval zebrafish as a transparent g",
    "url": "https://arxiv.org/abs/2602.04492",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "BrainVista: Modeling Naturalistic Brain Dynamics as Multimodal Next-Token Prediction",
    "summary": "arXiv:2602.04512v1 Announce Type: cross Abstract: Naturalistic fMRI characterizes the brain as a dynamic predictive engine driven by continuous sensory streams. However, modeling the causal forward evolution in realistic neural simulation is impeded by the timescale mismatch between multimodal inputs and the complex topology of cortical networks. T",
    "url": "https://arxiv.org/abs/2602.04512",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Learning the Value Systems of Agents with Preference-based and Inverse Reinforcement Learning",
    "summary": "arXiv:2602.04518v1 Announce Type: cross Abstract: Agreement Technologies refer to open computer systems in which autonomous software agents interact with one another, typically on behalf of humans, in order to come to mutually acceptable agreements. With the advance of AI systems in recent years, it has become apparent that such agreements, in orde",
    "url": "https://arxiv.org/abs/2602.04518",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "SLUM-i: Semi-supervised Learning for Urban Mapping of Informal Settlements and Data Quality Benchmarking",
    "summary": "arXiv:2602.04525v1 Announce Type: cross Abstract: Rapid urban expansion has fueled the growth of informal settlements in major cities of low- and middle-income countries, with Lahore and Karachi in Pakistan and Mumbai in India serving as prominent examples. However, large-scale mapping of these settlements is severely constrained not only by the sc",
    "url": "https://arxiv.org/abs/2602.04525",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "LycheeDecode: Accelerating Long-Context LLM Inference via Hybrid-Head Sparse Decoding",
    "summary": "arXiv:2602.04541v1 Announce Type: cross Abstract: The proliferation of long-context large language models (LLMs) exposes a key bottleneck: the rapidly expanding key-value cache during decoding, which imposes heavy memory and latency costs. While recent approaches attempt to alleviate this by sharing a single set of crucial tokens across layers, suc",
    "url": "https://arxiv.org/abs/2602.04541",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Continual Learning through Control Minimization",
    "summary": "arXiv:2602.04542v1 Announce Type: cross Abstract: Catastrophic forgetting remains a fundamental challenge for neural networks when tasks are trained sequentially. In this work, we reformulate continual learning as a control problem where learning and preservation signals compete within neural activity dynamics. We convert regularization penalties i",
    "url": "https://arxiv.org/abs/2602.04542",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "OmniRad: A Radiological Foundation Model for Multi-Task Medical Image Analysis",
    "summary": "arXiv:2602.04547v1 Announce Type: cross Abstract: Radiological analysis increasingly benefits from pretrained visual representations that can support heterogeneous downstream tasks across imaging modalities. In this work, we introduce OmniRad, a self-supervised radiological foundation model pretrained on 1.2 million medical images, designed with ra",
    "url": "https://arxiv.org/abs/2602.04547",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Dual Mind World Model Inspired Network Digital Twin for Access Scheduling",
    "summary": "arXiv:2602.04566v1 Announce Type: cross Abstract: Emerging networked systems such as industrial IoT and real-time cyber-physical infrastructures demand intelligent scheduling strategies capable of adapting to dynamic traffic, deadlines, and interference constraints. In this work, we present a novel Digital Twin-enabled scheduling framework inspired",
    "url": "https://arxiv.org/abs/2602.04566",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Trust The Typical",
    "summary": "arXiv:2602.04581v1 Announce Type: cross Abstract: Current approaches to LLM safety fundamentally rely on a brittle cat-and-mouse game of identifying and blocking known threats via guardrails. We argue for a fresh approach: robust safety comes not from enumerating what is harmful, but from deeply understanding what is safe. We introduce Trust The Ty",
    "url": "https://arxiv.org/abs/2602.04581",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "VILLAIN at AVerImaTeC: Verifying Image-Text Claims via Multi-Agent Collaboration",
    "summary": "arXiv:2602.04587v1 Announce Type: cross Abstract: This paper describes VILLAIN, a multimodal fact-checking system that verifies image-text claims through prompt-based multi-agent collaboration. For the AVerImaTeC shared task, VILLAIN employs vision-language model agents across multiple stages of fact-checking. Textual and visual evidence is retriev",
    "url": "https://arxiv.org/abs/2602.04587",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "RexBERT: Context Specialized Bidirectional Encoders for E-commerce",
    "summary": "arXiv:2602.04605v1 Announce Type: cross Abstract: Encoder-only transformers remain indispensable in retrieval, classification, and ranking systems where latency, stability, and cost are paramount. Most general purpose encoders, however, are trained on generic corpora with limited coverage of specialized domains. We introduce RexBERT, a family of BE",
    "url": "https://arxiv.org/abs/2602.04605",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "A Human-Centered Privacy Approach (HCP) to AI",
    "summary": "arXiv:2602.04616v1 Announce Type: cross Abstract: As the paradigm of Human-Centered AI (HCAI) gains prominence, its benefits to society are accompanied by significant ethical concerns, one of which is the protection of individual privacy. This chapter provides a comprehensive overview of privacy within HCAI, proposing a human-centered privacy (HCP)",
    "url": "https://arxiv.org/abs/2602.04616",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Towards Structured, State-Aware, and Execution-Grounded Reasoning for Software Engineering Agents",
    "summary": "arXiv:2602.04640v1 Announce Type: cross Abstract: Software Engineering (SE) agents have shown promising abilities in supporting various SE tasks. Current SE agents remain fundamentally reactive, making decisions mainly based on conversation history and the most recent response. However, this reactive design provides no explicit structure or persist",
    "url": "https://arxiv.org/abs/2602.04640",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Rethinking the Design Space of Reinforcement Learning for Diffusion Models: On the Importance of Likelihood Estimation Beyond Loss Design",
    "summary": "arXiv:2602.04663v1 Announce Type: cross Abstract: Reinforcement learning has been widely applied to diffusion and flow models for visual tasks such as text-to-image generation. However, these tasks remain challenging because diffusion models have intractable likelihoods, which creates a barrier for directly applying popular policy-gradient type met",
    "url": "https://arxiv.org/abs/2602.04663",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Delving into Muon and Beyond: Deep Analysis and Extensions",
    "summary": "arXiv:2602.04669v1 Announce Type: cross Abstract: The Muon optimizer has recently attracted considerable attention for its strong empirical performance and use of orthogonalized updates on matrix-shaped parameters, yet its underlying mechanisms and relationship to adaptive optimizers such as Adam remain insufficiently understood. In this work, we a",
    "url": "https://arxiv.org/abs/2602.04669",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Overstating Attitudes, Ignoring Networks: LLM Biases in Simulating Misinformation Susceptibility",
    "summary": "arXiv:2602.04674v1 Announce Type: cross Abstract: Large language models (LLMs) are increasingly used as proxies for human judgment in computational social science, yet their ability to reproduce patterns of susceptibility to misinformation remains unclear. We test whether LLM-simulated survey respondents, prompted with participant profiles drawn fr",
    "url": "https://arxiv.org/abs/2602.04674",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Let Experts Feel Uncertainty: A Multi-Expert Label Distribution Approach to Probabilistic Time Series Forecasting",
    "summary": "arXiv:2602.04678v1 Announce Type: cross Abstract: Time series forecasting in real-world applications requires both high predictive accuracy and interpretable uncertainty quantification. Traditional point prediction methods often fail to capture the inherent uncertainty in time series data, while existing probabilistic approaches struggle to balance",
    "url": "https://arxiv.org/abs/2602.04678",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Audio ControlNet for Fine-Grained Audio Generation and Editing",
    "summary": "arXiv:2602.04680v1 Announce Type: cross Abstract: We study the fine-grained text-to-audio (T2A) generation task. While recent models can synthesize high-quality audio from text descriptions, they often lack precise control over attributes such as loudness, pitch, and sound events. Unlike prior approaches that retrain models for specific control typ",
    "url": "https://arxiv.org/abs/2602.04680",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "DRMOT: A Dataset and Framework for RGBD Referring Multi-Object Tracking",
    "summary": "arXiv:2602.04692v1 Announce Type: cross Abstract: Referring Multi-Object Tracking (RMOT) aims to track specific targets based on language descriptions and is vital for interactive AI systems such as robotics and autonomous driving. However, existing RMOT models rely solely on 2D RGB data, making it challenging to accurately detect and associate tar",
    "url": "https://arxiv.org/abs/2602.04692",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Addressing Corpus Knowledge Poisoning Attacks on RAG Using Sparse Attention",
    "summary": "arXiv:2602.04711v2 Announce Type: cross Abstract: Retrieval Augmented Generation (RAG) is a highly effective paradigm for keeping LLM-based responses up-to-date and reducing the likelihood of hallucinations. Yet, RAG was recently shown to be quite vulnerable to corpus knowledge poisoning: an attacker injects misleading documents to the corpus to st",
    "url": "https://arxiv.org/abs/2602.04711",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation",
    "summary": "arXiv:2602.04712v1 Announce Type: cross Abstract: We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, w",
    "url": "https://arxiv.org/abs/2602.04712",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Adaptive Prompt Elicitation for Text-to-Image Generation",
    "summary": "arXiv:2602.04713v1 Announce Type: cross Abstract: Aligning text-to-image generation with user intent remains challenging, for users who provide ambiguous inputs and struggle with model idiosyncrasies. We propose Adaptive Prompt Elicitation (APE), a technique that adaptively asks visual queries to help users refine prompts without extensive writing.",
    "url": "https://arxiv.org/abs/2602.04713",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Identifying Intervenable and Interpretable Features via Orthogonality Regularization",
    "summary": "arXiv:2602.04718v1 Announce Type: cross Abstract: With recent progress on fine-tuning language models around a fixed sparse autoencoder, we disentangle the decoder matrix into almost orthogonal features. This reduces interference and superposition between the features, while keeping performance on the target dataset essentially unchanged. Our ortho",
    "url": "https://arxiv.org/abs/2602.04718",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Supporting software engineering tasks with agentic AI: Demonstration on document retrieval and test scenario generation",
    "summary": "arXiv:2602.04726v1 Announce Type: cross Abstract: The introduction of large language models ignited great retooling and rethinking of the software development models. The ensuing response of software engineering research yielded a massive body of tools and approaches. In this paper, we join the hassle by introducing agentic AI solutions for two tas",
    "url": "https://arxiv.org/abs/2602.04726",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "From Data to Behavior: Predicting Unintended Model Behaviors Before Training",
    "summary": "arXiv:2602.04735v1 Announce Type: cross Abstract: Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduc",
    "url": "https://arxiv.org/abs/2602.04735",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases",
    "summary": "arXiv:2602.04739v1 Announce Type: cross Abstract: Multimodal large language models (MLLMs) are increasingly deployed in real-world systems, yet their safety under adversarial prompting remains underexplored. We present a two-phase evaluation of MLLM harmlessness using a fixed benchmark of 726 adversarial prompts authored by 26 professional red team",
    "url": "https://arxiv.org/abs/2602.04739",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Exploiting contextual information to improve stance detection in informal political discourse with LLMs",
    "summary": "arXiv:2602.04750v1 Announce Type: cross Abstract: This study investigates the use of Large Language Models (LLMs) for political stance detection in informal online discourse, where language is often sarcastic, ambiguous, and context-dependent. We explore whether providing contextual information, specifically user profile summaries derived from hist",
    "url": "https://arxiv.org/abs/2602.04750",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Comparative Insights on Adversarial Machine Learning from Industry and Academia: A User-Study Approach",
    "summary": "arXiv:2602.04753v1 Announce Type: cross Abstract: An exponential growth of Machine Learning and its Generative AI applications brings with it significant security challenges, often referred to as Adversarial Machine Learning (AML). In this paper, we conducted two comprehensive studies to explore the perspectives of industry professionals and studen",
    "url": "https://arxiv.org/abs/2602.04753",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "When Silence Is Golden: Can LLMs Learn to Abstain in Temporal QA and Beyond?",
    "summary": "arXiv:2602.04755v1 Announce Type: cross Abstract: Large language models (LLMs) rarely admit uncertainty, often producing fluent but misleading answers, rather than abstaining (i.e., refusing to answer). This weakness is even evident in temporal question answering, where models frequently ignore time-sensitive evidence and conflate facts across diff",
    "url": "https://arxiv.org/abs/2602.04755",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Active Asymmetric Multi-Agent Multimodal Learning under Uncertainty",
    "summary": "arXiv:2602.04763v1 Announce Type: cross Abstract: Multi-agent systems are increasingly equipped with heterogeneous multimodal sensors, enabling richer perception but introducing modality-specific and agent-dependent uncertainty. Existing multi-agent collaboration frameworks typically reason at the agent level, assume homogeneous sensing, and handle",
    "url": "https://arxiv.org/abs/2602.04763",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Billion-Scale Graph Foundation Models",
    "summary": "arXiv:2602.04768v1 Announce Type: cross Abstract: Graph-structured data underpins many critical applications. While foundation models have transformed language and vision via large-scale pretraining and lightweight adaptation, extending this paradigm to general, real-world graphs is challenging. In this work, we present Graph Billion- Foundation-Fu",
    "url": "https://arxiv.org/abs/2602.04768",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Team, Then Trim: An Assembly-Line LLM Framework for High-Quality Tabular Data Generation",
    "summary": "arXiv:2602.04785v1 Announce Type: cross Abstract: While tabular data is fundamental to many real-world machine learning (ML) applications, acquiring high-quality tabular data is usually labor-intensive and expensive. Limited by the scarcity of observations, tabular datasets often exhibit critical deficiencies, such as class imbalance, selection bia",
    "url": "https://arxiv.org/abs/2602.04785",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging",
    "summary": "arXiv:2602.04805v1 Announce Type: cross Abstract: The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is ",
    "url": "https://arxiv.org/abs/2602.04805",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Beyond Rewards in Reinforcement Learning for Cyber Defence",
    "summary": "arXiv:2602.04809v1 Announce Type: cross Abstract: Recent years have seen an explosion of interest in autonomous cyber defence agents trained to defend computer networks using deep reinforcement learning. These agents are typically trained in cyber gym environments using dense, highly engineered reward functions which combine many penalties and ince",
    "url": "https://arxiv.org/abs/2602.04809",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization",
    "summary": "arXiv:2602.04811v1 Announce Type: cross Abstract: True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-trainin",
    "url": "https://arxiv.org/abs/2602.04811",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Toward Reliable and Explainable Nail Disease Classification: Leveraging Adversarial Training and Grad-CAM Visualization",
    "summary": "arXiv:2602.04820v1 Announce Type: cross Abstract: Human nail diseases are gradually observed over all age groups, especially among older individuals, often going ignored until they become severe. Early detection and accurate diagnosis of such conditions are important because they sometimes reveal our body's health problems. But it is challenging du",
    "url": "https://arxiv.org/abs/2602.04820",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Safe Urban Traffic Control via Uncertainty-Aware Conformal Prediction and World-Model Reinforcement Learning",
    "summary": "arXiv:2602.04821v1 Announce Type: cross Abstract: Urban traffic management demands systems that simultaneously predict future conditions, detect anomalies, and take safe corrective actions -- all while providing reliability guarantees. We present STREAM-RL, a unified framework that introduces three novel algorithmic contributions: (1) PU-GAT+, an U",
    "url": "https://arxiv.org/abs/2602.04821",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "It's not a Lottery, it's a Race: Understanding How Gradient Descent Adapts the Network's Capacity to the Task",
    "summary": "arXiv:2602.04832v1 Announce Type: cross Abstract: Our theoretical understanding of neural networks is lagging behind their empirical success. One of the important unexplained phenomena is why and how, during the process of training with gradient descent, the theoretical capacity of neural networks is reduced to an effective capacity that fits the t",
    "url": "https://arxiv.org/abs/2602.04832",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "El Agente Estructural: An Artificially Intelligent Molecular Editor",
    "summary": "arXiv:2602.04849v1 Announce Type: cross Abstract: We present El Agente Estructural, a multimodal, natural-language-driven geometry-generation and manipulation agent for autonomous chemistry and molecular modelling. Unlike molecular generation or editing via generative models, Estructural mimics how human experts directly manipulate molecular system",
    "url": "https://arxiv.org/abs/2602.04849",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "El Agente Quntur: A research collaborator agent for quantum chemistry",
    "summary": "arXiv:2602.04850v1 Announce Type: cross Abstract: Quantum chemistry is a foundational enabling tool for the fields of chemistry, materials science, computational biology and others. Despite of its power, the practical application of quantum chemistry simulations remains in the hands of qualified experts due to methodological complexity, software he",
    "url": "https://arxiv.org/abs/2602.04850",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "From Evaluation to Design: Using Potential Energy Surface Smoothness Metrics to Guide Machine Learning Interatomic Potential Architectures",
    "summary": "arXiv:2602.04861v1 Announce Type: cross Abstract: Machine Learning Interatomic Potentials (MLIPs) sometimes fail to reproduce the physical smoothness of the quantum potential energy surface (PES), leading to erroneous behavior in downstream simulations that standard energy and force regression evaluations can miss. Existing evaluations, such as mic",
    "url": "https://arxiv.org/abs/2602.04861",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Subliminal Effects in Your Data: A General Mechanism via Log-Linearity",
    "summary": "arXiv:2602.04863v1 Announce Type: cross Abstract: Training modern large language models (LLMs) has become a veritable smorgasbord of algorithms and datasets designed to elicit particular behaviors, making it critical to develop techniques to understand the effects of datasets on the model's properties. This is exacerbated by recent experiments that",
    "url": "https://arxiv.org/abs/2602.04863",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "CRoSS: A Continual Robotic Simulation Suite for Scalable Reinforcement Learning with High Task Diversity and Realistic Physics Simulation",
    "summary": "arXiv:2602.04868v1 Announce Type: cross Abstract: Continual reinforcement learning (CRL) requires agents to learn from a sequence of tasks without forgetting previously acquired policies. In this work, we introduce a novel benchmark suite for CRL based on realistically simulated robots in the Gazebo simulator. Our Continual Robotic Simulation Suite",
    "url": "https://arxiv.org/abs/2602.04868",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Multi-layer Cross-Attention is Provably Optimal for Multi-modal In-context Learning",
    "summary": "arXiv:2602.04872v1 Announce Type: cross Abstract: Recent progress has rapidly advanced our understanding of the mechanisms underlying in-context learning in modern attention-based neural networks. However, existing results focus exclusively on unimodal data; in contrast, the theoretical underpinnings of in-context learning for multi-modal data rema",
    "url": "https://arxiv.org/abs/2602.04872",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Rethinking the Trust Region in LLM Reinforcement Learning",
    "summary": "arXiv:2602.04879v1 Announce Type: cross Abstract: Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large",
    "url": "https://arxiv.org/abs/2602.04879",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Contrastive Continual Learning for Model Adaptability in Internet of Things",
    "summary": "arXiv:2602.04881v1 Announce Type: cross Abstract: Internet of Things (IoT) deployments operate in nonstationary, dynamic environments where factors such as sensor drift, evolving user behavior, and heterogeneous user privacy requirements can affect application utility. Continual learning (CL) addresses this by adapting models over time without cata",
    "url": "https://arxiv.org/abs/2602.04881",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Protein Autoregressive Modeling via Multiscale Structure Generation",
    "summary": "arXiv:2602.04883v1 Announce Type: cross Abstract: We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and ",
    "url": "https://arxiv.org/abs/2602.04883",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Benchmarking Large Language Models for Diagnosing Students' Cognitive Skills from Handwritten Math Work",
    "summary": "arXiv:2504.00843v2 Announce Type: replace Abstract: Students' handwritten math work provides a rich resource for diagnosing cognitive skills, as it captures intermediate reasoning beyond final answers. We investigate how current large language models (LLMs) perform in diagnosing cognitive skills from such work. However, student responses vary widel",
    "url": "https://arxiv.org/abs/2504.00843",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "OmniCellTOSG: The First Cell Text-Omic Signaling Graphs Dataset for Graph Language Foundation Modeling",
    "summary": "arXiv:2504.02148v2 Announce Type: replace Abstract: With the rapid growth of large-scale single-cell omic datasets, omic foundation models (FMs) have emerged as powerful tools for advancing research in life sciences and precision medicine. However, most existing omic FMs rely primarily on numerical transcriptomic data by sorting genes as sequences,",
    "url": "https://arxiv.org/abs/2504.02148",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Toward Multiphysics-Informed Machine Learning for Sustainable Data Center Operations: Intelligence Evolution with Deployable Solutions for Computing Infrastructure",
    "summary": "arXiv:2505.19414v2 Announce Type: replace Abstract: The revolution in artificial intelligence (AI) has brought sustainable challenges in data center management due to the high carbon emissions and short cooling response time associated with high-power density racks. While machine learning (ML) offers promise for intelligent management, its adoption",
    "url": "https://arxiv.org/abs/2505.19414",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Can LLMs Reconcile Knowledge Conflicts in Counterfactual Reasoning",
    "summary": "arXiv:2506.15732v4 Announce Type: replace Abstract: Large Language Models have been shown to contain extensive world knowledge in their parameters, enabling impressive performance on many knowledge intensive tasks. However, when deployed in novel settings, LLMs often encounter situations where they must integrate parametric knowledge with new or un",
    "url": "https://arxiv.org/abs/2506.15732",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE",
    "summary": "arXiv:2507.21802v5 Announce Type: replace Abstract: Although GRPO substantially enhances flow matching models in human preference alignment of image generation, methods such as FlowGRPO and DanceGRPO still exhibit inefficiency due to the necessity of sampling and optimizing over all denoising steps specified by the Markov Decision Process (MDP). In",
    "url": "https://arxiv.org/abs/2507.21802",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Building Scaffolding Dialogue Data with LLM-Simulated Novices",
    "summary": "arXiv:2508.04428v2 Announce Type: replace Abstract: High-quality, multi-turn instructional dialogues between novices and experts are essential for developing AI systems that support teaching, learning, and decision-making. These dialogues often involve scaffolding -- the process by which an expert supports a novice's thinking through questions, fee",
    "url": "https://arxiv.org/abs/2508.04428",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision",
    "summary": "arXiv:2508.08688v3 Announce Type: replace Abstract: Vision-language models (VLMs) have made significant strides in reasoning, yet they often struggle with complex multimodal tasks and tend to generate overly verbose outputs. A key limitation is their reliance on chain-of-thought (CoT) reasoning, despite many tasks benefiting from alternative topolo",
    "url": "https://arxiv.org/abs/2508.08688",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Transduction is All You Need for Structured Data Workflows",
    "summary": "arXiv:2508.15610v3 Announce Type: replace Abstract: This paper introduces Agentics, a functional agentic AI framework for building LLM-based structured data workflow pipelines. Designed for both research and practical applications, Agentics offers a new data-centric paradigm in which agents are embedded within data types, enabling logical transduct",
    "url": "https://arxiv.org/abs/2508.15610",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Information Templates: A New Paradigm for Intelligent Active Feature Acquisition",
    "summary": "arXiv:2508.18380v2 Announce Type: replace Abstract: Active feature acquisition (AFA) is an instance-adaptive paradigm in which, at inference time, a policy sequentially chooses which features to acquire (at a cost) before predicting. Existing approaches either train reinforcement learning policies, which deal with a difficult MDP, or greedy policie",
    "url": "https://arxiv.org/abs/2508.18380",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "A Novel Framework for Uncertainty-Driven Adaptive Exploration",
    "summary": "arXiv:2509.03219v5 Announce Type: replace Abstract: Adaptive exploration methods propose ways to learn complex policies via alternating between exploration and exploitation. An important question for such methods is to determine the appropriate moment to switch between exploration and exploitation and vice versa. This is critical in domains that re",
    "url": "https://arxiv.org/abs/2509.03219",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization",
    "summary": "arXiv:2509.12434v3 Announce Type: replace Abstract: Software engineering presents complex, multi-step challenges for Large Language Models (LLMs), requiring reasoning over large codebases and coordinated tool use. The difficulty of these tasks is exemplified by benchmarks like SWE-bench, where current LLMs still struggle to resolve real-world issue",
    "url": "https://arxiv.org/abs/2509.12434",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Plug-and-Play Emotion Graphs for Compositional Prompting in Zero-Shot Speech Emotion Recognition",
    "summary": "arXiv:2509.25458v2 Announce Type: replace Abstract: Large audio-language models (LALMs) exhibit strong zero-shot performance across speech tasks but struggle with speech emotion recognition (SER) due to weak paralinguistic modeling and limited cross-modal reasoning. We propose Compositional Chain-of-Thought Prompting for Emotion Reasoning (CCoT-Emo",
    "url": "https://arxiv.org/abs/2509.25458",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Scaling Agents for Computer Use",
    "summary": "arXiv:2510.02250v2 Announce Type: replace Abstract: Computer-use agents (CUAs) hold promise for automating everyday digital tasks, but their performance on long-horizon, complex problems remains unreliable. Single-rollout execution is brittle, with small errors compounding over time and leading to high variance in outcomes. While prior work has att",
    "url": "https://arxiv.org/abs/2510.02250",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "How Catastrophic is Your LLM? Certifying Risk in Conversation",
    "summary": "arXiv:2510.03969v3 Announce Type: replace Abstract: Large Language Models (LLMs) can produce catastrophic responses in conversational settings that pose serious risks to public safety and security. Existing evaluations often fail to fully reveal these vulnerabilities because they rely on fixed attack prompt sequences, lack statistical guarantees, a",
    "url": "https://arxiv.org/abs/2510.03969",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing",
    "summary": "arXiv:2510.04670v3 Announce Type: replace Abstract: Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic Framework for Multimodal fMRI Response Encoding), an agnostic interface that standardizes time-aligned post-fusion tokens from varied encoders, a",
    "url": "https://arxiv.org/abs/2510.04670",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "DeepAgent: A General Reasoning Agent with Scalable Toolsets",
    "summary": "arXiv:2510.21618v3 Announce Type: replace Abstract: Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduc",
    "url": "https://arxiv.org/abs/2510.21618",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Mixed-Density Diffuser: Efficient Planning with Non-Uniform Temporal Resolution",
    "summary": "arXiv:2510.23026v4 Announce Type: replace Abstract: Recent studies demonstrate that diffusion planners benefit from sparse-step planning over single-step planning. Training models to skip steps in their trajectories helps capture long-term dependencies without additional memory or computational cost. However, predicting excessively sparse plans deg",
    "url": "https://arxiv.org/abs/2510.23026",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching",
    "summary": "arXiv:2511.00640v2 Announce Type: replace Abstract: Large Reasoning Models (LRMs) achieve remarkable inference-time improvements through parallel thinking. However, existing methods rely on redundant sampling of reasoning trajectories, failing to effectively explore the reasoning space to uncover high-quality solutions. To address these limitations",
    "url": "https://arxiv.org/abs/2511.00640",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Extending RLVR to Open-Ended Tasks via Verifiable Multiple-Choice Reformulation",
    "summary": "arXiv:2511.02463v3 Announce Type: replace Abstract: Reinforcement Learning with Verifiable Rewards(RLVR) has demonstrated great potential in enhancing the reasoning capabilities of large language models (LLMs). However, its success has thus far been largely confined to the mathematical and programming domains with clear and automatically checkable ",
    "url": "https://arxiv.org/abs/2511.02463",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
    "summary": "arXiv:2511.08585v3 Announce Type: replace Abstract: The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual",
    "url": "https://arxiv.org/abs/2511.08585",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "CastMind: An Interaction-Driven Agentic Reasoning Framework for Cognition-Inspired Time Series Forecasting",
    "summary": "arXiv:2511.08947v2 Announce Type: replace Abstract: Time series forecasting plays a crucial role in decision-making across many real-world applications. Despite substantial progress, most existing methods still treat forecasting as a static, single-pass regression problem. In contrast, human experts form predictions through iterative reasoning that",
    "url": "https://arxiv.org/abs/2511.08947",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Incremental Maintenance of DatalogMTL Materialisations",
    "summary": "arXiv:2511.12169v3 Announce Type: replace Abstract: DatalogMTL extends the classical Datalog language with metric temporal logic (MTL), enabling expressive reasoning over temporal data. While existing reasoning approaches, such as materialisation based and automata based methods, offer soundness and completeness, they lack support for handling effi",
    "url": "https://arxiv.org/abs/2511.12169",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "M^3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark",
    "summary": "arXiv:2511.17729v4 Announce Type: replace Abstract: We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resou",
    "url": "https://arxiv.org/abs/2511.17729",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Sample from What You See: Visuomotor Policy Learning via Diffusion Bridge with Observation-Embedded Stochastic Differential Equation",
    "summary": "arXiv:2512.07212v2 Announce Type: replace Abstract: Imitation learning with diffusion models has advanced robotic control by capturing the multi-modal action distributions. However, existing methods typically treat observations only as high-level conditions to the denoising network, rather than integrating them into the stochastic dynamics of the d",
    "url": "https://arxiv.org/abs/2512.07212",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "User-Feedback-Driven Adaptation for Vision-and-Language Navigation",
    "summary": "arXiv:2512.10322v2 Announce Type: replace Abstract: Real-world deployment of Vision-and-Language Navigation (VLN) agents is constrained by the scarcity of reliable supervision after offline training. While recent adaptation methods attempt to mitigate distribution shifts via environment-driven self-supervision (e.g., entropy minimization), these si",
    "url": "https://arxiv.org/abs/2512.10322",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation",
    "summary": "arXiv:2512.21066v2 Announce Type: replace Abstract: Explainable artificial intelligence (XAI) enables data-driven understanding of factor associations with response variables, yet communicating XAI outputs to laypersons remains challenging, hindering trust in AI-based predictions. Large language models (LLMs) have emerged as promising tools for tra",
    "url": "https://arxiv.org/abs/2512.21066",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines",
    "summary": "arXiv:2601.09465v2 Announce Type: replace Abstract: While LLM-based agents have shown promise for deep research, most existing approaches rely on fixed workflows that struggle to adapt to real-world, open-ended queries. Recent work therefore explores self-evolution by allowing agents to rewrite their own code or prompts to improve problem-solving a",
    "url": "https://arxiv.org/abs/2601.09465",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Resilient Routing: Risk-Aware Dynamic Routing in Smart Logistics via Spatiotemporal Graph Learning",
    "summary": "arXiv:2601.13632v2 Announce Type: replace Abstract: With the rapid development of the e-commerce industry, the logistics network is experiencing unprecedented pressure. The traditional static routing strategy most time cannot tolerate the traffic congestion and fluctuating retail demand. In this paper, we propose a Risk-Aware Dynamic Routing(RADR) ",
    "url": "https://arxiv.org/abs/2601.13632",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "DEEPMED: Building a Medical DeepResearch Agent via Multi-hop Med-Search Data and Turn-Controlled Agentic Training & Inference",
    "summary": "arXiv:2601.18496v2 Announce Type: replace Abstract: Medical reasoning models remain constrained by parametric knowledge and are thus susceptible to forgetting and hallucinations. DeepResearch (DR) models ground outputs in verifiable evidence from tools and perform strongly in general domains, but their direct transfer to medical field yields relati",
    "url": "https://arxiv.org/abs/2601.18496",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection",
    "summary": "arXiv:2601.19245v2 Announce Type: replace Abstract: Hallucination detection is critical for deploying large language models (LLMs) in real-world applications. Existing hallucination detection methods achieve strong performance when the training and test data come from the same domain, but they suffer from poor cross-domain generalization. In this p",
    "url": "https://arxiv.org/abs/2601.19245",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving",
    "summary": "arXiv:2601.21164v2 Announce Type: replace Abstract: Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by th",
    "url": "https://arxiv.org/abs/2601.21164",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization",
    "summary": "arXiv:2601.21358v2 Announce Type: replace Abstract: Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning w",
    "url": "https://arxiv.org/abs/2601.21358",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning",
    "summary": "arXiv:2601.21468v2 Announce Type: replace Abstract: Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value deta",
    "url": "https://arxiv.org/abs/2601.21468",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic",
    "summary": "arXiv:2601.21972v2 Announce Type: replace Abstract: Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as ag",
    "url": "https://arxiv.org/abs/2601.21972",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Scaling Multiagent Systems with Process Rewards",
    "summary": "arXiv:2601.23228v2 Announce Type: replace Abstract: While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multia",
    "url": "https://arxiv.org/abs/2601.23228",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "ConvexBench: Can LLMs Recognize Convex Functions?",
    "summary": "arXiv:2602.01075v2 Announce Type: replace Abstract: Convex analysis is a modern branch of mathematics with many applications. As Large Language Models (LLMs) start to automate research-level math and sciences, it is important for LLMs to demonstrate the ability to understand and reason with convexity. We introduce \\cb, a scalable and mechanically v",
    "url": "https://arxiv.org/abs/2602.01075",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "CreditAudit: 2$^\\text{nd}$ Dimension for LLM Evaluation and Selection",
    "summary": "arXiv:2602.02515v2 Announce Type: replace Abstract: Leaderboard scores on public benchmarks have been steadily rising and converging, with many frontier language models now separated by only marginal differences. However, these scores often fail to match users' day to day experience, because system prompts, output protocols, and interaction modes e",
    "url": "https://arxiv.org/abs/2602.02515",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Building Interpretable Models for Moral Decision-Making",
    "summary": "arXiv:2602.03351v2 Announce Type: replace Abstract: We build a custom transformer model to study how neural networks make moral decisions on trolley-style dilemmas. The model processes structured scenarios using embeddings that encode who is affected, how many people, and which outcome they belong to. Our 2-layer architecture achieves 77% accuracy ",
    "url": "https://arxiv.org/abs/2602.03351",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Multi-Excitation Projective Simulation with a Many-Body Physics Inspired Inductive Bias",
    "summary": "arXiv:2402.10192v4 Announce Type: replace-cross Abstract: With the impressive progress of deep learning, applications relying on machine learning are increasingly being integrated into daily life. However, most deep learning models have an opaque, oracle-like nature making it difficult to interpret and understand their decisions. This problem led t",
    "url": "https://arxiv.org/abs/2402.10192",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Policy Learning with a Language Bottleneck",
    "summary": "arXiv:2405.04118v3 Announce Type: replace-cross Abstract: Modern AI systems such as self-driving cars and game-playing agents achieve superhuman performance, but often lack human-like generalization, interpretability, and inter-operability with human users. Inspired by the rich interactions between language and decision-making in humans, we introdu",
    "url": "https://arxiv.org/abs/2405.04118",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "No Screening is More Efficient with Multiple Objects",
    "summary": "arXiv:2408.10077v3 Announce Type: replace-cross Abstract: We study efficient mechanism design for allocating multiple heterogeneous objects. The aim is to maximize the residual surplus, the total value generated from an allocation minus the costs of screening. We discover a robust trend indicating that no-screening mechanisms, such as serial dictat",
    "url": "https://arxiv.org/abs/2408.10077",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Deep Multimodal Learning with Missing Modality: A Survey",
    "summary": "arXiv:2409.07825v4 Announce Type: replace-cross Abstract: During multimodal model training and testing, certain data modalities may be absent due to sensor limitations, cost constraints, privacy concerns, or data loss, negatively affecting performance. Multimodal learning techniques designed to handle missing modalities can mitigate this by ensurin",
    "url": "https://arxiv.org/abs/2409.07825",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Learning to Explore with Lagrangians for Bandits under Unknown Linear Constraints",
    "summary": "arXiv:2410.18844v2 Announce Type: replace-cross Abstract: Pure exploration in bandits formalises multiple real-world problems, such as tuning hyper-parameters or conducting user studies to test a set of items, where different safety, resource, and fairness constraints on the decision space naturally appear. We study these problems as pure explorati",
    "url": "https://arxiv.org/abs/2410.18844",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "LLM-ABBA: Understanding time series via symbolic approximation",
    "summary": "arXiv:2411.18506v5 Announce Type: replace-cross Abstract: The success of large language models (LLMs) for time series has been demonstrated in previous work. Utilizing a symbolic time series representation, one can efficiently bridge the gap between LLMs and time series. However, the remaining challenge is to exploit the semantic information hidden",
    "url": "https://arxiv.org/abs/2411.18506",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "AI-Powered CPS-Enabled Vulnerable-User-Aware Urban Transportation Digital Twin: Methods and Applications",
    "summary": "arXiv:2501.10396v3 Announce Type: replace-cross Abstract: We present methods and applications for the development of digital twins (DT) for urban traffic management. While the majority of studies on the DT focus on its ``eyes,\" which is the emerging sensing and perception like object detection and tracking, what really distinguishes the DT from a t",
    "url": "https://arxiv.org/abs/2501.10396",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "DISCOVER: Identifying Patterns of Daily Living in Human Activities from Smart Home Data",
    "summary": "arXiv:2503.01733v3 Announce Type: replace-cross Abstract: Smart homes equipped with ambient sensors offer a transformative approach to continuous health monitoring and assisted living. Traditional research in this domain primarily focuses on Human Activity Recognition (HAR), which relies on mapping sensor data to a closed set of predefined activity",
    "url": "https://arxiv.org/abs/2503.01733",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Persuade Me if You Can: A Framework for Evaluating Persuasion Effectiveness and Susceptibility Among Large Language Models",
    "summary": "arXiv:2503.01829v3 Announce Type: replace-cross Abstract: Large Language Models (LLMs) demonstrate persuasive capabilities that rival human-level persuasion. While these capabilities can be used for social good, they also present risks of potential misuse. Beyond the concern of how LLMs persuade others, their own susceptibility to persuasion poses ",
    "url": "https://arxiv.org/abs/2503.01829",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Large Language Model as Meta-Surrogate for Data-Driven Many-Task Optimization: A Proof-of-Principle Study",
    "summary": "arXiv:2503.08301v3 Announce Type: replace-cross Abstract: In many-task optimization scenarios, surrogate models are valuable for mitigating the computational burden of repeated fitness evaluations across tasks. This study proposes a novel meta-surrogate framework to assist many-task optimization, by leveraging the knowledge transfer strengths and e",
    "url": "https://arxiv.org/abs/2503.08301",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Explainable Sentiment Analysis with DeepSeek-R1: Performance, Efficiency, and Few-Shot Learning",
    "summary": "arXiv:2503.11655v5 Announce Type: replace-cross Abstract: Large language models (LLMs) have transformed sentiment analysis, yet balancing accuracy, efficiency, and explainability remains a critical challenge. This study presents the first comprehensive evaluation of DeepSeek-R1--an open-source reasoning model--against OpenAI's GPT-4o and GPT-4o-min",
    "url": "https://arxiv.org/abs/2503.11655",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "LLM Agents for Education: Advances and Applications",
    "summary": "arXiv:2503.11733v2 Announce Type: replace-cross Abstract: Large Language Model (LLM) agents are transforming education by automating complex pedagogical tasks and enhancing both teaching and learning processes. In this survey, we present a systematic review of recent advances in applying LLM agents to address key challenges in educational settings,",
    "url": "https://arxiv.org/abs/2503.11733",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "AccidentSim: Generating Vehicle Collision Videos with Physically Realistic Collision Trajectories from Real-World Accident Reports",
    "summary": "arXiv:2503.20654v3 Announce Type: replace-cross Abstract: Collecting real-world vehicle accident videos for autonomous driving research is challenging due to their rarity and complexity. While existing driving video generation methods may produce visually realistic videos, they often fail to deliver physically realistic simulations because they lac",
    "url": "https://arxiv.org/abs/2503.20654",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Beyond speculation: Measuring the growing presence of LLM-generated texts in multilingual disinformation",
    "summary": "arXiv:2503.23242v2 Announce Type: replace-cross Abstract: Increased sophistication of large language models (LLMs) and the consequent quality of generated multilingual text raises concerns about potential disinformation misuse. While humans struggle to distinguish LLM-generated content from human-written texts, the scholarly debate about their impa",
    "url": "https://arxiv.org/abs/2503.23242",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Adaptive Helpfulness-Harmlessness Alignment with Preference Vectors",
    "summary": "arXiv:2504.20106v3 Announce Type: replace-cross Abstract: Ensuring that large language models (LLMs) are both helpful and harmless is a critical challenge, as overly strict constraints can lead to excessive refusals, while permissive models risk generating harmful content. Existing approaches, such as reinforcement learning from human feedback (RLH",
    "url": "https://arxiv.org/abs/2504.20106",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Dynamic and Distributed Routing in IoT Networks based on Multi-Objective Q-Learning",
    "summary": "arXiv:2505.00918v4 Announce Type: replace-cross Abstract: IoT networks often face conflicting routing goals such as maximizing packet delivery, minimizing delay, and conserving limited battery energy. These priorities can also change dynamically: for example, an emergency alert requires high reliability, while routine monitoring prioritizes energy ",
    "url": "https://arxiv.org/abs/2505.00918",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs",
    "summary": "arXiv:2505.13697v4 Announce Type: replace-cross Abstract: Reinforcement learning based post-training of large language models (LLMs) has recently gained attention, particularly following the release of DeepSeek R1, which applied GRPO for fine-tuning. Amid the growing claims around improved reasoning abilities attributed to RL post-training, we crit",
    "url": "https://arxiv.org/abs/2505.13697",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Are Graph Attention Networks Able to Model Structural Information?",
    "summary": "arXiv:2505.21288v2 Announce Type: replace-cross Abstract: Graph Attention Networks (GATs) have emerged as powerful models for learning expressive representations from such data by adaptively weighting neighboring nodes through attention mechanisms. However, most existing approaches primarily rely on node attributes and direct neighborhood connectio",
    "url": "https://arxiv.org/abs/2505.21288",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "CodeSense: a Real-World Benchmark and Dataset for Code Semantic Reasoning",
    "summary": "arXiv:2506.00750v3 Announce Type: replace-cross Abstract: Understanding and reasoning about code semantics is essential for enhancing code LLMs' abilities to solve real-world software engineering (SE) tasks. Although several code reasoning benchmarks exist, most rely on synthetic datasets or educational coding problems and focus on coarse-grained r",
    "url": "https://arxiv.org/abs/2506.00750",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "GRAM: Spatial general-purpose audio representation models for real-world applications",
    "summary": "arXiv:2506.00934v5 Announce Type: replace-cross Abstract: Audio foundation models learn general-purpose audio representations that facilitate a wide range of downstream tasks. While the performance of these models has greatly increased for conventional single-channel, dry audio clips, their success in real-world acoustic environments with reverbera",
    "url": "https://arxiv.org/abs/2506.00934",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving",
    "summary": "arXiv:2506.01374v5 Announce Type: replace-cross Abstract: While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compiler",
    "url": "https://arxiv.org/abs/2506.01374",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Graph Persistence goes Spectral",
    "summary": "arXiv:2506.06571v3 Announce Type: replace-cross Abstract: Including intricate topological information (e.g., cycles) provably enhances the expressivity of message-passing graph neural networks (GNNs) beyond the Weisfeiler-Leman (WL) hierarchy. Consequently, Persistent Homology (PH) methods are increasingly employed for graph representation learning",
    "url": "https://arxiv.org/abs/2506.06571",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO",
    "summary": "arXiv:2506.07464v5 Announce Type: replace-cross Abstract: Recent works have demonstrated the effectiveness of reinforcement learning (RL)-based post-training for enhancing the reasoning capabilities of large language models (LLMs). In particular, Group Relative Policy Optimization (GRPO) has shown impressive success using a PPO-style reinforcement ",
    "url": "https://arxiv.org/abs/2506.07464",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "TRACE: Transparent Web Reliability Assessment with Contextual Explanations",
    "summary": "arXiv:2506.12072v3 Announce Type: replace-cross Abstract: In an era of AI-generated misinformation flooding the web, existing tools struggle to empower users with nuanced, transparent assessments of content credibility. They often default to binary (true/false) classifications without contextual justifications, leaving users vulnerable to disinform",
    "url": "https://arxiv.org/abs/2506.12072",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Taking the GP Out of the Loop",
    "summary": "arXiv:2506.12818v2 Announce Type: replace-cross Abstract: Bayesian optimization (BO) has traditionally solved black-box problems where function evaluation is expensive and, therefore, observations are few. Recently, however, there has been growing interest in applying BO to problems where function evaluation is cheaper and observations are more ple",
    "url": "https://arxiv.org/abs/2506.12818",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Accurate and scalable exchange-correlation with deep learning",
    "summary": "arXiv:2506.14665v5 Announce Type: replace-cross Abstract: Density Functional Theory (DFT) is the most widely used electronic structure method for predicting the properties of molecules and materials. Although DFT is, in principle, an exact reformulation of the Schr\\\"odinger equation, practical applications rely on approximations to the unknown exch",
    "url": "https://arxiv.org/abs/2506.14665",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks",
    "summary": "arXiv:2506.21142v2 Announce Type: replace-cross Abstract: The growing integration of UAVs into civilian airspace underscores the need for resilient and intelligent intrusion detection systems (IDS), as traditional anomaly detection methods often fail to identify novel threats. A common approach treats unfamiliar attacks as out-of-distribution (OOD)",
    "url": "https://arxiv.org/abs/2506.21142",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "AI-Generated Video Detection via Perceptual Straightening",
    "summary": "arXiv:2507.00583v4 Announce Type: replace-cross Abstract: The rapid advancement of generative AI enables highly realistic synthetic videos, posing significant challenges for content authentication and raising urgent concerns about misuse. Existing detection methods often struggle with generalization and capturing subtle temporal inconsistencies. We",
    "url": "https://arxiv.org/abs/2507.00583",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Geometry-aware 4D Video Generation for Robot Manipulation",
    "summary": "arXiv:2507.01099v3 Announce Type: replace-cross Abstract: Understanding and predicting dynamics of the physical world can enhance a robot's ability to plan and interact effectively in complex environments. While recent video generation models have shown strong potential in modeling dynamic scenes, generating videos that are both temporally coherent",
    "url": "https://arxiv.org/abs/2507.01099",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders",
    "summary": "arXiv:2507.03262v3 Announce Type: replace-cross Abstract: Recent multimodal large language models (MLLMs) increasingly integrate multiple vision encoders to improve performance on various benchmarks, assuming that diverse pretraining objectives yield complementary visual signals. However, we show this assumption often fails in practice. Through sys",
    "url": "https://arxiv.org/abs/2507.03262",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy",
    "summary": "arXiv:2507.06969v4 Announce Type: replace-cross Abstract: Differentially private (DP) mechanisms are difficult to interpret and calibrate because existing methods for mapping standard privacy parameters to concrete privacy risks -- re-identification, attribute inference, and data reconstruction -- are both overly pessimistic and inconsistent. In th",
    "url": "https://arxiv.org/abs/2507.06969",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Neural Concept Verifier: Scaling Prover-Verifier Games via Concept Encodings",
    "summary": "arXiv:2507.07532v3 Announce Type: replace-cross Abstract: While Prover-Verifier Games (PVGs) offer a promising path toward verifiability in nonlinear classification models, they have not yet been applied to complex inputs such as high-dimensional images. Conversely, expressive concept encodings effectively allow to translate such data into interpre",
    "url": "https://arxiv.org/abs/2507.07532",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "DPO Unchained: Your Training Algorithm is Secretly Disentangled in Human Choice Theory",
    "summary": "arXiv:2507.07855v3 Announce Type: replace-cross Abstract: Normative theories allow one to elicit key parts of a ML algorithm from first principles, which is crucial at a time of championed scrutiny for ML work. Direct Preference Optimization (DPO) cleverly bypasses reward modeling by making an explicit link with a specific normative model of human ",
    "url": "https://arxiv.org/abs/2507.07855",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for Cross-Lingual Multi-Emotion Detection in Short Texts",
    "summary": "arXiv:2507.08499v2 Announce Type: replace-cross Abstract: This paper presents our system for SemEval 2025 Task 11: Bridging the Gap in Text-Based Emotion Detection (Track A), which focuses on multi-label emotion detection in short texts. We propose a feature-centric framework that dynamically adapts document representations and learning algorithms ",
    "url": "https://arxiv.org/abs/2507.08499",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Multiple Choice Learning of Low-Rank Adapters for Language Modeling",
    "summary": "arXiv:2507.10419v2 Announce Type: replace-cross Abstract: We propose LoRA-MCL, a training scheme that extends next-token prediction in language models with a method designed to decode diverse, plausible sentence continuations at inference time. Traditional language modeling is an intrinsically ill-posed problem: given a context, multiple ``futures'",
    "url": "https://arxiv.org/abs/2507.10419",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder",
    "summary": "arXiv:2507.13551v2 Announce Type: replace-cross Abstract: Formal thought disorder (FTD), a hallmark of schizophrenia spectrum disorders, manifests as incoherent speech and poses challenges for clinical assessment. Traditional clinical rating scales, though validated, are resource-intensive and lack scalability. Automated speech recognition (ASR) al",
    "url": "https://arxiv.org/abs/2507.13551",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "The Invisible Leash: Why RLVR May or May Not Escape Its Origin",
    "summary": "arXiv:2507.14843v4 Announce Type: replace-cross Abstract: Recent advances highlight Reinforcement Learning with Verifiable Rewards (RLVR) as a promising method for enhancing LLMs' capabilities. However, it remains unclear whether the current practice of RLVR truly expands a model's reasoning boundary or mainly amplifies high-reward outputs that the",
    "url": "https://arxiv.org/abs/2507.14843",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Quantization-Aware Neuromorphic Architecture for Skin Disease Classification on Resource-Constrained Devices",
    "summary": "arXiv:2507.15958v4 Announce Type: replace-cross Abstract: On-device skin lesion analysis is constrained by the compute and energy cost of conventional CNN inference and by the need to update models as new patient data become available. Neuromorphic processors provide event-driven sparse computation and support on-chip incremental learning, yet depl",
    "url": "https://arxiv.org/abs/2507.15958",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Analysis of Fourier Neural Operators via Effective Field Theory",
    "summary": "arXiv:2507.21833v3 Announce Type: replace-cross Abstract: Fourier Neural Operators (FNOs) have emerged as leading surrogates for solver operators for various functional problems, yet their stability, generalization and frequency behavior lack a principled explanation. We present a systematic effective field theory analysis of FNOs in an infinite-di",
    "url": "https://arxiv.org/abs/2507.21833",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs",
    "summary": "arXiv:2508.03365v3 Announce Type: replace-cross Abstract: As large language models (LLMs) become increasingly integrated into daily life, audio has emerged as a key interface for human-AI interaction. However, this convenience also introduces new vulnerabilities, making audio a potential attack surface for adversaries. Our research introduces Whisp",
    "url": "https://arxiv.org/abs/2508.03365",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Towards Universal Neural Likelihood Inference",
    "summary": "arXiv:2508.09100v2 Announce Type: replace-cross Abstract: We introduce universal neural likelihood inference (UNLI): enabling a single model to provide data-grounded, conditional likelihood predictions for arbitrary targets given any collection of observed features, across diverse domains and tasks. To achieve UNLI over heterogeneous tabular data, ",
    "url": "https://arxiv.org/abs/2508.09100",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Mutually Assured Deregulation",
    "summary": "arXiv:2508.12300v3 Announce Type: replace-cross Abstract: We have convinced ourselves that the way to make AI safe is to make it unsafe. Since 2022, policymakers worldwide have embraced the Regulation Sacrifice - the belief that dismantling safety oversight will deliver security through AI dominance. Fearing China or USA will gain advantage, nation",
    "url": "https://arxiv.org/abs/2508.12300",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Input-Time Scaling: Adding Noise and Irrelevance into Less-Is-More Drastically Improves Reasoning Performance and Efficiency",
    "summary": "arXiv:2508.13654v5 Announce Type: replace-cross Abstract: Large Language Models (LLMs) excel at reasoning, traditionally requiring high-quality large-scale data and extensive training. Recent works reveal a very appealing Less-Is-More phenomenon where very small, carefully curated high-quality datasets match resource-intensive approaches. In this w",
    "url": "https://arxiv.org/abs/2508.13654",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Toward Substantive Intersectional Algorithmic Fairness: Desiderata for a Feminist Approach",
    "summary": "arXiv:2508.17944v2 Announce Type: replace-cross Abstract: People's experiences of discrimination are often shaped by multiple intersecting factors, yet algorithmic fairness research rarely reflects this complexity. While intersectionality offers tools for understanding how forms of oppression interact, current approaches to intersectional algorithm",
    "url": "https://arxiv.org/abs/2508.17944",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning",
    "summary": "arXiv:2509.06165v5 Announce Type: replace-cross Abstract: Video Scene Graph Generation (VidSGG) aims to represent dynamic visual content by detecting objects and modeling their temporal interactions as structured graphs. Prior studies typically target either coarse-grained box-level or fine-grained panoptic pixel-level VidSGG, often requiring task-",
    "url": "https://arxiv.org/abs/2509.06165",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Thermal Imaging-based Real-time Fall Detection using Motion Flow and Attention-enhanced Convolutional Recurrent Architecture",
    "summary": "arXiv:2509.16479v2 Announce Type: replace-cross Abstract: Falls among seniors are a major public health issue. Existing solutions using wearable sensors, ambient sensors, and RGB-based vision systems face challenges in reliability, user compliance, and practicality. Studies indicate that stakeholders, such as older adults and eldercare facilities, ",
    "url": "https://arxiv.org/abs/2509.16479",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "MapCoder-Lite: Distilling Multi-Agent Coding into a Single Small LLM",
    "summary": "arXiv:2509.17489v2 Announce Type: replace-cross Abstract: Large language models (LLMs) have advanced code generation from single-function tasks to competitive-programming problems, but existing multi-agent solutions either rely on costly large-scale (>30B) models or collapse when downsized to small open-source models. We present MapCoder-Lite, a fr",
    "url": "https://arxiv.org/abs/2509.17489",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Anticipatory Evaluation of Language Models",
    "summary": "arXiv:2509.20645v3 Announce Type: replace-cross Abstract: Progress in large language models is increasingly constrained by an evaluation bottleneck: benchmarks must be built and models run before iteration can begin. We investigate whether evaluation outcomes can be forecast before any experiments are conducted. Specifically, we study text-only per",
    "url": "https://arxiv.org/abs/2509.20645",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Less Precise Can Be More Reliable: A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy",
    "summary": "arXiv:2509.21173v4 Announce Type: replace-cross Abstract: Vision-Language Models (VLMs) such as CLIP have revolutionized zero-shot classification and safety-critical tasks, including Out-of-Distribution (OOD) detection. However, their high computational cost hinders efficient real-world deployment. While quantization is a standard solution for effi",
    "url": "https://arxiv.org/abs/2509.21173",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents",
    "summary": "arXiv:2509.23040v3 Announce Type: replace-cross Abstract: Large language models face challenges in long-context question answering, where key evidence of a query may be dispersed across millions of tokens. Existing works equip large language models with a memory buffer that is dynamically updated via a linear document scan, also known as the \"memor",
    "url": "https://arxiv.org/abs/2509.23040",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy",
    "summary": "arXiv:2509.24385v2 Announce Type: replace-cross Abstract: Recent developments in Multimodal Large Language Models (MLLMs) have significantly improved Vision-Language (VL) reasoning in 2D domains. However, extending these capabilities to 3D scene understanding remains a major challenge. Existing 3D Multimodal Large Language Models (3D-MLLMs) often d",
    "url": "https://arxiv.org/abs/2509.24385",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Causal-Adapter: Taming Text-to-Image Diffusion for Faithful Counterfactual Generation",
    "summary": "arXiv:2509.24798v5 Announce Type: replace-cross Abstract: We present Causal-Adapter, a modular framework that adapts frozen text-to-image diffusion backbones for counterfactual image generation. Our method supports causal interventions on target attributes and consistently propagates their effects to causal dependents while preserving the core iden",
    "url": "https://arxiv.org/abs/2509.24798",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "EMO-TTA: Improving Test-Time Adaptation of Audio-Language Models for Speech Emotion Recognition",
    "summary": "arXiv:2509.25495v2 Announce Type: replace-cross Abstract: Speech emotion recognition (SER) with audio-language models (ALMs) remains vulnerable to distribution shifts at test time, leading to performance degradation in out-of-domain scenarios. Test-time adaptation (TTA) provides a promising solution but often relies on gradient-based updates or pro",
    "url": "https://arxiv.org/abs/2509.25495",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression",
    "summary": "arXiv:2510.02345v3 Announce Type: replace-cross Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load imbalance, parameter redundancy, and communication overhead. We introduce a unified framework based on dynamic expert clustering and structured compression to address these issues cohesively. Our method employs an ",
    "url": "https://arxiv.org/abs/2510.02345",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks",
    "summary": "arXiv:2510.02712v4 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their robustness in extended multi-turn dialogues remains poorly understood. Existing evaluation frameworks focus on static benchmarks and single-turn assessments, failing to capture the temporal dynamics of conversation",
    "url": "https://arxiv.org/abs/2510.02712",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Cooperative Flexibility Exchange: Fair and Comfort-Aware Decentralized Resource Allocation",
    "summary": "arXiv:2510.04192v2 Announce Type: replace-cross Abstract: The growing electricity demand and use of smart appliances are placing pressure on power grids, making efficient energy management more important than ever. The existing energy management systems often prioritize system efficiency (balanced energy demand and supply) at the expense of consume",
    "url": "https://arxiv.org/abs/2510.04192",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates",
    "summary": "arXiv:2510.04769v2 Announce Type: replace-cross Abstract: Many machine learning algorithms rely on iterative updates of uncertainty representations, ranging from variational inference and expectation-maximization, to reinforcement learning, continual learning, and multi-agent learning. In the presence of imprecision and ambiguity, credal sets -- cl",
    "url": "https://arxiv.org/abs/2510.04769",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments",
    "summary": "arXiv:2510.09712v3 Announce Type: replace-cross Abstract: Online fake news profoundly distorts public judgment and erodes trust in social platforms. While existing detectors achieve competitive performance on benchmark datasets, they remain notably vulnerable to malicious comments designed specifically to induce misclassification. This evolving thr",
    "url": "https://arxiv.org/abs/2510.09712",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Y-Shaped Generative Flows",
    "summary": "arXiv:2510.11955v3 Announce Type: replace-cross Abstract: Modern continuous-time generative models typically induce \\emph{V-shaped} flows: each sample travels independently along a nearly straight trajectory from the prior to the data. Although effective, this independent movement overlooks the hierarchical structures that exist in real-world data.",
    "url": "https://arxiv.org/abs/2510.11955",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection",
    "summary": "arXiv:2510.13893v2 Announce Type: replace-cross Abstract: Jailbreaking techniques pose a significant threat to the safety of Large Language Models (LLMs). Existing defenses typically focus on single-turn attacks, lack coverage across languages, and rely on limited taxonomies that either fail to capture the full diversity of attack strategies or emp",
    "url": "https://arxiv.org/abs/2510.13893",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "LiDAR-based 3D Change Detection at City Scale",
    "summary": "arXiv:2510.21112v2 Announce Type: replace-cross Abstract: High-definition 3D city maps enable city planning and change detection, which is essential for municipal compliance, map maintenance, and asset monitoring, including both built structures and urban greenery. Conventional Digital Surface Model (DSM) and image differencing are sensitive to ver",
    "url": "https://arxiv.org/abs/2510.21112",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "A Research Roadmap for Augmenting Software Engineering Processes and Software Products with Generative AI",
    "summary": "arXiv:2510.26275v3 Announce Type: replace-cross Abstract: Generative AI (GenAI) is rapidly transforming software engineering (SE) practices, influencing how SE processes are executed, as well as how software systems are developed, operated, and evolved. This paper applies design science research to build a roadmap for GenAI-augmented SE. The proces",
    "url": "https://arxiv.org/abs/2510.26275",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Comparing Task-Agnostic Embedding Models for Tabular Data",
    "summary": "arXiv:2511.14276v2 Announce Type: replace-cross Abstract: Recent foundation models for tabular data achieve strong task-specific performance via in-context learning. Nevertheless, they focus on direct prediction by encapsulating both representation learning and task-specific inference inside a single, resource-intensive network. This work specifica",
    "url": "https://arxiv.org/abs/2511.14276",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Semantics as a Shield: Label Disguise Defense (LDD) against Prompt Injection in LLM Sentiment Classification",
    "summary": "arXiv:2511.21752v2 Announce Type: replace-cross Abstract: Large language models are increasingly used for text classification tasks such as sentiment analysis, yet their reliance on natural language prompts exposes them to prompt injection attacks. In particular, class-directive injections exploit knowledge of the model's label set (e.g., positive ",
    "url": "https://arxiv.org/abs/2511.21752",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes",
    "summary": "arXiv:2512.00771v2 Announce Type: replace-cross Abstract: Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, e",
    "url": "https://arxiv.org/abs/2512.00771",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering",
    "summary": "arXiv:2512.06655v2 Announce Type: replace-cross Abstract: Large language models (LLMs) face critical safety challenges, as they can be manipulated to generate harmful content through adversarial prompts and jailbreak attacks. Many defenses are typically either black-box guardrails that filter outputs, or internals-based methods that steer hidden ac",
    "url": "https://arxiv.org/abs/2512.06655",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Near--Real-Time Conflict-Related Fire Detection Using Unsupervised Deep Learning and Satellite Imagery",
    "summary": "arXiv:2512.07925v2 Announce Type: replace-cross Abstract: Ongoing armed conflict in Sudan highlights the need for rapid monitoring of conflict-related fire damage. Recent advances in deep learning and high-frequency satellite imagery enable near--real-time assessment of active fires and burn scars in war zones. This study presents a near--real-time",
    "url": "https://arxiv.org/abs/2512.07925",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "InfoTok: Adaptive Discrete Video Tokenizer via Information-Theoretic Compression",
    "summary": "arXiv:2512.16975v2 Announce Type: replace-cross Abstract: Accurate and efficient discrete video tokenization is essential for long video sequences processing. Yet, the inherent complexity and variable information density of videos present a significant bottleneck for current tokenizers, which rigidly compress all content at a fixed rate, leading to",
    "url": "https://arxiv.org/abs/2512.16975",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale",
    "summary": "arXiv:2512.19097v2 Announce Type: replace-cross Abstract: Unifying the vast heterogeneity of brain signals into a single foundation model is a longstanding challenge in neuroscience. Yet, even as large-scale pretraining becomes feasible, the field lacks principled guidance on how to scale electrophysiological foundation models under realistic data ",
    "url": "https://arxiv.org/abs/2512.19097",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Synergizing Kolmogorov-Arnold Networks with Dynamic Adaptive Weighting for High-Frequency and Multi-Scale PDE Solutions",
    "summary": "arXiv:2512.22283v2 Announce Type: replace-cross Abstract: PINNs enhance scientific computing by incorporating physical laws into neural network structures, leading to significant advancements in scientific computing. However, PINNs struggle with multi-scale and high-frequency problems due to pathological gradient flow and spectral bias, which sever",
    "url": "https://arxiv.org/abs/2512.22283",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents",
    "summary": "arXiv:2512.22387v2 Announce Type: replace-cross Abstract: The rise of Large Language Models (LLMs) as coding agents promises to accelerate software development, but their impact on generated code reproducibility remains largely unexplored. This paper presents an empirical study investigating whether LLM-generated code can be executed successfully i",
    "url": "https://arxiv.org/abs/2512.22387",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
    "summary": "arXiv:2601.01874v2 Announce Type: replace-cross Abstract: Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and i",
    "url": "https://arxiv.org/abs/2601.01874",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "The World is Not Mono: Enabling Spatial Understanding in Large Audio-Language Models",
    "summary": "arXiv:2601.02954v2 Announce Type: replace-cross Abstract: Existing large audio-language models perceive the world as \"mono\"-a single stream of audio that ignores the critical spatial dimension (\"where\") required for universal audio scene analysis (ASA). To bridge this gap, we first introduce a hierarchical framework for audio scene analysis. Guided",
    "url": "https://arxiv.org/abs/2601.02954",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "GPU-Accelerated ANNS: Quantized for Speed, Built for Change",
    "summary": "arXiv:2601.07048v3 Announce Type: replace-cross Abstract: Approximate nearest neighbor search (ANNS) is a core problem in machine learning and information retrieval applications. GPUs offer a promising path to high-performance ANNS: they provide massive parallelism for distance computations, are readily available, and can co-locate with downstream ",
    "url": "https://arxiv.org/abs/2601.07048",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Attention Consistency Regularization for Interpretable Early-Exit Neural Networks",
    "summary": "arXiv:2601.08891v2 Announce Type: replace-cross Abstract: Early-exit neural networks enable adaptive inference by allowing predictions at intermediate layers, reducing computational cost. However, early exits often lack interpretability and may focus on different features than deeper layers, limiting trust and explainability. This paper presents Ex",
    "url": "https://arxiv.org/abs/2601.08891",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Bridging Cognitive Neuroscience and Graph Intelligence: Hippocampus-Inspired Multi-View Hypergraph Learning for Web Finance Fraud",
    "summary": "arXiv:2601.11073v2 Announce Type: replace-cross Abstract: Online financial services constitute an essential component of contemporary web ecosystems, yet their openness introduces substantial exposure to fraud that harms vulnerable users and weakens trust in digital finance. Such threats have become a significant web harm that erodes societal fairn",
    "url": "https://arxiv.org/abs/2601.11073",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "ConceptCaps: a Distilled Concept Dataset for Interpretability in Music Models",
    "summary": "arXiv:2601.14157v3 Announce Type: replace-cross Abstract: Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure: tags are sparse, noisy, or ill-defined. We introduce ConceptCaps, a dataset of 21k music-caption-tags triplets with exp",
    "url": "https://arxiv.org/abs/2601.14157",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Attention Is Not Retention: The Orthogonality Constraint in Infinite-Context Architectures",
    "summary": "arXiv:2601.15313v2 Announce Type: replace-cross Abstract: Biological memory solves a problem that eludes current AI: storing specific episodic facts without corrupting general semantic knowledge. Complementary Learning Systems theory explains this through two subsystems - a fast hippocampal system using sparse, pattern-separated representations for",
    "url": "https://arxiv.org/abs/2601.15313",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Low-Dimensional Adaptation of Rectified Flow: A Diffusion and Stochastic Localization Perspective",
    "summary": "arXiv:2601.15500v2 Announce Type: replace-cross Abstract: In recent years, Rectified flow (RF) has gained considerable popularity largely due to its generation efficiency and state-of-the-art performance. In this paper, we investigate the degree to which RF automatically adapts to the intrinsic low dimensionality of the support of the target distri",
    "url": "https://arxiv.org/abs/2601.15500",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "When Does Adaptation Win? Scaling Laws for Meta-Learning in Quantum Control",
    "summary": "arXiv:2601.18973v3 Announce Type: replace-cross Abstract: Quantum hardware suffers from intrinsic device heterogeneity and environmental drift, forcing practitioners to choose between suboptimal non-adaptive controllers or costly per-device recalibration. We derive a scaling law lower bound for meta-learning showing that the adaptation gain (expect",
    "url": "https://arxiv.org/abs/2601.18973",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Stingy Context: 18:1 Hierarchical Code Compression for LLM Auto-Coding",
    "summary": "arXiv:2601.19929v2 Announce Type: replace-cross Abstract: We introduce Stingy Context, a hierarchical tree-based compression scheme achieving 18:1 reduction in LLM context for auto-coding tasks. Using our TREEFRAG exploit decomposition, we reduce a real source code base of 239k tokens to 11k tokens while preserving task fidelity. Empirical results ",
    "url": "https://arxiv.org/abs/2601.19929",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "CLEAR-Mamba:Towards Accurate, Adaptive and Trustworthy Multi-Sequence Ophthalmic Angiography Classification",
    "summary": "arXiv:2601.20601v2 Announce Type: replace-cross Abstract: Medical image classification is a core task in computer-aided diagnosis (CAD), playing a pivotal role in early disease detection, treatment planning, and patient prognosis assessment. In ophthalmic practice, fluorescein fundus angiography (FFA) and indocyanine green angiography (ICGA) provid",
    "url": "https://arxiv.org/abs/2601.20601",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Zenith: Scaling up Ranking Models for Billion-scale Livestreaming Recommendation",
    "summary": "arXiv:2601.21285v3 Announce Type: replace-cross Abstract: Accurately capturing feature interactions is essential in recommender systems, and recent trends show that scaling up model capacity could be a key driver for next-level predictive performance. While prior work has explored various model architectures to capture multi-granularity feature int",
    "url": "https://arxiv.org/abs/2601.21285",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Self-Improving Pretraining: using post-trained models to pretrain better models",
    "summary": "arXiv:2601.21343v2 Announce Type: replace-cross Abstract: Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully cu",
    "url": "https://arxiv.org/abs/2601.21343",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "From Consistency to Complementarity: Aligned and Disentangled Multi-modal Learning for Time Series Understanding and Reasoning",
    "summary": "arXiv:2601.21436v2 Announce Type: replace-cross Abstract: Advances in multi-modal large language models (MLLMs) have inspired time series understanding and reasoning tasks, that enable natural language querying over time series, producing textual analyses of complex temporal dynamics. Recent attempts hybridize numerical time series with their visua",
    "url": "https://arxiv.org/abs/2601.21436",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Optimization, Generalization and Differential Privacy Bounds for Gradient Descent on Kolmogorov-Arnold Networks",
    "summary": "arXiv:2601.22409v2 Announce Type: replace-cross Abstract: Kolmogorov--Arnold Networks (KANs) have recently emerged as a structured alternative to standard MLPs, yet a principled theory for their training dynamics, generalization, and privacy properties remains limited. In this paper, we analyze gradient descent (GD) for training two-layer KANs and ",
    "url": "https://arxiv.org/abs/2601.22409",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Are LLM Evaluators Really Narcissists? Sanity Checking Self-Preference Evaluations",
    "summary": "arXiv:2601.22548v2 Announce Type: replace-cross Abstract: Recent research has shown that large language models (LLMs) favor their own outputs when acting as judges, undermining the integrity of automated post-training and evaluation workflows. However, it is difficult to disentangle which evaluation biases are explained by narcissism versus general",
    "url": "https://arxiv.org/abs/2601.22548",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization",
    "summary": "arXiv:2601.23174v2 Announce Type: replace-cross Abstract: Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing u",
    "url": "https://arxiv.org/abs/2601.23174",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "ProDCARL: Reinforcement Learning-Aligned Diffusion Models for De Novo Antimicrobial Peptide Design",
    "summary": "arXiv:2602.00157v2 Announce Type: replace-cross Abstract: Antimicrobial resistance threatens healthcare sustainability and motivates low-cost computational discovery of antimicrobial peptides (AMPs). De novo peptide generation must optimize antimicrobial activity and safety through low predicted toxicity, but likelihood-trained generators do not en",
    "url": "https://arxiv.org/abs/2602.00157",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "RAPTOR: Ridge-Adaptive Logistic Probes",
    "summary": "arXiv:2602.00158v2 Announce Type: replace-cross Abstract: Probing studies what information is encoded in a frozen LLM's layer representations by training a lightweight predictor on top of them. Beyond analysis, probes are often used operationally in probe-then-steer pipelines: a learned concept vector is extracted from a probe and injected via addi",
    "url": "https://arxiv.org/abs/2602.00158",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing",
    "summary": "arXiv:2602.00906v4 Announce Type: replace-cross Abstract: Large language models often hallucinate with high confidence on \"random facts\" that lack inferable patterns. We formalize the memorization of such facts as a membership testing problem, unifying the discrete error metrics of Bloom filters with the continuous log-loss of LLMs. By analyzing th",
    "url": "https://arxiv.org/abs/2602.00906",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "LASS-ODE: Scaling ODE Computations to Connect Foundation Models with Dynamical Physical Systems",
    "summary": "arXiv:2602.01009v2 Announce Type: replace-cross Abstract: Foundation models have transformed language, vision, and time series data analysis, yet progress on dynamic predictions for physical systems remains limited. Given the complexity of physical constraints, two challenges stand out. $(i)$ Physics-computation scalability: physics-informed learni",
    "url": "https://arxiv.org/abs/2602.01009",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "TxRay: Agentic Postmortem of Live Blockchain Attacks",
    "summary": "arXiv:2602.01317v3 Announce Type: replace-cross Abstract: Decentralized Finance (DeFi) has turned blockchains into financial infrastructure, allowing anyone to trade, lend, and build protocols without intermediaries, but this openness exposes pools of value controlled by code. Within five years, the DeFi ecosystem has lost over 15.75B USD to report",
    "url": "https://arxiv.org/abs/2602.01317",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner",
    "summary": "arXiv:2602.01705v2 Announce Type: replace-cross Abstract: Recent reinforcement learning (RL) methods improve LLM reasoning by optimizing discrete Chain-of-Thought (CoT) generation; however, exploration in token space often suffers from diversity collapse as policy entropy decreases due to mode elicitation behavior in discrete RL. To mitigate this i",
    "url": "https://arxiv.org/abs/2602.01705",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics",
    "summary": "arXiv:2602.02343v2 Announce Type: replace-cross Abstract: Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frame",
    "url": "https://arxiv.org/abs/2602.02343",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
    "summary": "arXiv:2602.02437v2 Announce Type: replace-cross Abstract: Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework tha",
    "url": "https://arxiv.org/abs/2602.02437",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "ProphetKV: User-Query-Driven Selective Recomputation for Efficient KV Cache Reuse in Retrieval-Augmented Generation",
    "summary": "arXiv:2602.02579v3 Announce Type: replace-cross Abstract: The prefill stage of long-context Retrieval-Augmented Generation (RAG) is severely bottlenecked by computational overhead. To mitigate this, recent methods assemble pre-calculated KV caches of retrieved RAG documents (by a user query) and reprocess selected tokens to recover cross-attention ",
    "url": "https://arxiv.org/abs/2602.02579",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "RAP: KV-Cache Compression via RoPE-Aligned Pruning",
    "summary": "arXiv:2602.02599v2 Announce Type: replace-cross Abstract: Long-context inference in large language models is increasingly bottlenecked by the memory and compute cost of the KV-Cache. Low-rank factorization compresses KV projections by writing $W \\approx A * B$, where A produces latent KV states and B can be absorbed into downstream weights. In mode",
    "url": "https://arxiv.org/abs/2602.02599",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently",
    "summary": "arXiv:2602.02619v2 Announce Type: replace-cross Abstract: While Large Language Models (LLMs) excel at short-term tasks, scaling them to long-horizon agentic workflows remains challenging. The core bottleneck lies in the scarcity of training data that captures authentic long-dependency structures and cross-stage evolutionary dynamics--existing synth",
    "url": "https://arxiv.org/abs/2602.02619",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "WAXAL: A Large-Scale Multilingual African Language Speech Corpus",
    "summary": "arXiv:2602.02734v2 Announce Type: replace-cross Abstract: The advancement of speech technology has predominantly favored high-resource languages, creating a significant digital divide for speakers of most Sub-Saharan African languages. To address this gap, we introduce WAXAL, a large-scale, openly accessible speech dataset for 21 languages represen",
    "url": "https://arxiv.org/abs/2602.02734",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Tabula RASA: Exposing and Breaking the Relational Bottleneck in Transformers",
    "summary": "arXiv:2602.02834v2 Announce Type: replace-cross Abstract: Transformers achieve remarkable performance across many domains, yet struggle with tasks requiring multi-hop relational reasoning over structured data. We analyze this limitation through circuit complexity: standard transformers are $\\mathsf{TC}^0$-complete and cannot solve graph connectivit",
    "url": "https://arxiv.org/abs/2602.02834",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs",
    "summary": "arXiv:2602.03048v2 Announce Type: replace-cross Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning. However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptiv",
    "url": "https://arxiv.org/abs/2602.03048",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "\"I'm happy even though it's not real\": GenAI Photo Editing as a Remembering Experience",
    "summary": "arXiv:2602.03104v2 Announce Type: replace-cross Abstract: Generative Artificial Intelligence (GenAI) is increasingly integrated into photo applications on personal devices, making editing photographs easier than ever while potentially influencing the memories they represent. This study explores how and why people use GenAI to edit personal photos a",
    "url": "https://arxiv.org/abs/2602.03104",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Not All Negative Samples Are Equal: LLMs Learn Better from Plausible Reasoning",
    "summary": "arXiv:2602.03516v2 Announce Type: replace-cross Abstract: Learning from negative samples holds great promise for improving Large Language Model (LLM) reasoning capability, yet existing methods treat all incorrect responses as equally informative, overlooking the crucial role of sample quality. To address this, we propose Plausible Negative Samples ",
    "url": "https://arxiv.org/abs/2602.03516",
    "source": "Arxiv AI",
    "published_at": "2026-02-06T05:00:00+00:00"
  },
  {
    "title": "Rethinking imitation learning with Predictive Inverse Dynamics Models",
    "summary": "This research looks at why Predictive Inverse Dynamics Models often outperform standard Behavior Cloning in imitation learning. By using simple predictions of what happens next, PIDMs reduce ambiguity and learn from far fewer demonstrations. The post Rethinking imitation learning with Predictive Inverse Dynamics Models appeared first on Microsoft R",
    "url": "https://www.microsoft.com/en-us/research/blog/rethinking-imitation-learning-with-predictive-inverse-dynamics-models/",
    "source": "Microsoft Research",
    "published_at": "2026-02-05T17:00:00+00:00"
  },
  {
    "title": "Introducing SyGra Studio",
    "summary": "",
    "url": "https://huggingface.co/blog/ServiceNow-AI/sygra-studio",
    "source": "Hugging Face Blog",
    "published_at": "2026-02-05T16:52:28+00:00"
  }
]