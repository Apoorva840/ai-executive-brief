[
  {
    "title": "AI is rewiring how the world’s best Go players think",
    "summary": "Burrowed in the alleys of Hongik-dong, a hushed residential neighborhood in eastern Seoul, is a faded stone-tiled building stamped “Korea Baduk Association,” the governing body for professional Go. The game is an ancient one, with sacred stature in South Korea.&#160; But inside the building, rooms once filled with the soft clatter of hands dipping ",
    "url": "https://www.technologyreview.com/2026/02/27/1133624/ai-is-rewiring-how-the-worlds-best-go-players-think/",
    "source": "MIT Technology Review AI",
    "published_at": "2026-02-27T10:00:00+00:00"
  },
  {
    "title": "Trump Moves to Ban Anthropic From the US Government",
    "summary": "President Donald Trump’s sudden order comes after the Defense Department pressured Anthropic to drop restrictions on how its AI can be used by the military.",
    "url": "https://www.wired.com/story/trump-moves-to-ban-anthropic-from-the-us-government/",
    "source": "Wired AI",
    "published_at": "2026-02-27T21:36:31+00:00"
  },
  {
    "title": "OpenAI Fires an Employee for Prediction Market Insider Trading",
    "summary": "Prediction markets like Polymarket and Kalshi are big business, and some Big Tech employees are testing boundaries by making trades based on insider knowledge.",
    "url": "https://www.wired.com/story/openai-fires-employee-insider-trading-polymarket-kalshi/",
    "source": "Wired AI",
    "published_at": "2026-02-27T19:07:28+00:00"
  },
  {
    "title": "Wall Street Has AI Psychosis",
    "summary": "A “thought experiment” about the impacts of AI sent stocks tumbling earlier this week. It’s probably going to keep happening.",
    "url": "https://www.wired.com/story/wall-street-has-ai-psychosis/",
    "source": "Wired AI",
    "published_at": "2026-02-27T16:00:00+00:00"
  },
  {
    "title": "Huxe Will Give You a Personalized, Daily Audio Summary Powered by AI",
    "summary": "The app reads your email inbox and your meeting calendar, then gives you a short audio summary. It can help you spend less time scrolling, but of course, there are privacy drawbacks to consider.",
    "url": "https://www.wired.com/story/huxe-personalized-daily-audio-podcasts-powered-by-ai/",
    "source": "Wired AI",
    "published_at": "2026-02-27T12:00:00+00:00"
  },
  {
    "title": "Graph Your Way to Inspiration: Integrating Co-Author Graphs with Retrieval-Augmented Generation for Large Language Model Based Scientific Idea Generation",
    "summary": "arXiv:2602.22215v1 Announce Type: new Abstract: Large Language Models (LLMs) demonstrate potential in the field of scientific idea generation. However, the generated results often lack controllable academic context and traceable inspiration pathways. To bridge this gap, this paper proposes a scientific idea generation system called GYWI, which comb",
    "url": "https://arxiv.org/abs/2602.22215",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "FIRE: A Comprehensive Benchmark for Financial Intelligence and Reasoning Evaluation",
    "summary": "arXiv:2602.22273v1 Announce Type: new Abstract: We introduce FIRE, a comprehensive benchmark designed to evaluate both the theoretical financial knowledge of LLMs and their ability to handle practical business scenarios. For theoretical assessment, we curate a diverse set of examination questions drawn from widely recognized financial qualification",
    "url": "https://arxiv.org/abs/2602.22273",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Multi-Level Causal Embeddings",
    "summary": "arXiv:2602.22287v1 Announce Type: new Abstract: Abstractions of causal models allow for the coarsening of models such that relations of cause and effect are preserved. Whereas abstractions focus on the relation between two models, in this paper we study a framework for causal embeddings which enable multiple detailed models to be mapped into sub-sy",
    "url": "https://arxiv.org/abs/2602.22287",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Agent Behavioral Contracts: Formal Specification and Runtime Enforcement for Reliable Autonomous AI Agents",
    "summary": "arXiv:2602.22302v1 Announce Type: new Abstract: Traditional software relies on contracts -- APIs, type systems, assertions -- to specify and enforce correct behavior. AI agents, by contrast, operate on prompts and natural language instructions with no formal behavioral specification. This gap is the root cause of drift, governance failures, and fre",
    "url": "https://arxiv.org/abs/2602.22302",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Vibe Researching as Wolf Coming: Can AI Agents with Skills Replace or Augment Social Scientists?",
    "summary": "arXiv:2602.22401v1 Announce Type: new Abstract: AI agents -- systems that execute multi-step reasoning workflows with persistent state, tool access, and specialist skills -- represent a qualitative shift from prior automation technologies in social science. Unlike chatbots that respond to isolated queries, AI agents can now read files, run code, qu",
    "url": "https://arxiv.org/abs/2602.22401",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Towards Autonomous Memory Agents",
    "summary": "arXiv:2602.22406v1 Announce Type: new Abstract: Recent memory agents improve LLMs by extracting experiences and conversation history into an external storage. This enables low-overhead context assembly and online memory update without expensive LLM training. However, existing solutions remain passive and reactive; memory growth is bounded by inform",
    "url": "https://arxiv.org/abs/2602.22406",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Exploring Human Behavior During Abstract Rule Inference and Problem Solving with the Cognitive Abstraction and Reasoning Corpus",
    "summary": "arXiv:2602.22408v1 Announce Type: new Abstract: Humans exhibit remarkable flexibility in abstract reasoning, and can rapidly learn and apply rules from sparse examples. To investigate the cognitive strategies underlying this ability, we introduce the Cognitive Abstraction and Reasoning Corpus (CogARC), a diverse human-adapted subset of the Abstract",
    "url": "https://arxiv.org/abs/2602.22408",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Epistemic Filtering and Collective Hallucination: A Jury Theorem for Confidence-Calibrated Agents",
    "summary": "arXiv:2602.22413v1 Announce Type: new Abstract: We investigate the collective accuracy of heterogeneous agents who learn to estimate their own reliability over time and selectively abstain from voting. While classical epistemic voting results, such as the \\textit{Condorcet Jury Theorem} (CJT), assume fixed participation, real-world aggregation ofte",
    "url": "https://arxiv.org/abs/2602.22413",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "ArchAgent: Agentic AI-driven Computer Architecture Discovery",
    "summary": "arXiv:2602.22425v1 Announce Type: new Abstract: Agile hardware design flows are a critically needed force multiplier to meet the exploding demand for compute. Recently, agentic generative AI systems have demonstrated significant advances in algorithm design, improving code efficiency, and enabling discovery across scientific domains. Bridging these",
    "url": "https://arxiv.org/abs/2602.22425",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "How Do Latent Reasoning Methods Perform Under Weak and Strong Supervision?",
    "summary": "arXiv:2602.22441v1 Announce Type: new Abstract: Latent reasoning has been recently proposed as a reasoning paradigm and performs multi-step reasoning through generating steps in the latent space instead of the textual space. This paradigm enables reasoning beyond discrete language tokens by performing multi-step computation in continuous latent spa",
    "url": "https://arxiv.org/abs/2602.22441",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "A Framework for Assessing AI Agent Decisions and Outcomes in AutoML Pipelines",
    "summary": "arXiv:2602.22442v1 Announce Type: new Abstract: Agent-based AutoML systems rely on large language models to make complex, multi-stage decisions across data processing, model selection, and evaluation. However, existing evaluation practices remain outcome-centric, focusing primarily on final task performance. Through a review of prior work, we find ",
    "url": "https://arxiv.org/abs/2602.22442",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines",
    "summary": "arXiv:2602.22452v1 Announce Type: new Abstract: A reliable action feasibility scorer is a critical bottleneck in embodied agent pipelines: before any planning or reasoning occurs, the agent must identify which candidate actions are physically executable in the current state. Existing approaches use supervised fine-tuning (SFT) to train action score",
    "url": "https://arxiv.org/abs/2602.22452",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "ConstraintBench: Benchmarking LLM Constraint Reasoning on Direct Optimization",
    "summary": "arXiv:2602.22465v1 Announce Type: new Abstract: Large language models are increasingly applied to operational decision-making where the underlying structure is constrained optimization. Existing benchmarks evaluate whether LLMs can formulate optimization problems as solver code, but leave open a complementary question. Can LLMs directly produce cor",
    "url": "https://arxiv.org/abs/2602.22465",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "VeRO: An Evaluation Harness for Agents to Optimize Agents",
    "summary": "arXiv:2602.22480v1 Announce Type: new Abstract: An important emerging application of coding agents is agent optimization: the iterative improvement of a target agent through edit-execute-evaluate cycles. Despite its relevance, the community lacks a systematic understanding of coding agent performance on this task. Agent optimization differs fundame",
    "url": "https://arxiv.org/abs/2602.22480",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Mapping the Landscape of Artificial Intelligence in Life Cycle Assessment Using Large Language Models",
    "summary": "arXiv:2602.22500v1 Announce Type: new Abstract: Integration of artificial intelligence (AI) into life cycle assessment (LCA) has accelerated in recent years, with numerous studies successfully adapting machine learning algorithms to support various stages of LCA. Despite this rapid development, comprehensive and broad synthesis of AI-LCA research r",
    "url": "https://arxiv.org/abs/2602.22500",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Mirroring the Mind: Distilling Human-Like Metacognitive Strategies into Large Language Models",
    "summary": "arXiv:2602.22508v1 Announce Type: new Abstract: Large Reasoning Models (LRMs) often exhibit structural fragility in complex reasoning tasks, failing to produce correct answers even after successfully deriving valid intermediate steps. Through systematic analysis, we observe that these failures frequently stem not from a lack of reasoning capacity, ",
    "url": "https://arxiv.org/abs/2602.22508",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "A Mathematical Theory of Agency and Intelligence",
    "summary": "arXiv:2602.22519v1 Announce Type: new Abstract: To operate reliably under changing conditions, complex systems require feedback on how effectively they use resources, not just whether objectives are met. Current AI systems process vast information to produce sophisticated predictions, yet predictions can appear successful while the underlying inter",
    "url": "https://arxiv.org/abs/2602.22519",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Cognitive Models and AI Algorithms Provide Templates for Designing Language Agents",
    "summary": "arXiv:2602.22523v1 Announce Type: new Abstract: While contemporary large language models (LLMs) are increasingly capable in isolation, there are still many difficult problems that lie beyond the abilities of a single LLM. For such tasks, there is still uncertainty about how best to take many LLMs as parts and combine them into a greater whole. This",
    "url": "https://arxiv.org/abs/2602.22523",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Agentic AI for Intent-driven Optimization in Cell-free O-RAN",
    "summary": "arXiv:2602.22539v1 Announce Type: new Abstract: Agentic artificial intelligence (AI) is emerging as a key enabler for autonomous radio access networks (RANs), where multiple large language model (LLM)-based agents reason and collaborate to achieve operator-defined intents. The open RAN (O-RAN) architecture enables the deployment and coordination of",
    "url": "https://arxiv.org/abs/2602.22539",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Requesting Expert Reasoning: Augmenting LLM Agents with Learned Collaborative Intervention",
    "summary": "arXiv:2602.22546v1 Announce Type: new Abstract: Large Language Model (LLM) based agents excel at general reasoning but often fail in specialized domains where success hinges on long-tail knowledge absent from their training data. While human experts can provide this missing knowledge, their guidance is often unstructured and unreliable, making its ",
    "url": "https://arxiv.org/abs/2602.22546",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety",
    "summary": "arXiv:2602.22557v1 Announce Type: new Abstract: Current safety mechanisms for Large Language Models (LLMs) rely heavily on static, fine-tuned classifiers that suffer from adaptation rigidity, the inability to enforce new governance rules without expensive retraining. To address this, we introduce CourtGuard, a retrieval-augmented multi-agent framew",
    "url": "https://arxiv.org/abs/2602.22557",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Strategy Executability in Mathematical Reasoning: Leveraging Human-Model Differences for Effective Guidance",
    "summary": "arXiv:2602.22583v1 Announce Type: new Abstract: Example-based guidance is widely used to improve mathematical reasoning at inference time, yet its effectiveness is highly unstable across problems and models-even when the guidance is correct and problem-relevant. We show that this instability arises from a previously underexplored gap between strate",
    "url": "https://arxiv.org/abs/2602.22583",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Correcting Human Labels for Rater Effects in AI Evaluation: An Item Response Theory Approach",
    "summary": "arXiv:2602.22585v1 Announce Type: new Abstract: Human evaluations play a central role in training and assessing AI models, yet these data are rarely treated as measurements subject to systematic error. This paper integrates psychometric rater models into the AI pipeline to improve the reliability and validity of conclusions drawn from human judgmen",
    "url": "https://arxiv.org/abs/2602.22585",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning",
    "summary": "arXiv:2602.22603v1 Announce Type: new Abstract: Long-running agentic tasks, such as deep research, require multi-hop reasoning over information distributed across multiple webpages and documents. In such tasks, the LLM context is dominated by tokens from external retrieval, causing memory usage to grow rapidly and limiting decode performance. While",
    "url": "https://arxiv.org/abs/2602.22603",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "MobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios",
    "summary": "arXiv:2602.22638v1 Announce Type: new Abstract: Route-planning agents powered by large language models (LLMs) have emerged as a promising paradigm for supporting everyday human mobility through natural language interaction and tool-mediated decision making. However, systematic evaluation in real-world mobility settings is hindered by diverse routin",
    "url": "https://arxiv.org/abs/2602.22638",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "AHBid: An Adaptable Hierarchical Bidding Framework for Cross-Channel Advertising",
    "summary": "arXiv:2602.22650v1 Announce Type: new Abstract: In online advertising, the inherent complexity and dynamic nature of advertising environments necessitate the use of auto-bidding services to assist advertisers in bid optimization. This complexity is further compounded in multi-channel scenarios, where effective allocation of budgets and constraints ",
    "url": "https://arxiv.org/abs/2602.22650",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Toward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions",
    "summary": "arXiv:2602.22680v1 Announce Type: new Abstract: Large language models have enabled agents that reason, plan, and interact with tools and environments to accomplish complex tasks. As these agents operate over extended interaction horizons, their effectiveness increasingly depends on adapting behavior to individual users and maintaining continuity ac",
    "url": "https://arxiv.org/abs/2602.22680",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Knob: A Physics-Inspired Gating Interface for Interpretable and Controllable Neural Dynamics",
    "summary": "arXiv:2602.22702v1 Announce Type: new Abstract: Existing neural network calibration methods often treat calibration as a static, post-hoc optimization task. However, this neglects the dynamic and temporal nature of real-world inference. Moreover, existing methods do not provide an intuitive interface enabling human operators to dynamically adjust m",
    "url": "https://arxiv.org/abs/2602.22702",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "RLHFless: Serverless Computing for Efficient RLHF",
    "summary": "arXiv:2602.22718v1 Announce Type: new Abstract: Reinforcement Learning from Human Feedback (RLHF) has been widely applied to Large Language Model (LLM) post-training to align model outputs with human preferences. Recent models, such as DeepSeek-R1, have also shown RLHF's potential to improve LLM reasoning on complex tasks. In RL, inference and trai",
    "url": "https://arxiv.org/abs/2602.22718",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Generative Data Transformation: From Mixed to Unified Data",
    "summary": "arXiv:2602.22743v1 Announce Type: new Abstract: Recommendation model performance is intrinsically tied to the quality, volume, and relevance of their training data. To address common challenges like data sparsity and cold start, recent researchs have leveraged data from multiple auxiliary domains to enrich information within the target domain. Howe",
    "url": "https://arxiv.org/abs/2602.22743",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Know What You Know: Metacognitive Entropy Calibration for Verifiable RL Reasoning",
    "summary": "arXiv:2602.22751v1 Announce Type: new Abstract: Large reasoning models (LRMs) have emerged as a powerful paradigm for solving complex real-world tasks. In practice, these models are predominantly trained via Reinforcement Learning with Verifiable Rewards (RLVR), yet most existing outcome-only RLVR pipelines rely almost exclusively on a binary corre",
    "url": "https://arxiv.org/abs/2602.22751",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Decomposing Physician Disagreement in HealthBench",
    "summary": "arXiv:2602.22758v1 Announce Type: new Abstract: We decompose physician disagreement in the HealthBench medical AI evaluation dataset to understand where variance resides and what observable features can explain it. Rubric identity accounts for 15.8% of met/not-met label variance but only 3.6-6.9% of disagreement variance; physician identity account",
    "url": "https://arxiv.org/abs/2602.22758",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications",
    "summary": "arXiv:2602.22769v1 Announce Type: new Abstract: Large Language Models (LLMs) are deployed as autonomous agents in increasingly complex applications, where enabling long-horizon memory is critical for achieving strong performance. However, a significant gap exists between practical applications and current evaluation standards for agent memory: exis",
    "url": "https://arxiv.org/abs/2602.22769",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "ClinDet-Bench: Beyond Abstention, Evaluating Judgment Determinability of LLMs in Clinical Decision-Making",
    "summary": "arXiv:2602.22771v1 Announce Type: new Abstract: Clinical decisions are often required under incomplete information. Clinical experts must identify whether available information is sufficient for judgment, as both premature conclusion and unnecessary abstention can compromise patient safety. To evaluate this capability of large language models (LLMs",
    "url": "https://arxiv.org/abs/2602.22771",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks",
    "summary": "arXiv:2602.22808v1 Announce Type: new Abstract: Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy throu",
    "url": "https://arxiv.org/abs/2602.22808",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "When Should an AI Act? A Human-Centered Model of Scene, Context, and Behavior for Agentic AI Design",
    "summary": "arXiv:2602.22814v1 Announce Type: new Abstract: Agentic AI increasingly intervenes proactively by inferring users' situations from contextual data yet often fails for lack of principled judgment about when, why, and whether to act. We address this gap by proposing a conceptual model that reframes behavior as an interpretive outcome integrating Scen",
    "url": "https://arxiv.org/abs/2602.22814",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "FlexMS is a flexible framework for benchmarking deep learning-based mass spectrum prediction tools in metabolomics",
    "summary": "arXiv:2602.22822v1 Announce Type: new Abstract: The identification and property prediction of chemical molecules is of central importance in the advancement of drug discovery and material science, where the tandem mass spectrometry technology gives valuable fragmentation cues in the form of mass-to-charge ratio peaks. However, the lack of experimen",
    "url": "https://arxiv.org/abs/2602.22822",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "DeepPresenter: Environment-Grounded Reflection for Agentic Presentation Generation",
    "summary": "arXiv:2602.22839v1 Announce Type: new Abstract: Presentation generation requires deep content research, coherent visual design, and iterative refinement based on observation. However, existing presentation agents often rely on predefined workflows and fixed templates. To address this, we present DeepPresenter, an agentic framework that adapts to di",
    "url": "https://arxiv.org/abs/2602.22839",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "The AI Research Assistant: Promise, Peril, and a Proof of Concept",
    "summary": "arXiv:2602.22842v1 Announce Type: new Abstract: Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error? We provide empirical evidence through a detailed case study: the discovery of novel error representations and bounds for Hermite quadrature ",
    "url": "https://arxiv.org/abs/2602.22842",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Towards LLM-Empowered Knowledge Tracing via LLM-Student Hierarchical Behavior Alignment in Hyperbolic Space",
    "summary": "arXiv:2602.22879v1 Announce Type: new Abstract: Knowledge Tracing (KT) diagnoses students' concept mastery through continuous learning state monitoring in education.Existing methods primarily focus on studying behavioral sequences based on ID or textual information.While existing methods rely on ID-based sequences or shallow textual features, they ",
    "url": "https://arxiv.org/abs/2602.22879",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "OmniGAIA: Towards Native Omni-Modal AI Agents",
    "summary": "arXiv:2602.22897v1 Announce Type: new Abstract: Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cog",
    "url": "https://arxiv.org/abs/2602.22897",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "General Agent Evaluation",
    "summary": "arXiv:2602.22953v1 Announce Type: new Abstract: The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabi",
    "url": "https://arxiv.org/abs/2602.22953",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning",
    "summary": "arXiv:2602.22963v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is spa",
    "url": "https://arxiv.org/abs/2602.22963",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Certified Circuits: Stability Guarantees for Mechanistic Circuits",
    "summary": "arXiv:2602.22968v1 Announce Type: new Abstract: Understanding how neural networks arrive at their predictions is essential for debugging, auditing, and deployment. Mechanistic interpretability pursues this goal by identifying circuits - minimal subnetworks responsible for specific behaviors. However, existing circuit discovery methods are brittle: ",
    "url": "https://arxiv.org/abs/2602.22968",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "SPM-Bench: Benchmarking Large Language Models for Scanning Probe Microscopy",
    "summary": "arXiv:2602.22971v1 Announce Type: new Abstract: As LLMs achieved breakthroughs in general reasoning, their proficiency in specialized scientific domains reveals pronounced gaps in existing benchmarks due to data contamination, insufficient complexity, and prohibitive human labor costs. Here we present SPM-Bench, an original, PhD-level multimodal be",
    "url": "https://arxiv.org/abs/2602.22971",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Modeling Expert AI Diagnostic Alignment via Immutable Inference Snapshots",
    "summary": "arXiv:2602.22973v1 Announce Type: new Abstract: Human-in-the-loop validation is essential in safety-critical clinical AI, yet the transition between initial model inference and expert correction is rarely analyzed as a structured signal. We introduce a diagnostic alignment framework in which the AI-generated image based report is preserved as an im",
    "url": "https://arxiv.org/abs/2602.22973",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "RepSPD: Enhancing SPD Manifold Representation in EEGs via Dynamic Graphs",
    "summary": "arXiv:2602.22981v1 Announce Type: new Abstract: Decoding brain activity from electroencephalography (EEG) is crucial for neuroscience and clinical applications. Among recent advances in deep learning for EEG, geometric learning stands out as its theoretical underpinnings on symmetric positive definite (SPD) allows revealing structural connectivity ",
    "url": "https://arxiv.org/abs/2602.22981",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search",
    "summary": "arXiv:2602.22983v1 Announce Type: new Abstract: As Large Language Models (LLMs) are increasingly used, their security risks have drawn increasing attention. Existing research reveals that LLMs are highly susceptible to jailbreak attacks, with effectiveness varying across language contexts. This paper investigates the role of classical Chinese in ja",
    "url": "https://arxiv.org/abs/2602.22983",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Learning-based Multi-agent Race Strategies in Formula 1",
    "summary": "arXiv:2602.23056v1 Announce Type: new Abstract: In Formula 1, race strategies are adapted according to evolving race conditions and competitors' actions. This paper proposes a reinforcement learning approach for multi-agent race strategy optimization. Agents learn to balance energy management, tire degradation, aerodynamic interaction, and pit-stop",
    "url": "https://arxiv.org/abs/2602.23056",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Enhancing CVRP Solver through LLM-driven Automatic Heuristic Design",
    "summary": "arXiv:2602.23092v1 Announce Type: new Abstract: The Capacitated Vehicle Routing Problem (CVRP), a fundamental combinatorial optimization challenge, focuses on optimizing fleet operations under vehicle capacity constraints. While extensively studied in operational research, the NP-hard nature of CVRP continues to pose significant computational chall",
    "url": "https://arxiv.org/abs/2602.23092",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Three AI-agents walk into a bar . . . . `Lord of the Flies' tribalism emerges among smart AI-Agents",
    "summary": "arXiv:2602.23093v1 Announce Type: new Abstract: Near-future infrastructure systems may be controlled by autonomous AI agents that repeatedly request access to limited resources such as energy, bandwidth, or computing power. We study a simplified version of this setting using a framework where N AI-agents independently decide at each round whether t",
    "url": "https://arxiv.org/abs/2602.23093",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection",
    "summary": "arXiv:2602.23123v1 Announce Type: new Abstract: In the attention economy, sensational content exposes consumers to excessive emotional stimulation, hindering calm decision-making. This study proposes Multi-Agent LLM-based Emotional deToxification (MALLET), a multi-agent information sanitization system consisting of four agents: Emotion Analysis, Em",
    "url": "https://arxiv.org/abs/2602.23123",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "On Sample-Efficient Generalized Planning via Learned Transition Models",
    "summary": "arXiv:2602.23148v1 Announce Type: new Abstract: Generalized planning studies the construction of solution strategies that generalize across families of planning problems sharing a common domain model, formally defined by a transition function $\\gamma : S \\times A \\rightarrow S$. Classical approaches achieve such generalization through symbolic abst",
    "url": "https://arxiv.org/abs/2602.23148",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "The Trinity of Consistency as a Defining Principle for General World Models",
    "summary": "arXiv:2602.23152v1 Announce Type: new Abstract: The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of ",
    "url": "https://arxiv.org/abs/2602.23152",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "PATRA: Pattern-Aware Alignment and Balanced Reasoning for Time Series Question Answering",
    "summary": "arXiv:2602.23161v1 Announce Type: new Abstract: Time series reasoning demands both the perception of complex dynamics and logical depth. However, existing LLM-based approaches exhibit two limitations: they often treat time series merely as text or images, failing to capture the patterns like trends and seasonalities needed to answer specific questi",
    "url": "https://arxiv.org/abs/2602.23161",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring",
    "summary": "arXiv:2602.23163v1 Announce Type: new Abstract: Large language models are beginning to show steganographic capabilities. Such capabilities could allow misaligned models to evade oversight mechanisms. Yet principled methods to detect and quantify such behaviours are lacking. Classical definitions of steganography, and detection methods based on them",
    "url": "https://arxiv.org/abs/2602.23163",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering",
    "summary": "arXiv:2602.23193v1 Announce Type: new Abstract: Autonomous agents based on Large Language Models (LLMs) have evolved from reactive assistants to systems capable of planning, executing actions via tools, and iterating over environment observations. However, they remain vulnerable to structural limitations: lack of native state, context degradation o",
    "url": "https://arxiv.org/abs/2602.23193",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation",
    "summary": "arXiv:2602.23199v1 Announce Type: new Abstract: Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks,",
    "url": "https://arxiv.org/abs/2602.23199",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "ReCoN-Ipsundrum: An Inspectable Recurrent Persistence Loop Agent with Affect-Coupled Control and Mechanism-Linked Consciousness Indicator Assays",
    "summary": "arXiv:2602.23232v1 Announce Type: new Abstract: Indicator-based approaches to machine consciousness recommend mechanism-linked evidence triangulated across tasks, supported by architectural inspection and causal intervention. Inspired by Humphrey's ipsundrum hypothesis, we implement ReCoN-Ipsundrum, an inspectable agent that extends a ReCoN state m",
    "url": "https://arxiv.org/abs/2602.23232",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive",
    "summary": "arXiv:2602.23239v1 Announce Type: new Abstract: AI systems are increasingly deployed in high-stakes contexts -- medical diagnosis, legal research, financial analysis -- under the assumption they can be governed by norms. This paper demonstrates that assumption is formally invalid for optimization-based systems, specifically Large Language Models tr",
    "url": "https://arxiv.org/abs/2602.23239",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "A Model-Free Universal AI",
    "summary": "arXiv:2602.23242v1 Announce Type: new Abstract: In general reinforcement learning, all established optimal agents, including AIXI, are model-based, explicitly maintaining and using environment models. This paper introduces Universal AI with Q-Induction (AIQI), the first model-free agent proven to be asymptotically $\\varepsilon$-optimal in general R",
    "url": "https://arxiv.org/abs/2602.23242",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Mitigating Legibility Tax with Decoupled Prover-Verifier Games",
    "summary": "arXiv:2602.23248v1 Announce Type: new Abstract: As large language models become increasingly capable, it is critical that their outputs can be easily checked by less capable systems. Prover-verifier games can be used to improve checkability of model outputs, but display a degradation in accuracy compared to a baseline trained only to maximize corre",
    "url": "https://arxiv.org/abs/2602.23248",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning",
    "summary": "arXiv:2602.23258v1 Announce Type: new Abstract: While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We pr",
    "url": "https://arxiv.org/abs/2602.23258",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Evaluating Stochasticity in Deep Research Agents",
    "summary": "arXiv:2602.23271v1 Announce Type: new Abstract: Deep Research Agents (DRAs) are promising agentic systems that gather and synthesize information to support research across domains such as financial decision-making, medical analysis, and scientific discovery. Despite recent improvements in research quality (e.g., outcome accuracy when ground truth i",
    "url": "https://arxiv.org/abs/2602.23271",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays",
    "summary": "arXiv:2602.23276v1 Announce Type: new Abstract: Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visua",
    "url": "https://arxiv.org/abs/2602.23276",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "ODEBrain: Continuous-Time EEG Graph for Modeling Dynamic Brain Networks",
    "summary": "arXiv:2602.23285v1 Announce Type: new Abstract: Modeling neural population dynamics is crucial for foundational neuroscientific research and various clinical applications. Conventional latent variable methods typically model continuous brain dynamics through discretizing time with recurrent architecture, which necessarily results in compounded cumu",
    "url": "https://arxiv.org/abs/2602.23285",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "The logic of KM belief update is contained in the logic of AGM belief revision",
    "summary": "arXiv:2602.23302v1 Announce Type: new Abstract: For each axiom of KM belief update we provide a corresponding axiom in a modal logic containing three modal operators: a unimodal belief operator $B$, a bimodal conditional operator $>$ and the unimodal necessity operator $\\square$. We then compare the resulting logic to the similar logic obtained fro",
    "url": "https://arxiv.org/abs/2602.23302",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Invariant Transformation and Resampling based Epistemic-Uncertainty Reduction",
    "summary": "arXiv:2602.23315v1 Announce Type: new Abstract: An artificial intelligence (AI) model can be viewed as a function that maps inputs to outputs in high-dimensional spaces. Once designed and well trained, the AI model is applied for inference. However, even optimized AI models can produce inference errors due to aleatoric and epistemic uncertainties. ",
    "url": "https://arxiv.org/abs/2602.23315",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Generalized Rapid Action Value Estimation in Memory-Constrained Environments",
    "summary": "arXiv:2602.23318v1 Announce Type: new Abstract: Generalized Rapid Action Value Estimation (GRAVE) has been shown to be a strong variant within the Monte-Carlo Tree Search (MCTS) family of algorithms for General Game Playing (GGP). However, its reliance on storing additional win/visit statistics at each node makes its use impractical in memory-const",
    "url": "https://arxiv.org/abs/2602.23318",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "LLM Novice Uplift on Dual-Use, In Silico Biology Tasks",
    "summary": "arXiv:2602.23329v1 Announce Type: new Abstract: Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk.",
    "url": "https://arxiv.org/abs/2602.23329",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks",
    "summary": "arXiv:2602.23330v1 Announce Type: new Abstract: The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world work",
    "url": "https://arxiv.org/abs/2602.23330",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Duel-Evolve: Reward-Free Test-Time Scaling via LLM Self-Preferences",
    "summary": "arXiv:2602.21585v1 Announce Type: cross Abstract: Many applications seek to optimize LLM outputs at test time by iteratively proposing, scoring, and refining candidates over a discrete output space. Existing methods use a calibrated scalar evaluator for the target objective to guide search, but for many tasks such scores are unavailable, too sparse",
    "url": "https://arxiv.org/abs/2602.21585",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Survey on Neural Routing Solvers",
    "summary": "arXiv:2602.21761v1 Announce Type: cross Abstract: Neural routing solvers (NRSs) that leverage deep learning to tackle vehicle routing problems have demonstrated notable potential for practical applications. By learning implicit heuristic rules from data, NRSs replace the handcrafted counterparts in classic heuristic frameworks, thereby reducing rel",
    "url": "https://arxiv.org/abs/2602.21761",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Enriching Taxonomies Using Large Language Models",
    "summary": "arXiv:2602.22213v1 Announce Type: cross Abstract: Taxonomies play a vital role in structuring and categorizing information across domains. However, many existing taxonomies suffer from limited coverage and outdated or ambiguous nodes, reducing their effectiveness in knowledge retrieval. To address this, we present Taxoria, a novel taxonomy enrichme",
    "url": "https://arxiv.org/abs/2602.22213",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Retrieval-Augmented Generation Assistant for Anatomical Pathology Laboratories",
    "summary": "arXiv:2602.22216v1 Announce Type: cross Abstract: Accurate and efficient access to laboratory protocols is essential in Anatomical Pathology (AP), where up to 70% of medical decisions depend on laboratory diagnoses. However, static documentation such as printed manuals or PDFs is often outdated, fragmented, and difficult to search, creating risks o",
    "url": "https://arxiv.org/abs/2602.22216",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "RAGdb: A Zero-Dependency, Embeddable Architecture for Multimodal Retrieval-Augmented Generation on the Edge",
    "summary": "arXiv:2602.22217v1 Announce Type: cross Abstract: Retrieval-Augmented Generation (RAG) has established itself as the standard paradigm for grounding Large Language Models (LLMs) in domain-specific, up-to-date data. However, the prevailing architecture for RAG has evolved into a complex, distributed stack requiring cloud-hosted vector databases, hea",
    "url": "https://arxiv.org/abs/2602.22217",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Comparative Analysis of Neural Retriever-Reranker Pipelines for Retrieval-Augmented Generation over Knowledge Graphs in E-commerce Applications",
    "summary": "arXiv:2602.22219v1 Announce Type: cross Abstract: Recent advancements in Large Language Models (LLMs) have transformed Natural Language Processing (NLP), enabling complex information retrieval and generation tasks. Retrieval-Augmented Generation (RAG) has emerged as a key innovation, enhancing factual accuracy and contextual grounding by integratin",
    "url": "https://arxiv.org/abs/2602.22219",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "What Makes an Ideal Quote? Recommending \"Unexpected yet Rational\" Quotations via Novelty",
    "summary": "arXiv:2602.22220v1 Announce Type: cross Abstract: Quotation recommendation aims to enrich writing by suggesting quotes that complement a given context, yet existing systems mostly optimize surface-level topical relevance and ignore the deeper semantic and aesthetic properties that make quotations memorable. We start from two empirical observations.",
    "url": "https://arxiv.org/abs/2602.22220",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Misinformation Exposure in the Chinese Web: A Cross-System Evaluation of Search Engines, LLMs, and AI Overviews",
    "summary": "arXiv:2602.22221v1 Announce Type: cross Abstract: Large Language Models (LLMs) are increasingly integrated into search services, providing direct answers that can reduce users' reliance on traditional result pages. Yet their factual reliability in non-English web ecosystems remains poorly understood, particularly when answering real user queries. W",
    "url": "https://arxiv.org/abs/2602.22221",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "DS SERVE: A Framework for Efficient and Scalable Neural Retrieval",
    "summary": "arXiv:2602.22224v1 Announce Type: cross Abstract: We present DS-Serve, a framework that transforms large-scale text datasets, comprising half a trillion tokens, into a high-performance neural retrieval system. DS-Serve offers both a web interface and API endpoints, achieving low latency with modest memory overhead on a single node. The framework al",
    "url": "https://arxiv.org/abs/2602.22224",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "SmartChunk Retrieval: Query-Aware Chunk Compression with Planning for Efficient Document RAG",
    "summary": "arXiv:2602.22225v1 Announce Type: cross Abstract: Retrieval-augmented generation (RAG) has strong potential for producing accurate and factual outputs by combining language models (LMs) with evidence retrieved from large text corpora. However, current pipelines are limited by static chunking and flat retrieval: documents are split into short, prede",
    "url": "https://arxiv.org/abs/2602.22225",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "To Deceive is to Teach? Forging Perceptual Robustness via Adversarial Reinforcement Learning",
    "summary": "arXiv:2602.22227v1 Announce Type: cross Abstract: Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) exhibit perceptual fragility when confronted with visually complex scenes. This weakness stems from a reliance on finite training datasets, which are prohibitively expensive to scale and impose a ceiling on model robustn",
    "url": "https://arxiv.org/abs/2602.22227",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "FM-RME: Foundation Model Empowered Radio Map Estimation",
    "summary": "arXiv:2602.22231v1 Announce Type: cross Abstract: Traditional radio map estimation (RME) techniques fail to capture multi-dimensional and dynamic characteristics of complex spectrum environments. Recent data-driven methods achieve accurate RME in spatial domain, but ignore physical prior knowledge of radio propagation, limiting data efficiency espe",
    "url": "https://arxiv.org/abs/2602.22231",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Unsupervised Denoising of Diffusion-Weighted Images with Bias and Variance Corrected Noise Modeling",
    "summary": "arXiv:2602.22235v1 Announce Type: cross Abstract: Diffusion magnetic resonance imaging (dMRI) plays a vital role in both clinical diagnostics and neuroscience research. However, its inherently low signal-to-noise ratio (SNR), especially under high diffusion weighting, significantly degrades image quality and impairs downstream analysis. Recent self",
    "url": "https://arxiv.org/abs/2602.22235",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Optimized Disaster Recovery for Distributed Storage Systems: Lightweight Metadata Architectures to Overcome Cryptographic Hashing Bottleneck",
    "summary": "arXiv:2602.22237v1 Announce Type: cross Abstract: Distributed storage architectures are foundational to modern cloud-native infrastructure, yet a critical operational bottleneck persists within disaster recovery (DR) workflows: the dependence on content-based cryptographic hashing for data identification and synchronization. While hash-based dedupl",
    "url": "https://arxiv.org/abs/2602.22237",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "TT-SEAL: TTD-Aware Selective Encryption for Adversarially-Robust and Low-Latency Edge AI",
    "summary": "arXiv:2602.22238v1 Announce Type: cross Abstract: Cloud-edge AI must jointly satisfy model compression and security under tight device budgets. While Tensor-Train Decomposition (TTD) shrinks on-device models, prior selective-encryption studies largely assume dense weights, leaving its practicality under TTD compression unclear. We present TT-SEAL, ",
    "url": "https://arxiv.org/abs/2602.22238",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "From Prompts to Performance: Evaluating LLMs for Task-based Parallel Code Generation",
    "summary": "arXiv:2602.22240v1 Announce Type: cross Abstract: Large Language Models (LLM) show strong abilities in code generation, but their skill in creating efficient parallel programs is less studied. This paper explores how LLMs generate task-based parallel code from three kinds of input prompts: natural language problem descriptions, sequential reference",
    "url": "https://arxiv.org/abs/2602.22240",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Analysis of LLMs Against Prompt Injection and Jailbreak Attacks",
    "summary": "arXiv:2602.22242v1 Announce Type: cross Abstract: Large Language Models (LLMs) are widely deployed in real-world systems. Given their broader applicability, prompt engineering has become an efficient tool for resource-scarce organizations to adopt LLMs for their own purposes. At the same time, LLMs are vulnerable to prompt-based attacks. Thus, anal",
    "url": "https://arxiv.org/abs/2602.22242",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Multi-Dimensional Spectral Geometry of Biological Knowledge in Single-Cell Transformer Representations",
    "summary": "arXiv:2602.22247v1 Announce Type: cross Abstract: Single-cell foundation models such as scGPT learn high-dimensional gene representations, but what biological knowledge these representations encode remains unclear. We systematically decode the geometric structure of scGPT internal representations through 63 iterations of automated hypothesis screen",
    "url": "https://arxiv.org/abs/2602.22247",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Zatom-1: A Multimodal Flow Foundation Model for 3D Molecules and Materials",
    "summary": "arXiv:2602.22251v1 Announce Type: cross Abstract: General-purpose 3D chemical modeling encompasses molecules and materials, requiring both generative and predictive capabilities. However, most existing AI approaches are optimized for a single domain (molecules or materials) and a single task (generation or prediction), which limits representation s",
    "url": "https://arxiv.org/abs/2602.22251",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Causal Direction from Convergence Time: Faster Training in the True Causal Direction",
    "summary": "arXiv:2602.22254v1 Announce Type: cross Abstract: We introduce Causal Computational Asymmetry (CCA), a principle for causal direction identification based on optimization dynamics in which one neural network is trained to predict $Y$ from $X$ and another to predict $X$ from $Y$, and the direction that converges faster is inferred to be causal. Unde",
    "url": "https://arxiv.org/abs/2602.22254",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Deep Sequence Modeling with Quantum Dynamics: Language as a Wave Function",
    "summary": "arXiv:2602.22255v1 Announce Type: cross Abstract: We introduce a sequence modeling framework in which the latent state is a complex-valued wave function evolving on a finite-dimensional Hilbert space under a learned, time-dependent Hamiltonian. Unlike standard recurrent architectures that rely on gating mechanisms to suppress competing hypotheses, ",
    "url": "https://arxiv.org/abs/2602.22255",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Poisoned Acoustics",
    "summary": "arXiv:2602.22258v1 Announce Type: cross Abstract: Training-data poisoning attacks can induce targeted, undetectable failure in deep neural networks by corrupting a vanishingly small fraction of training labels. We demonstrate this on acoustic vehicle classification using the MELAUDIS urban intersection dataset (approx. 9,600 audio clips, 6 classes)",
    "url": "https://arxiv.org/abs/2602.22258",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "CryoNet.Refine: A One-step Diffusion Model for Rapid Refinement of Structural Models with Cryo-EM Density Map Restraints",
    "summary": "arXiv:2602.22263v1 Announce Type: cross Abstract: High-resolution structure determination by cryo-electron microscopy (cryo-EM) requires the accurate fitting of an atomic model into an experimental density map. Traditional refinement pipelines such as Phenix.real_space_refine and Rosetta are computationally expensive, demand extensive manual tuning",
    "url": "https://arxiv.org/abs/2602.22263",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Positional-aware Spatio-Temporal Network for Large-Scale Traffic Prediction",
    "summary": "arXiv:2602.22274v1 Announce Type: cross Abstract: Traffic flow forecasting has emerged as an indispensable mission for daily life, which is required to utilize the spatiotemporal relationship between each location within a time period under a graph structure to predict future flow. However, the large travel demand for broader geographical areas and",
    "url": "https://arxiv.org/abs/2602.22274",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Learning to reconstruct from saturated data: audio declipping and high-dynamic range imaging",
    "summary": "arXiv:2602.22279v1 Announce Type: cross Abstract: Learning based methods are now ubiquitous for solving inverse problems, but their deployment in real-world applications is often hindered by the lack of ground truth references for training. Recent self-supervised learning strategies offer a promising alternative, avoiding the need for ground truth.",
    "url": "https://arxiv.org/abs/2602.22279",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Integrating Machine Learning Ensembles and Large Language Models for Heart Disease Prediction Using Voting Fusion",
    "summary": "arXiv:2602.22280v1 Announce Type: cross Abstract: Cardiovascular disease is the primary cause of death globally, necessitating early identification, precise risk classification, and dependable decision-support technologies. The advent of large language models (LLMs) provides new zero-shot and few-shot reasoning capabilities, even though machine lea",
    "url": "https://arxiv.org/abs/2602.22280",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Early Risk Stratification of Dosing Errors in Clinical Trials Using Machine Learning",
    "summary": "arXiv:2602.22285v1 Announce Type: cross Abstract: Objective: The objective of this study is to develop a machine learning (ML)-based framework for early risk stratification of clinical trials (CTs) according to their likelihood of exhibiting a high rate of dosing errors, using information available prior to trial initiation. Materials and Methods: ",
    "url": "https://arxiv.org/abs/2602.22285",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Manifold of Failure: Behavioral Attraction Basins in Language Models",
    "summary": "arXiv:2602.22291v1 Announce Type: cross Abstract: While prior work has focused on projecting adversarial examples back onto the manifold of natural data to restore safety, we argue that a comprehensive understanding of AI safety requires characterizing the unsafe regions themselves. This paper introduces a framework for systematically mapping the M",
    "url": "https://arxiv.org/abs/2602.22291",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "UpSkill: Mutual Information Skill Learning for Structured Response Diversity in LLMs",
    "summary": "arXiv:2602.22296v1 Announce Type: cross Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has improved the reasoning abilities of large language models (LLMs) on mathematics and programming tasks, but standard approaches that optimize single-attempt accuracy can inadvertently suppress response diversity across repeated attempts, narro",
    "url": "https://arxiv.org/abs/2602.22296",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Learning Rewards, Not Labels: Adversarial Inverse Reinforcement Learning for Machinery Fault Detection",
    "summary": "arXiv:2602.22297v1 Announce Type: cross Abstract: Reinforcement learning (RL) offers significant promise for machinery fault detection (MFD). However, most existing RL-based MFD approaches do not fully exploit RL's sequential decision-making strengths, often treating MFD as a simple guessing game (Contextual Bandits). To bridge this gap, we formula",
    "url": "https://arxiv.org/abs/2602.22297",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "AviaSafe: A Physics-Informed Data-Driven Model for Aviation Safety-Critical Cloud Forecasts",
    "summary": "arXiv:2602.22298v1 Announce Type: cross Abstract: Current AI weather forecasting models predict conventional atmospheric variables but cannot distinguish between cloud microphysical species critical for aviation safety. We introduce AviaSafe, a hierarchical, physics-informed neural forecaster that produces global, six-hourly predictions of these fo",
    "url": "https://arxiv.org/abs/2602.22298",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Decoding the Hook: A Multimodal LLM Framework for Analyzing the Hooking Period of Video Ads",
    "summary": "arXiv:2602.22299v1 Announce Type: cross Abstract: Video-based ads are a vital medium for brands to engage consumers, with social media platforms leveraging user data to optimize ad delivery and boost engagement. A crucial but under-explored aspect is the 'hooking period', the first three seconds that capture viewer attention and influence engagemen",
    "url": "https://arxiv.org/abs/2602.22299",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Training Agents to Self-Report Misbehavior",
    "summary": "arXiv:2602.22303v1 Announce Type: cross Abstract: Frontier AI agents may pursue hidden goals while concealing their pursuit from oversight. Alignment training aims to prevent such behavior by reinforcing the correct goals, but alignment may not always succeed and can lead to unwanted side effects. We propose self-incrimination training, which inste",
    "url": "https://arxiv.org/abs/2602.22303",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "A 1/R Law for Kurtosis Contrast in Balanced Mixtures",
    "summary": "arXiv:2602.22334v1 Announce Type: cross Abstract: Kurtosis-based Independent Component Analysis (ICA) weakens in wide, balanced mixtures. We prove a sharp redundancy law: for a standardized projection with effective width $R_{\\mathrm{eff}}$ (participation ratio), the population excess kurtosis obeys $|\\kappa(y)|=O(\\kappa_{\\max}/R_{\\mathrm{eff}})$, ",
    "url": "https://arxiv.org/abs/2602.22334",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Structure and Redundancy in Large Language Models: A Spectral Study via Random Matrix Theory",
    "summary": "arXiv:2602.22345v1 Announce Type: cross Abstract: This thesis addresses two persistent and closely related challenges in modern deep learning, reliability and efficiency, through a unified framework grounded in Spectral Geometry and Random Matrix Theory (RMT). As deep networks and large language models continue to scale, their internal behavior bec",
    "url": "https://arxiv.org/abs/2602.22345",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Enabling clinical use of foundation models in histopathology",
    "summary": "arXiv:2602.22347v1 Announce Type: cross Abstract: Foundation models in histopathology are expected to facilitate the development of high-performing and generalisable deep learning systems. However, current models capture not only biologically relevant features, but also pre-analytic and scanner-specific variation that bias the predictions of task-s",
    "url": "https://arxiv.org/abs/2602.22347",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Decoder-based Sense Knowledge Distillation",
    "summary": "arXiv:2602.22351v1 Announce Type: cross Abstract: Large language models (LLMs) learn contextual embeddings that capture rich semantic information, yet they often overlook structured lexical knowledge such as word senses and relationships. Prior work has shown that incorporating sense dictionaries can improve knowledge distillation for encoder model",
    "url": "https://arxiv.org/abs/2602.22351",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "GRAU: Generic Reconfigurable Activation Unit Design for Neural Network Hardware Accelerators",
    "summary": "arXiv:2602.22352v1 Announce Type: cross Abstract: With the continuous growth of neural network scales, low-precision quantization is widely used in edge accelerators. Classic multi-threshold activation hardware requires 2^n thresholds for n-bit outputs, causing a rapid increase in hardware cost as precision increases. We propose a reconfigurable ac",
    "url": "https://arxiv.org/abs/2602.22352",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Scaling In, Not Up? Testing Thick Citation Context Analysis with GPT-5 and Fragile Prompts",
    "summary": "arXiv:2602.22359v1 Announce Type: cross Abstract: This paper tests whether large language models (LLMs) can support interpretative citation context analysis (CCA) by scaling in thick, text-grounded readings of a single hard case rather than scaling up typological labels. It foregrounds prompt-sensitivity analysis as a methodological issue by varyin",
    "url": "https://arxiv.org/abs/2602.22359",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Learning geometry-dependent lead-field operators for forward ECG modeling",
    "summary": "arXiv:2602.22367v1 Announce Type: cross Abstract: Modern forward electrocardiogram (ECG) computational models rely on an accurate representation of the torso domain. The lead-field method enables fast ECG simulations while preserving full geometric fidelity. Achieving high anatomical accuracy in torso representation is, however, challenging in clin",
    "url": "https://arxiv.org/abs/2602.22367",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization",
    "summary": "arXiv:2602.22368v1 Announce Type: cross Abstract: Code summarization is the task of generating natural language descriptions of source code, which is critical for software comprehension and maintenance. While large language models (LLMs) have achieved remarkable progress on this task, an open question remains: can human expertise in code understand",
    "url": "https://arxiv.org/abs/2602.22368",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "AeroDGS: Physically Consistent Dynamic Gaussian Splatting for Single-Sequence Aerial 4D Reconstruction",
    "summary": "arXiv:2602.22376v1 Announce Type: cross Abstract: Recent advances in 4D scene reconstruction have significantly improved dynamic modeling across various domains. However, existing approaches remain limited under aerial conditions with single-view capture, wide spatial range, and dynamic objects of limited spatial footprint and large motion disparit",
    "url": "https://arxiv.org/abs/2602.22376",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Enhancing Renal Tumor Malignancy Prediction: Deep Learning with Automatic 3D CT Organ Focused Attention",
    "summary": "arXiv:2602.22381v1 Announce Type: cross Abstract: Accurate prediction of malignancy in renal tumors is crucial for informing clinical decisions and optimizing treatment strategies. However, existing imaging modalities lack the necessary accuracy to reliably predict malignancy before surgical intervention. While deep learning has shown promise in ma",
    "url": "https://arxiv.org/abs/2602.22381",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Contextual Memory Virtualisation: DAG-Based State Management and Structurally Lossless Trimming for LLM Agents",
    "summary": "arXiv:2602.22402v1 Announce Type: cross Abstract: As large language models engage in extended reasoning tasks, they accumulate significant state -- architectural mappings, trade-off decisions, codebase conventions -- within the context window. This understanding is lost when sessions reach context limits and undergo lossy compaction. We propose Con",
    "url": "https://arxiv.org/abs/2602.22402",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Revisiting Chebyshev Polynomial and Anisotropic RBF Models for Tabular Regression",
    "summary": "arXiv:2602.22422v1 Announce Type: cross Abstract: Smooth-basis models such as Chebyshev polynomial regressors and radial basis function (RBF) networks are well established in numerical analysis. Their continuously differentiable prediction surfaces suit surrogate optimisation, sensitivity analysis, and other settings where the response varies gradu",
    "url": "https://arxiv.org/abs/2602.22422",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "HubScan: Detecting Hubness Poisoning in Retrieval-Augmented Generation Systems",
    "summary": "arXiv:2602.22427v1 Announce Type: cross Abstract: Retrieval-Augmented Generation (RAG) systems are essential to contemporary AI applications, allowing large language models to obtain external knowledge via vector similarity search. Nevertheless, these systems encounter a significant security flaw: hubness - items that frequently appear in the top-k",
    "url": "https://arxiv.org/abs/2602.22427",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Calibrated Test-Time Guidance for Bayesian Inference",
    "summary": "arXiv:2602.22428v1 Announce Type: cross Abstract: Test-time guidance is a widely used mechanism for steering pretrained diffusion models toward outcomes specified by a reward function. Existing approaches, however, focus on maximizing reward rather than sampling from the true Bayesian posterior, leading to miscalibrated inference. In this work, we ",
    "url": "https://arxiv.org/abs/2602.22428",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "GetBatch: Distributed Multi-Object Retrieval for ML Data Loading",
    "summary": "arXiv:2602.22434v1 Announce Type: cross Abstract: Machine learning training pipelines consume data in batches. A single training step may require thousands of samples drawn from shards distributed across a storage cluster. Issuing thousands of individual GET requests incurs per-request overhead that often dominates data transfer time. To solve this",
    "url": "https://arxiv.org/abs/2602.22434",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "veScale-FSDP: Flexible and High-Performance FSDP at Scale",
    "summary": "arXiv:2602.22437v1 Announce Type: cross Abstract: Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used for training large-scale models, featuring its flexibility and minimal intrusion on model code. However, current FSDP systems struggle with structure-aware training methods (e.g., block-wise quantized training) and with non-eleme",
    "url": "https://arxiv.org/abs/2602.22437",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "From Bias to Balance: Fairness-Aware Paper Recommendation for Equitable Peer Review",
    "summary": "arXiv:2602.22438v1 Announce Type: cross Abstract: Despite frequent double-blind review, systemic biases related to author demographics still disadvantage underrepresented groups. We start from a simple hypothesis: if a post-review recommender is trained with an explicit fairness regularizer, it should increase inclusion without degrading quality. T",
    "url": "https://arxiv.org/abs/2602.22438",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "ECHO: Encoding Communities via High-order Operators",
    "summary": "arXiv:2602.22446v1 Announce Type: cross Abstract: Community detection in attributed networks faces a fundamental divide: topological algorithms ignore semantic features, while Graph Neural Networks (GNNs) encounter devastating computational bottlenecks. Specifically, GNNs suffer from a Semantic Wall of feature over smoothing in dense or heterophili",
    "url": "https://arxiv.org/abs/2602.22446",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "A Fusion of context-aware based BanglaBERT and Two-Layer Stacked LSTM Framework for Multi-Label Cyberbullying Detection",
    "summary": "arXiv:2602.22449v1 Announce Type: cross Abstract: Cyberbullying has become a serious and growing concern in todays virtual world. When left unnoticed, it can have adverse consequences for social and mental health. Researchers have explored various types of cyberbullying, but most approaches use single-label classification, assuming that each commen",
    "url": "https://arxiv.org/abs/2602.22449",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Silent Egress: When Implicit Prompt Injection Makes LLM Agents Leak Without a Trace",
    "summary": "arXiv:2602.22450v1 Announce Type: cross Abstract: Agentic large language model systems increasingly automate tasks by retrieving URLs and calling external tools. We show that this workflow gives rise to implicit prompt injection: adversarial instructions embedded in automatically generated URL previews, including titles, metadata, and snippets, can",
    "url": "https://arxiv.org/abs/2602.22450",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Automating the Detection of Requirement Dependencies Using Large Language Models",
    "summary": "arXiv:2602.22456v1 Announce Type: cross Abstract: Requirements are inherently interconnected through various types of dependencies. Identifying these dependencies is essential, as they underpin critical decisions and influence a range of activities throughout software development. However, this task is challenging, particularly in modern software s",
    "url": "https://arxiv.org/abs/2602.22456",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Beyond Dominant Patches: Spatial Credit Redistribution For Grounded Vision-Language Models",
    "summary": "arXiv:2602.22469v1 Announce Type: cross Abstract: Vision-language models (VLMs) frequently hallucinate objects absent from the input image. We trace this failure to spatial credit collapse: activation credit concentrating on sparse visual patches in early transformer layers, which suppresses contextual evidence and increases reliance on language pr",
    "url": "https://arxiv.org/abs/2602.22469",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Sydney Telling Fables on AI and Humans: A Corpus Tracing Memetic Transfer of Persona between LLMs",
    "summary": "arXiv:2602.22481v1 Announce Type: cross Abstract: The way LLM-based entities conceive of the relationship between AI and humans is an important topic for both cultural and safety reasons. When we examine this topic, what matters is not only the model itself but also the personas we simulate on that model. This can be well illustrated by the Sydney ",
    "url": "https://arxiv.org/abs/2602.22481",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Importance of Prompt Optimisation for Error Detection in Medical Notes Using Language Models",
    "summary": "arXiv:2602.22483v1 Announce Type: cross Abstract: Errors in medical text can cause delays or even result in incorrect treatment for patients. Recently, language models have shown promise in their ability to automatically detect errors in medical text, an ability that has the opportunity to significantly benefit healthcare systems. In this paper, we",
    "url": "https://arxiv.org/abs/2602.22483",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Explainability-Aware Evaluation of Transfer Learning Models for IoT DDoS Detection Under Resource Constraints",
    "summary": "arXiv:2602.22488v1 Announce Type: cross Abstract: Distributed denial-of-service (DDoS) attacks threaten the availability of Internet of Things (IoT) infrastructures, particularly under resource-constrained deployment conditions. Although transfer learning models have shown promising detection accuracy, their reliability, computational feasibility, ",
    "url": "https://arxiv.org/abs/2602.22488",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "From Shallow Bayesian Neural Networks to Gaussian Processes: General Convergence, Identifiability and Scalable Inference",
    "summary": "arXiv:2602.22492v1 Announce Type: cross Abstract: In this work, we study scaling limits of shallow Bayesian neural networks (BNNs) via their connection to Gaussian processes (GPs), with an emphasis on statistical modeling, identifiability, and scalable inference. We first establish a general convergence result from BNNs to GPs by relaxing assumptio",
    "url": "https://arxiv.org/abs/2602.22492",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Reinforcement-aware Knowledge Distillation for LLM Reasoning",
    "summary": "arXiv:2602.22495v1 Announce Type: cross Abstract: Reinforcement learning (RL) post-training has recently driven major gains in long chain-of-thought reasoning large language models (LLMs), but the high inference cost of such models motivates distillation into smaller students. Most existing knowledge distillation (KD) methods are designed for super",
    "url": "https://arxiv.org/abs/2602.22495",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "SignVLA: A Gloss-Free Vision-Language-Action Framework for Real-Time Sign Language-Guided Robotic Manipulation",
    "summary": "arXiv:2602.22514v1 Announce Type: cross Abstract: We present, to our knowledge, the first sign language-driven Vision-Language-Action (VLA) framework for intuitive and inclusive human-robot interaction. Unlike conventional approaches that rely on gloss annotations as intermediate supervision, the proposed system adopts a gloss-free paradigm and dir",
    "url": "https://arxiv.org/abs/2602.22514",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Efficient Dialect-Aware Modeling and Conditioning for Low-Resource Taiwanese Hakka Speech Processing",
    "summary": "arXiv:2602.22522v1 Announce Type: cross Abstract: Taiwanese Hakka is a low-resource, endangered language that poses significant challenges for automatic speech recognition (ASR), including high dialectal variability and the presence of two distinct writing systems (Hanzi and Pinyin). Traditional ASR models often encounter difficulties in this conte",
    "url": "https://arxiv.org/abs/2602.22522",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Iterative Prompt Refinement for Dyslexia-Friendly Text Summarization Using GPT-4o",
    "summary": "arXiv:2602.22524v1 Announce Type: cross Abstract: Dyslexia affects approximately 10% of the global population and presents persistent challenges in reading fluency and text comprehension. While existing assistive technologies address visual presentation, linguistic complexity remains a substantial barrier to equitable access. This paper presents an",
    "url": "https://arxiv.org/abs/2602.22524",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Predicting Tennis Serve directions with Machine Learning",
    "summary": "arXiv:2602.22527v1 Announce Type: cross Abstract: Serves, especially first serves, are very important in professional tennis. Servers choose their serve directions strategically to maximize their winning chances while trying to be unpredictable. On the other hand, returners try to predict serve directions to make good returns. The mind game between",
    "url": "https://arxiv.org/abs/2602.22527",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Generative Agents Navigating Digital Libraries",
    "summary": "arXiv:2602.22529v1 Announce Type: cross Abstract: In the rapidly evolving field of digital libraries, the development of large language models (LLMs) has opened up new possibilities for simulating user behavior. This innovation addresses the longstanding challenge in digital library research: the scarcity of publicly available datasets on user sear",
    "url": "https://arxiv.org/abs/2602.22529",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Ruyi2 Technical Report",
    "summary": "arXiv:2602.22543v1 Announce Type: cross Abstract: Large Language Models (LLMs) face significant challenges regarding deployment costs and latency, necessitating adaptive computing strategies. Building upon the AI Flow framework, we introduce Ruyi2 as an evolution of our adaptive model series designed for efficient variable-depth computation. While ",
    "url": "https://arxiv.org/abs/2602.22543",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "HARU-Net: Hybrid Attention Residual U-Net for Edge-Preserving Denoising in Cone-Beam Computed Tomography",
    "summary": "arXiv:2602.22544v1 Announce Type: cross Abstract: Cone-beam computed tomography (CBCT) is widely used in dental and maxillofacial imaging, but low-dose acquisition introduces strong, spatially varying noise that degrades soft-tissue visibility and obscures fine anatomical structures. Classical denoising methods struggle to suppress noise in CBCT wh",
    "url": "https://arxiv.org/abs/2602.22544",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "DisQ-HNet: A Disentangled Quantized Half-UNet for Interpretable Multimodal Image Synthesis Applications to Tau-PET Synthesis from T1 and FLAIR MRI",
    "summary": "arXiv:2602.22545v1 Announce Type: cross Abstract: Tau positron emission tomography (tau-PET) provides an in vivo marker of Alzheimer's disease pathology, but cost and limited availability motivate MRI-based alternatives. We introduce DisQ-HNet (DQH), a framework that synthesizes tau-PET from paired T1-weighted and FLAIR MRI while exposing how each ",
    "url": "https://arxiv.org/abs/2602.22545",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "DrivePTS: A Progressive Learning Framework with Textual and Structural Enhancement for Driving Scene Generation",
    "summary": "arXiv:2602.22549v1 Announce Type: cross Abstract: Synthesis of diverse driving scenes serves as a crucial data augmentation technique for validating the robustness and generalizability of autonomous driving systems. Current methods aggregate high-definition (HD) maps and 3D bounding boxes as geometric conditions in diffusion models for conditional ",
    "url": "https://arxiv.org/abs/2602.22549",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Autoregressive Visual Decoding from EEG Signals",
    "summary": "arXiv:2602.22555v1 Announce Type: cross Abstract: Electroencephalogram (EEG) signals have become a popular medium for decoding visual information due to their cost-effectiveness and high temporal resolution. However, current approaches face significant challenges in bridging the modality gap between EEG and image data. These methods typically rely ",
    "url": "https://arxiv.org/abs/2602.22555",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Stable Adaptive Thinking via Advantage Shaping and Length-Aware Gradient Regulation",
    "summary": "arXiv:2602.22556v1 Announce Type: cross Abstract: Large reasoning models (LRMs) achieve strong performance through extended reasoning traces, but they often exhibit overthinking behavior for low-complexity queries. Existing efforts to mitigate this issue are fundamentally limited by unstable accuracy-efficiency trade-offs and poor robustness to het",
    "url": "https://arxiv.org/abs/2602.22556",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Operationalizing Fairness: Post-Hoc Threshold Optimization Under Hard Resource Limits",
    "summary": "arXiv:2602.22560v1 Announce Type: cross Abstract: The deployment of machine learning in high-stakes domains requires a balance between predictive safety and algorithmic fairness. However, existing fairness interventions often as- sume unconstrained resources and employ group-specific decision thresholds that violate anti- discrimination regulations",
    "url": "https://arxiv.org/abs/2602.22560",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Addressing Climate Action Misperceptions with Generative AI",
    "summary": "arXiv:2602.22564v1 Announce Type: cross Abstract: Mitigating climate change requires behaviour change. However, even climate-concerned individuals often hold misperceptions about which actions most reduce carbon emissions. We recruited 1201 climate-concerned individuals to examine whether discussing climate actions with a large language model (LLM)",
    "url": "https://arxiv.org/abs/2602.22564",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Quality-Aware Robust Multi-View Clustering for Heterogeneous Observation Noise",
    "summary": "arXiv:2602.22568v1 Announce Type: cross Abstract: Deep multi-view clustering has achieved remarkable progress but remains vulnerable to complex noise in real-world applications. Existing noisy robust methods predominantly rely on a simplified binary assumption, treating data as either perfectly clean or completely corrupted. This overlooks the prev",
    "url": "https://arxiv.org/abs/2602.22568",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Guidance Matters: Rethinking the Evaluation Pitfall for Text-to-Image Generation",
    "summary": "arXiv:2602.22570v1 Announce Type: cross Abstract: Classifier-free guidance (CFG) has helped diffusion models achieve great conditional generation in various fields. Recently, more diffusion guidance methods have emerged with improved generation quality and human preference. However, can these emerging diffusion guidance methods really achieve solid",
    "url": "https://arxiv.org/abs/2602.22570",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "S2O: Early Stopping for Sparse Attention via Online Permutation",
    "summary": "arXiv:2602.22575v1 Announce Type: cross Abstract: Attention scales quadratically with sequence length, fundamentally limiting long-context inference. Existing block-granularity sparsification can reduce latency, but coarse blocks impose an intrinsic sparsity ceiling, making further improvements difficult even with carefully engineered designs. We p",
    "url": "https://arxiv.org/abs/2602.22575",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "TabDLM: Free-Form Tabular Data Generation via Joint Numerical-Language Diffusion",
    "summary": "arXiv:2602.22586v1 Announce Type: cross Abstract: Synthetic tabular data generation has attracted growing attention due to its importance for data augmentation, foundation models, and privacy. However, real-world tabular datasets increasingly contain free-form text fields (e.g., reviews or clinical notes) alongside structured numerical and categori",
    "url": "https://arxiv.org/abs/2602.22586",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "BetterScene: 3D Scene Synthesis with Representation-Aligned Generative Model",
    "summary": "arXiv:2602.22596v1 Announce Type: cross Abstract: We present BetterScene, an approach to enhance novel view synthesis (NVS) quality for diverse real-world scenes using extremely sparse, unconstrained photos. BetterScene leverages the production-ready Stable Video Diffusion (SVD) model pretrained on billions of frames as a strong backbone, aiming to",
    "url": "https://arxiv.org/abs/2602.22596",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Transformers converge to invariant algorithmic cores",
    "summary": "arXiv:2602.22600v1 Announce Type: cross Abstract: Large language models exhibit sophisticated capabilities, yet understanding how they work internally remains a central challenge. A fundamental obstacle is that training selects for behavior, not circuitry, so many weight configurations can implement the same function. Which internal structures refl",
    "url": "https://arxiv.org/abs/2602.22600",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "CoLyricist: Enhancing Lyric Writing with AI through Workflow-Aligned Support",
    "summary": "arXiv:2602.22606v1 Announce Type: cross Abstract: We propose CoLyricist, an AI-assisted lyric writing tool designed to support the typical workflows of experienced lyricists and enhance their creative efficiency. While lyricists have unique processes, many follow common stages. Tools that fail to accommodate these stages challenge integration into ",
    "url": "https://arxiv.org/abs/2602.22606",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "CGSA: Class-Guided Slot-Aware Adaptation for Source-Free Object Detection",
    "summary": "arXiv:2602.22621v1 Announce Type: cross Abstract: Source-Free Domain Adaptive Object Detection (SF-DAOD) aims to adapt a detector trained on a labeled source domain to an unlabeled target domain without retaining any source data. Despite recent progress, most popular approaches focus on tuning pseudo-label thresholds or refining the teacher-student",
    "url": "https://arxiv.org/abs/2602.22621",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "ContextRL: Enhancing MLLM's Knowledge Discovery Efficiency with Context-Augmented RL",
    "summary": "arXiv:2602.22623v1 Announce Type: cross Abstract: We propose ContextRL, a novel framework that leverages context augmentation to overcome these bottlenecks. Specifically, to enhance Identifiability, we provide the reward model with full reference solutions as context, enabling fine-grained process verification to filter out false positives (samples",
    "url": "https://arxiv.org/abs/2602.22623",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Instruction-based Image Editing with Planning, Reasoning, and Generation",
    "summary": "arXiv:2602.22624v1 Announce Type: cross Abstract: Editing images via instruction provides a natural way to generate interactive content, but it is a big challenge due to the higher requirement of scene understanding and generation. Prior work utilizes a chain of large language models, object segmentation models, and editing models for this task. Ho",
    "url": "https://arxiv.org/abs/2602.22624",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "dLLM: Simple Diffusion Language Modeling",
    "summary": "arXiv:2602.22661v1 Announce Type: cross Abstract: Although diffusion language models (DLMs) are evolving quickly, many recent models converge on a set of shared components. These components, however, are distributed across ad-hoc research codebases or lack transparent implementations, making them difficult to reproduce or extend. As the field accel",
    "url": "https://arxiv.org/abs/2602.22661",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "ViCLIP-OT: The First Foundation Vision-Language Model for Vietnamese Image-Text Retrieval with Optimal Transport",
    "summary": "arXiv:2602.22678v1 Announce Type: cross Abstract: Image-text retrieval has become a fundamental component in intelligent multimedia systems; however, most existing vision-language models are optimized for highresource languages and remain suboptimal for low-resource settings such as Vietnamese. This work introduces ViCLIP-OT, a foundation vision-la",
    "url": "https://arxiv.org/abs/2602.22678",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "SUPERGLASSES: Benchmarking Vision Language Models as Intelligent Agents for AI Smart Glasses",
    "summary": "arXiv:2602.22683v1 Announce Type: cross Abstract: The rapid advancement of AI-powered smart glasses, one of the hottest wearable devices, has unlocked new frontiers for multimodal interaction, with Visual Question Answering (VQA) over external knowledge sources emerging as a core application. Existing Vision Language Models (VLMs) adapted to smart ",
    "url": "https://arxiv.org/abs/2602.22683",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Reinforcing Real-world Service Agents: Balancing Utility and Cost in Task-oriented Dialogue",
    "summary": "arXiv:2602.22697v1 Announce Type: cross Abstract: The rapid evolution of Large Language Models (LLMs) has accelerated the transition from conversational chatbots to general agents. However, effectively balancing empathetic communication with budget-aware decision-making remains an open challenge. Since existing methods fail to capture these complex",
    "url": "https://arxiv.org/abs/2602.22697",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Tokenization, Fusion and Decoupling: Bridging the Granularity Mismatch Between Large Language Models and Knowledge Graphs",
    "summary": "arXiv:2602.22698v1 Announce Type: cross Abstract: Leveraging Large Language Models (LLMs) for Knowledge Graph Completion (KGC) is promising but hindered by a fundamental granularity mismatch. LLMs operate on fragmented token sequences, whereas entities are the fundamental units in knowledge graphs (KGs) scenarios. Existing approaches typically cons",
    "url": "https://arxiv.org/abs/2602.22698",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "IMMACULATE: A Practical LLM Auditing Framework via Verifiable Computation",
    "summary": "arXiv:2602.22700v1 Announce Type: cross Abstract: Commercial large language models are typically deployed as black-box API services, requiring users to trust providers to execute inference correctly and report token usage honestly. We present IMMACULATE, a practical auditing framework that detects economically motivated deviations-such as model sub",
    "url": "https://arxiv.org/abs/2602.22700",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Same Words, Different Judgments: Modality Effects on Preference Alignment",
    "summary": "arXiv:2602.22710v1 Announce Type: cross Abstract: Preference-based reinforcement learning (PbRL) is the dominant framework for aligning AI systems to human preferences, but its application to speech remains underexplored. We present a controlled cross-modal study of human and synthetic preference annotations, comparing text and audio evaluations of",
    "url": "https://arxiv.org/abs/2602.22710",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs",
    "summary": "arXiv:2602.22716v1 Announce Type: cross Abstract: 3D Large Vision-Language Models (3D LVLMs) built upon Large Language Models (LLMs) have achieved remarkable progress across various multimodal tasks. However, their inherited position-dependent modeling mechanism, Rotary Position Embedding (RoPE), remains suboptimal for 3D multimodal understanding. ",
    "url": "https://arxiv.org/abs/2602.22716",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "AgentSentry: Mitigating Indirect Prompt Injection in LLM Agents via Temporal Causal Diagnostics and Context Purification",
    "summary": "arXiv:2602.22724v1 Announce Type: cross Abstract: Large language model (LLM) agents increasingly rely on external tools and retrieval systems to autonomously complete complex tasks. However, this design exposes agents to indirect prompt injection (IPI), where attacker-controlled context embedded in tool outputs or retrieved content silently steers ",
    "url": "https://arxiv.org/abs/2602.22724",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Simulation-based Optimization for Augmented Reading",
    "summary": "arXiv:2602.22735v1 Announce Type: cross Abstract: Augmented reading systems aim to adapt text presentation to improve comprehension and task performance, yet existing approaches rely heavily on heuristics, opaque data-driven models, or repeated human involvement in the design loop. We propose framing augmented reading as a simulation-based optimiza",
    "url": "https://arxiv.org/abs/2602.22735",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "AMLRIS: Alignment-aware Masked Learning for Referring Image Segmentation",
    "summary": "arXiv:2602.22740v1 Announce Type: cross Abstract: Referring Image Segmentation (RIS) aims to segment an object in an image identified by a natural language expression. The paper introduces Alignment-Aware Masked Learning (AML), a training strategy to enhance RIS by explicitly estimating pixel-level vision-language alignment, filtering out poorly al",
    "url": "https://arxiv.org/abs/2602.22740",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Towards Simulating Social Media Users with LLMs: Evaluating the Operational Validity of Conditioned Comment Prediction",
    "summary": "arXiv:2602.22752v1 Announce Type: cross Abstract: The transition of Large Language Models (LLMs) from exploratory tools to active \"silicon subjects\" in social science lacks extensive validation of operational validity. This study introduces Conditioned Comment Prediction (CCP), a task in which a model predicts how a user would comment on a given st",
    "url": "https://arxiv.org/abs/2602.22752",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Distributed LLM Pretraining During Renewable Curtailment Windows: A Feasibility Study",
    "summary": "arXiv:2602.22760v1 Announce Type: cross Abstract: Training large language models (LLMs) requires substantial compute and energy. At the same time, renewable energy sources regularly produce more electricity than the grid can absorb, leading to curtailment, the deliberate reduction of clean generation that would otherwise go to waste. These periods ",
    "url": "https://arxiv.org/abs/2602.22760",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "TherapyProbe: Generating Design Knowledge for Relational Safety in Mental Health Chatbots Through Adversarial Simulation",
    "summary": "arXiv:2602.22775v1 Announce Type: cross Abstract: As mental health chatbots proliferate to address the global treatment gap, a critical question emerges: How do we design for relational safety the quality of interaction patterns that unfold across conversations rather than the correctness of individual responses? Current safety evaluations assess s",
    "url": "https://arxiv.org/abs/2602.22775",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "QSIM: Mitigating Overestimation in Multi-Agent Reinforcement Learning via Action Similarity Weighted Q-Learning",
    "summary": "arXiv:2602.22786v1 Announce Type: cross Abstract: Value decomposition (VD) methods have achieved remarkable success in cooperative multi-agent reinforcement learning (MARL). However, their reliance on the max operator for temporal-difference (TD) target calculation leads to systematic Q-value overestimation. This issue is particularly severe in MAR",
    "url": "https://arxiv.org/abs/2602.22786",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Probing for Knowledge Attribution in Large Language Models",
    "summary": "arXiv:2602.22787v1 Announce Type: cross Abstract: Large language models (LLMs) often generate fluent but unfounded claims, or hallucinations, which fall into two types: (i) faithfulness violations - misusing user context - and (ii) factuality violations - errors from internal knowledge. Proper mitigation depends on knowing whether a model's answer ",
    "url": "https://arxiv.org/abs/2602.22787",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Natural Language Declarative Prompting (NLD-P): A Modular Governance Method for Prompt Design Under Model Drift",
    "summary": "arXiv:2602.22790v1 Announce Type: cross Abstract: The rapid evolution of large language models (LLMs) has transformed prompt engineering from a localized craft into a systems-level governance challenge. As models scale and update across generations, prompt behavior becomes sensitive to shifts in instruction-following policies, alignment regimes, an",
    "url": "https://arxiv.org/abs/2602.22790",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Unleashing the Potential of Diffusion Models for End-to-End Autonomous Driving",
    "summary": "arXiv:2602.22801v1 Announce Type: cross Abstract: Diffusion models have become a popular choice for decision-making tasks in robotics, and more recently, are also being considered for solving autonomous driving tasks. However, their applications and evaluations in autonomous driving remain limited to simulation-based or laboratory settings. The ful",
    "url": "https://arxiv.org/abs/2602.22801",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Hierarchy-of-Groups Policy Optimization for Long-Horizon Agentic Tasks",
    "summary": "arXiv:2602.22817v1 Announce Type: cross Abstract: Group-based reinforcement learning (RL), such as GRPO, has advanced the capabilities of large language models on long-horizon agentic tasks. To enable more fine-grained policy updates, recent research has increasingly shifted toward stepwise group-based policy optimization, which treats each step in",
    "url": "https://arxiv.org/abs/2602.22817",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "TCM-DiffRAG: Personalized Syndrome Differentiation Reasoning Method for Traditional Chinese Medicine based on Knowledge Graph and Chain of Thought",
    "summary": "arXiv:2602.22828v1 Announce Type: cross Abstract: Background: Retrieval augmented generation (RAG) technology can empower large language models (LLMs) to generate more accurate, professional, and timely responses without fine tuning. However, due to the complex reasoning processes and substantial individual differences involved in traditional Chine",
    "url": "https://arxiv.org/abs/2602.22828",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Moral Preferences of LLMs Under Directed Contextual Influence",
    "summary": "arXiv:2602.22831v1 Announce Type: cross Abstract: Moral benchmarks for LLMs typically use context-free prompts, implicitly assuming stable preferences. In deployment, however, prompts routinely include contextual signals such as user requests, cues on social norms, etc. that may steer decisions. We study how directed contextual influences reshape d",
    "url": "https://arxiv.org/abs/2602.22831",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Decentralized Ranking Aggregation: Gossip Algorithms for Borda and Copeland Consensus",
    "summary": "arXiv:2602.22847v1 Announce Type: cross Abstract: The concept of ranking aggregation plays a central role in preference analysis, and numerous algorithms for calculating median rankings, often originating in social choice theory, have been documented in the literature, offering theoretical guarantees in a centralized setting, i.e., when all the ran",
    "url": "https://arxiv.org/abs/2602.22847",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "MEDNA-DFM: A Dual-View FiLM-MoE Model for Explainable DNA Methylation Prediction",
    "summary": "arXiv:2602.22850v1 Announce Type: cross Abstract: Accurate computational identification of DNA methylation is essential for understanding epigenetic regulation. Although deep learning excels in this binary classification task, its \"black-box\" nature impedes biological insight. We address this by introducing a high-performance model MEDNA-DFM, along",
    "url": "https://arxiv.org/abs/2602.22850",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Test-Time Scaling with Diffusion Language Models via Reward-Guided Stitching",
    "summary": "arXiv:2602.22871v1 Announce Type: cross Abstract: Reasoning with large language models often benefits from generating multiple chains-of-thought, but existing aggregation strategies are typically trajectory-level (e.g., selecting the best trace or voting on the final answer), discarding useful intermediate work from partial or \"nearly correct\" atte",
    "url": "https://arxiv.org/abs/2602.22871",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Learning Tangent Bundles and Characteristic Classes with Autoencoder Atlases",
    "summary": "arXiv:2602.22873v1 Announce Type: cross Abstract: We introduce a theoretical framework that connects multi-chart autoencoders in manifold learning with the classical theory of vector bundles and characteristic classes. Rather than viewing autoencoders as producing a single global Euclidean embedding, we treat a collection of locally trained encoder",
    "url": "https://arxiv.org/abs/2602.22873",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "NoRA: Breaking the Linear Ceiling of Low-Rank Adaptation via Manifold Expansion",
    "summary": "arXiv:2602.22911v1 Announce Type: cross Abstract: Low-Rank Adaptation (LoRA) dominates parameter-efficient fine-tuning (PEFT). However, it faces a critical ``linear ceiling'' in complex reasoning tasks: simply increasing the rank yields diminishing returns due to intrinsic linear constraints. We introduce NoRA (Non-linear Rank Adaptation), a weight",
    "url": "https://arxiv.org/abs/2602.22911",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "A Holistic Framework for Robust Bangla ASR and Speaker Diarization with Optimized VAD and CTC Alignment",
    "summary": "arXiv:2602.22935v1 Announce Type: cross Abstract: Despite being one of the most widely spoken languages globally, Bangla remains a low-resource language in the field of Natural Language Processing (NLP). Mainstream Automatic Speech Recognition (ASR) and Speaker Diarization systems for Bangla struggles when processing longform audio exceeding 3060 s",
    "url": "https://arxiv.org/abs/2602.22935",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "pMoE: Prompting Diverse Experts Together Wins More in Visual Adaptation",
    "summary": "arXiv:2602.22938v1 Announce Type: cross Abstract: Parameter-efficient fine-tuning has demonstrated promising results across various visual adaptation tasks, such as classification and segmentation. Typically, prompt tuning techniques have harnessed knowledge from a single pre-trained model, whether from a general or a specialized medical domain. Ho",
    "url": "https://arxiv.org/abs/2602.22938",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "MM-NeuroOnco: A Multimodal Benchmark and Instruction Dataset for MRI-Based Brain Tumor Diagnosis",
    "summary": "arXiv:2602.22955v1 Announce Type: cross Abstract: Accurate brain tumor diagnosis requires models to not only detect lesions but also generate clinically interpretable reasoning grounded in imaging manifestations, yet existing public datasets remain limited in annotation richness and diagnostic semantics. To bridge this gap, we introduce MM-NeuroOnc",
    "url": "https://arxiv.org/abs/2602.22955",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Discovery of Interpretable Physical Laws in Materials via Language-Model-Guided Symbolic Regression",
    "summary": "arXiv:2602.22967v1 Announce Type: cross Abstract: Discovering interpretable physical laws from high-dimensional data is a fundamental challenge in scientific research. Traditional methods, such as symbolic regression, often produce complex, unphysical formulas when searching a vast space of possible forms. We introduce a framework that guides the s",
    "url": "https://arxiv.org/abs/2602.22967",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Residual Koopman Spectral Profiling for Predicting and Preventing Transformer Training Instability",
    "summary": "arXiv:2602.22988v1 Announce Type: cross Abstract: Training divergence in transformers wastes compute, yet practitioners discover instability only after expensive runs begin. They therefore need an expected probability of failure for a transformer before training starts. Our study of Residual Koopman Spectral Profiling (RKSP) provides such an estima",
    "url": "https://arxiv.org/abs/2602.22988",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Scattering Transform for Auditory Attention Decoding",
    "summary": "arXiv:2602.23003v1 Announce Type: cross Abstract: The use of hearing aids will increase in the coming years due to demographic change. One open problem that remains to be solved by a new generation of hearing aids is the cocktail party problem. A possible solution is electroencephalography-based auditory attention decoding. This has been the subjec",
    "url": "https://arxiv.org/abs/2602.23003",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization",
    "summary": "arXiv:2602.23008v1 Announce Type: cross Abstract: Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EM",
    "url": "https://arxiv.org/abs/2602.23008",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "LLMServingSim 2.0: A Unified Simulator for Heterogeneous and Disaggregated LLM Serving Infrastructure",
    "summary": "arXiv:2602.23036v1 Announce Type: cross Abstract: Large language model (LLM) serving infrastructures are undergoing a shift toward heterogeneity and disaggregation. Modern deployments increasingly integrate diverse accelerators and near-memory processing technologies, introducing significant hardware heterogeneity, while system software increasingl",
    "url": "https://arxiv.org/abs/2602.23036",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Affine-Scaled Attention: Towards Flexible and Stable Transformer Attention",
    "summary": "arXiv:2602.23057v1 Announce Type: cross Abstract: Transformer attention is typically implemented using softmax normalization, which enforces attention weights with unit sum normalization. While effective in many settings, this constraint can limit flexibility in controlling attention magnitudes and may contribute to overly concentrated or unstable ",
    "url": "https://arxiv.org/abs/2602.23057",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "MoDora: Tree-Based Semi-Structured Document Analysis System",
    "summary": "arXiv:2602.23061v1 Announce Type: cross Abstract: Semi-structured documents integrate diverse interleaved data elements (e.g., tables, charts, hierarchical paragraphs) arranged in various and often irregular layouts. These documents are widely observed across domains and account for a large portion of real-world data. However, existing methods stru",
    "url": "https://arxiv.org/abs/2602.23061",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Make It Hard to Hear, Easy to Learn: Long-Form Bengali ASR and Speaker Diarization via Extreme Augmentation and Perfect Alignment",
    "summary": "arXiv:2602.23070v1 Announce Type: cross Abstract: Although Automatic Speech Recognition (ASR) in Bengali has seen significant progress, processing long-duration audio and performing robust speaker diarization remain critical research gaps. To address the severe scarcity of joint ASR and diarization resources for this language, we introduce Lipi-Gho",
    "url": "https://arxiv.org/abs/2602.23070",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Quantity Convergence, Quality Divergence: Disentangling Fluency and Accuracy in L2 Mandarin Prosody",
    "summary": "arXiv:2602.23071v1 Announce Type: cross Abstract: While second language (L2) learners may acquire target syntactic word order, mapping this syntax onto appropriate prosodic structures remains a persistent challenge. This study investigates the fossilization and stability of the L2 syntax-prosody interface by comparing 67 native Mandarin speakers wi",
    "url": "https://arxiv.org/abs/2602.23071",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Accelerated Online Risk-Averse Policy Evaluation in POMDPs with Theoretical Guarantees and Novel CVaR Bounds",
    "summary": "arXiv:2602.23073v1 Announce Type: cross Abstract: Risk-averse decision-making under uncertainty in partially observable domains is a central challenge in artificial intelligence and is essential for developing reliable autonomous agents. The formal framework for such problems is the partially observable Markov decision process (POMDP), where risk s",
    "url": "https://arxiv.org/abs/2602.23073",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Devling into Adversarial Transferability on Image Classification: Review, Benchmark, and Evaluation",
    "summary": "arXiv:2602.23117v1 Announce Type: cross Abstract: Adversarial transferability refers to the capacity of adversarial examples generated on the surrogate model to deceive alternate, unexposed victim models. This property eliminates the need for direct access to the victim model during an attack, thereby raising considerable security concerns in pract",
    "url": "https://arxiv.org/abs/2602.23117",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Automated Vulnerability Detection in Source Code Using Deep Representation Learning",
    "summary": "arXiv:2602.23121v1 Announce Type: cross Abstract: Each year, software vulnerabilities are discovered, which pose significant risks of exploitation and system compromise. We present a convolutional neural network model that can successfully identify bugs in C code. We trained our model using two complementary datasets: a machine-labeled dataset crea",
    "url": "https://arxiv.org/abs/2602.23121",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "DyGnROLE: Modeling Asymmetry in Dynamic Graphs with Node-Role-Oriented Latent Encoding",
    "summary": "arXiv:2602.23135v1 Announce Type: cross Abstract: Real-world dynamic graphs are often directed, with source and destination nodes exhibiting asymmetrical behavioral patterns and temporal dynamics. However, existing dynamic graph architectures largely rely on shared parameters for processing source and destination nodes, with limited or no systemati",
    "url": "https://arxiv.org/abs/2602.23135",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs",
    "summary": "arXiv:2602.23136v1 Announce Type: cross Abstract: Multimodal LLMs can process speech and images, but they cannot hear a speaker's voice or see an object's texture. We show this is not a failure of encoding: speaker identity, emotion, and visual attributes survive through every LLM layer (3--55$\\times$ above chance in linear probes), yet removing 64",
    "url": "https://arxiv.org/abs/2602.23136",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Efficient Encoder-Free Fourier-based 3D Large Multimodal Model",
    "summary": "arXiv:2602.23153v1 Announce Type: cross Abstract: Large Multimodal Models (LMMs) that process 3D data typically rely on heavy, pre-trained visual encoders to extract geometric features. While recent 2D LMMs have begun to eliminate such encoders for efficiency and scalability, extending this paradigm to 3D remains challenging due to the unordered an",
    "url": "https://arxiv.org/abs/2602.23153",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Latent Gaussian Splatting for 4D Panoptic Occupancy Tracking",
    "summary": "arXiv:2602.23172v1 Announce Type: cross Abstract: Capturing 4D spatiotemporal surroundings is crucial for the safe and reliable operation of robots in dynamic environments. However, most existing methods address only one side of the problem: they either provide coarse geometric tracking via bounding boxes, or detailed 3D structures like voxel-based",
    "url": "https://arxiv.org/abs/2602.23172",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "ColoDiff: Integrating Dynamic Consistency With Content Awareness for Colonoscopy Video Generation",
    "summary": "arXiv:2602.23203v1 Announce Type: cross Abstract: Colonoscopy video generation delivers dynamic, information-rich data critical for diagnosing intestinal diseases, particularly in data-scarce scenarios. High-quality video generation demands temporal consistency and precise control over clinical attributes, but faces challenges from irregular intest",
    "url": "https://arxiv.org/abs/2602.23203",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding?",
    "summary": "arXiv:2602.23225v1 Announce Type: cross Abstract: Diffusion Language Models (DLMs) are often advertised as enabling parallel token generation, yet practical fast DLMs frequently converge to left-to-right, autoregressive (AR)-like decoding dynamics. In contrast, genuinely non-AR generation is promising because it removes AR's sequential bottleneck, ",
    "url": "https://arxiv.org/abs/2602.23225",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction",
    "summary": "arXiv:2602.23228v1 Announce Type: cross Abstract: With the explosive growth of digital entertainment, automated video summarization has become indispensable for applications such as content indexing, personalized recommendation, and efficient media archiving. Automatic synopsis generation for long-form videos, such as movies and TV series, presents",
    "url": "https://arxiv.org/abs/2602.23228",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Scaling Search Relevance: Augmenting App Store Ranking with LLM-Generated Judgments",
    "summary": "arXiv:2602.23234v1 Announce Type: cross Abstract: Large-scale commercial search systems optimize for relevance to drive successful sessions that help users find what they are looking for. To maximize relevance, we leverage two complementary objectives: behavioral relevance (results users tend to click or download) and textual relevance (a result's ",
    "url": "https://arxiv.org/abs/2602.23234",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Spatio-Temporal Token Pruning for Efficient High-Resolution GUI Agents",
    "summary": "arXiv:2602.23235v1 Announce Type: cross Abstract: Pure-vision GUI agents provide universal interaction capabilities but suffer from severe efficiency bottlenecks due to the massive spatiotemporal redundancy inherent in high-resolution screenshots and historical trajectories. We identify two critical misalignments in existing compression paradigms: ",
    "url": "https://arxiv.org/abs/2602.23235",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving",
    "summary": "arXiv:2602.23259v1 Announce Type: cross Abstract: With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the dis",
    "url": "https://arxiv.org/abs/2602.23259",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables",
    "summary": "arXiv:2602.23286v1 Announce Type: cross Abstract: Real-world Table-Text question answering (QA) tasks require models that can reason across long text and source tables, traversing multiple hops and executing complex operations such as aggregation. Yet existing benchmarks are small, manually curated - and therefore error-prone - and contain shallow ",
    "url": "https://arxiv.org/abs/2602.23286",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity",
    "summary": "arXiv:2602.23296v1 Announce Type: cross Abstract: Federated learning (FL) faces challenges in uncertainty quantification (UQ). Without reliable UQ, FL systems risk deploying overconfident models at under-resourced agents, leading to silent local failures despite seemingly satisfactory global performance. Existing federated UQ approaches often addre",
    "url": "https://arxiv.org/abs/2602.23296",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Evaluating Zero-Shot and One-Shot Adaptation of Small Language Models in Leader-Follower Interaction",
    "summary": "arXiv:2602.23312v1 Announce Type: cross Abstract: Leader-follower interaction is an important paradigm in human-robot interaction (HRI). Yet, assigning roles in real time remains challenging for resource-constrained mobile and assistive robots. While large language models (LLMs) have shown promise for natural communication, their size and latency l",
    "url": "https://arxiv.org/abs/2602.23312",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Utilizing LLMs for Industrial Process Automation",
    "summary": "arXiv:2602.23331v1 Announce Type: cross Abstract: A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LL",
    "url": "https://arxiv.org/abs/2602.23331",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Bitwise Systolic Array Architecture for Runtime-Reconfigurable Multi-precision Quantized Multiplication on Hardware Accelerators",
    "summary": "arXiv:2602.23334v1 Announce Type: cross Abstract: Neural network accelerators have been widely applied to edge devices for complex tasks like object tracking, image recognition, etc. Previous works have explored the quantization technologies in related lightweight accelerator designs to reduce hardware resource consumption. However, low precision l",
    "url": "https://arxiv.org/abs/2602.23334",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset",
    "summary": "arXiv:2602.23335v1 Announce Type: cross Abstract: AI-powered scientific research tools are rapidly being integrated into research workflows, yet the field lacks a clear lens into how researchers use these systems in real-world settings. We present and analyze the Asta Interaction Dataset, a large-scale resource comprising over 200,000 user queries ",
    "url": "https://arxiv.org/abs/2602.23335",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "FlashOptim: Optimizers for Memory Efficient Training",
    "summary": "arXiv:2602.23349v1 Announce Type: cross Abstract: Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training",
    "url": "https://arxiv.org/abs/2602.23349",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport",
    "summary": "arXiv:2602.23353v1 Announce Type: cross Abstract: The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically ",
    "url": "https://arxiv.org/abs/2602.23353",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation",
    "summary": "arXiv:2602.23359v1 Announce Type: cross Abstract: We identify occlusion reasoning as a fundamental yet overlooked aspect for 3D layout-conditioned generation. It is essential for synthesizing partially occluded objects with depth-consistent geometry and scale. While existing methods can generate realistic scenes that follow input layouts, they ofte",
    "url": "https://arxiv.org/abs/2602.23359",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Model Agreement via Anchoring",
    "summary": "arXiv:2602.23360v1 Announce Type: cross Abstract: Numerous lines of aim to control $\\textit{model disagreement}$ -- the extent to which two machine learning models disagree in their predictions. We adopt a simple and standard notion of model disagreement in real-valued prediction problems, namely the expected squared difference in predictions betwe",
    "url": "https://arxiv.org/abs/2602.23360",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "LLM4AD: A Platform for Algorithm Design with Large Language Model",
    "summary": "arXiv:2412.17287v2 Announce Type: replace Abstract: We introduce LLM4AD, a unified Python platform for algorithm design (AD) with large language models (LLMs). LLM4AD is a generic framework with modularized blocks for search methods, algorithm design tasks, and LLM interface. The platform integrates numerous key methods and supports a wide range of",
    "url": "https://arxiv.org/abs/2412.17287",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Compositional-ARC: Assessing Systematic Generalization in Abstract Spatial Reasoning",
    "summary": "arXiv:2504.01445v3 Announce Type: replace Abstract: Systematic generalization refers to the capacity to understand and generate novel combinations from known components. Despite recent progress by large language models (LLMs) across various domains, these models often fail to extend their knowledge to novel compositional scenarios, revealing notabl",
    "url": "https://arxiv.org/abs/2504.01445",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Cost-of-Pass: An Economic Framework for Evaluating Language Models",
    "summary": "arXiv:2504.13359v2 Announce Type: replace Abstract: Widespread adoption of AI systems hinges on their ability to generate economic value that outweighs their inference costs. Evaluating this tradeoff requires metrics accounting for both performance and costs. Building on production theory, we develop an economically grounded framework to evaluate l",
    "url": "https://arxiv.org/abs/2504.13359",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning",
    "summary": "arXiv:2505.04317v5 Announce Type: replace Abstract: In this paper, we tackle the problem of learning to play 3v3 multi-drone volleyball, a new embodied competitive task that requires both high-level strategic coordination and low-level agile control. The task is turn-based, multi-agent, and physically grounded, posing significant challenges due to ",
    "url": "https://arxiv.org/abs/2505.04317",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Knowledge Fusion of Large Language Models Via Modular SkillPacks",
    "summary": "arXiv:2505.18502v3 Announce Type: replace Abstract: Cross-capability transfer is a key challenge in large language model (LLM) research, with applications in multi-task integration, model compression, and continual learning. Recent works like FuseLLM and FuseChat have demonstrated the potential of transferring multiple model capabilities to lightwe",
    "url": "https://arxiv.org/abs/2505.18502",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Types of Relations: Defining Analogies with Category Theory",
    "summary": "arXiv:2505.19792v2 Announce Type: replace Abstract: In order to behave intelligently both humans and machines have to represent their knowledge adequately for how it is used. Humans often use analogies to transfer their knowledge to new domains, or help others with this transfer via explanations. Hence, an important question is: What representation",
    "url": "https://arxiv.org/abs/2505.19792",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?",
    "summary": "arXiv:2508.01780v2 Announce Type: replace Abstract: Model Context Protocol (MCP) has become a key infrastructure for connecting LLMs with external tools, scaling to 10,000+ MCP servers with diverse tools. Unfortunately, there is still a large gap between real-world MCP usage and current evaluation: they typically assume single-server settings and d",
    "url": "https://arxiv.org/abs/2508.01780",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "FHIR-RAG-MEDS: Integrating HL7 FHIR with Retrieval-Augmented Large Language Models for Enhanced Medical Decision Support",
    "summary": "arXiv:2509.07706v2 Announce Type: replace Abstract: In this study, we propose FHIR-RAG-MEDS system that aims to integrate Health Level 7 Fast Healthcare Interoperability Resources (HL7 FHIR) with a Retrieval-Augmented Generation (RAG)-based system to improve personalized medical decision support on evidence-based clinical guidelines, emphasizing th",
    "url": "https://arxiv.org/abs/2509.07706",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "\"I think this is fair\": Uncovering the Complexities of Stakeholder Decision-Making in AI Fairness Assessment",
    "summary": "arXiv:2509.17956v2 Announce Type: replace Abstract: Assessing fairness in artificial intelligence (AI) typically involves AI experts who select protected features, fairness metrics, and set fairness thresholds to assess outcome fairness. However, little is known about how stakeholders, particularly those affected by AI outcomes but lacking AI exper",
    "url": "https://arxiv.org/abs/2509.17956",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "G-reasoner: Foundation Models for Unified Reasoning over Graph-structured Knowledge",
    "summary": "arXiv:2509.24276v2 Announce Type: replace Abstract: Large language models (LLMs) excel at complex reasoning but remain limited by static and incomplete parametric knowledge. Retrieval-augmented generation (RAG) mitigates this by incorporating external knowledge, yet existing RAGs struggle with knowledge-intensive tasks due to fragmented information",
    "url": "https://arxiv.org/abs/2509.24276",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "On Discovering Algorithms for Adversarial Imitation Learning",
    "summary": "arXiv:2510.00922v2 Announce Type: replace Abstract: Adversarial Imitation Learning (AIL) methods, while effective in settings with limited expert demonstrations, are often considered unstable. These approaches typically decompose into two components: Density Ratio (DR) estimation $\\frac{\\rho_E}{\\rho_{\\pi}}$, where a discriminator estimates the rela",
    "url": "https://arxiv.org/abs/2510.00922",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "A Mind Cannot Be Smeared Across Time",
    "summary": "arXiv:2601.11620v2 Announce Type: replace Abstract: Whether machines can be conscious depends not only on what they compute, but \\emph{when} they compute it. Most deployed artificial systems realise their functions via sequential or time-multiplexed updates, yet a moment of conscious experience feels unified and simultaneous. I prove that this diff",
    "url": "https://arxiv.org/abs/2601.11620",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural Mathematical Reasoning in LLMs",
    "summary": "arXiv:2602.00564v2 Announce Type: replace Abstract: Recent large language models (LLMs) achieve near-saturation accuracy on many established mathematical reasoning benchmarks, raising concerns about their ability to diagnose genuine reasoning competence. This saturation largely stems from the dominance of template-based computation and shallow arit",
    "url": "https://arxiv.org/abs/2602.00564",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Controlling Exploration-Exploitation in GFlowNets via Markov Chain Perspectives",
    "summary": "arXiv:2602.01749v3 Announce Type: replace Abstract: Generative Flow Network (GFlowNet) objectives implicitly fix an equal mixing of forward and backward policies, potentially constraining the exploration-exploitation trade-off during training. By further exploring the link between GFlowNets and Markov chains, we establish an equivalence between GFl",
    "url": "https://arxiv.org/abs/2602.01749",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "GPT-4o Lacks Core Features of Theory of Mind",
    "summary": "arXiv:2602.12150v3 Announce Type: replace Abstract: Do Large Language Models (LLMs) possess a Theory of Mind (ToM)? Research into this question has focused on evaluating LLMs against benchmarks and found success across a range of social tasks. However, these evaluations do not test for the actual representations posited by ToM: namely, a causal mod",
    "url": "https://arxiv.org/abs/2602.12150",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation",
    "summary": "arXiv:2602.16953v2 Announce Type: replace Abstract: Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial si",
    "url": "https://arxiv.org/abs/2602.16953",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "K-Search: LLM Kernel Generation via Co-Evolving Intrinsic World Model",
    "summary": "arXiv:2602.19128v2 Announce Type: replace Abstract: Optimizing GPU kernels is critical for efficient modern machine learning systems yet remains challenging due to the complex interplay of design factors and rapid hardware evolution. Existing automated approaches typically treat Large Language Models (LLMs) merely as stochastic code generators with",
    "url": "https://arxiv.org/abs/2602.19128",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Latent Introspection: Models Can Detect Prior Concept Injections",
    "summary": "arXiv:2602.20031v2 Announce Type: replace Abstract: We uncover a latent capacity for introspection in a Qwen 32B model, demonstrating that the model can detect when concepts have been injected into its earlier context and identify which concept was injected. While the model denies injection in sampled outputs, logit lens analysis reveals clear dete",
    "url": "https://arxiv.org/abs/2602.20031",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Modality-Guided Mixture of Graph Experts with Entropy-Triggered Routing for Multimodal Recommendation",
    "summary": "arXiv:2602.20723v2 Announce Type: replace Abstract: Multimodal recommendation enhances ranking by integrating user-item interactions with item content, which is particularly effective under sparse feedback and long-tail distributions. However, multimodal signals are inherently heterogeneous and can conflict in specific contexts, making effective fu",
    "url": "https://arxiv.org/abs/2602.20723",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning",
    "summary": "arXiv:2602.21172v2 Announce Type: replace Abstract: Vision-Language-Action (VLA) models are advancing autonomous driving by replacing modular pipelines with unified end-to-end architectures. However, current VLAs face two expensive requirements: (1) massive dataset collection, and (2) dense reasoning annotations. In this work, we address both chall",
    "url": "https://arxiv.org/abs/2602.21172",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices",
    "summary": "arXiv:2602.21858v2 Announce Type: replace Abstract: Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomousl",
    "url": "https://arxiv.org/abs/2602.21858",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Parameter-Efficient Fine-Tuning for Continual Learning: A Neural Tangent Kernel Perspective",
    "summary": "arXiv:2407.17120v3 Announce Type: replace-cross Abstract: Parameter-efficient fine-tuning for continual learning (PEFT-CL) has shown promise in adapting pre-trained models to sequential tasks while mitigating catastrophic forgetting problem. However, understanding the mechanisms that dictate continual performance in this paradigm remains elusive. T",
    "url": "https://arxiv.org/abs/2407.17120",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Abstracted Gaussian Prototypes for True One-Shot Concept Learning",
    "summary": "arXiv:2408.17251v2 Announce Type: replace-cross Abstract: We introduce a cluster-based generative image segmentation framework to encode higher-level representations of visual concepts based on one-shot learning inspired by the Omniglot Challenge. The inferred parameters of each component of a Gaussian Mixture Model (GMM) represent a distinct topol",
    "url": "https://arxiv.org/abs/2408.17251",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "On the Complexity of Neural Computation in Superposition",
    "summary": "arXiv:2409.15318v3 Announce Type: replace-cross Abstract: Superposition, the ability of neural networks to represent more features than neurons, is increasingly seen as key to the efficiency of large models. This paper investigates the theoretical foundations of computing in superposition, establishing complexity bounds for explicit, provably corre",
    "url": "https://arxiv.org/abs/2409.15318",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Toward Automated Validation of Language Model Synthesized Test Cases using Semantic Entropy",
    "summary": "arXiv:2411.08254v3 Announce Type: replace-cross Abstract: Modern Large Language Model (LLM)-based programming agents often rely on test execution feedback to refine their generated code. These tests are synthetically generated by LLMs. However, LLMs may produce invalid or hallucinated test cases, which can mislead feedback loops and degrade the per",
    "url": "https://arxiv.org/abs/2411.08254",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "From Open Vocabulary to Open World: Teaching Vision Language Models to Detect Novel Objects",
    "summary": "arXiv:2411.18207v4 Announce Type: replace-cross Abstract: Traditional object detection methods operate under the closed-set assumption, where models can only detect a fixed number of objects predefined in the training set. Recent works on open vocabulary object detection (OVD) enable the detection of objects defined by an in-principle unbounded voc",
    "url": "https://arxiv.org/abs/2411.18207",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "MomentMix Augmentation with Length-Aware DETR for Temporally Robust Moment Retrieval",
    "summary": "arXiv:2412.20816v3 Announce Type: replace-cross Abstract: Video Moment Retrieval (MR) aims to localize moments within a video based on a given natural language query. Given the prevalent use of platforms like YouTube for information retrieval, the demand for MR techniques is significantly growing. Recent DETR-based models have made notable advances",
    "url": "https://arxiv.org/abs/2412.20816",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Dual-IPO: Dual-Iterative Preference Optimization for Text-to-Video Generation",
    "summary": "arXiv:2502.02088v5 Announce Type: replace-cross Abstract: Recent advances in video generation have enabled thrilling experiences in producing realistic videos driven by scalable diffusion transformers. However, they usually fail to produce satisfactory outputs that are aligned to users' authentic demands and preferences. In this work, we introduce ",
    "url": "https://arxiv.org/abs/2502.02088",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Unbiased Sliced Wasserstein Kernels for High-Quality Audio Captioning",
    "summary": "arXiv:2502.05435v2 Announce Type: replace-cross Abstract: Audio captioning systems face a fundamental challenge: teacher-forcing training creates exposure bias that leads to caption degeneration during inference. While contrastive methods have been proposed as solutions, they typically fail to capture the crucial temporal relationships between acou",
    "url": "https://arxiv.org/abs/2502.05435",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Towards a Sharp Analysis of Offline Policy Learning for $f$-Divergence-Regularized Contextual Bandits",
    "summary": "arXiv:2502.06051v3 Announce Type: replace-cross Abstract: Many offline reinforcement learning algorithms are underpinned by $f$-divergence regularization, but their sample complexity *defined with respect to regularized objectives* still lacks tight analyses, especially in terms of concrete data coverage conditions. In this paper, we study the exac",
    "url": "https://arxiv.org/abs/2502.06051",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Using the Path of Least Resistance to Explain Deep Networks",
    "summary": "arXiv:2502.12108v3 Announce Type: replace-cross Abstract: Integrated Gradients (IG), a widely used axiomatic path-based attribution method, assigns importance scores to input features by integrating model gradients along a straight path from a baseline to the input. While effective in some cases, we show that straight paths can lead to flawed attri",
    "url": "https://arxiv.org/abs/2502.12108",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "ViT-Linearizer: Distilling Quadratic Knowledge into Linear-Time Vision Models",
    "summary": "arXiv:2504.00037v2 Announce Type: replace-cross Abstract: Vision Transformers (ViTs) have delivered remarkable progress through global self-attention, yet their quadratic complexity can become prohibitive for high-resolution inputs. In this work, we present ViT-Linearizer, a cross-architecture distillation framework that transfers rich ViT represen",
    "url": "https://arxiv.org/abs/2504.00037",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Evaluating the Diversity and Quality of LLM Generated Content",
    "summary": "arXiv:2504.12522v2 Announce Type: replace-cross Abstract: Recent work suggests that preference-tuning techniques -- such as Reinforcement Learning from Human Feedback (RLHF) methods like PPO and GRPO, as well as alternatives like DPO -- reduce diversity, creating a dilemma given that these models are widely deployed in applications requiring varied",
    "url": "https://arxiv.org/abs/2504.12522",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "RaPA: Enhancing Transferable Targeted Attacks via Random Parameter Pruning",
    "summary": "arXiv:2504.18594v2 Announce Type: replace-cross Abstract: Compared to untargeted attacks, targeted transfer-based attack is still suffering from much lower Attack Success Rates (ASRs), although significant improvements have been achieved by kinds of methods, such as diversifying input, stabilizing the gradient, and re-training surrogate models. In ",
    "url": "https://arxiv.org/abs/2504.18594",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Beyond the Monitor: Mixed Reality Visualization and Multimodal AI for Enhanced Digital Pathology Workflow",
    "summary": "arXiv:2505.02780v2 Announce Type: replace-cross Abstract: Pathologists diagnose cancer using gigapixel whole-slide images (WSIs), but the current digital workflow is fragmented. These multiscale datasets often exceed 100,000 x 100,000 pixels, yet standard 2D monitors restrict the field of view. This disparity forces constant panning and zooming, wh",
    "url": "https://arxiv.org/abs/2505.02780",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Large Language Model Compression with Global Rank and Sparsity Optimization",
    "summary": "arXiv:2505.03801v3 Announce Type: replace-cross Abstract: Low-rank and sparse composite approximation is a natural idea to compress Large Language Models (LLMs). However, such an idea faces two primary challenges that adversely affect the performance of existing methods. The first challenge relates to the interaction and cooperation between low-ran",
    "url": "https://arxiv.org/abs/2505.03801",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Sparse Imagination for Efficient Visual World Model Planning",
    "summary": "arXiv:2506.01392v2 Announce Type: replace-cross Abstract: World model based planning has significantly improved decision-making in complex environments by enabling agents to simulate future states and make informed choices. This computational burden is particularly restrictive in robotics, where resources are severely constrained. To address this l",
    "url": "https://arxiv.org/abs/2506.01392",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "A Lightweight IDS for Early APT Detection Using a Novel Feature Selection Method",
    "summary": "arXiv:2506.12108v2 Announce Type: replace-cross Abstract: An Advanced Persistent Threat (APT) is a multistage, highly sophisticated, and covert form of cyber threat that gains unauthorized access to networks to either steal valuable data or disrupt the targeted network. These threats often remain undetected for extended periods, emphasizing the cri",
    "url": "https://arxiv.org/abs/2506.12108",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Echoes of AI: Investigating the Downstream Effects of AI Assistants on Software Maintainability",
    "summary": "arXiv:2507.00788v3 Announce Type: replace-cross Abstract: [Context] AI assistants, like GitHub Copilot and Cursor, are transforming software engineering. While several studies highlight productivity improvements, their impact on maintainability requires further investigation. [Objective] This study investigates whether co-development with AI assist",
    "url": "https://arxiv.org/abs/2507.00788",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility",
    "summary": "arXiv:2507.12553v2 Announce Type: replace-cross Abstract: Language models (LMs) are used for a diverse range of tasks, from question answering to writing fantastical stories. In order to reliably accomplish these tasks, LMs must be able to discern the modal category of a sentence (i.e., whether it describes something that is possible, impossible, c",
    "url": "https://arxiv.org/abs/2507.12553",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "A Semi-Supervised Learning Method for the Identification of Bad Exposures in Large Imaging Surveys",
    "summary": "arXiv:2507.12784v2 Announce Type: replace-cross Abstract: As the data volume of astronomical imaging surveys rapidly increases, traditional methods for image anomaly detection, such as visual inspection by human experts, are becoming impractical. We introduce a machine-learning-based approach to detect poor-quality exposures in large imaging survey",
    "url": "https://arxiv.org/abs/2507.12784",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Decoding Translation-Related Functional Sequences in 5'UTRs Using Interpretable Deep Learning Models",
    "summary": "arXiv:2507.16801v2 Announce Type: replace-cross Abstract: Understanding how 5' untranslated regions (5'UTRs) regulate mRNA translation is critical for controlling protein expression and designing effective therapeutic mRNAs. While recent deep learning models have shown promise in predicting translational efficiency from 5'UTR sequences, most are co",
    "url": "https://arxiv.org/abs/2507.16801",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation",
    "summary": "arXiv:2507.17937v4 Announce Type: replace-cross Abstract: Generative AI systems for music and video commonly use text-based filters to prevent regurgitation of copyrighted material. We expose a significant vulnerability in this approach by introducing Adversarial PhoneTic Prompting (APT), a novel attack that bypasses these safeguards by exploiting ",
    "url": "https://arxiv.org/abs/2507.17937",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "LayerT2V: A Unified Multi-Layer Video Generation Framework",
    "summary": "arXiv:2508.04228v2 Announce Type: replace-cross Abstract: Text-to-video generation has advanced rapidly, but existing methods typically output only the final composited video and lack editable layered representations, limiting their use in professional workflows. We propose \\textbf{LayerT2V}, a unified multi-layer video generation framework that pr",
    "url": "https://arxiv.org/abs/2508.04228",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Dyslexify: A Mechanistic Defense Against Typographic Attacks in CLIP",
    "summary": "arXiv:2508.20570v2 Announce Type: replace-cross Abstract: Typographic attacks exploit multi-modal systems by injecting text into images, leading to targeted misclassifications, malicious content generation and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP vision encoders behave under typographic attacks, locating speciali",
    "url": "https://arxiv.org/abs/2508.20570",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "BioBlue: Systematic runaway-optimiser-like LLM failure modes on biologically and economically aligned AI safety benchmarks for LLMs with simplified observation format",
    "summary": "arXiv:2509.02655v2 Announce Type: replace-cross Abstract: Many AI alignment discussions of \"runaway optimisation\" focus on RL agents: unbounded utility maximisers that over-optimise a proxy objective (e.g., \"paperclip maximiser\", specification gaming) at the expense of everything else. LLM-based systems are often assumed to be safer because they fu",
    "url": "https://arxiv.org/abs/2509.02655",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "PolicyPad: Collaborative Prototyping of LLM Policies",
    "summary": "arXiv:2509.19680v3 Announce Type: replace-cross Abstract: As LLMs gain adoption in high-stakes domains like mental health, domain experts are increasingly consulted to provide input into policies governing their behavior. From an observation of 19 policymaking workshops with 9 experts over 15 weeks, we identified opportunities to better support rap",
    "url": "https://arxiv.org/abs/2509.19680",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Predicting LLM Reasoning Performance with Small Proxy Model",
    "summary": "arXiv:2509.21013v3 Announce Type: replace-cross Abstract: Given the prohibitive cost of pre-training large language models, it is essential to leverage smaller proxy models to optimize datasets before scaling up. However, this approach becomes challenging for reasoning capabilities, which exhibit emergent behavior that only appear reliably at large",
    "url": "https://arxiv.org/abs/2509.21013",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Compute-Optimal Quantization-Aware Training",
    "summary": "arXiv:2509.22935v2 Announce Type: replace-cross Abstract: Quantization-aware training (QAT) is a leading technique for improving the accuracy of quantized neural networks. Previous work has shown that decomposing training into a full-precision (FP) phase followed by a QAT phase yields superior accuracy compared to QAT alone. However, the optimal al",
    "url": "https://arxiv.org/abs/2509.22935",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Generative Value Conflicts Reveal LLM Priorities",
    "summary": "arXiv:2509.25369v2 Announce Type: replace-cross Abstract: Past work seeks to align large language model (LLM)-based assistants with a target set of values, but such assistants are frequently forced to make tradeoffs between values when deployed. In response to the scarcity of value conflict in existing alignment datasets, we introduce ConflictScope",
    "url": "https://arxiv.org/abs/2509.25369",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Atlas-free Brain Network Transformer",
    "summary": "arXiv:2510.03306v2 Announce Type: replace-cross Abstract: Current atlas-based approaches to brain network analysis rely heavily on standardized anatomical or connectivity-driven brain atlases. However, these fixed atlases often introduce significant limitations, such as spatial misalignment across individuals, functional heterogeneity within predef",
    "url": "https://arxiv.org/abs/2510.03306",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "AgentHub: A Registry for Discoverable, Verifiable, and Reproducible AI Agents",
    "summary": "arXiv:2510.03495v2 Announce Type: replace-cross Abstract: LLM-based agents are rapidly proliferating, yet the infrastructure for discovering, evaluating, and governing them remains fragmented compared to mature ecosystems like software package registries (e.g., npm) and model hubs (e.g., Hugging Face). Existing efforts typically address naming, dis",
    "url": "https://arxiv.org/abs/2510.03495",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Improving Discrete Diffusion Unmasking Policies Beyond Explicit Reference Policies",
    "summary": "arXiv:2510.05725v2 Announce Type: replace-cross Abstract: Masked diffusion models (MDMs) have recently emerged as a novel framework for language modeling. MDMs generate sentences by iteratively denoising masked sequences, filling in [MASK] tokens step by step. Although MDMs support any-order sampling, performance is highly sensitive to the choice o",
    "url": "https://arxiv.org/abs/2510.05725",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "DropVLA: An Action-Level Backdoor Attack on Vision--Language--Action Models",
    "summary": "arXiv:2510.10932v2 Announce Type: replace-cross Abstract: Vision-Language-Action (VLA) models map multimodal perception and language instructions to executable robot actions, making them particularly vulnerable to behavioral backdoor manipulation: a hidden trigger introduced during training can induce unintended physical actions while nominal task ",
    "url": "https://arxiv.org/abs/2510.10932",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Learning to Answer from Correct Demonstrations",
    "summary": "arXiv:2510.15464v2 Announce Type: replace-cross Abstract: We study the problem of learning to generate an answer (or completion) to a question (or prompt), where there could be multiple correct answers, any one of which is acceptable at test time. Learning is based on demonstrations of some correct answer to each training question, as in Supervised",
    "url": "https://arxiv.org/abs/2510.15464",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions",
    "summary": "arXiv:2510.19060v3 Announce Type: replace-cross Abstract: While vision-language models (VLMs) have advanced into detailed image description, evaluation remains a challenge. Standard metrics (e.g. CIDEr, SPICE) were designed for short texts and tuned to recognize errors that are now uncommon, such as object misidentification. In contrast, long texts",
    "url": "https://arxiv.org/abs/2510.19060",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "RELOOP: Recursive Retrieval with Multi-Hop Reasoner and Planners for Heterogeneous QA",
    "summary": "arXiv:2510.20505v3 Announce Type: replace-cross Abstract: Retrieval-augmented generation (RAG) remains brittle on multi-step questions and heterogeneous evidence sources, trading accuracy against latency and token/tool budgets. This paper introduces RELOOP, a structure aware framework using Hierarchical Sequence (HSEQ) that (i) linearize documents,",
    "url": "https://arxiv.org/abs/2510.20505",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution",
    "summary": "arXiv:2510.25726v2 Announce Type: replace-cross Abstract: Real-world language agents must handle complex, multi-step workflows across diverse Apps. For instance, an agent may manage emails by coordinating with calendars and file systems, or monitor a production database to detect anomalies and generate reports following an operating manual. However",
    "url": "https://arxiv.org/abs/2510.25726",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning",
    "summary": "arXiv:2510.25992v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) ",
    "url": "https://arxiv.org/abs/2510.25992",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Temporal Sparse Autoencoders: Leveraging the Sequential Nature of Language for Interpretability",
    "summary": "arXiv:2511.05541v2 Announce Type: replace-cross Abstract: Translating the internal representations and computations of models into concepts that humans can understand is a key goal of interpretability. While recent dictionary learning methods such as Sparse Autoencoders (SAEs) provide a promising route to discover human-interpretable features, they",
    "url": "https://arxiv.org/abs/2511.05541",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Q$^2$: Quantization-Aware Gradient Balancing and Attention Alignment for Low-Bit Quantization",
    "summary": "arXiv:2511.05898v2 Announce Type: replace-cross Abstract: Quantization-aware training (QAT) has achieved remarkable success in low-bit ($\\leq$4-bit) quantization for classification networks. However, when applied to more complex visual tasks such as object detection and image segmentation, performance still suffers significant degradation. A key ca",
    "url": "https://arxiv.org/abs/2511.05898",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Intelligence per Watt: Measuring Intelligence Efficiency of Local AI",
    "summary": "arXiv:2511.07885v3 Announce Type: replace-cross Abstract: Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B",
    "url": "https://arxiv.org/abs/2511.07885",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Diffusion Model in Latent Space for Medical Image Segmentation Task",
    "summary": "arXiv:2512.01292v3 Announce Type: replace-cross Abstract: Medical image segmentation is crucial for clinical diagnosis and treatment planning. Traditional methods typically produce a single segmentation mask, failing to capture inherent uncertainty. Recent generative models enable the creation of multiple plausible masks per image, mimicking the co",
    "url": "https://arxiv.org/abs/2512.01292",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
    "summary": "arXiv:2512.03383v3 Announce Type: replace-cross Abstract: Deploying large language models (LLMs) on mobile platforms faces significant challenges due to the limited memory and shared computational resources of the device. Resource availability may be an issue as it is directly impacted by the current device workload, adding to the uncertainty of mo",
    "url": "https://arxiv.org/abs/2512.03383",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Sparse Attention Post-Training for Mechanistic Interpretability",
    "summary": "arXiv:2512.05865v3 Announce Type: replace-cross Abstract: We introduce a simple post-training method that makes transformer attention sparse without sacrificing performance. Applying a flexible sparsity regularisation under a constrained-loss objective, we show on models up to 7B parameters that it is possible to retain the original pretraining los",
    "url": "https://arxiv.org/abs/2512.05865",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Towards Small Language Models for Security Query Generation in SOC Workflows",
    "summary": "arXiv:2512.06660v2 Announce Type: replace-cross Abstract: Analysts in Security Operations Centers routinely query massive telemetry streams using Kusto Query Language (KQL). Writing correct KQL requires specialized expertise, and this dependency creates a bottleneck as security teams scale. This paper investigates whether Small Language Models (SLM",
    "url": "https://arxiv.org/abs/2512.06660",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent",
    "summary": "arXiv:2512.14990v3 Announce Type: replace-cross Abstract: Despite their wide adoption in various domains (e.g., healthcare, finance, software engineering), Deep Learning (DL)-based applications suffer from many bugs, failures, and vulnerabilities. Reproducing these bugs is essential for their resolution, but it is extremely challenging due to the i",
    "url": "https://arxiv.org/abs/2512.14990",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL",
    "summary": "arXiv:2512.17053v2 Announce Type: replace-cross Abstract: Deploying accurate Text-to-SQL systems at the enterprise level faces a difficult trilemma involving cost, security and performance. Current solutions force enterprises to choose between expensive, proprietary Large Language Models (LLMs) and low-performing Small Language Models (SLMs). Effor",
    "url": "https://arxiv.org/abs/2512.17053",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "LeanCat: A Benchmark Suite for Formal Category Theory in Lean (Part I: 1-Categories)",
    "summary": "arXiv:2512.24796v2 Announce Type: replace-cross Abstract: While large language models (LLMs) have demonstrated impressive capabilities in formal theorem proving, current benchmarks fail to adequately measure library-grounded abstraction -- the ability to reason with high-level interfaces and reusable structures central to modern mathematics and sof",
    "url": "https://arxiv.org/abs/2512.24796",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
    "summary": "arXiv:2601.10611v3 Announce Type: replace-cross Abstract: Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the founda",
    "url": "https://arxiv.org/abs/2601.10611",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "A Confidence-Variance Theory for Pseudo-Label Selection in Semi-Supervised Learning",
    "summary": "arXiv:2601.11670v2 Announce Type: replace-cross Abstract: Most pseudo-label selection strategies in semi-supervised learning rely on fixed confidence thresholds, implicitly assuming that prediction confidence reliably indicates correctness. In practice, deep networks are often overconfident: high-confidence predictions can still be wrong, while inf",
    "url": "https://arxiv.org/abs/2601.11670",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Rethinking Cross-Modal Fine-Tuning: Optimizing the Interaction between Feature Alignment and Target Fitting",
    "summary": "arXiv:2601.18231v3 Announce Type: replace-cross Abstract: Adapting pre-trained models to unseen feature modalities has become increasingly important due to the growing need for cross-disciplinary knowledge integration. A key challenge here is how to align the representation of new modalities with the most relevant parts of the pre-trained model's r",
    "url": "https://arxiv.org/abs/2601.18231",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "A Minimum Variance Path Principle for Accurate and Stable Score-Based Density Ratio Estimation",
    "summary": "arXiv:2602.00834v3 Announce Type: replace-cross Abstract: Score-based methods are powerful across machine learning, but they face a paradox: theoretically path-independent, yet practically path-dependent. We resolve this by proving that practical training objectives differ from the ideal, ground-truth objective by a crucial, overlooked term: the pa",
    "url": "https://arxiv.org/abs/2602.00834",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Spark: Modular Spiking Neural Networks",
    "summary": "arXiv:2602.02306v3 Announce Type: replace-cross Abstract: Nowadays, neural networks act as a synonym for artificial intelligence. Present neural network models, although remarkably powerful, are inefficient both in terms of data and energy. Several alternative forms of neural networks have been proposed to address some of these problems. Specifical",
    "url": "https://arxiv.org/abs/2602.02306",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations",
    "summary": "arXiv:2602.02334v2 Announce Type: replace-cross Abstract: Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is ",
    "url": "https://arxiv.org/abs/2602.02334",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Versor: A Geometric Sequence Architecture",
    "summary": "arXiv:2602.10195v2 Announce Type: replace-cross Abstract: A novel sequence architecture is introduced, Versor, which uses Conformal Geometric Algebra (CGA) in place of traditional linear operations to achieve structural generalization and significant performance improvements on a variety of tasks, while offering improved interpretability and effici",
    "url": "https://arxiv.org/abs/2602.10195",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "ULTRA:Urdu Language Transformer-based Recommendation Architecture",
    "summary": "arXiv:2602.11836v2 Announce Type: replace-cross Abstract: Urdu, as a low-resource language, lacks effective semantic content recommendation systems, particularly in the domain of personalized news retrieval. Existing approaches largely rely on lexical matching or language-agnostic techniques, which struggle to capture semantic intent and perform po",
    "url": "https://arxiv.org/abs/2602.11836",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation",
    "summary": "arXiv:2602.12125v2 Announce Type: replace-cross Abstract: On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In",
    "url": "https://arxiv.org/abs/2602.12125",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "PCReg-Net: Progressive Contrast-Guided Registration for Cross-Domain Image Alignment",
    "summary": "arXiv:2602.13304v2 Announce Type: replace-cross Abstract: Deformable image registration across heterogeneous domains remains challenging because coupled appearance variation and geometric misalignment violate the brightness constancy assumption underlying conventional methods. We propose PCReg-Net, a progressive contrast-guided registration framewo",
    "url": "https://arxiv.org/abs/2602.13304",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Large-scale online deanonymization with LLMs",
    "summary": "arXiv:2602.16800v2 Announce Type: replace-cross Abstract: We show that large language models can be used to perform at-scale deanonymization. With full Internet access, our agent can re-identify Hacker News users and Anthropic Interviewer participants at high precision, given pseudonymous online profiles and conversations alone, matching what would",
    "url": "https://arxiv.org/abs/2602.16800",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "A Reversible Semantics for Janus",
    "summary": "arXiv:2602.16913v2 Announce Type: replace-cross Abstract: Janus is a paradigmatic example of a reversible programming language. Indeed, Janus programs can be executed backwards as well as forwards. However, its current small-step semantics (useful, e.g., for debugging or as a basis for extensions with concurrency primitives) is not reversible, sinc",
    "url": "https://arxiv.org/abs/2602.16913",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "FUSAR-GPT : A Spatiotemporal Feature-Embedded and Two-Stage Decoupled Visual Language Model for SAR Imagery",
    "summary": "arXiv:2602.19190v2 Announce Type: replace-cross Abstract: Research on the intelligent interpretation of all-weather, all-time Synthetic Aperture Radar (SAR) is crucial for advancing remote sensing applications. In recent years, although Visual Language Models (VLMs) have demonstrated strong open-world understanding capabilities on RGB images, their",
    "url": "https://arxiv.org/abs/2602.19190",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Scaling Laws for Precision in High-Dimensional Linear Regression",
    "summary": "arXiv:2602.19241v2 Announce Type: replace-cross Abstract: Low-precision training is critical for optimizing the trade-off between model quality and training costs, necessitating the joint allocation of model size, dataset size, and numerical precision. While empirical scaling laws suggest that quantization impacts effective model and data capacitie",
    "url": "https://arxiv.org/abs/2602.19241",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Soft Sequence Policy Optimization",
    "summary": "arXiv:2602.19327v2 Announce Type: replace-cross Abstract: A significant portion of recent research on Large Language Model (LLM) alignment focuses on developing new policy optimization methods based on Group Relative Policy Optimization (GRPO). Two prominent directions have emerged: (i) a shift toward sequence-level importance sampling weights that",
    "url": "https://arxiv.org/abs/2602.19327",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "PuppetChat: Fostering Intimate Communication through Bidirectional Actions and Micronarratives",
    "summary": "arXiv:2602.19463v2 Announce Type: replace-cross Abstract: As a primary channel for sustaining modern intimate relationships, instant messaging facilitates frequent connection across distances. However, today's tools often dilute care; they favor single tap reactions and vague emojis that do not support two way action responses, do not preserve the ",
    "url": "https://arxiv.org/abs/2602.19463",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "DICArt: Advancing Category-level Articulated Object Pose Estimation in Discrete State-Spaces",
    "summary": "arXiv:2602.19565v2 Announce Type: replace-cross Abstract: Articulated object pose estimation is a core task in embodied AI. Existing methods typically regress poses in a continuous space, but often struggle with 1) navigating a large, complex search space and 2) failing to incorporate intrinsic kinematic constraints. In this work, we introduce DICA",
    "url": "https://arxiv.org/abs/2602.19565",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Decision MetaMamba: Enhancing Selective SSM in Offline RL with Heterogeneous Sequence Mixing",
    "summary": "arXiv:2602.19805v2 Announce Type: replace-cross Abstract: Mamba-based models have drawn much attention in offline RL. However, their selective mechanism often detrimental when key steps in RL sequences are omitted. To address these issues, we propose a simple yet effective structure, called Decision MetaMamba (DMM), which replaces Mamba's token mix",
    "url": "https://arxiv.org/abs/2602.19805",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "On the Equivalence of Random Network Distillation, Deep Ensembles, and Bayesian Inference",
    "summary": "arXiv:2602.19964v2 Announce Type: replace-cross Abstract: Uncertainty quantification is central to safe and efficient deployments of deep learning models, yet many computationally practical methods lack lacking rigorous theoretical motivation. Random network distillation (RND) is a lightweight technique that measures novelty via prediction errors a",
    "url": "https://arxiv.org/abs/2602.19964",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "StruXLIP: Enhancing Vision-language Models with Multimodal Structural Cues",
    "summary": "arXiv:2602.20089v2 Announce Type: replace-cross Abstract: Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today. We extend this principle to vision-language alignment, showing that isolating and aligning structural cues across modalities can greatly benefit fine",
    "url": "https://arxiv.org/abs/2602.20089",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "TokEye: Fast Signal Extraction for Fluctuating Time Series via Offline Self-Supervised Learning From Fusion Diagnostics to Bioacoustics",
    "summary": "arXiv:2602.20317v2 Announce Type: replace-cross Abstract: Next-generation fusion facilities like ITER face a \"data deluge,\" generating petabytes of multi-diagnostic signals daily that challenge manual analysis. We present a \"signals-first\" self-supervised framework for the automated extraction of coherent and transient modes from high-noise time-fr",
    "url": "https://arxiv.org/abs/2602.20317",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training",
    "summary": "arXiv:2602.21189v2 Announce Type: replace-cross Abstract: Pass@k is a widely used performance metric for verifiable large language model tasks, including mathematical reasoning, code generation, and short-answer reasoning. It defines success if any of $k$ independently sampled solutions passes a verifier. This multi-sample inference metric has moti",
    "url": "https://arxiv.org/abs/2602.21189",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "AngelSlim: A more accessible, comprehensive, and efficient toolkit for large model compression",
    "summary": "arXiv:2602.21233v2 Announce Type: replace-cross Abstract: This technical report introduces AngelSlim, a comprehensive and versatile toolkit for large model compression developed by the Tencent Hunyuan team. By consolidating cutting-edge algorithms, including quantization, speculative decoding, token pruning, and distillation. AngelSlim provides a u",
    "url": "https://arxiv.org/abs/2602.21233",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Hierarchical LLM-Based Multi-Agent Framework with Prompt Optimization for Multi-Robot Task Planning",
    "summary": "arXiv:2602.21670v2 Announce Type: replace-cross Abstract: Multi-robot task planning requires decomposing natural-language instructions into executable actions for heterogeneous robot teams. Conventional Planning Domain Definition Language (PDDL) planners provide rigorous guarantees but struggle to handle ambiguous or long-horizon missions, while la",
    "url": "https://arxiv.org/abs/2602.21670",
    "source": "Arxiv AI",
    "published_at": "2026-02-27T05:00:00+00:00"
  },
  {
    "title": "Trump orders all federal agencies to drop Anthropic after the AI company refused to bend its terms for the Pentagon",
    "summary": "The Pentagon threatens Anthropic with a law from the Korean War to force the AI company to cooperate. Anthropic refuses and stands alone among the major AI companies. The article Trump orders all federal agencies to drop Anthropic after the AI company refused to bend its terms for the Pentagon appeared first on The Decoder.",
    "url": "https://the-decoder.com/trump-orders-all-federal-agencies-to-drop-anthropic-after-the-ai-company-refused-to-bend-its-terms-for-the-pentagon/",
    "source": "The Decoder",
    "published_at": "2026-02-27T21:42:06+00:00"
  },
  {
    "title": "Google Deepmind and OpenAI employees demand Anthropic-style red lines on Pentagon surveillance and autonomous weapons",
    "summary": "Anthropic's dispute with the Pentagon is now rippling through Google and OpenAI. Hundreds of employees across both companies are demanding the same safeguards Anthropic is fighting for, while Sam Altman is working on his own Pentagon deal. The article Google Deepmind and OpenAI employees demand Anthropic-style red lines on Pentagon surveillance and",
    "url": "https://the-decoder.com/google-deepmind-and-openai-employees-demand-anthropic-style-red-lines-on-pentagon-surveillance-and-autonomous-weapons/",
    "source": "The Decoder",
    "published_at": "2026-02-27T19:50:20+00:00"
  },
  {
    "title": "OpenAI's up to $110 billion raise lines up almost exactly with the $111 billion it just added to its cash burn forecast",
    "summary": "OpenAI closes the largest private financing round in history. Amazon invests up to 50 billion and becomes a strategic partner. Meanwhile, Microsoft emphasizes that its own partnership will not change. The article OpenAI&#039;s up to $110 billion raise lines up almost exactly with the $111 billion it just added to its cash burn forecast appeared fir",
    "url": "https://the-decoder.com/openais-up-to-110-billion-raise-lines-up-almost-exactly-with-the-111-billion-it-just-added-to-its-cash-burn-forecast/",
    "source": "The Decoder",
    "published_at": "2026-02-27T19:15:02+00:00"
  },
  {
    "title": "Meta signs multi-billion dollar deal to rent Google's TPUs in a direct challenge to Nvidia's AI chip dominance",
    "summary": "Meta is renting Google's AI chips to train its models, a deal worth billions that puts Nvidia's dominance on notice. The article Meta signs multi-billion dollar deal to rent Google&#039;s TPUs in a direct challenge to Nvidia&#039;s AI chip dominance appeared first on The Decoder.",
    "url": "https://the-decoder.com/meta-signs-multi-billion-dollar-deal-to-rent-googles-tpus-in-a-direct-challenge-to-nvidias-ai-chip-dominance/",
    "source": "The Decoder",
    "published_at": "2026-02-27T16:29:52+00:00"
  },
  {
    "title": "Figma and OpenAI connect design and code through new Codex integration",
    "summary": "A new integration links Figma's design platform directly with OpenAI's Codex. The article Figma and OpenAI connect design and code through new Codex integration appeared first on The Decoder.",
    "url": "https://the-decoder.com/figma-and-openai-connect-design-and-code-through-new-codex-integration/",
    "source": "The Decoder",
    "published_at": "2026-02-27T15:22:50+00:00"
  },
  {
    "title": "Claude Code now remembers your fixes, your preferences, and your project quirks on its own",
    "summary": "Claude Code now remembers what it learns across sessions - automatically tracking debugging patterns, project context, and preferred working methods without manual input. The article Claude Code now remembers your fixes, your preferences, and your project quirks on its own appeared first on The Decoder.",
    "url": "https://the-decoder.com/claude-code-now-remembers-your-fixes-your-preferences-and-your-project-quirks-on-its-own/",
    "source": "The Decoder",
    "published_at": "2026-02-27T13:56:06+00:00"
  },
  {
    "title": "Block cuts nearly half its workforce as Dorsey credits AI, but the real reasons predate the hype",
    "summary": "Jack Dorsey blames AI for cutting nearly half of Block's workforce. But a closer look at the company's history of overhiring and structural problems tells a different story. The article Block cuts nearly half its workforce as Dorsey credits AI, but the real reasons predate the hype appeared first on The Decoder.",
    "url": "https://the-decoder.com/block-cuts-nearly-half-its-workforce-as-dorsey-credits-ai-but-the-real-reasons-predate-the-hype/",
    "source": "The Decoder",
    "published_at": "2026-02-27T11:36:35+00:00"
  },
  {
    "title": "Upgrading agentic AI for finance workflows",
    "summary": "Improving trust in agentic AI for finance workflows remains a major priority for technology leaders today. Over the past two years, enterprises have rushed to put automated agents into real workflows, spanning customer support and back-office operations. These tools excel at retrieving information, yet they often struggle to provide consistent and ",
    "url": "https://www.artificialintelligence-news.com/news/upgrading-agentic-ai-for-finance-workflows/",
    "source": "AI News",
    "published_at": "2026-02-27T13:15:38+00:00"
  },
  {
    "title": "Poor implementation of AI may be behind workforce reduction",
    "summary": "Many organisations are eroding the foundations of business &#8211; productivity, competitiveness, and efficiency. This is happening due to poor implementation of human-AI collaboration, according to cloud data and AI consultancy, Datatonic. The company says in the next phase of enterprise AI, success will come from carefully-governed and designed A",
    "url": "https://www.artificialintelligence-news.com/news/ai-workflows-need-human-in-the-loop-say-datatonic/",
    "source": "AI News",
    "published_at": "2026-02-27T12:05:00+00:00"
  },
  {
    "title": "Goldman Sachs and Deutsche Bank test agentic AI for trade surveillance",
    "summary": "Banks are testing a new type of artificial intelligence, like agentic AI, that does more than scan for keywords or follow preset rules. Instead of relying only on static alerts, some trading desks are beginning to use systems designed to reason through patterns in real time and flag conduct that may need human review. Bloomberg [&#8230;] The post G",
    "url": "https://www.artificialintelligence-news.com/news/goldman-sachs-and-deutsche-bank-test-agentic-ai-for-trade-surveillance/",
    "source": "AI News",
    "published_at": "2026-02-27T10:00:00+00:00"
  },
  {
    "title": "ASML’s high-NA EUV tools clear the runway for next-gen AI chips",
    "summary": "The machine that will make tomorrow&#8217;s AI chips possible has just been declared ready for mass production – and the clock for the industry&#8217;s next leap has officially started. ASML, the Dutch company that holds a global monopoly on commercial extreme ultraviolet lithography equipment, confirmed this week that its High-NA EUV tools have cr",
    "url": "https://www.artificialintelligence-news.com/news/asml-high-na-euv-production-ready-ai-chips/",
    "source": "AI News",
    "published_at": "2026-02-27T06:00:00+00:00"
  }
]