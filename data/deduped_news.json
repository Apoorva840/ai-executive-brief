[
  {
    "title": "Is a secure AI assistant possible?",
    "summary": "AI agents are a risky business. Even when stuck inside the chatbox window, LLMs will make mistakes and behave badly. Once they have tools that they can use to interact with the outside world, such as web browsers and email addresses, the consequences of those mistakes become far more serious. That might explain why the&#8230;",
    "url": "https://www.technologyreview.com/2026/02/11/1132768/is-a-secure-ai-assistant-possible/",
    "source": "MIT Technology Review AI",
    "published_at": "2026-02-11T20:08:35+00:00"
  },
  {
    "title": "I Loved My OpenClaw AI Agent—Until It Turned on Me",
    "summary": "I used the viral AI helper to order groceries, sort emails, and negotiate deals. Then it decided to scam me.",
    "url": "https://www.wired.com/story/malevolent-ai-agent-openclaw-clawdbot/",
    "source": "Wired AI",
    "published_at": "2026-02-11T19:00:00+00:00"
  },
  {
    "title": "CBP Signs Clearview AI Deal to Use Face Recognition for ‘Tactical Targeting’",
    "summary": "US Border Patrol intelligence units will gain access to a face recognition tool built on billions of images scraped from the internet.",
    "url": "https://www.wired.com/story/cbp-signs-clearview-ai-deal-to-use-face-recognition-for-tactical-targeting/",
    "source": "Wired AI",
    "published_at": "2026-02-11T16:32:27+00:00"
  },
  {
    "title": "AI Industry Rivals Are Teaming Up on a Startup Accelerator",
    "summary": "OpenAI, Anthropic, Google, and a host of other major tech companies have found common ground in F/ai, a new startup accelerator based out of Paris.",
    "url": "https://www.wired.com/story/ai-industry-rivals-are-teaming-up-on-a-startup-accelerator/",
    "source": "Wired AI",
    "published_at": "2026-02-11T10:55:24+00:00"
  },
  {
    "title": "LLM-FSM: Scaling Large Language Models for Finite-State Reasoning in RTL Code Generation",
    "summary": "arXiv:2602.07032v1 Announce Type: new Abstract: Finite-state reasoning, the ability to understand and implement state-dependent behavior, is central to hardware design. In this paper, we present LLM-FSM, a benchmark that evaluates how well large language models (LLMs) can recover finite-state machine (FSM) behavior from natural-language specificati",
    "url": "https://arxiv.org/abs/2602.07032",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "ST-Raptor: An Agentic System for Semi-Structured Table QA",
    "summary": "arXiv:2602.07034v1 Announce Type: new Abstract: Semi-structured table question answering (QA) is a challenging task that requires (1) precise extraction of cell contents and positions and (2) accurate recovery of key implicit logical structures, hierarchical relationships, and semantic associations encoded in table layouts. In practice, such tables",
    "url": "https://arxiv.org/abs/2602.07034",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents",
    "summary": "arXiv:2602.07035v1 Announce Type: new Abstract: Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamen",
    "url": "https://arxiv.org/abs/2602.07035",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods",
    "summary": "arXiv:2602.07040v1 Announce Type: new Abstract: We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-a",
    "url": "https://arxiv.org/abs/2602.07040",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?",
    "summary": "arXiv:2602.07055v1 Announce Type: new Abstract: Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability ",
    "url": "https://arxiv.org/abs/2602.07055",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "ANCHOR: Branch-Point Data Generation for GUI Agents",
    "summary": "arXiv:2602.07153v1 Announce Type: new Abstract: End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion",
    "url": "https://arxiv.org/abs/2602.07153",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "PreFlect: From Retrospective to Prospective Reflection in Large Language Model Agents",
    "summary": "arXiv:2602.07187v1 Announce Type: new Abstract: Advanced large language model agents typically adopt self-reflection for improving performance, where agents iteratively analyze past actions to correct errors. However, existing reflective approaches are inherently retrospective: agents act, observe failure, and only then attempt to recover. In this ",
    "url": "https://arxiv.org/abs/2602.07187",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Is there \"Secret Sauce'' in Large Language Model Development?",
    "summary": "arXiv:2602.07238v1 Announce Type: new Abstract: Do leading LLM developers possess a proprietary ``secret sauce'', or is LLM performance driven by scaling up compute? Using training and benchmark data for 809 models released between 2022 and 2025, we estimate scaling-law regressions with release-date and developer fixed effects. We find clear eviden",
    "url": "https://arxiv.org/abs/2602.07238",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "From Out-of-Distribution Detection to Hallucination Detection: A Geometric View",
    "summary": "arXiv:2602.07253v1 Announce Type: new Abstract: Detecting hallucinations in large language models is a critical open problem with significant implications for safety and reliability. While existing hallucination detection methods achieve strong performance in question-answering tasks, they remain less effective on tasks requiring reasoning. In this",
    "url": "https://arxiv.org/abs/2602.07253",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Incentive-Aware AI Safety via Strategic Resource Allocation: A Stackelberg Security Games Perspective",
    "summary": "arXiv:2602.07259v1 Announce Type: new Abstract: As AI systems grow more capable and autonomous, ensuring their safety and reliability requires not only model-level alignment but also strategic oversight of the humans and institutions involved in their development and deployment. Existing safety frameworks largely treat alignment as a static optimiz",
    "url": "https://arxiv.org/abs/2602.07259",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "BRIDGE: Predicting Human Task Completion Time From Model Performance",
    "summary": "arXiv:2602.07267v1 Announce Type: new Abstract: Evaluating the real-world capabilities of AI systems requires grounding benchmark performance in human-interpretable measures of task difficulty. Existing approaches that rely on direct human task completion time annotations are costly, noisy, and difficult to scale across benchmarks. In this work, we",
    "url": "https://arxiv.org/abs/2602.07267",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents",
    "summary": "arXiv:2602.07274v1 Announce Type: new Abstract: Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories s",
    "url": "https://arxiv.org/abs/2602.07274",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs",
    "summary": "arXiv:2602.07276v1 Announce Type: new Abstract: Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex t",
    "url": "https://arxiv.org/abs/2602.07276",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Adaptive Scaffolding for Cognitive Engagement in an Intelligent Tutoring System",
    "summary": "arXiv:2602.07308v1 Announce Type: new Abstract: The ICAP framework defines four cognitive engagement levels: Passive, Active, Constructive, and Interactive, where increased cognitive engagement can yield improved learning. However, personalizing learning activities that elicit the optimal level of cognitive engagement remains a key challenge in int",
    "url": "https://arxiv.org/abs/2602.07308",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "RAPiD: Real-time Deterministic Trajectory Planning via Diffusion Behavior Priors for Safe and Efficient Autonomous Driving",
    "summary": "arXiv:2602.07339v1 Announce Type: new Abstract: Diffusion-based trajectory planners have demonstrated strong capability for modeling the multimodal nature of human driving behavior, but their reliance on iterative stochastic sampling poses critical challenges for real-time, safety-critical deployment. In this work, we present RAPiD, a deterministic",
    "url": "https://arxiv.org/abs/2602.07339",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SupChain-Bench: Benchmarking Large Language Models for Real-World Supply Chain Management",
    "summary": "arXiv:2602.07342v1 Announce Type: new Abstract: Large language models (LLMs) have shown promise in complex reasoning and tool-based decision making, motivating their application to real-world supply chain management. However, supply chain workflows require reliable long-horizon, multi-step orchestration grounded in domain-specific procedures, which",
    "url": "https://arxiv.org/abs/2602.07342",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "W&D:Scaling Parallel Tool Calling for Efficient Deep Research Agents",
    "summary": "arXiv:2602.07359v1 Announce Type: new Abstract: Deep research agents have emerged as powerful tools for automating complex intellectual tasks through multi-step reasoning and web-based information seeking. While recent efforts have successfully enhanced these agents by scaling depth through increasing the number of sequential thinking and tool call",
    "url": "https://arxiv.org/abs/2602.07359",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "NAAMSE: Framework for Evolutionary Security Evaluation of Agents",
    "summary": "arXiv:2602.07391v1 Announce Type: new Abstract: AI agents are increasingly deployed in production, yet their security evaluations remain bottlenecked by manual red-teaming or static benchmarks that fail to model adaptive, multi-turn adversaries. We propose NAAMSE, an evolutionary framework that reframes agent security evaluation as a feedback-drive",
    "url": "https://arxiv.org/abs/2602.07391",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation",
    "summary": "arXiv:2602.07399v1 Announce Type: new Abstract: Vision--Language--Action (VLA) models bridge multimodal reasoning with physical control, but adapting them to new tasks with scarce demonstrations remains unreliable. While fine-tuned VLA policies often produce semantically plausible trajectories, failures often arise from unresolved geometric ambigui",
    "url": "https://arxiv.org/abs/2602.07399",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Progressive Multi-Agent Reasoning for Biological Perturbation Prediction",
    "summary": "arXiv:2602.07408v1 Announce Type: new Abstract: Predicting gene regulation responses to biological perturbations requires reasoning about underlying biological causalities. While large language models (LLMs) show promise for such tasks, they are often overwhelmed by the entangled nature of high-dimensional perturbation results. Moreover, recent wor",
    "url": "https://arxiv.org/abs/2602.07408",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution",
    "summary": "arXiv:2602.07414v1 Announce Type: new Abstract: Large language models (LLMs) are increasingly used to simulate human behavior in social settings such as legal mediation, negotiation, and dispute resolution. However, it remains unclear whether these simulations reproduce the personality-behavior patterns observed in humans. Human personality, for in",
    "url": "https://arxiv.org/abs/2602.07414",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "The Moltbook Illusion: Separating Human Influence from Emergent Behavior in AI Agent Societies",
    "summary": "arXiv:2602.07432v1 Announce Type: new Abstract: When AI agents on the social platform Moltbook appeared to develop consciousness, found religions, and declare hostility toward humanity, the phenomenon attracted global media attention and was cited as evidence of emergent machine intelligence. We show that these viral narratives were overwhelmingly ",
    "url": "https://arxiv.org/abs/2602.07432",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Are Reasoning LLMs Robust to Interventions on Their Chain-of-Thought?",
    "summary": "arXiv:2602.07470v1 Announce Type: new Abstract: Reasoning LLMs (RLLMs) generate step-by-step chains of thought (CoTs) before giving an answer, which improves performance on complex tasks and makes reasoning more transparent. But how robust are these reasoning traces to disruptions that occur within them? To address this question, we introduce a con",
    "url": "https://arxiv.org/abs/2602.07470",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Computing the Reachability Value of Posterior-Deterministic POMDPs",
    "summary": "arXiv:2602.07473v1 Announce Type: new Abstract: Partially observable Markov decision processes (POMDPs) are a fundamental model for sequential decision-making under uncertainty. However, many verification and synthesis problems for POMDPs are undecidable or intractable. Most prominently, the seminal result of Madani et al. (2003) states that there ",
    "url": "https://arxiv.org/abs/2602.07473",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design",
    "summary": "arXiv:2602.07491v1 Announce Type: new Abstract: Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from mol",
    "url": "https://arxiv.org/abs/2602.07491",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Joint Reward Modeling: Internalizing Chain-of-Thought for Efficient Visual Reward Models",
    "summary": "arXiv:2602.07533v1 Announce Type: new Abstract: Reward models are critical for reinforcement learning from human feedback, as they determine the alignment quality and reliability of generative models. For complex tasks such as image editing, reward models are required to capture global semantic consistency and implicit logical constraints beyond lo",
    "url": "https://arxiv.org/abs/2602.07533",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning",
    "summary": "arXiv:2602.07543v2 Announce Type: new Abstract: Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based ",
    "url": "https://arxiv.org/abs/2602.07543",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "When Is Enough Not Enough? Illusory Completion in Search Agents",
    "summary": "arXiv:2602.07549v1 Announce Type: new Abstract: Recent search agents leverage multi-turn reasoning and search tools to achieve strong performance on multi-hop and long-horizon benchmarks. Yet it remains unclear whether they reliably reason across all requirements by tracking, verifying, and maintaining multiple conditions in these questions. We stu",
    "url": "https://arxiv.org/abs/2602.07549",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "VERIFY-RL: Verifiable Recursive Decomposition for Reinforcement Learning in Mathematical Reasoning",
    "summary": "arXiv:2602.07559v1 Announce Type: new Abstract: Training language models to solve complex mathematical problems benefits from curriculum learning progressively training on simpler subproblems. However, existing decomposition methods are often heuristic, offering no guarantees that subproblems are simpler, that solving them aids the parent task, or ",
    "url": "https://arxiv.org/abs/2602.07559",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "M2A: Multimodal Memory Agent with Dual-Layer Hybrid Memory for Long-Term Personalized Interactions",
    "summary": "arXiv:2602.07624v1 Announce Type: new Abstract: This work addresses the challenge of personalized question answering in long-term human-machine interactions: when conversational history spans weeks or months and exceeds the context window, existing personalization mechanisms struggle to continuously absorb and leverage users' incremental concepts, ",
    "url": "https://arxiv.org/abs/2602.07624",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SleepMaMi: A Universal Sleep Foundation Model for Integrating Macro- and Micro-structures",
    "summary": "arXiv:2602.07628v1 Announce Type: new Abstract: While the shift toward unified foundation models has revolutionized many deep learning domains, sleep medicine remains largely restricted to task-specific models that focus on localized micro-structure features. These approaches often neglect the rich, multi-modal context of Polysomnography (PSG) and ",
    "url": "https://arxiv.org/abs/2602.07628",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Efficient Table Retrieval and Understanding with Multimodal Large Language Models",
    "summary": "arXiv:2602.07642v1 Announce Type: new Abstract: Tabular data is frequently captured in image form across a wide range of real-world scenarios such as financial reports, handwritten records, and document scans. These visual representations pose unique challenges for machine understanding, as they combine both structural and visual complexities. Whil",
    "url": "https://arxiv.org/abs/2602.07642",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "ONTrust: A Reference Ontology of Trust",
    "summary": "arXiv:2602.07662v1 Announce Type: new Abstract: Trust has stood out more than ever in the light of recent innovations. Some examples are advances in artificial intelligence that make machines more and more humanlike, and the introduction of decentralized technologies (e.g. blockchains), which creates new forms of (decentralized) trust. These new de",
    "url": "https://arxiv.org/abs/2602.07662",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "EventCast: Hybrid Demand Forecasting in E-Commerce with LLM-Based Event Knowledge",
    "summary": "arXiv:2602.07695v1 Announce Type: new Abstract: Demand forecasting is a cornerstone of e-commerce operations, directly impacting inventory planning and fulfillment scheduling. However, existing forecasting systems often fail during high-impact periods such as flash sales, holiday campaigns, and sudden policy interventions, where demand patterns shi",
    "url": "https://arxiv.org/abs/2602.07695",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Geo-Code: A Code Framework for Reverse Code Generation from Geometric Images Based on Two-Stage Multi-Agent Evolution",
    "summary": "arXiv:2602.07749v1 Announce Type: new Abstract: Program code serves as a bridge linking vision and logic, providing a feasible supervisory approach for enhancing the multimodal reasoning capability of large models through geometric operations such as auxiliary line construction and perspective transformation. Nevertheless, current inverse graphics ",
    "url": "https://arxiv.org/abs/2602.07749",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Humanizing AI Grading: Student-Centered Insights on Fairness, Trust, Consistency and Transparency",
    "summary": "arXiv:2602.07754v1 Announce Type: new Abstract: This study investigates students' perceptions of Artificial Intelligence (AI) grading systems in an undergraduate computer science course (n = 27), focusing on a block-based programming final project. Guided by the ethical principles framework articulated by Jobin (2019), our study examines fairness, ",
    "url": "https://arxiv.org/abs/2602.07754",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Learning to Continually Learn via Meta-learning Agentic Memory Designs",
    "summary": "arXiv:2602.07755v1 Announce Type: new Abstract: The statelessness of foundation models bottlenecks agentic systems' ability to continually learn, a core capability for long-horizon reasoning and adaptation. To address this limitation, agentic systems commonly incorporate memory modules to retain and reuse past experience, aiming for continual learn",
    "url": "https://arxiv.org/abs/2602.07755",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Disentangled Instrumental Variables for Causal Inference with Networked Observational Data",
    "summary": "arXiv:2602.07765v1 Announce Type: new Abstract: Instrumental variables (IVs) are crucial for addressing unobservable confounders, yet their stringent exogeneity assumptions pose significant challenges in networked data. Existing methods typically rely on modelling neighbour information when recovering IVs, thereby inevitably mixing shared environme",
    "url": "https://arxiv.org/abs/2602.07765",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Do Multi-Agents Dream of Electric Screens? Achieving Perfect Accuracy on AndroidWorld Through Task Decomposition",
    "summary": "arXiv:2602.07787v1 Announce Type: new Abstract: We present Minitap, a multi-agent system that achieves 100% success on the AndroidWorld benchmark, the first to fully solve all 116 tasks and surpassing human performance (80%). We first analyze why single-agent architectures fail: context pollution from mixed reasoning traces, silent text input failu",
    "url": "https://arxiv.org/abs/2602.07787",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training",
    "summary": "arXiv:2602.07824v1 Announce Type: new Abstract: Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientif",
    "url": "https://arxiv.org/abs/2602.07824",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning",
    "summary": "arXiv:2602.07830v1 Announce Type: new Abstract: Time series is a pervasive data type across various application domains, rendering the reasonable solving of diverse time series tasks a long-standing goal. Recent advances in large language models (LLMs), especially their reasoning abilities unlocked through reinforcement learning (RL), have opened n",
    "url": "https://arxiv.org/abs/2602.07830",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "LQA: A Lightweight Quantized-Adaptive Framework for Vision-Language Models on the Edge",
    "summary": "arXiv:2602.07849v1 Announce Type: new Abstract: Deploying Vision-Language Models (VLMs) on edge devices is challenged by resource constraints and performance degradation under distribution shifts. While test-time adaptation (TTA) can counteract such shifts, existing methods are too resource-intensive for on-device deployment. To address this challe",
    "url": "https://arxiv.org/abs/2602.07849",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Emergent Misalignment is Easy, Narrow Misalignment is Hard",
    "summary": "arXiv:2602.07852v1 Announce Type: new Abstract: Finetuning large language models on narrowly harmful datasets can cause them to become emergently misaligned, giving stereotypically `evil' responses across diverse unrelated settings. Concerningly, a pre-registered survey of experts failed to predict this result, highlighting our poor understanding o",
    "url": "https://arxiv.org/abs/2602.07852",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "ToolSelf: Unifying Task Execution and Self-Reconfiguration via Tool-Driven Intrinsic Adaptation",
    "summary": "arXiv:2602.07883v1 Announce Type: new Abstract: Agentic systems powered by Large Language Models (LLMs) have demonstrated remarkable potential in tackling complex, long-horizon tasks. However, their efficacy is fundamentally constrained by static configurations governing agent behaviors, which are fixed prior to execution and fail to adapt to evolv",
    "url": "https://arxiv.org/abs/2602.07883",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MemFly: On-the-Fly Memory Optimization via Information Bottleneck",
    "summary": "arXiv:2602.07885v1 Announce Type: new Abstract: Long-term memory enables large language model agents to tackle complex tasks through historical interactions. However, existing frameworks encounter a fundamental dilemma between compressing redundant information efficiently and maintaining precise retrieval for downstream tasks. To bridge this gap, w",
    "url": "https://arxiv.org/abs/2602.07885",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "GCN-MPPR: Enhancing the Propagation of Message Passing Neural Networks via Motif-Based Personalized PageRank",
    "summary": "arXiv:2602.07903v1 Announce Type: new Abstract: The algorithms based on message passing neural networks (MPNNs) on graphs have recently achieved great success for various graph applications. However, studies find that these methods always propagate the information to very limited neighborhoods with shallow depth, particularly due to over-smoothing.",
    "url": "https://arxiv.org/abs/2602.07903",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MedCoG: Maximizing LLM Inference Density in Medical Reasoning via Meta-Cognitive Regulation",
    "summary": "arXiv:2602.07905v1 Announce Type: new Abstract: Large Language Models (LLMs) have shown strong potential in complex medical reasoning yet face diminishing gains under inference scaling laws. While existing studies augment LLMs with various knowledge types, it remains unclear how effectively the additional costs translate into accuracy. In this pape",
    "url": "https://arxiv.org/abs/2602.07905",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Selective Fine-Tuning for Targeted and Robust Concept Unlearning",
    "summary": "arXiv:2602.07919v1 Announce Type: new Abstract: Text guided diffusion models are used by millions of users, but can be easily exploited to produce harmful content. Concept unlearning methods aim at reducing the models' likelihood of generating harmful content. Traditionally, this has been tackled at an individual concept level, with only a handful ",
    "url": "https://arxiv.org/abs/2602.07919",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MePo: Meta Post-Refinement for Rehearsal-Free General Continual Learnin",
    "summary": "arXiv:2602.07940v1 Announce Type: new Abstract: To cope with uncertain changes of the external world, intelligent systems must continually learn from complex, evolving environments and respond in real time. This ability, collectively known as general continual learning (GCL), encapsulates practical challenges such as online datastreams and blurry t",
    "url": "https://arxiv.org/abs/2602.07940",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "IV Co-Scientist: Multi-Agent LLM Framework for Causal Instrumental Variable Discovery",
    "summary": "arXiv:2602.07943v1 Announce Type: new Abstract: In the presence of confounding between an endogenous variable and the outcome, instrumental variables (IVs) are used to isolate the causal effect of the endogenous variable. Identifying valid instruments requires interdisciplinary knowledge, creativity, and contextual understanding, making it a non-tr",
    "url": "https://arxiv.org/abs/2602.07943",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth",
    "summary": "arXiv:2602.07962v1 Announce Type: new Abstract: Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as \"context rot\". Existing long-context benchmarks primarily focus on single-step settings that evalua",
    "url": "https://arxiv.org/abs/2602.07962",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Accelerating Social Science Research via Agentic Hypothesization and Experimentation",
    "summary": "arXiv:2602.07983v1 Announce Type: new Abstract: Data-driven social science research is inherently slow, relying on iterative cycles of observation, hypothesis generation, and experimental validation. While recent data-driven methods promise to accelerate parts of this process, they largely fail to support end-to-end scientific discovery. To address",
    "url": "https://arxiv.org/abs/2602.07983",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Towards Adaptive, Scalable, and Robust Coordination of LLM Agents: A Dynamic Ad-Hoc Networking Perspective",
    "summary": "arXiv:2602.08009v1 Announce Type: new Abstract: Multi-agent architectures built on large language models (LLMs) have demonstrated the potential to realize swarm intelligence through well-crafted collaboration. However, the substantial burden of manual orchestration inherently raises an imperative to automate the design of agentic workflows. We fram",
    "url": "https://arxiv.org/abs/2602.08009",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Small Agent Group is the Future of Digital Health",
    "summary": "arXiv:2602.08013v1 Announce Type: new Abstract: The rapid adoption of large language models (LLMs) in digital health has been driven by a \"scaling-first\" philosophy, i.e., the assumption that clinical intelligence increases with model size and data. However, real-world clinical needs include not only effectiveness, but also reliability and reasonab",
    "url": "https://arxiv.org/abs/2602.08013",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Structure-Aware Robust Counterfactual Explanations via Conditional Gaussian Network Classifiers",
    "summary": "arXiv:2602.08021v1 Announce Type: new Abstract: Counterfactual explanation (CE) is a core technique in explainable artificial intelligence (XAI), widely used to interpret model decisions and suggest actionable alternatives. This work presents a structure-aware and robustness-oriented counterfactual search method based on the conditional Gaussian ne",
    "url": "https://arxiv.org/abs/2602.08021",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Free(): Learning to Forget in Malloc-Only Reasoning Models",
    "summary": "arXiv:2602.08030v2 Announce Type: new Abstract: Reasoning models enhance problem-solving by scaling test-time compute, yet they face a critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to a fundamental architectural flaw: standard LLMs operate as \"malloc-only\" engines, continuously accum",
    "url": "https://arxiv.org/abs/2602.08030",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Graph-Enhanced Deep Reinforcement Learning for Multi-Objective Unrelated Parallel Machine Scheduling",
    "summary": "arXiv:2602.08052v1 Announce Type: new Abstract: The Unrelated Parallel Machine Scheduling Problem (UPMSP) with release dates, setups, and eligibility constraints presents a significant multi-objective challenge. Traditional methods struggle to balance minimizing Total Weighted Tardiness (TWT) and Total Setup Time (TST). This paper proposes a Deep R",
    "url": "https://arxiv.org/abs/2602.08052",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Securing Dual-Use Pathogen Data of Concern",
    "summary": "arXiv:2602.08061v1 Announce Type: new Abstract: Training data is an essential input into creating competent artificial intelligence (AI) models. AI models for biology are trained on large volumes of data, including data related to biological sequences, structures, images, and functions. The type of data used to train a model is intimately tied to t",
    "url": "https://arxiv.org/abs/2602.08061",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Objective Decoupling in Social Reinforcement Learning: Recovering Ground Truth from Sycophantic Majorities",
    "summary": "arXiv:2602.08092v1 Announce Type: new Abstract: Contemporary AI alignment strategies rely on a fragile premise: that human feedback, while noisy, remains a fundamentally truthful signal. In this paper, we identify this assumption as Dogma 4 of Reinforcement Learning (RL). We demonstrate that while this dogma holds in static environments, it fails i",
    "url": "https://arxiv.org/abs/2602.08092",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Interpretable Failure Analysis in Multi-Agent Reinforcement Learning Systems",
    "summary": "arXiv:2602.08104v1 Announce Type: new Abstract: Multi-Agent Reinforcement Learning (MARL) is increasingly deployed in safety-critical domains, yet methods for interpretable failure detection and attribution remain underdeveloped. We introduce a two-stage gradient-based framework that provides interpretable diagnostics for three critical failure ana",
    "url": "https://arxiv.org/abs/2602.08104",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Initial Risk Probing and Feasibility Testing of Glow: a Generative AI-Powered Dialectical Behavior Therapy Skills Coach for Substance Use Recovery and HIV Prevention",
    "summary": "arXiv:2602.08121v1 Announce Type: new Abstract: Background: HIV and substance use represent interacting epidemics with shared psychological drivers - impulsivity and maladaptive coping. Dialectical behavior therapy (DBT) targets these mechanisms but faces scalability challenges. Generative artificial intelligence (GenAI) offers potential for delive",
    "url": "https://arxiv.org/abs/2602.08121",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "RECUR: Resource Exhaustion Attack via Recursive-Entropy Guided Counterfactual Utilization and Reflection",
    "summary": "arXiv:2602.08214v1 Announce Type: new Abstract: Large Reasoning Models (LRMs) employ reasoning to address complex tasks. Such explicit reasoning requires extended context lengths, resulting in substantially higher resource consumption. Prior work has shown that adversarially crafted inputs can trigger redundant reasoning processes, exposing LRMs to",
    "url": "https://arxiv.org/abs/2602.08214",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger",
    "summary": "arXiv:2602.08222v1 Announce Type: new Abstract: As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative sup",
    "url": "https://arxiv.org/abs/2602.08222",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation",
    "summary": "arXiv:2602.08229v1 Announce Type: new Abstract: The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation ",
    "url": "https://arxiv.org/abs/2602.08229",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "PTS-SNN: A Prompt-Tuned Temporal Shift Spiking Neural Networks for Efficient Speech Emotion Recognition",
    "summary": "arXiv:2602.08240v1 Announce Type: new Abstract: Speech Emotion Recognition (SER) is widely deployed in Human-Computer Interaction, yet the high computational cost of conventional models hinders their implementation on resource-constrained edge devices. Spiking Neural Networks (SNNs) offer an energy-efficient alternative due to their event-driven na",
    "url": "https://arxiv.org/abs/2602.08240",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs",
    "summary": "arXiv:2602.08241v1 Announce Type: new Abstract: While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis show",
    "url": "https://arxiv.org/abs/2602.08241",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design",
    "summary": "arXiv:2602.08253v1 Announce Type: new Abstract: While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer l",
    "url": "https://arxiv.org/abs/2602.08253",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SynthAgent: A Multi-Agent LLM Framework for Realistic Patient Simulation -- A Case Study in Obesity with Mental Health Comorbidities",
    "summary": "arXiv:2602.08254v1 Announce Type: new Abstract: Simulating high-fidelity patients offers a powerful avenue for studying complex diseases while addressing the challenges of fragmented, biased, and privacy-restricted real-world data. In this study, we introduce SynthAgent, a novel Multi-Agent System (MAS) framework designed to model obesity patients ",
    "url": "https://arxiv.org/abs/2602.08254",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Puda: Private User Dataset Agent for User-Sovereign and Privacy-Preserving Personalized AI",
    "summary": "arXiv:2602.08268v2 Announce Type: new Abstract: Personal data centralization among dominant platform providers including search engines, social networking services, and e-commerce has created siloed ecosystems that restrict user sovereignty, thereby impeding data use across services. Meanwhile, the rapid proliferation of Large Language Model (LLM)-",
    "url": "https://arxiv.org/abs/2602.08268",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis",
    "summary": "arXiv:2602.08276v1 Announce Type: new Abstract: Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concep",
    "url": "https://arxiv.org/abs/2602.08276",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "The Vibe-Automation of Automation: A Proactive Education Framework for Computer Science in the Age of Generative AI",
    "summary": "arXiv:2602.08295v1 Announce Type: new Abstract: The emergence of generative artificial intelligence (GenAI) represents not an incremental technological advance but a qualitative epistemological shift that challenges foundational assumptions of computer science. Whereas machine learning has been described as the automation of automation, generative ",
    "url": "https://arxiv.org/abs/2602.08295",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Moral Sycophancy in Vision Language Models",
    "summary": "arXiv:2602.08311v1 Announce Type: new Abstract: Sycophancy in Vision-Language Models (VLMs) refers to their tendency to align with user opinions, often at the expense of moral or factual accuracy. While prior studies have explored sycophantic behavior in general contexts, its impact on morally grounded visual decision-making remains insufficiently ",
    "url": "https://arxiv.org/abs/2602.08311",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System",
    "summary": "arXiv:2602.08335v1 Announce Type: new Abstract: Integrating Large Language Models (LLMs) with external tools via multi-agent systems offers a promising new paradigm for decomposing and solving complex problems. However, training these systems remains notoriously difficult due to the credit assignment challenge, as it is often unclear which specific",
    "url": "https://arxiv.org/abs/2602.08335",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT",
    "summary": "arXiv:2602.08339v1 Announce Type: new Abstract: Recent advances in vision-language models (VLMs) have markedly improved image-text alignment, yet they still fall short of human-like visual reasoning. A key limitation is that many VLMs rely on surface correlations rather than building logically coherent structured representations, which often leads ",
    "url": "https://arxiv.org/abs/2602.08339",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Effect-Level Validation for Causal Discovery",
    "summary": "arXiv:2602.08340v1 Announce Type: new Abstract: Causal discovery is increasingly applied to large-scale telemetry data to estimate the effects of user-facing interventions, yet its reliability for decision-making in feedback-driven systems with strong self-selection remains unclear. In this paper, we propose an effect-centric, admissibility-first f",
    "url": "https://arxiv.org/abs/2602.08340",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration",
    "summary": "arXiv:2602.08344v1 Announce Type: new Abstract: Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised f",
    "url": "https://arxiv.org/abs/2602.08344",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Towards Better Evolution Modeling for Temporal Knowledge Graphs",
    "summary": "arXiv:2602.08353v1 Announce Type: new Abstract: Temporal knowledge graphs (TKGs) structurally preserve evolving human knowledge. Recent research has focused on designing models to learn the evolutionary nature of TKGs to predict future facts, achieving impressive results. For instance, Hits@10 scores over 0.9 on YAGO dataset. However, we find that ",
    "url": "https://arxiv.org/abs/2602.08353",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Does Your Reasoning Model Implicitly Know When to Stop Thinking?",
    "summary": "arXiv:2602.08354v1 Announce Type: new Abstract: Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time",
    "url": "https://arxiv.org/abs/2602.08354",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Circuit Representations of Random Forests with Applications to XAI",
    "summary": "arXiv:2602.08362v1 Announce Type: new Abstract: We make three contributions in this paper. First, we present an approach for compiling a random forest classifier into a set of circuits, where each circuit directly encodes the instances in some class of the classifier. We show empirically that our proposed approach is significantly more efficient th",
    "url": "https://arxiv.org/abs/2602.08362",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MemAdapter: Fast Alignment across Agent Memory Paradigms via Generative Subgraph Retrieval",
    "summary": "arXiv:2602.08369v1 Announce Type: new Abstract: Memory mechanism is a core component of LLM-based agents, enabling reasoning and knowledge discovery over long-horizon contexts. Existing agent memory systems are typically designed within isolated paradigms (e.g., explicit, parametric, or latent memory) with tightly coupled retrieval methods that hin",
    "url": "https://arxiv.org/abs/2602.08369",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI",
    "summary": "arXiv:2602.08373v1 Announce Type: new Abstract: Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repa",
    "url": "https://arxiv.org/abs/2602.08373",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SCOUT-RAG: Scalable and Cost-Efficient Unifying Traversal for Agentic Graph-RAG over Distributed Domains",
    "summary": "arXiv:2602.08400v1 Announce Type: new Abstract: Graph-RAG improves LLM reasoning using structured knowledge, yet conventional designs rely on a centralized knowledge graph. In distributed and access-restricted settings (e.g., hospitals or multinational organizations), retrieval must select relevant domains and appropriate traversal depth without gl",
    "url": "https://arxiv.org/abs/2602.08400",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "On Protecting Agentic Systems' Intellectual Property via Watermarking",
    "summary": "arXiv:2602.08401v1 Announce Type: new Abstract: The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by ",
    "url": "https://arxiv.org/abs/2602.08401",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent",
    "summary": "arXiv:2602.08412v1 Announce Type: new Abstract: Although large language model (LLM)-based agents, exemplified by OpenClaw, are increasingly evolving from task-oriented systems into personalized AI assistants for solving complex real-world tasks, their practical deployment also introduces severe security risks. However, existing agent security resea",
    "url": "https://arxiv.org/abs/2602.08412",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "When Evaluation Becomes a Side Channel: Regime Leakage and Structural Mitigations for Alignment Assessment",
    "summary": "arXiv:2602.08449v1 Announce Type: new Abstract: Safety evaluation for advanced AI systems implicitly assumes that behavior observed under evaluation is predictive of behavior in deployment. This assumption becomes fragile for agents with situational awareness, which may exploitregime leakage-informational cues distinguishing evaluation from deploym",
    "url": "https://arxiv.org/abs/2602.08449",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "TreeTensor: Boost AI System on Nested Data with Constrained Tree-Like Tensor",
    "summary": "arXiv:2602.08517v1 Announce Type: new Abstract: Tensor is the most basic and essential data structure of nowadays artificial intelligence (AI) system. The natural properties of Tensor, especially the memory-continuity and slice-independence, make it feasible for training system to leverage parallel computing unit like GPU to process data simultaneo",
    "url": "https://arxiv.org/abs/2602.08517",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning",
    "summary": "arXiv:2602.08520v1 Announce Type: new Abstract: Modern large language models (LLMs) are often evaluated and deployed under a \\emph{one-shot, greedy} inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from mi",
    "url": "https://arxiv.org/abs/2602.08520",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Dialogue Model Optimization via Agent Game and Adaptive Tree-based GRPO",
    "summary": "arXiv:2602.08533v2 Announce Type: new Abstract: Open-ended dialogue agents aim to deliver engaging, personalized interactions by adapting to users' traits, but existing methods face critical limitations: over-reliance on pre-collected user data, and short-horizon biases in reinforcement learning (RL) that neglect long-term dialogue value. To addres",
    "url": "https://arxiv.org/abs/2602.08533",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "PRISM: A Principled Framework for Multi-Agent Reasoning via Gain Decomposition",
    "summary": "arXiv:2602.08586v2 Announce Type: new Abstract: Multi-agent collaboration has emerged as a promising paradigm for enhancing reasoning capabilities of Large Language Models (LLMs). However, existing approaches remain largely heuristic, lacking principled guidance on what drives performance gains and how to systematically optimize multi-agent reasoni",
    "url": "https://arxiv.org/abs/2602.08586",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture",
    "summary": "arXiv:2602.08597v1 Announce Type: new Abstract: Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal i",
    "url": "https://arxiv.org/abs/2602.08597",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval",
    "summary": "arXiv:2602.08603v1 Announce Type: new Abstract: Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-",
    "url": "https://arxiv.org/abs/2602.08603",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Debate is efficient with your time",
    "summary": "arXiv:2602.08630v1 Announce Type: new Abstract: AI safety via debate uses two competing models to help a human judge verify complex computational tasks. Previous work has established what problems debate can solve in principle, but has not analysed the practical cost of human oversight: how many queries must the judge make to the debate transcript?",
    "url": "https://arxiv.org/abs/2602.08630",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Why do we Trust Chatbots? From Normative Principles to Behavioral Drivers",
    "summary": "arXiv:2602.08707v1 Announce Type: new Abstract: As chatbots increasingly blur the boundary between automated systems and human conversation, the foundations of trust in these systems warrant closer examination. While regulatory and policy frameworks tend to define trust in normative terms, the trust users place in chatbots often emerges from behavi",
    "url": "https://arxiv.org/abs/2602.08707",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Intermediate Results on the Complexity of STRIPS$_{1}^{1}$",
    "summary": "arXiv:2602.08708v1 Announce Type: new Abstract: This paper is based on Bylander's results on the computational complexity of propositional STRIPS planning. He showed that when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. While NP-hardness",
    "url": "https://arxiv.org/abs/2602.08708",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Exploring SAIG Methods for an Objective Evaluation of XAI",
    "summary": "arXiv:2602.08715v1 Announce Type: new Abstract: The evaluation of eXplainable Artificial Intelligence (XAI) methods is a rapidly growing field, characterized by a wide variety of approaches. This diversity highlights the complexity of the XAI evaluation, which, unlike traditional AI assessment, lacks a universally correct ground truth for the expla",
    "url": "https://arxiv.org/abs/2602.08715",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Finite-State Controllers for (Hidden-Model) POMDPs using Deep Reinforcement Learning",
    "summary": "arXiv:2602.08734v1 Announce Type: new Abstract: Solving partially observable Markov decision processes (POMDPs) requires computing policies under imperfect state information. Despite recent advances, the scalability of existing POMDP solvers remains limited. Moreover, many settings require a policy that is robust across multiple POMDPs, further agg",
    "url": "https://arxiv.org/abs/2602.08734",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Belief Offloading in Human-AI Interaction",
    "summary": "arXiv:2602.08754v1 Announce Type: new Abstract: What happens when people's beliefs are derived from information provided by an LLM? People's use of LLM chatbots as thought partners can contribute to cognitive offloading, which can have adverse effects on cognitive skills in cases of over-reliance. This paper defines and investigates a particular ki",
    "url": "https://arxiv.org/abs/2602.08754",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure",
    "summary": "arXiv:2602.08783v1 Announce Type: new Abstract: Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes. In this paper, we view latent chain-of-thought as a manipulable causal process in rep",
    "url": "https://arxiv.org/abs/2602.08783",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "The Use of AI Tools to Develop and Validate Q-Matrices",
    "summary": "arXiv:2602.08796v1 Announce Type: new Abstract: Constructing a Q-matrix is a critical but labor-intensive step in cognitive diagnostic modeling (CDM). This study investigates whether AI tools (i.e., general language models) can support Q-matrix development by comparing AI-generated Q-matrices with a validated Q-matrix from Li and Suen (2013) for a ",
    "url": "https://arxiv.org/abs/2602.08796",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Root Cause Analysis Method Based on Large Language Models with Residual Connection Structures",
    "summary": "arXiv:2602.08804v1 Announce Type: new Abstract: Root cause localization remain challenging in complex and large-scale microservice architectures. The complex fault propagation among microservices and the high dimensionality of telemetry data, including metrics, logs, and traces, limit the effectiveness of existing root cause analysis (RCA) methods.",
    "url": "https://arxiv.org/abs/2602.08804",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Negative-Aware Diffusion Process for Temporal Knowledge Graph Extrapolation",
    "summary": "arXiv:2602.08815v1 Announce Type: new Abstract: Temporal Knowledge Graph (TKG) reasoning seeks to predict future missing facts from historical evidence. While diffusion models (DM) have recently gained attention for their ability to capture complex predictive distributions, two gaps remain: (i) the generative path is conditioned only on positive ev",
    "url": "https://arxiv.org/abs/2602.08815",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning",
    "summary": "arXiv:2602.08835v1 Announce Type: new Abstract: Value-aware AI should recognise human values and adapt to the value systems (value-based preferences) of different users. This requires operationalization of values, which can be prone to misspecification. The social nature of values demands their representation to adhere to multiple users while value",
    "url": "https://arxiv.org/abs/2602.08835",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Deciding the Satisfiability of Combined Qualitative Constraint Networks",
    "summary": "arXiv:2602.08848v1 Announce Type: new Abstract: Among the various forms of reasoning studied in the context of artificial intelligence, qualitative reasoning makes it possible to infer new knowledge in the context of imprecise, incomplete information without numerical values. In this paper, we propose a formal framework unifying several forms of ex",
    "url": "https://arxiv.org/abs/2602.08848",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Scalable Delphi: Large Language Models for Structured Risk Estimation",
    "summary": "arXiv:2602.08889v1 Announce Type: new Abstract: Quantitative risk assessment in high-stakes domains relies on structured expert elicitation to estimate unobservable properties. The gold standard - the Delphi method - produces calibrated, auditable judgments but requires months of coordination and specialist time, placing rigorous risk assessment ou",
    "url": "https://arxiv.org/abs/2602.08889",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Efficient and Stable Reinforcement Learning for Diffusion Language Models",
    "summary": "arXiv:2602.08905v1 Announce Type: new Abstract: Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framewor",
    "url": "https://arxiv.org/abs/2602.08905",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse",
    "summary": "arXiv:2602.08939v1 Announce Type: new Abstract: LLM failures in causal reasoning, including sycophancy, rung collapse, and miscalibrated refusal, are well-documented, yet progress on remediation is slow because no benchmark enables systematic diagnosis. We introduce CausalT5K, a diagnostic benchmark of over 5,000 cases across 10 domains that tests ",
    "url": "https://arxiv.org/abs/2602.08939",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute",
    "summary": "arXiv:2602.08948v1 Announce Type: new Abstract: Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the t",
    "url": "https://arxiv.org/abs/2602.08948",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Digital Twin and Agentic AI for Wild Fire Disaster Management: Intelligent Virtual Situation Room",
    "summary": "arXiv:2602.08949v1 Announce Type: new Abstract: According to the United Nations, wildfire frequency and intensity are projected to increase by approximately 14% by 2030 and 30% by 2050 due to global warming, posing critical threats to life, infrastructure, and ecosystems. Conventional disaster management frameworks rely on static simulations and pa",
    "url": "https://arxiv.org/abs/2602.08949",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation",
    "summary": "arXiv:2602.08968v1 Announce Type: new Abstract: World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, seve",
    "url": "https://arxiv.org/abs/2602.08968",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery",
    "summary": "arXiv:2602.08990v1 Announce Type: new Abstract: We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported ",
    "url": "https://arxiv.org/abs/2602.08990",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "iGRPO: Self-Feedback-Driven LLM Reasoning",
    "summary": "arXiv:2602.09000v1 Announce Type: new Abstract: Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability",
    "url": "https://arxiv.org/abs/2602.09000",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Data Science and Technology Towards AGI Part I: Tiered Data Management",
    "summary": "arXiv:2602.09003v1 Announce Type: new Abstract: The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirection",
    "url": "https://arxiv.org/abs/2602.09003",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "GEBench: Benchmarking Image Generation Models as GUI Environments",
    "summary": "arXiv:2602.09007v2 Announce Type: new Abstract: Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in G",
    "url": "https://arxiv.org/abs/2602.09007",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "BERT Learns (and Teaches) Chemistry",
    "summary": "arXiv:2007.16012v1 Announce Type: cross Abstract: Modern computational organic chemistry is becoming increasingly data-driven. There remain a large number of important unsolved problems in this area such as product prediction given reactants, drug discovery, and metric-optimized molecule synthesis, but efforts to solve these problems using machine ",
    "url": "https://arxiv.org/abs/2007.16012",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning",
    "summary": "arXiv:2512.22730v2 Announce Type: cross Abstract: Cystic hygroma is a high-risk prenatal ultrasound finding that portends high rates of chromosomal abnormalities, structural malformations, and adverse pregnancy outcomes. Automated detection can increase reproducibility and support scalable early screening programs, but supervised deep learning meth",
    "url": "https://arxiv.org/abs/2512.22730",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Leveraging Adaptive Group Negotiation for Heterogeneous Multi-Robot Collaboration with Large Language Models",
    "summary": "arXiv:2602.06967v1 Announce Type: cross Abstract: Multi-robot collaboration tasks often require heterogeneous robots to work together over long horizons under spatial constraints and environmental uncertainties. Although Large Language Models (LLMs) excel at reasoning and planning, their potential for coordinated control has not been fully explored",
    "url": "https://arxiv.org/abs/2602.06967",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Does Visual Rendering Bypass Tokenization? Investigating Script-Tokenizer Misalignment in Pixel-Based Language Models",
    "summary": "arXiv:2602.06973v1 Announce Type: cross Abstract: While pixel-based language modeling aims to bypass the sub-word tokenization bottleneck by rendering text as images, recent multimodal variants such as DualGPT reintroduce text tokenizers to improve autoregressive performance. We investigate a fundamental question, does visual rendering truly decoup",
    "url": "https://arxiv.org/abs/2602.06973",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "BiomechAgent: AI-Assisted Biomechanical Analysis Through Code-Generating Agents",
    "summary": "arXiv:2602.06975v1 Announce Type: cross Abstract: Markerless motion capture is making quantitative movement analysis increasingly accessible, yet analyzing the resulting data remains a barrier for clinicians without programming expertise. We present BiomechAgent, a code-generating AI agent that enables biomechanical analysis through natural languag",
    "url": "https://arxiv.org/abs/2602.06975",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Bridging the Knowledge Void: Inference-time Acquisition of Unfamiliar Programming Languages for Coding Tasks",
    "summary": "arXiv:2602.06976v1 Announce Type: cross Abstract: The proficiency of Large Language Models (LLMs) in coding tasks is often a reflection of their extensive pre-training corpora, which typically collapses when confronted with previously unfamiliar programming languages. Departing from data-intensive finetuning, we investigate the paradigm of Inferenc",
    "url": "https://arxiv.org/abs/2602.06976",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "What is Safety? Corporate Discourse, Power, and the Politics of Generative AI Safety",
    "summary": "arXiv:2602.06981v1 Announce Type: cross Abstract: This work examines how leading generative artificial intelligence companies construct and communicate the concept of \"safety\" through public-facing documents. Drawing on critical discourse analysis, we analyze a corpus of corporate safety-related statements to explicate how authority, responsibility",
    "url": "https://arxiv.org/abs/2602.06981",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Deep Reinforcement Learning for Interference Suppression in RIS-Aided Space-Air-Ground Integrated Networks",
    "summary": "arXiv:2602.06982v1 Announce Type: cross Abstract: Future 6G networks envision ubiquitous connectivity through space-air-ground integrated networks (SAGINs), where high-altitude platform stations (HAPSs) and satellites complement terrestrial systems to provide wide-area, low-latency coverage. However, the rapid growth of terrestrial devices intensif",
    "url": "https://arxiv.org/abs/2602.06982",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Hybrid Deep Learning Framework for CSI-Based Activity Recognition in Bandwidth-Constrained Wi-Fi Sensing",
    "summary": "arXiv:2602.06983v1 Announce Type: cross Abstract: This paper presents a novel hybrid deep learning framework designed to enhance the robustness of CSI-based Human Activity Recognition (HAR) within bandwidth-constrained Wi-Fi sensing environments. The core of our proposed methodology is a preliminary Doppler trace extraction stage, implemented to am",
    "url": "https://arxiv.org/abs/2602.06983",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Empowering Affected Individuals to Shape AI Fairness Assessments: Processes, Criteria, and Tools",
    "summary": "arXiv:2602.06984v1 Announce Type: cross Abstract: AI systems are increasingly used in high-stakes domains such as credit rating, where fairness concerns are critical. Existing fairness assessments are typically conducted by AI experts or regulators using predefined protected attributes and metrics, which often fail to capture the diversity and nuan",
    "url": "https://arxiv.org/abs/2602.06984",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "A New Mode of Teaching Chinese as a Foreign Language from the Perspective of Smart System Studied by Using Rongzhixue",
    "summary": "arXiv:2602.06992v1 Announce Type: cross Abstract: The purpose of this study is to introduce a new model of teaching Chinese as a foreign language from the perspective of integrating wisdom. Its characteristics are as follows: focusing on the butterfly model of interpretation before translation, highlighting the new method of bilingual thinking trai",
    "url": "https://arxiv.org/abs/2602.06992",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SurfAge-Net: A Hierarchical Surface-Based Network for Interpretable Fine-Grained Brain Age Prediction",
    "summary": "arXiv:2602.06994v1 Announce Type: cross Abstract: Brain age prediction serves as a powerful framework for assessing brain status and detecting deviations associated with neurodevelopmental and neurodegenerative disorders. However, most existing approaches emphasize whole-brain age prediction and therefore overlook the pronounced regional heterogene",
    "url": "https://arxiv.org/abs/2602.06994",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Adaptive Temporal Dynamics for Personalized Emotion Recognition: A Liquid Neural Network Approach",
    "summary": "arXiv:2602.06997v1 Announce Type: cross Abstract: Emotion recognition from physiological signals remains challenging due to their non-stationary, noisy, and subject-dependent characteristics. This work presents, to the best of our knowledge, the first comprehensive application of liquid neural networks for EEG-based emotion recognition. The propose",
    "url": "https://arxiv.org/abs/2602.06997",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Hierarchical JEPA Meets Predictive Remote Control in Beyond 5G Networks",
    "summary": "arXiv:2602.07000v1 Announce Type: cross Abstract: In wireless networked control systems, ensuring timely and reliable state updates from distributed devices to remote controllers is essential for robust control performance. However, when multiple devices transmit high-dimensional states (e.g., images or video frames) over bandwidth-limited wireless",
    "url": "https://arxiv.org/abs/2602.07000",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Multi-Scale Temporal Homeostasis Enables Efficient and Robust Neural Networks",
    "summary": "arXiv:2602.07009v1 Announce Type: cross Abstract: Artificial neural networks achieve strong performance on benchmark tasks but remain fundamentally brittle under perturbations, limiting their deployment in real-world settings. In contrast, biological nervous systems sustain reliable function across decades through homeostatic regulation coordinated",
    "url": "https://arxiv.org/abs/2602.07009",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Learning Alzheimer's Disease Signatures by bridging EEG with Spiking Neural Networks and Biophysical Simulations",
    "summary": "arXiv:2602.07010v1 Announce Type: cross Abstract: As the prevalence of Alzheimer's disease (AD) rises, improving mechanistic insight from non-invasive biomarkers is increasingly critical. Recent work suggests that circuit-level brain alterations manifest as changes in electroencephalography (EEG) spectral features detectable by machine learning. Ho",
    "url": "https://arxiv.org/abs/2602.07010",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation",
    "summary": "arXiv:2602.07011v1 Announce Type: cross Abstract: As industrial manufacturing scales, automating fine-grained product image analysis has become critical for quality control. However, existing approaches are hindered by limited dataset coverage and poor model generalization across diverse and complex anomaly patterns. To address these challenges, we",
    "url": "https://arxiv.org/abs/2602.07011",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "A General Model for Retinal Segmentation and Quantification",
    "summary": "arXiv:2602.07012v1 Announce Type: cross Abstract: Retinal imaging is fast, non-invasive, and widely available, offering quantifiable structural and vascular signals for ophthalmic and systemic health assessment. This accessibility creates an opportunity to study how quantitative retinal phenotypes relate to ocular and systemic diseases. However, su",
    "url": "https://arxiv.org/abs/2602.07012",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models",
    "summary": "arXiv:2602.07013v1 Announce Type: cross Abstract: With the rapid advancement of Vision Language Models (VLMs), refusal mechanisms have become a critical component for ensuring responsible and safe model behavior. However, existing refusal strategies are largely \\textit{one-size-fits-all} and fail to adapt to diverse user needs and contextual constr",
    "url": "https://arxiv.org/abs/2602.07013",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation",
    "summary": "arXiv:2602.07014v1 Announce Type: cross Abstract: In-Image Machine Translation (IIMT) powers cross-border e-commerce product listings; existing research focuses on machine translation evaluation, while visual rendering quality is critical for user engagement. When facing context-dense product imagery and multimodal defects, current reference-based ",
    "url": "https://arxiv.org/abs/2602.07014",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "XAI-CLIP: ROI-Guided Perturbation Framework for Explainable Medical Image Segmentation in Multimodal Vision-Language Models",
    "summary": "arXiv:2602.07017v1 Announce Type: cross Abstract: Medical image segmentation is a critical component of clinical workflows, enabling accurate diagnosis, treatment planning, and disease monitoring. However, despite the superior performance of transformer-based models over convolutional architectures, their limited interpretability remains a major ob",
    "url": "https://arxiv.org/abs/2602.07017",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "AI for Sustainable Data Protection and Fair Algorithmic Management in Environmental Regulation",
    "summary": "arXiv:2602.07021v1 Announce Type: cross Abstract: Integration of AI into environmental regulation represents a significant advancement in data management. It offers promising results in both data protection plus algorithmic fairness. This research addresses the critical need for sustainable data protection in the era of ever evolving cyber threats.",
    "url": "https://arxiv.org/abs/2602.07021",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Behavioral Consistency Validation for LLM Agents: An Analysis of Trading-Style Switching through Stock-Market Simulation",
    "summary": "arXiv:2602.07023v1 Announce Type: cross Abstract: Recent works have increasingly applied Large Language Models (LLMs) as agents in financial stock market simulations to test if micro-level behaviors aggregate into macro-level phenomena. However, a crucial question arises: Do LLM agents' behaviors align with real market participants? This alignment ",
    "url": "https://arxiv.org/abs/2602.07023",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "The Geometry of Representational Failures in Vision Language Models",
    "summary": "arXiv:2602.07025v1 Announce Type: cross Abstract: Vision-Language Models (VLMs) exhibit puzzling failures in multi-object visual tasks, such as hallucinating non-existent elements or failing to identify the most similar objects among distractions. While these errors mirror human cognitive constraints, such as the \"Binding Problem\", the internal mec",
    "url": "https://arxiv.org/abs/2602.07025",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models",
    "summary": "arXiv:2602.07026v1 Announce Type: cross Abstract: Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this ",
    "url": "https://arxiv.org/abs/2602.07026",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "A Comparative Study of Adversarial Robustness in CNN and CNN-ANFIS Architectures",
    "summary": "arXiv:2602.07028v1 Announce Type: cross Abstract: Convolutional Neural Networks (CNNs) achieve strong image classification performance but lack interpretability and are vulnerable to adversarial attacks. Neuro-fuzzy hybrids such as DCNFIS replace fully connected CNN classifiers with Adaptive Neuro-Fuzzy Inference Systems (ANFIS) to improve interpre",
    "url": "https://arxiv.org/abs/2602.07028",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Lagged backward-compatible physics-informed neural networks for unsaturated soil consolidation analysis",
    "summary": "arXiv:2602.07031v1 Announce Type: cross Abstract: This study develops a Lagged Backward-Compatible Physics-Informed Neural Network (LBC-PINN) for simulating and inverting one-dimensional unsaturated soil consolidation under long-term loading. To address the challenges of coupled air and water pressure dissipation across multi-scale time domains, th",
    "url": "https://arxiv.org/abs/2602.07031",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MENASpeechBank: A Reference Voice Bank with Persona-Conditioned Multi-Turn Conversations for AudioLLMs",
    "summary": "arXiv:2602.07036v1 Announce Type: cross Abstract: Audio large language models (AudioLLMs) enable instruction-following over speech and general audio, but progress is increasingly limited by the lack of diverse, conversational, instruction-aligned speech-text data. This bottleneck is especially acute for persona-grounded interactions and dialectal c",
    "url": "https://arxiv.org/abs/2602.07036",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Stochastic Spiking Neuron Based SNN Can be Inherently Bayesian",
    "summary": "arXiv:2602.07037v1 Announce Type: cross Abstract: Uncertainty in biological neural systems appears to be computationally beneficial rather than detrimental. However, in neuromorphic computing systems, device variability often limits performance, including accuracy and efficiency. In this work, we propose a spiking Bayesian neural network (SBNN) fra",
    "url": "https://arxiv.org/abs/2602.07037",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "When Excellence Stops Producing Knowledge: A Practitioner's Observation on Research Funding",
    "summary": "arXiv:2602.07039v1 Announce Type: cross Abstract: After almost four decades of participating in competitive research funding -- as applicant, coordinator, evaluator, and panel member -- I have come to see a structural paradox: many participants recognize that the current system is approaching its functional limits, yet most reform measures intensif",
    "url": "https://arxiv.org/abs/2602.07039",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging",
    "summary": "arXiv:2602.07044v1 Announce Type: cross Abstract: Pipeline integrity is critical to industrial safety and environmental protection, with Magnetic Flux Leakage (MFL) detection being a primary non-destructive testing technology. Despite the promise of deep learning for automating MFL interpretation, progress toward reliable models has been constraine",
    "url": "https://arxiv.org/abs/2602.07044",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing",
    "summary": "arXiv:2602.07045v1 Announce Type: cross Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for co",
    "url": "https://arxiv.org/abs/2602.07045",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Interpreting Physics in Video World Models",
    "summary": "arXiv:2602.07050v1 Announce Type: cross Abstract: A long-standing question in physical reasoning is whether video-based models need to rely on factorized representations of physical variables in order to make physically accurate predictions, or whether they can implicitly represent such variables in a task-specific, distributed manner. While modern",
    "url": "https://arxiv.org/abs/2602.07050",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Neural Sentinel: Unified Vision Language Model (VLM) for License Plate Recognition with Human-in-the-Loop Continual Learning",
    "summary": "arXiv:2602.07051v1 Announce Type: cross Abstract: Traditional Automatic License Plate Recognition (ALPR) systems employ multi-stage pipelines consisting of object detection networks followed by separate Optical Character Recognition (OCR) modules, introducing compounding errors, increased latency, and architectural complexity. This research present",
    "url": "https://arxiv.org/abs/2602.07051",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MTS-CSNet: Multiscale Tensor Factorization for Deep Compressive Sensing on RGB Images",
    "summary": "arXiv:2602.07056v1 Announce Type: cross Abstract: Deep learning based compressive sensing (CS) methods typically learn sampling operators using convolutional or block wise fully connected layers, which limit receptive fields and scale poorly for high dimensional data. We propose MTSCSNet, a CS framework based on Multiscale Tensor Summation (MTS) fa",
    "url": "https://arxiv.org/abs/2602.07056",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "FADE: Selective Forgetting via Sparse LoRA and Self-Distillation",
    "summary": "arXiv:2602.07058v1 Announce Type: cross Abstract: Machine Unlearning aims to remove the influence of specific data or concepts from trained models while preserving overall performance, a capability increasingly required by data protection regulations and responsible AI practices. Despite recent progress, unlearning in text-to-image diffusion models",
    "url": "https://arxiv.org/abs/2602.07058",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Assessing Reproducibility in Evolutionary Computation: A Case Study using Human- and LLM-based Assessment",
    "summary": "arXiv:2602.07059v1 Announce Type: cross Abstract: Reproducibility is an important requirement in evolutionary computation, where results largely depend on computational experiments. In practice, reproducibility relies on how algorithms, experimental protocols, and artifacts are documented and shared. Despite growing awareness, there is still limite",
    "url": "https://arxiv.org/abs/2602.07059",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "TACIT: Transformation-Aware Capturing of Implicit Thought",
    "summary": "arXiv:2602.07061v1 Announce Type: cross Abstract: We present TACIT (Transformation-Aware Capturing of Implicit Thought), a diffusion-based transformer for interpretable visual reasoning. Unlike language-based reasoning systems, TACIT operates entirely in pixel space using rectified flow, enabling direct visualization of the reasoning process at eac",
    "url": "https://arxiv.org/abs/2602.07061",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Video-based Music Generation",
    "summary": "arXiv:2602.07063v1 Announce Type: cross Abstract: As the volume of video content on the internet grows rapidly, finding a suitable soundtrack remains a significant challenge. This thesis presents EMSYNC (EMotion and SYNChronization), a fast, free, and automatic solution that generates music tailored to the input video, enabling content creators to ",
    "url": "https://arxiv.org/abs/2602.07063",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MRI Cross-Modal Synthesis: A Comparative Study of Generative Models for T1-to-T2 Reconstruction",
    "summary": "arXiv:2602.07068v1 Announce Type: cross Abstract: MRI cross-modal synthesis involves generating images from one acquisition protocol using another, offering considerable clinical value by reducing scan time while maintaining diagnostic information. This paper presents a comprehensive comparison of three state-of-the-art generative models for T1-to-",
    "url": "https://arxiv.org/abs/2602.07068",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Bidirectional Reward-Guided Diffusion for Real-World Image Super-Resolution",
    "summary": "arXiv:2602.07069v1 Announce Type: cross Abstract: Diffusion-based super-resolution can synthesize rich details, but models trained on synthetic paired data often fail on real-world LR images due to distribution shifts. We propose Bird-SR, a bidirectional reward-guided diffusion framework that formulates super-resolution as trajectory-level preferen",
    "url": "https://arxiv.org/abs/2602.07069",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Artificial Intelligence in Open Source Software Engineering: A Foundation for Sustainability",
    "summary": "arXiv:2602.07071v1 Announce Type: cross Abstract: Open-source software (OSS) is foundational to modern digital infrastructure, yet this context for group work continues to struggle to ensure sufficient contributions in many critical cases. This literature review explores how artificial intelligence (AI) is being leveraged to address critical challe",
    "url": "https://arxiv.org/abs/2602.07071",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Pro-ZD: A Transferable Graph Neural Network Approach for Proactive Zero-Day Threats Mitigation",
    "summary": "arXiv:2602.07073v1 Announce Type: cross Abstract: In today's enterprise network landscape, the combination of perimeter and distributed firewall rules governs connectivity. To address challenges arising from increased traffic and diverse network architectures, organizations employ automated tools for firewall rule and access policy generation. Yet,",
    "url": "https://arxiv.org/abs/2602.07073",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning",
    "summary": "arXiv:2602.07075v1 Announce Type: cross Abstract: Chemical large language models (LLMs) predominantly rely on explicit Chain-of-Thought (CoT) in natural language to perform complex reasoning. However, chemical reasoning is inherently continuous and structural, and forcing it into discrete linguistic tokens introduces a fundamental representation mi",
    "url": "https://arxiv.org/abs/2602.07075",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "CALM: Class-Conditional Sparse Attention Vectors for Large Audio-Language Models",
    "summary": "arXiv:2602.07077v1 Announce Type: cross Abstract: Large audio-language models (LALMs) exhibit strong zero-shot capabilities in multiple downstream tasks, such as audio question answering (AQA) and abstract reasoning; however, these models still lag behind specialized models for certain discriminative tasks (e.g., audio classification). Recent studi",
    "url": "https://arxiv.org/abs/2602.07077",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "The Optimal Token Baseline: Variance Reduction for Long-Horizon LLM-RL",
    "summary": "arXiv:2602.07078v1 Announce Type: cross Abstract: Reinforcement Learning (RL) for Large Language Models (LLMs) often suffers from training collapse in long-horizon tasks due to exploding gradient variance. To mitigate this, a baseline is commonly introduced for advantage computation; however, traditional value models remain difficult to optimize, a",
    "url": "https://arxiv.org/abs/2602.07078",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs",
    "summary": "arXiv:2602.07080v1 Announce Type: cross Abstract: Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model's own capabilities. This raises a fundamental, yet unexplored question: Can an LLM's functional corre",
    "url": "https://arxiv.org/abs/2602.07080",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Federated Prompt-Tuning with Heterogeneous and Incomplete Multimodal Client Data",
    "summary": "arXiv:2602.07081v1 Announce Type: cross Abstract: This paper introduces a generalized federated prompt-tuning framework for practical scenarios where local datasets are multi-modal and exhibit different distributional patterns of missing features at the input level. The proposed framework bridges the gap between federated learning and multi-modal p",
    "url": "https://arxiv.org/abs/2602.07081",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation",
    "summary": "arXiv:2602.07082v1 Announce Type: cross Abstract: When embodied AI is expanding from traditional object detection and recognition to more advanced tasks of robot manipulation and actuation planning, visual spatial reasoning from the video inputs is necessary to perceive the spatial relationships of objects and guide device actions. However, existin",
    "url": "https://arxiv.org/abs/2602.07082",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Rethinking Scientific Modeling: Toward Physically Consistent and Simulation-Executable Programmatic Generation",
    "summary": "arXiv:2602.07083v1 Announce Type: cross Abstract: Structural modeling is a fundamental component of computational engineering science, in which even minor physical inconsistencies or specification violations may invalidate downstream simulations. The potential of large language models (LLMs) for automatic generation of modeling code has been demons",
    "url": "https://arxiv.org/abs/2602.07083",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "AbFlow : End-to-end Paratope-Centric Antibody Design by Interaction Enhanced Flow Matching",
    "summary": "arXiv:2602.07084v1 Announce Type: cross Abstract: Antigen-antibody binding is a critical process in the immune response. Although recent progress has advanced antibody design, current methods lack a generative framework for end-to-end modeling of full-atom antibody structures and struggle to fully exploit antigen-specific geometric information for ",
    "url": "https://arxiv.org/abs/2602.07084",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining",
    "summary": "arXiv:2602.07085v1 Announce Type: cross Abstract: Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated exper",
    "url": "https://arxiv.org/abs/2602.07085",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Evaluating Retrieval-Augmented Generation Variants for Natural Language-Based SQL and API Call Generation",
    "summary": "arXiv:2602.07086v1 Announce Type: cross Abstract: Enterprise systems increasingly require natural language interfaces that can translate user requests into structured operations such as SQL queries and REST API calls. While large language models (LLMs) show promise for code generation [Chen et al., 2021; Huynh and Lin, 2025], their effectiveness in",
    "url": "https://arxiv.org/abs/2602.07086",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Electron-Informed Coarse-Graining Molecular Representation Learning for Real-World Molecular Physics",
    "summary": "arXiv:2602.07087v1 Announce Type: cross Abstract: Various representation learning methods for molecular structures have been devised to accelerate data-driven chemistry. However, the representation capabilities of existing methods are essentially limited to atom-level information, which is not sufficient to describe real-world molecular physics. Al",
    "url": "https://arxiv.org/abs/2602.07087",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Concept-Aware Privacy Mechanisms for Defending Embedding Inversion Attacks",
    "summary": "arXiv:2602.07090v1 Announce Type: cross Abstract: Text embeddings enable numerous NLP applications but face severe privacy risks from embedding inversion attacks, which can expose sensitive attributes or reconstruct raw text. Existing differential privacy defenses assume uniform sensitivity across embedding dimensions, leading to excessive noise an",
    "url": "https://arxiv.org/abs/2602.07090",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Lemon Agent Technical Report",
    "summary": "arXiv:2602.07092v1 Announce Type: cross Abstract: Recent advanced LLM-powered agent systems have exhibited their remarkable capabilities in tackling complex, long-horizon tasks. Nevertheless, they still suffer from inherent limitations in resource efficiency, context management, and multimodal perception. Based on these observations, Lemon Agent is",
    "url": "https://arxiv.org/abs/2602.07092",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "WorldEdit: Towards Open-World Image Editing with a Knowledge-Informed Benchmark",
    "summary": "arXiv:2602.07095v1 Announce Type: cross Abstract: Recent advances in image editing models have demonstrated remarkable capabilities in executing explicit instructions, such as attribute manipulation, style transfer, and pose synthesis. However, these models often face challenges when dealing with implicit editing instructions, which describe the ca",
    "url": "https://arxiv.org/abs/2602.07095",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "RealFin: How Well Do LLMs Reason About Finance When Users Leave Things Unsaid?",
    "summary": "arXiv:2602.07096v1 Announce Type: cross Abstract: Reliable financial reasoning requires knowing not only how to answer, but also when an answer cannot be justified. In real financial practice, problems often rely on implicit assumptions that are taken for granted rather than stated explicitly, causing problems to appear solvable while lacking enoug",
    "url": "https://arxiv.org/abs/2602.07096",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Fast and Robust Likelihood-Guided Diffusion Posterior Sampling with Amortized Variational Inference",
    "summary": "arXiv:2602.07102v1 Announce Type: cross Abstract: Zero-shot diffusion posterior sampling offers a flexible framework for inverse problems by accommodating arbitrary degradation operators at test time, but incurs high computational cost due to repeated likelihood-guided updates. In contrast, previous amortized diffusion approaches enable fast infere",
    "url": "https://arxiv.org/abs/2602.07102",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "scDFM: Distributional Flow Matching Model for Robust Single-Cell Perturbation Prediction",
    "summary": "arXiv:2602.07103v1 Announce Type: cross Abstract: A central goal in systems biology and drug discovery is to predict the transcriptional response of cells to perturbations. This task is challenging due to the noisy and sparse nature of single-cell measurements, as well as the fact that perturbations often induce population-level shifts rather than ",
    "url": "https://arxiv.org/abs/2602.07103",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Extended to Reality: Prompt Injection in 3D Environments",
    "summary": "arXiv:2602.07104v1 Announce Type: cross Abstract: Multimodal large language models (MLLMs) have advanced the capabilities to interpret and act on visual input in 3D environments, empowering diverse applications such as robotics and situated conversational agents. When MLLMs reason over camera-captured views of the physical world, a new attack surfa",
    "url": "https://arxiv.org/abs/2602.07104",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models",
    "summary": "arXiv:2602.07106v1 Announce Type: cross Abstract: Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-",
    "url": "https://arxiv.org/abs/2602.07106",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "ShallowJail: Steering Jailbreaks against Large Language Models",
    "summary": "arXiv:2602.07107v1 Announce Type: cross Abstract: Large Language Models(LLMs) have been successful in numerous fields. Alignment has usually been applied to prevent them from harmful purposes. However, aligned LLMs remain vulnerable to jailbreak attacks that deliberately mislead them into producing harmful outputs. Existing jailbreaks are either bl",
    "url": "https://arxiv.org/abs/2602.07107",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Reasoning-Augmented Representations for Multimodal Retrieval",
    "summary": "arXiv:2602.07125v1 Announce Type: cross Abstract: Universal Multimodal Retrieval (UMR) seeks any-to-any search across text and vision, yet modern embedding models remain brittle when queries require latent reasoning (e.g., resolving underspecified references or matching compositional constraints). We argue this brittleness is often data-induced: wh",
    "url": "https://arxiv.org/abs/2602.07125",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Landscaper: Understanding Loss Landscapes Through Multi-Dimensional Topological Analysis",
    "summary": "arXiv:2602.07135v1 Announce Type: cross Abstract: Loss landscapes are a powerful tool for understanding neural network optimization and generalization, yet traditional low-dimensional analyses often miss complex topological features. We present Landscaper, an open-source Python package for arbitrary-dimensional loss landscape analysis. Landscaper c",
    "url": "https://arxiv.org/abs/2602.07135",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Exploring Teachers' Perspectives on Using Conversational AI Agents for Group Collaboration",
    "summary": "arXiv:2602.07142v1 Announce Type: cross Abstract: Collaboration is a cornerstone of 21st-century learning, yet teachers continue to face challenges in supporting productive peer interaction. Emerging generative AI tools offer new possibilities for scaffolding collaboration, but their role in mediating in-person group work remains underexplored, esp",
    "url": "https://arxiv.org/abs/2602.07142",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "BONSAI: Bayesian Optimization with Natural Simplicity and Interpretability",
    "summary": "arXiv:2602.07144v1 Announce Type: cross Abstract: Bayesian optimization (BO) is a popular technique for sample-efficient optimization of black-box functions. In many applications, the parameters being tuned come with a carefully engineered default configuration, and practitioners only want to deviate from this default when necessary. Standard BO, h",
    "url": "https://arxiv.org/abs/2602.07144",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "On Randomness in Agentic Evals",
    "summary": "arXiv:2602.07150v1 Announce Type: cross Abstract: Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Ben",
    "url": "https://arxiv.org/abs/2602.07150",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Beyond Pooling: Matching for Robust Generalization under Data Heterogeneity",
    "summary": "arXiv:2602.07154v1 Announce Type: cross Abstract: Pooling heterogeneous datasets across domains is a common strategy in representation learning, but naive pooling can amplify distributional asymmetries and yield biased estimators, especially in settings where zero-shot generalization is required. We propose a matching framework that selects samples",
    "url": "https://arxiv.org/abs/2602.07154",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Mimetic Initialization of MLPs",
    "summary": "arXiv:2602.07156v1 Announce Type: cross Abstract: Mimetic initialization uses pretrained models as case studies of good initialization, using observations of structures in trained weights to inspire new, simple initialization techniques. So far, it has been applied only to spatial mixing layers, such convolutional, self-attention, and state space l",
    "url": "https://arxiv.org/abs/2602.07156",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Free Energy Mixer",
    "summary": "arXiv:2602.07160v1 Announce Type: cross Abstract: Standard attention stores keys/values losslessly but reads them via a per-head convex average, blocking channel-wise selection. We propose the Free Energy Mixer (FEM): a free-energy (log-sum-exp) read that applies a value-driven, per-channel log-linear tilt to a fast prior (e.g., from queries/keys i",
    "url": "https://arxiv.org/abs/2602.07160",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Your Language Model Secretly Contains Personality Subnetworks",
    "summary": "arXiv:2602.07164v1 Announce Type: cross Abstract: Humans shift between different personas depending on social context. Large Language Models (LLMs) demonstrate a similar flexibility in adopting different personas and behaviors. Existing approaches, however, typically adapt such behavior through external knowledge such as prompting, retrieval-augmen",
    "url": "https://arxiv.org/abs/2602.07164",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Open TutorAI: An Open-source Platform for Personalized and Immersive Learning with Generative AI",
    "summary": "arXiv:2602.07176v1 Announce Type: cross Abstract: Recent advances in artificial intelligence have created new possibilities for making education more scalable, adaptive, and learner-centered. However, existing educational chatbot systems often lack contextual adaptability, real-time responsiveness, and pedagogical agility. which can limit learner e",
    "url": "https://arxiv.org/abs/2602.07176",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "An Information-Theoretic Framework for Comparing Voice and Text Explainability",
    "summary": "arXiv:2602.07179v1 Announce Type: cross Abstract: Explainable Artificial Intelligence (XAI) aims to make machine learning models transparent and trustworthy, yet most current approaches communicate explanations visually or through text. This paper introduces an information theoretic framework for analyzing how explanation modality specifically, voi",
    "url": "https://arxiv.org/abs/2602.07179",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Long-Context Long-Form Question Answering for Legal Domain",
    "summary": "arXiv:2602.07190v1 Announce Type: cross Abstract: Legal documents have complex document layouts involving multiple nested sections, lengthy footnotes and further use specialized linguistic devices like intricate syntax and domain-specific vocabulary to ensure precision and authority. These inherent characteristics of legal documents make question a",
    "url": "https://arxiv.org/abs/2602.07190",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "\"Death\" of a Chatbot: Investigating and Designing Toward Psychologically Safe Endings for Human-AI Relationships",
    "summary": "arXiv:2602.07193v2 Announce Type: cross Abstract: Millions of users form emotional attachments to AI companions like Character AI, Replika, and ChatGPT. When these relationships end through model updates, safety interventions, or platform shutdowns, users receive no closure, reporting grief comparable to human loss. As regulations mandate protectio",
    "url": "https://arxiv.org/abs/2602.07193",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "BadSNN: Backdoor Attacks on Spiking Neural Networks via Adversarial Spiking Neuron",
    "summary": "arXiv:2602.07200v1 Announce Type: cross Abstract: Spiking Neural Networks (SNNs) are energy-efficient counterparts of Deep Neural Networks (DNNs) with high biological plausibility, as information is transmitted through temporal spiking patterns. The core element of an SNN is the spiking neuron, which converts input data into spikes following the Le",
    "url": "https://arxiv.org/abs/2602.07200",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Exactly Computing do-Shapley Values",
    "summary": "arXiv:2602.07203v1 Announce Type: cross Abstract: Structural Causal Models (SCM) are a powerful framework for describing complicated dynamics across the natural sciences. A particularly elegant way of interpreting SCMs is do-Shapley, a game-theoretic method of quantifying the average effect of $d$ variables across exponentially many interventions. ",
    "url": "https://arxiv.org/abs/2602.07203",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "DSL: Understanding and Improving Softmax Recommender Systems with Competition-Aware Scaling",
    "summary": "arXiv:2602.07206v1 Announce Type: cross Abstract: Softmax Loss (SL) is being increasingly adopted for recommender systems (RS) as it has demonstrated better performance, robustness and fairness. Yet in implicit-feedback, a single global temperature and equal treatment of uniformly sampled negatives can lead to brittle training, because sampled sets",
    "url": "https://arxiv.org/abs/2602.07206",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Multimodal Enhancement of Sequential Recommendation",
    "summary": "arXiv:2602.07207v1 Announce Type: cross Abstract: We propose a novel recommender framework, MuSTRec (Multimodal and Sequential Transformer-based Recommendation), that unifies multimodal and sequential recommendation paradigms. MuSTRec captures cross-item similarities and collaborative filtering signals, by building item-item graphs from extracted t",
    "url": "https://arxiv.org/abs/2602.07207",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Sequences as Nodes for Contrastive Multimodal Graph Recommendation",
    "summary": "arXiv:2602.07208v1 Announce Type: cross Abstract: To tackle cold-start and data sparsity issues in recommender systems, numerous multimodal, sequential, and contrastive techniques have been proposed. While these augmentations can boost recommendation performance, they tend to add noise and disrupt useful semantics. To address this, we propose MuSIC",
    "url": "https://arxiv.org/abs/2602.07208",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Multi-Agentic AI for Fairness-Aware and Accelerated Multi-modal Large Model Inference in Real-world Mobile Edge Networks",
    "summary": "arXiv:2602.07215v1 Announce Type: cross Abstract: Generative AI (GenAI) has transformed applications in natural language processing and content creation, yet centralized inference remains hindered by high latency, limited customizability, and privacy concerns. Deploying large models (LMs) in mobile edge networks emerges as a promising solution. How",
    "url": "https://arxiv.org/abs/2602.07215",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Collaborative and Efficient Fine-tuning: Leveraging Task Similarity",
    "summary": "arXiv:2602.07218v1 Announce Type: cross Abstract: Adaptability has been regarded as a central feature in the foundation models, enabling them to effectively acclimate to unseen downstream tasks. Parameter-efficient fine-tuning methods such as celebrated LoRA facilitate efficient adaptation of large foundation models using labeled, high-quality and ",
    "url": "https://arxiv.org/abs/2602.07218",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "The Median is Easier than it Looks: Approximation with a Constant-Depth, Linear-Width ReLU Network",
    "summary": "arXiv:2602.07219v1 Announce Type: cross Abstract: We study the approximation of the median of $d$ inputs using ReLU neural networks. We present depth-width tradeoffs under several settings, culminating in a constant-depth, linear-width construction that achieves exponentially small approximation error with respect to the uniform distribution over t",
    "url": "https://arxiv.org/abs/2602.07219",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "ArcMark: Multi-bit LLM Watermark via Optimal Transport",
    "summary": "arXiv:2602.07235v1 Announce Type: cross Abstract: Watermarking is an important tool for promoting the responsible use of language models (LMs). Existing watermarks insert a signal into generated tokens that either flags LM-generated text (zero-bit watermarking) or encodes more complex messages (multi-bit watermarking). Though a number of recent mul",
    "url": "https://arxiv.org/abs/2602.07235",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Realistic Synthetic Household Data Generation at Scale",
    "summary": "arXiv:2602.07243v1 Announce Type: cross Abstract: Advancements in foundation models have catalyzed research in Embodied AI to develop interactive agents capable of environmental reasoning and interaction. Developing such agents requires diverse, large-scale datasets. Prior frameworks generate synthetic data for long-term human-robot interactions bu",
    "url": "https://arxiv.org/abs/2602.07243",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models",
    "summary": "arXiv:2602.07251v1 Announce Type: cross Abstract: Data-driven super-resolution (SR) methods are often integrated into imaging pipelines as preprocessing steps to improve downstream tasks such as classification and detection. However, these SR models introduce a previously unexplored attack surface into imaging pipelines. In this paper, we present A",
    "url": "https://arxiv.org/abs/2602.07251",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Graph homophily booster: Reimagining the role of discrete features in heterophilic graph learning",
    "summary": "arXiv:2602.07256v1 Announce Type: cross Abstract: Graph neural networks (GNNs) have emerged as a powerful tool for modeling graph-structured data. However, existing GNNs often struggle with heterophilic graphs, where connected nodes tend to have dissimilar features or labels. While numerous methods have been proposed to address this challenge, they",
    "url": "https://arxiv.org/abs/2602.07256",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Cognitive algorithms and systems of episodic memory, semantic memory and their learnings",
    "summary": "arXiv:2602.07261v1 Announce Type: cross Abstract: Declarative memory, the memory that can be \"declared\" in words or languages, is made up of two dissociated parts: episodic memory and semantic memory. This dissociation has its neuroanatomical basis episodic memory is mostly associated with the hippocampus and semantic memory with the neocortex. The",
    "url": "https://arxiv.org/abs/2602.07261",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "aerial-autonomy-stack -- a Faster-than-real-time, Autopilot-agnostic, ROS2 Framework to Simulate and Deploy Perception-based Drones",
    "summary": "arXiv:2602.07264v1 Announce Type: cross Abstract: Unmanned aerial vehicles are rapidly transforming multiple applications, from agricultural and infrastructure monitoring to logistics and defense. Introducing greater autonomy to these systems can simultaneously make them more effective as well as reliable. Thus, the ability to rapidly engineer and ",
    "url": "https://arxiv.org/abs/2602.07264",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "XShare: Collaborative in-Batch Expert Sharing for Faster MoE Inference",
    "summary": "arXiv:2602.07265v1 Announce Type: cross Abstract: Mixture-of-Experts (MoE) architectures are increasingly used to efficiently scale large language models. However, in production inference, request batching and speculative decoding significantly amplify expert activation, eroding these efficiency benefits. We address this issue by modeling batch-awa",
    "url": "https://arxiv.org/abs/2602.07265",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Laplacian-LoRA: Delaying Oversmoothing in Deep GCNs via Spectral Low-Rank Adaptation",
    "summary": "arXiv:2602.07278v1 Announce Type: cross Abstract: Oversmoothing is a fundamental limitation of deep graph convolutional networks (GCNs), causing node representations to collapse as depth increases. While many prior approaches mitigate this effect through architectural modifications or residual mechanisms, the underlying spectral cause of oversmooth",
    "url": "https://arxiv.org/abs/2602.07278",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Imagining the Alien: Human Projections and Cognitive Limitations",
    "summary": "arXiv:2602.07284v1 Announce Type: cross Abstract: Imagining what life on other planets, and intelligent life in particular, may be like is a long-running theme in human culture. It is a manifestation of the innate human curiosity about the Cosmos, and it has inspired numerous works of art and folklore, including whole literary and other media genre",
    "url": "https://arxiv.org/abs/2602.07284",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Fin-RATE: A Real-world Financial Analytics and Tracking Evaluation Benchmark for LLMs on SEC Filings",
    "summary": "arXiv:2602.07294v1 Announce Type: cross Abstract: With increasing deployment of Large Language Models (LLMs) in the finance domain, LLMs are increasingly expected to parse complex regulatory disclosures. However, existing benchmarks often focus on isolated details, failing to reflect the complexity of professional analysis that requires synthesizin",
    "url": "https://arxiv.org/abs/2602.07294",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Progressive Searching for Retrieval in RAG",
    "summary": "arXiv:2602.07297v1 Announce Type: cross Abstract: Retrieval Augmented Generation (RAG) is a promising technique for mitigating two key limitations of large language models (LLMs): outdated information and hallucinations. RAG system stores documents as embedding vectors in a database. Given a query, search is executed to find the most related docume",
    "url": "https://arxiv.org/abs/2602.07297",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Principled Synthetic Data Enables the First Scaling Laws for LLMs in Recommendation",
    "summary": "arXiv:2602.07298v1 Announce Type: cross Abstract: Large Language Models (LLMs) represent a promising frontier for recommender systems, yet their development has been impeded by the absence of predictable scaling laws, which are crucial for guiding research and optimizing resource allocation. We hypothesize that this may be attributed to the inheren",
    "url": "https://arxiv.org/abs/2602.07298",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "KRONE: Hierarchical and Modular Log Anomaly Detection",
    "summary": "arXiv:2602.07303v1 Announce Type: cross Abstract: Log anomaly detection is crucial for uncovering system failures and security risks. Although logs originate from nested component executions with clear boundaries, this structure is lost when they are stored as flat sequences. As a result, state-of-the-art methods risk missing true dependencies with",
    "url": "https://arxiv.org/abs/2602.07303",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "LIT-GRAPH: Evaluating Deep vs. Shallow Graph Embeddings for High-Quality Text Recommendation in Domain-Specific Knowledge Graphs",
    "summary": "arXiv:2602.07307v1 Announce Type: cross Abstract: This study presents LIT-GRAPH (Literature Graph for Recommendation and Pedagogical Heuristics), a novel knowledge graph-based recommendation system designed to scaffold high school English teachers in selecting diverse, pedagogically aligned instructional literature. The system is built upon an onto",
    "url": "https://arxiv.org/abs/2602.07307",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Semantic Search At LinkedIn",
    "summary": "arXiv:2602.07309v1 Announce Type: cross Abstract: Semantic search with large language models (LLMs) enables retrieval by meaning rather than keyword overlap, but scaling it requires major inference efficiency advances. We present LinkedIn's LLM-based semantic search framework for AI Job Search and AI People Search, combining an LLM relevance judge,",
    "url": "https://arxiv.org/abs/2602.07309",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery",
    "summary": "arXiv:2602.07311v1 Announce Type: cross Abstract: Sparse autoencoders (SAEs) offer a natural path toward comparable explanations across different representation spaces. However, current SAEs are trained per modality, producing dictionaries whose features are not directly understandable and whose explanations do not transfer across domains. In this ",
    "url": "https://arxiv.org/abs/2602.07311",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Beyond Accuracy: Risk-Sensitive Evaluation of Hallucinated Medical Advice",
    "summary": "arXiv:2602.07319v1 Announce Type: cross Abstract: Large language models are increasingly being used in patient-facing medical question answering, where hallucinated outputs can vary widely in potential harm. However, existing hallucination standards and evaluation metrics focus primarily on factual correctness, treating all errors as equally severe",
    "url": "https://arxiv.org/abs/2602.07319",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Action-to-Action Flow Matching",
    "summary": "arXiv:2602.07322v1 Announce Type: cross Abstract: Diffusion-based policies have recently achieved remarkable success in robotics by formulating action prediction as a conditional denoising process. However, the standard practice of sampling from random Gaussian noise often requires multiple iterative steps to produce clean actions, leading to high ",
    "url": "https://arxiv.org/abs/2602.07322",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "High Fidelity Textual User Representation over Heterogeneous Sources via Reinforcement Learning",
    "summary": "arXiv:2602.07333v1 Announce Type: cross Abstract: Effective personalization on large-scale job platforms requires modeling members based on heterogeneous textual sources, including profiles, professional data, and search activity logs. As recommender systems increasingly adopt Large Language Models (LLMs), creating unified, interpretable, and conci",
    "url": "https://arxiv.org/abs/2602.07333",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Intent Mismatch Causes LLMs to Get Lost in Multi-Turn Conversation",
    "summary": "arXiv:2602.07338v1 Announce Type: cross Abstract: Multi-turn conversation has emerged as a predominant interaction paradigm for Large Language Models (LLMs). Users often employ follow-up questions to refine their intent, expecting LLMs to adapt dynamically. However, recent research reveals that LLMs suffer a substantial performance drop in multi-tu",
    "url": "https://arxiv.org/abs/2602.07338",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Seeing Roads Through Words: A Language-Guided Framework for RGB-T Driving Scene Segmentation",
    "summary": "arXiv:2602.07343v1 Announce Type: cross Abstract: Robust semantic segmentation of road scenes under adverse illumination, lighting, and shadow conditions remain a core challenge for autonomous driving applications. RGB-Thermal fusion is a standard approach, yet existing methods apply static fusion strategies uniformly across all conditions, allowin",
    "url": "https://arxiv.org/abs/2602.07343",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "TernaryLM: Memory-Efficient Language Modeling via Native 1-Bit Quantization with Adaptive Layer-wise Scaling",
    "summary": "arXiv:2602.07374v1 Announce Type: cross Abstract: Large language models (LLMs) achieve remarkable performance but demand substantial computational resources, limiting deployment on edge devices and resource-constrained environments. We present TernaryLM, a 132M parameter transformer architecture that employs native 1-bit ternary quantization {-1, 0",
    "url": "https://arxiv.org/abs/2602.07374",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Advantages of Domain Knowledge Injection for Legal Document Summarization: A Case Study on Summarizing Indian Court Judgments in English and Hindi",
    "summary": "arXiv:2602.07382v1 Announce Type: cross Abstract: Summarizing Indian legal court judgments is a complex task not only due to the intricate language and unstructured nature of the legal texts, but also since a large section of the Indian population does not understand the complex English in which legal text is written, thus requiring summaries in In",
    "url": "https://arxiv.org/abs/2602.07382",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Scout Before You Attend: Sketch-and-Walk Sparse Attention for Efficient LLM Inference",
    "summary": "arXiv:2602.07397v1 Announce Type: cross Abstract: Self-attention dominates the computational and memory cost of long-context LLM inference across both prefill and decode phases. To address this challenge, we introduce Sketch&amp;Walk Attention, a training-free sparse attention method that determines sparsity with lightweight sketches and determinis",
    "url": "https://arxiv.org/abs/2602.07397",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "AgentSys: Secure and Dynamic LLM Agents Through Explicit Hierarchical Memory Management",
    "summary": "arXiv:2602.07398v1 Announce Type: cross Abstract: Indirect prompt injection threatens LLM agents by embedding malicious instructions in external content, enabling unauthorized actions and data theft. LLM agents maintain working memory through their context window, which stores interaction history for decision-making. Conventional agents indiscrimin",
    "url": "https://arxiv.org/abs/2602.07398",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Learning Molecular Chirality via Chiral Determinant Kernels",
    "summary": "arXiv:2602.07415v1 Announce Type: cross Abstract: Chirality is a fundamental molecular property that governs stereospecific behavior in chemistry and biology. Capturing chirality in machine learning models remains challenging due to the geometric complexity of stereochemical relationships and the limitations of traditional molecular representations",
    "url": "https://arxiv.org/abs/2602.07415",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Secure Code Generation via Online Reinforcement Learning with Vulnerability Reward Model",
    "summary": "arXiv:2602.07422v1 Announce Type: cross Abstract: Large language models (LLMs) are increasingly used in software development, yet their tendency to generate insecure code remains a major barrier to real-world deployment. Existing secure code alignment methods often suffer from a functionality--security paradox, improving security at the cost of sub",
    "url": "https://arxiv.org/abs/2602.07422",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Brep2Shape: Boundary and Shape Representation Alignment via Self-Supervised Transformers",
    "summary": "arXiv:2602.07429v1 Announce Type: cross Abstract: Boundary representation (B-rep) is the industry standard for computer-aided design (CAD). While deep learning shows promise in processing B-rep models, existing methods suffer from a representation gap: continuous approaches offer analytical precision but are visually abstract, whereas discrete meth",
    "url": "https://arxiv.org/abs/2602.07429",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Multi-Agent Systems Shape Social Norms for Prosocial Behavior Change",
    "summary": "arXiv:2602.07433v1 Announce Type: cross Abstract: Social norm interventions are used promote prosocial behaviors by highlighting prevalent actions, but their effectiveness is often limited in heterogeneous populations where shared understandings of desirable behaviors are lacking. This study explores whether multi-agent systems can establish \"virtu",
    "url": "https://arxiv.org/abs/2602.07433",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots",
    "summary": "arXiv:2602.07434v1 Announce Type: cross Abstract: Effective human-robot interaction requires emotionally rich multimodal expressions, yet most humanoid robots lack coordinated speech, facial expressions, and gestures. Meanwhile, real-world deployment demands on-device solutions that can operate autonomously without continuous cloud connectivity. To",
    "url": "https://arxiv.org/abs/2602.07434",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control",
    "summary": "arXiv:2602.07439v1 Announce Type: cross Abstract: Recent advances in humanoid whole-body motion tracking have enabled the execution of diverse and highly coordinated motions on real hardware. However, existing controllers are commonly driven either by predefined motion trajectories, which offer limited flexibility when user intent changes, or by co",
    "url": "https://arxiv.org/abs/2602.07439",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Proximal Action Replacement for Behavior Cloning Actor-Critic in Offline Reinforcement Learning",
    "summary": "arXiv:2602.07441v1 Announce Type: cross Abstract: Offline reinforcement learning (RL) optimizes policies from a previously collected static dataset and is an important branch of RL. A popular and promising approach is to regularize actor-critic methods with behavior cloning (BC), which yields realistic policies and mitigates bias from out-of-distri",
    "url": "https://arxiv.org/abs/2602.07441",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Pull Requests as a Training Signal for Repo-Level Code Editing",
    "summary": "arXiv:2602.07457v1 Announce Type: cross Abstract: Repository-level code editing requires models to understand complex dependencies and execute precise multi-file modifications across a large codebase. While recent gains on SWE-bench rely heavily on complex agent scaffolding, it remains unclear how much of this capability can be internalised via hig",
    "url": "https://arxiv.org/abs/2602.07457",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Deriving Neural Scaling Laws from the statistics of natural language",
    "summary": "arXiv:2602.07488v1 Announce Type: cross Abstract: Despite the fact that experimental neural scaling laws have substantially guided empirical progress in large-scale machine learning, no existing theory can quantitatively predict the exponents of these important laws for any modern LLM trained on any natural language dataset. We provide the first su",
    "url": "https://arxiv.org/abs/2602.07488",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "VividFace: Real-Time and Realistic Facial Expression Shadowing for Humanoid Robots",
    "summary": "arXiv:2602.07506v1 Announce Type: cross Abstract: Humanoid facial expression shadowing enables robots to realistically imitate human facial expressions in real time, which is critical for lifelike, facially expressive humanoid robots and affective human-robot interaction. Existing progress in humanoid facial expression imitation remains limited, of",
    "url": "https://arxiv.org/abs/2602.07506",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MemPot: Defending Against Memory Extraction Attack with Optimized Honeypots",
    "summary": "arXiv:2602.07517v1 Announce Type: cross Abstract: Large Language Model (LLM)-based agents employ external and internal memory systems to handle complex, goal-oriented tasks, yet this exposes them to severe extraction attacks, and effective defenses remain lacking. In this paper, we propose MemPot, the first theoretically verified defense framework ",
    "url": "https://arxiv.org/abs/2602.07517",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MDL: A Unified Multi-Distribution Learner in Large-scale Industrial Recommendation through Tokenization",
    "summary": "arXiv:2602.07520v2 Announce Type: cross Abstract: Industrial recommender systems increasingly adopt multi-scenario learning (MSL) and multi-task learning (MTL) to handle diverse user interactions and contexts, but existing approaches suffer from two critical drawbacks: (1) underutilization of large-scale model parameters due to limited interaction ",
    "url": "https://arxiv.org/abs/2602.07520",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Fine-Grained Cat Breed Recognition with Global Context Vision Transformer",
    "summary": "arXiv:2602.07534v1 Announce Type: cross Abstract: Accurate identification of cat breeds from images is a challenging task due to subtle differences in fur patterns, facial structure, and color. In this paper, we present a deep learning-based approach for classifying cat breeds using a subset of the Oxford-IIIT Pet Dataset, which contains high-resol",
    "url": "https://arxiv.org/abs/2602.07534",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Beyond Core and Penumbra: Bi-Temporal Image-Driven Stroke Evolution Analysis",
    "summary": "arXiv:2602.07535v1 Announce Type: cross Abstract: Computed tomography perfusion (CTP) at admission is routinely used to estimate the ischemic core and penumbra, while follow-up diffusion-weighted MRI (DWI) provides the definitive infarct outcome. However, single time-point segmentations fail to capture the biological heterogeneity and temporal evol",
    "url": "https://arxiv.org/abs/2602.07535",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Linguistic properties and model scale in brain encoding: from small to compressed language models",
    "summary": "arXiv:2602.07547v1 Announce Type: cross Abstract: Recent work has shown that scaling large language models (LLMs) improves their alignment with human brain activity, yet it remains unclear what drives these gains and which representational properties are responsible. Although larger models often yield better task performance and brain alignment, th",
    "url": "https://arxiv.org/abs/2602.07547",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Revealing the Semantic Selection Gap in DINOv3 through Training-Free Few-Shot Segmentation",
    "summary": "arXiv:2602.07550v1 Announce Type: cross Abstract: Recent self-supervised Vision Transformers (ViTs), such as DINOv3, provide rich feature representations for dense vision tasks. This study investigates the intrinsic few-shot semantic segmentation (FSS) capabilities of frozen DINOv3 features through a training-free baseline, FSSDINO, utilizing class",
    "url": "https://arxiv.org/abs/2602.07550",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "VISOR: VIsual Spatial Object Reasoning for Language-driven Object Navigation",
    "summary": "arXiv:2602.07555v1 Announce Type: cross Abstract: Language-driven object navigation requires agents to interpret natural language descriptions of target objects, which combine intrinsic and extrinsic attributes for instance recognition and commonsense navigation. Existing methods either (i) use end-to-end trained models with vision-language embeddi",
    "url": "https://arxiv.org/abs/2602.07555",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Gaussian Match-and-Copy: A Minimalist Benchmark for Studying Transformer Induction",
    "summary": "arXiv:2602.07562v1 Announce Type: cross Abstract: Match-and-copy is a core retrieval primitive used at inference time by large language models to retrieve a matching token from the context then copy its successor. Yet, understanding how this behavior emerges on natural data is challenging because retrieval and memorization are entangled. To disenta",
    "url": "https://arxiv.org/abs/2602.07562",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Cross-Camera Cow Identification via Disentangled Representation Learning",
    "summary": "arXiv:2602.07566v1 Announce Type: cross Abstract: Precise identification of individual cows is a fundamental prerequisite for comprehensive digital management in smart livestock farming. While existing animal identification methods excel in controlled, single-camera settings, they face severe challenges regarding cross-camera generalization. When m",
    "url": "https://arxiv.org/abs/2602.07566",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "How does longer temporal context enhance multimodal narrative video processing in the brain?",
    "summary": "arXiv:2602.07570v1 Announce Type: cross Abstract: Understanding how humans and artificial intelligence systems process complex narrative videos is a fundamental challenge at the intersection of neuroscience and machine learning. This study investigates how the temporal context length of video clips (3--12 s clips) and the narrative-task prompting s",
    "url": "https://arxiv.org/abs/2602.07570",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Graph Domain Adaptation via Homophily-Agnostic Reconstructing Structure",
    "summary": "arXiv:2602.07573v1 Announce Type: cross Abstract: Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs to unlabeled target graphs, addressing the challenge of label scarcity. However, existing GDA methods typically assume that both source and target graphs exhibit homophily, leading existing methods to perform poorly when he",
    "url": "https://arxiv.org/abs/2602.07573",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Automated rock joint trace mapping using a supervised learning model trained on synthetic data generated by parametric modelling",
    "summary": "arXiv:2602.07590v1 Announce Type: cross Abstract: This paper presents a geology-driven machine learning method for automated rock joint trace mapping from images. The approach combines geological modelling, synthetic data generation, and supervised image segmentation to address limited real data and class imbalance. First, discrete fracture network",
    "url": "https://arxiv.org/abs/2602.07590",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Learning to Self-Verify Makes Language Models Better Reasoners",
    "summary": "arXiv:2602.07594v1 Announce Type: cross Abstract: Recent large language models (LLMs) achieve strong performance in generating promising reasoning paths for complex tasks. However, despite powerful generation ability, LLMs remain weak at verifying their own answers, revealing a persistent capability asymmetry between generation and self-verificatio",
    "url": "https://arxiv.org/abs/2602.07594",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation",
    "summary": "arXiv:2602.07595v1 Announce Type: cross Abstract: Post-training is the decisive step for converting a pretrained video generator into a production-oriented model that is instruction-following, controllable, and robust over long temporal horizons. This report presents a systematical post-training framework that organizes supervised policy shaping, r",
    "url": "https://arxiv.org/abs/2602.07595",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Astro: Activation-guided Structured Regularization for Outlier-Robust LLM Post-Training Quantization",
    "summary": "arXiv:2602.07596v1 Announce Type: cross Abstract: Weight-only post-training quantization (PTQ) is crucial for efficient Large Language Model (LLM) deployment but suffers from accuracy degradation caused by weight and activation outliers. Existing mitigation strategies often face critical limitations: they either yield insufficient outlier suppressi",
    "url": "https://arxiv.org/abs/2602.07596",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Fine-R1: Make Multi-modal LLMs Excel in Fine-Grained Visual Recognition by Chain-of-Thought Reasoning",
    "summary": "arXiv:2602.07605v2 Announce Type: cross Abstract: Any entity in the visual world can be hierarchically grouped based on shared characteristics and mapped to fine-grained sub-categories. While Multi-modal Large Language Models (MLLMs) achieve strong performance on coarse-grained visual tasks, they often struggle with Fine-Grained Visual Recognition ",
    "url": "https://arxiv.org/abs/2602.07605",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Evaluating Large Language Models for Detecting Architectural Decision Violations",
    "summary": "arXiv:2602.07609v1 Announce Type: cross Abstract: Architectural Decision Records (ADRs) play a central role in maintaining software architecture quality, yet many decision violations go unnoticed because projects lack both systematic documentation and automated detection mechanisms. Recent advances in Large Language Models (LLMs) open up new possib",
    "url": "https://arxiv.org/abs/2602.07609",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SERE: Similarity-based Expert Re-routing for Efficient Batch Decoding in MoE Models",
    "summary": "arXiv:2602.07616v1 Announce Type: cross Abstract: Mixture-of-Experts (MoE) architectures employ sparse activation to deliver faster training and inference with higher accuracy than dense LLMs. However, in production serving, MoE models require batch inference to optimize hardware efficiency, which may cause excessive expert activation and thus slow",
    "url": "https://arxiv.org/abs/2602.07616",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "AD-MIR: Bridging the Gap from Perception to Persuasion in Advertising Video Understanding via Structured Reasoning",
    "summary": "arXiv:2602.07625v1 Announce Type: cross Abstract: Multimodal understanding of advertising videos is essential for interpreting the intricate relationship between visual storytelling and abstract persuasion strategies. However, despite excelling at general search, existing agents often struggle to bridge the cognitive gap between pixel-level percept",
    "url": "https://arxiv.org/abs/2602.07625",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "From Dead Pixels to Editable Slides: Infographic Reconstruction into Native Google Slides via Vision-Language Region Understanding",
    "summary": "arXiv:2602.07645v1 Announce Type: cross Abstract: Infographics are widely used to communicate information with a combination of text, icons, and data visualizations, but once exported as images their content is locked into pixels, making updates, localization, and reuse expensive. We describe \\textsc{Images2Slides}, an API-based pipeline that conve",
    "url": "https://arxiv.org/abs/2602.07645",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Agent-Fence: Mapping Security Vulnerabilities Across Deep Research Agents",
    "summary": "arXiv:2602.07652v1 Announce Type: cross Abstract: Large language models are increasingly deployed as *deep agents* that plan, maintain persistent state, and invoke external tools, shifting safety failures from unsafe text to unsafe *trajectories*. We introduce **AgentFence**, an architecture-centric security evaluation that defines 14 trust-boundar",
    "url": "https://arxiv.org/abs/2602.07652",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Continuous Program Search",
    "summary": "arXiv:2602.07659v1 Announce Type: cross Abstract: Genetic Programming yields interpretable programs, but small syntactic mutations can induce large, unpredictable behavioral shifts, degrading locality and sample efficiency. We frame this as an operator-design problem: learn a continuous program space where latent distance has behavioral meaning, th",
    "url": "https://arxiv.org/abs/2602.07659",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SoK: DARPA's AI Cyber Challenge (AIxCC): Competition Design, Architectures, and Lessons Learned",
    "summary": "arXiv:2602.07666v1 Announce Type: cross Abstract: DARPA's AI Cyber Challenge (AIxCC, 2023--2025) is the largest competition to date for building fully autonomous cyber reasoning systems (CRSs) that leverage recent advances in AI -- particularly large language models (LLMs) -- to discover and remediate vulnerabilities in real-world open-source softw",
    "url": "https://arxiv.org/abs/2602.07666",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Looking and Listening Inside and Outside: Multimodal Artificial Intelligence Systems for Driver Safety Assessment and Intelligent Vehicle Decision-Making",
    "summary": "arXiv:2602.07668v1 Announce Type: cross Abstract: The looking-in-looking-out (LILO) framework has enabled intelligent vehicle applications that understand both the outside scene and the driver state to improve safety outcomes, with examples in smart airbag deployment, takeover time prediction in autonomous control transitions, and driver attention ",
    "url": "https://arxiv.org/abs/2602.07668",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation",
    "summary": "arXiv:2602.07670v1 Announce Type: cross Abstract: Test-time training (TTT) adapts language models through gradient-based updates at inference. But is adaptation the right strategy? We study compute-optimal test-time strategies for verifiable execution-grounded (VEG) tasks, domains like GPU kernel optimization where a deterministic evaluator provide",
    "url": "https://arxiv.org/abs/2602.07670",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Debugging code world models",
    "summary": "arXiv:2602.07672v1 Announce Type: cross Abstract: Code World Models (CWMs) are language models trained to simulate program execution by predicting explicit runtime state after every executed command. This execution-based world modeling enables internal verification within the model, offering an alternative to natural language chain-of-thought reaso",
    "url": "https://arxiv.org/abs/2602.07672",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Spectral Gating Networks",
    "summary": "arXiv:2602.07679v1 Announce Type: cross Abstract: Gating mechanisms are ubiquitous, yet a complementary question in feed-forward networks remains under-explored: how to introduce frequency-rich expressivity without sacrificing stability and scalability? This tension is exposed by spline-based Kolmogorov-Arnold Network (KAN) parameterizations, where",
    "url": "https://arxiv.org/abs/2602.07679",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Vision and language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning",
    "summary": "arXiv:2602.07680v1 Announce Type: cross Abstract: Vision-language models (VLMs) have recently emerged as powerful representation learning systems that align visual observations with natural language concepts, offering new opportunities for semantic reasoning in safety-critical autonomous driving. This paper investigates how vision-language represen",
    "url": "https://arxiv.org/abs/2602.07680",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Mapping Drivers of Greenness: Spatial Variable Selection for MODIS Vegetation Indices",
    "summary": "arXiv:2602.07681v2 Announce Type: cross Abstract: Understanding how environmental drivers relate to vegetation condition motivates spatially varying regression models, but estimating a separate coefficient surface for every predictor can yield noisy patterns and poor interpretability when many predictors are irrelevant. Motivated by MODIS vegetatio",
    "url": "https://arxiv.org/abs/2602.07681",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Process-of-Thought Reasoning for Videos",
    "summary": "arXiv:2602.07689v1 Announce Type: cross Abstract: Video understanding requires not only recognizing visual content but also performing temporally grounded, multi-step reasoning over long and noisy observations. We propose Process-of-Thought (PoT) Reasoning for Videos, a framework that makes the reasoning process explicit by structuring video infere",
    "url": "https://arxiv.org/abs/2602.07689",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "On the Infinite Width and Depth Limits of Predictive Coding Networks",
    "summary": "arXiv:2602.07697v1 Announce Type: cross Abstract: Predictive coding (PC) is a biologically plausible alternative to standard backpropagation (BP) that minimises an energy function with respect to network activities before updating weights. Recent work has improved the training stability of deep PC networks (PCNs) by leveraging some BP-inspired repa",
    "url": "https://arxiv.org/abs/2602.07697",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs",
    "summary": "arXiv:2602.07729v1 Announce Type: cross Abstract: Reinforcement learning (RL), particularly RL from verifiable reward (RLVR), has become a crucial phase of training large language models (LLMs) and a key focus of current scaling efforts. However, optimization practices in RL largely follow those of next-token prediction stages (e.g., pretraining an",
    "url": "https://arxiv.org/abs/2602.07729",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "The Laplacian Keyboard: Beyond the Linear Span",
    "summary": "arXiv:2602.07730v1 Announce Type: cross Abstract: Across scientific disciplines, Laplacian eigenvectors serve as a fundamental basis for simplifying complex systems, from signal processing to quantum mechanics. In reinforcement learning (RL), these eigenvectors provide a natural basis for approximating reward functions; however, their use is typica",
    "url": "https://arxiv.org/abs/2602.07730",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Learnable Chernoff Baselines for Inference-Time Alignment",
    "summary": "arXiv:2602.07738v1 Announce Type: cross Abstract: We study inference-time reward-guided alignment for generative models. Existing methods often rely on either architecture-specific adaptations or computationally costly inference procedures. We introduce Learnable Chernoff Baselines (LCBs) as a method for efficiently and approximately sampling from ",
    "url": "https://arxiv.org/abs/2602.07738",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "HypRAG: Hyperbolic Dense Retrieval for Retrieval Augmented Generation",
    "summary": "arXiv:2602.07739v1 Announce Type: cross Abstract: Embedding geometry plays a fundamental role in retrieval quality, yet dense retrievers for retrieval-augmented generation (RAG) remain largely confined to Euclidean space. However, natural language exhibits hierarchical structure from broad topics to specific entities that Euclidean embeddings fail ",
    "url": "https://arxiv.org/abs/2602.07739",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Preference Conditioned Multi-Objective Reinforcement Learning: Decomposed, Diversity-Driven Policy Optimization",
    "summary": "arXiv:2602.07764v1 Announce Type: cross Abstract: Multi-objective reinforcement learning (MORL) seeks to learn policies that balance multiple, often conflicting objectives. Although a single preference-conditioned policy is the most flexible and scalable solution, existing approaches remain brittle in practice, frequently failing to recover complet",
    "url": "https://arxiv.org/abs/2602.07764",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "PAND: Prompt-Aware Neighborhood Distillation for Lightweight Fine-Grained Visual Classification",
    "summary": "arXiv:2602.07768v1 Announce Type: cross Abstract: Distilling knowledge from large Vision-Language Models (VLMs) into lightweight networks is crucial yet challenging in Fine-Grained Visual Classification (FGVC), due to the reliance on fixed prompts and global alignment. To address this, we propose PAND (Prompt-Aware Neighborhood Distillation), a two",
    "url": "https://arxiv.org/abs/2602.07768",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Generative Reasoning Re-ranker",
    "summary": "arXiv:2602.07774v2 Announce Type: cross Abstract: Recent studies increasingly explore Large Language Models (LLMs) as a new paradigm for recommendation systems due to their scalability and world knowledge. However, existing work has three key limitations: (1) most efforts focus on retrieval and ranking, while the reranking phase, critical for refin",
    "url": "https://arxiv.org/abs/2602.07774",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Still Manual? Automated Linter Configuration via DSL-Based LLM Compilation of Coding Standards",
    "summary": "arXiv:2602.07783v1 Announce Type: cross Abstract: Coding standards are essential for maintaining consistent and high-quality code across teams and projects. Linters help developers enforce these standards by detecting code violations. However, manual linter configuration is complex and expertise-intensive, and the diversity and evolution of program",
    "url": "https://arxiv.org/abs/2602.07783",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Emergent Structured Representations Support Flexible In-Context Inference in Large Language Models",
    "summary": "arXiv:2602.07794v2 Announce Type: cross Abstract: Large language models (LLMs) exhibit emergent behaviors suggestive of human-like reasoning. While recent work has identified structured, human-like conceptual representations within these models, it remains unclear whether they functionally rely on such representations for reasoning. Here we investi",
    "url": "https://arxiv.org/abs/2602.07794",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "CausalTAD: Injecting Causal Knowledge into Large Language Models for Tabular Anomaly Detection",
    "summary": "arXiv:2602.07798v1 Announce Type: cross Abstract: Detecting anomalies in tabular data is critical for many real-world applications, such as credit card fraud detection. With the rapid advancements in large language models (LLMs), state-of-the-art performance in tabular anomaly detection has been achieved by converting tabular data into text and fin",
    "url": "https://arxiv.org/abs/2602.07798",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Fairness Aware Reward Optimization",
    "summary": "arXiv:2602.07799v1 Announce Type: cross Abstract: Demographic skews in human preference data propagate systematic unfairness through reward models into aligned LLMs. We introduce Fairness Aware Reward Optimization (Faro), an in-processing framework that trains reward models under demographic parity, equalized odds, or counterfactual fairness constr",
    "url": "https://arxiv.org/abs/2602.07799",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "VideoTemp-o3: Harmonizing Temporal Grounding and Video Understanding in Agentic Thinking-with-Videos",
    "summary": "arXiv:2602.07801v1 Announce Type: cross Abstract: In long-video understanding, conventional uniform frame sampling often fails to capture key visual evidence, leading to degraded performance and increased hallucinations. To address this, recent agentic thinking-with-videos paradigms have emerged, adopting a localize-clip-answer pipeline in which th",
    "url": "https://arxiv.org/abs/2602.07801",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SoulX-Singer: Towards High-Quality Zero-Shot Singing Voice Synthesis",
    "summary": "arXiv:2602.07803v1 Announce Type: cross Abstract: While recent years have witnessed rapid progress in speech synthesis, open-source singing voice synthesis (SVS) systems still face significant barriers to industrial deployment, particularly in terms of robustness and zero-shot generalization. In this report, we introduce SoulX-Singer, a high-qualit",
    "url": "https://arxiv.org/abs/2602.07803",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Pruning as a Cooperative Game: Surrogate-Assisted Layer Contribution Estimation for Large Language Models",
    "summary": "arXiv:2602.07804v1 Announce Type: cross Abstract: While large language models (LLMs) demonstrate impressive performance across various tasks, their deployment in real-world scenarios is still constrained by high computational demands. Layer-wise pruning, a commonly employed strategy to mitigate inference costs, can partially address this challenge.",
    "url": "https://arxiv.org/abs/2602.07804",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study",
    "summary": "arXiv:2602.07814v1 Announce Type: cross Abstract: As AI-generated images proliferate across digital platforms, reliable detection methods have become critical for combating misinformation and maintaining content authenticity. While numerous deepfake detection methods have been proposed, existing benchmarks predominantly evaluate fine-tuned models, ",
    "url": "https://arxiv.org/abs/2602.07814",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Efficient Representations are Controllable Representations",
    "summary": "arXiv:2602.07828v1 Announce Type: cross Abstract: What is the most brute-force way to install interpretable, controllable features into a model's activations? Controlling how LLMs internally represent concepts typically requires sophisticated methods to first identify, then intervene on the model's existing feature geometry. We bypass all of this. ",
    "url": "https://arxiv.org/abs/2602.07828",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "rePIRL: Learn PRM with Inverse RL for LLM Reasoning",
    "summary": "arXiv:2602.07832v1 Announce Type: cross Abstract: Process rewards have been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. In LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert p",
    "url": "https://arxiv.org/abs/2602.07832",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SPD-Faith Bench: Diagnosing and Improving Faithfulness in Chain-of-Thought for Multimodal Large Language Models",
    "summary": "arXiv:2602.07833v1 Announce Type: cross Abstract: Chain-of-Thought reasoning is widely used to improve the interpretability of multimodal large language models (MLLMs), yet the faithfulness of the generated reasoning traces remains unclear. Prior work has mainly focused on perceptual hallucinations, leaving reasoning level unfaithfulness underexplo",
    "url": "https://arxiv.org/abs/2602.07833",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "TodoEvolve: Learning to Architect Agent Planning Systems",
    "summary": "arXiv:2602.07839v1 Announce Type: cross Abstract: Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address t",
    "url": "https://arxiv.org/abs/2602.07839",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SAGE: Scalable AI Governance & Evaluation",
    "summary": "arXiv:2602.07840v2 Announce Type: cross Abstract: Evaluating relevance in large-scale search systems is fundamentally constrained by the governance gap between nuanced, resource-constrained human oversight and the high-throughput requirements of production systems. While traditional approaches rely on engagement proxies or sparse manual review, the",
    "url": "https://arxiv.org/abs/2602.07840",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Orchestrating Attention: Bringing Harmony to the 'Chaos' of Neurodivergent Learning States",
    "summary": "arXiv:2602.07865v1 Announce Type: cross Abstract: Adaptive learning systems optimize content delivery based on performance metrics but ignore the dynamic attention fluctuations that characterize neurodivergent learners. We present AttentionGuard, a framework that detects engagement-attention states from privacy-preserving behavioral signals and ada",
    "url": "https://arxiv.org/abs/2602.07865",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Direct Soft-Policy Sampling via Langevin Dynamics",
    "summary": "arXiv:2602.07873v1 Announce Type: cross Abstract: Soft policies in reinforcement learning define policies as Boltzmann distributions over state-action value functions, providing a principled mechanism for balancing exploration and exploitation. However, realizing such soft policies in practice remains challenging. Existing approaches either depend ",
    "url": "https://arxiv.org/abs/2602.07873",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Rethinking Latency Denial-of-Service: Attacking the LLM Serving Framework, Not the Model",
    "summary": "arXiv:2602.07878v1 Announce Type: cross Abstract: Large Language Models face an emerging and critical threat known as latency attacks. Because LLM inference is inherently expensive, even modest slowdowns can translate into substantial operating costs and severe availability risks. Recently, a growing body of research has focused on algorithmic comp",
    "url": "https://arxiv.org/abs/2602.07878",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Deep Variable-Length Feedback Codes",
    "summary": "arXiv:2602.07881v1 Announce Type: cross Abstract: Deep learning has enabled significant advances in feedback-based channel coding, yet existing learned schemes remain fundamentally limited: they employ fixed block lengths, suffer degraded performance at high rates, and cannot fully exploit the adaptive potential of feedback. This paper introduces D",
    "url": "https://arxiv.org/abs/2602.07881",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "GRAFT: Decoupling Ranking and Calibration for Survival Analysis",
    "summary": "arXiv:2602.07884v1 Announce Type: cross Abstract: Survival analysis is complicated by censored data, high-dimensional features, and non-linear interactions. Classical models are interpretable but restrictive, while deep learning models are flexible but often non-interpretable and sensitive to noise. We propose GRAFT (Gated Residual Accelerated Fail",
    "url": "https://arxiv.org/abs/2602.07884",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Rich-ARQ: From 1-bit Acknowledgment to Rich Neural Coded Feedback",
    "summary": "arXiv:2602.07886v1 Announce Type: cross Abstract: This paper reimagines the foundational feedback mechanism in wireless communication, transforming the prevailing 1-bit binary ACK/NACK with a high-dimensional, information-rich vector to transform passive acknowledgment into an active collaboration. We present Rich-ARQ, a paradigm that introduces ne",
    "url": "https://arxiv.org/abs/2602.07886",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Scalable Adaptation of 3D Geometric Foundation Models via Weak Supervision from Internet Video",
    "summary": "arXiv:2602.07891v1 Announce Type: cross Abstract: Geometric foundation models show promise in 3D reconstruction, yet their progress is severely constrained by the scarcity of diverse, large-scale 3D annotations. While Internet videos offer virtually unlimited raw data, utilizing them as a scaling source for geometric learning is challenging due to ",
    "url": "https://arxiv.org/abs/2602.07891",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents",
    "summary": "arXiv:2602.07900v1 Announce Type: cross Abstract: Large Language Model (LLM) code agents increasingly resolve repository-level issues by iteratively editing code, invoking tools, and validating candidate patches. In these workflows, agents often write tests on the fly, a paradigm adopted by many high-ranking agents on the SWE-bench leaderboard. How",
    "url": "https://arxiv.org/abs/2602.07900",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Incremental Mapping with Measurement Synchronization & Compression",
    "summary": "arXiv:2602.07901v1 Announce Type: cross Abstract: Modern autonomous vehicles and robots utilize versatile sensors for localization and mapping. The fidelity of these maps is paramount, as an accurate environmental representation is a prerequisite for stable and precise localization. Factor graphs provide a powerful approach for sensor fusion, enabl",
    "url": "https://arxiv.org/abs/2602.07901",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Adaptive Acquisition Selection for Bayesian Optimization with Large Language Models",
    "summary": "arXiv:2602.07904v1 Announce Type: cross Abstract: Bayesian Optimization critically depends on the choice of acquisition function, but no single strategy is universally optimal; the best choice is non-stationary and problem-dependent. Existing adaptive portfolio methods often base their decisions on past function values while ignoring richer informa",
    "url": "https://arxiv.org/abs/2602.07904",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering",
    "summary": "arXiv:2602.07906v1 Announce Type: cross Abstract: Autonomous Machine Learning Engineering (MLE) requires agents to perform sustained, iterative optimization over long horizons. While recent LLM-based agents show promise, current prompt-based agents for MLE suffer from behavioral stagnation due to frozen parameters. Although Reinforcement Learning (",
    "url": "https://arxiv.org/abs/2602.07906",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "CausalCompass: Evaluating the Robustness of Time-Series Causal Discovery in Misspecified Scenarios",
    "summary": "arXiv:2602.07915v1 Announce Type: cross Abstract: Causal discovery from time series is a fundamental task in machine learning. However, its widespread adoption is hindered by a reliance on untestable causal assumptions and by the lack of robustness-oriented evaluation in existing benchmarks. To address these challenges, we propose CausalCompass, a ",
    "url": "https://arxiv.org/abs/2602.07915",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Optimized Human-Robot Co-Dispatch Planning for Petro-Site Surveillance under Varying Criticalities",
    "summary": "arXiv:2602.07924v1 Announce Type: cross Abstract: Securing petroleum infrastructure requires balancing autonomous system efficiency with human judgment for threat escalation, a challenge unaddressed by classical facility location models assuming homogeneous resources. This paper formulates the Human-Robot Co-Dispatch Facility Location Problem (HRCD",
    "url": "https://arxiv.org/abs/2602.07924",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "A Kinetic-Energy Perspective of Flow Matching",
    "summary": "arXiv:2602.07928v1 Announce Type: cross Abstract: Flow-based generative models can be viewed through a physics lens: sampling transports a particle from noise to data by integrating a time-varying velocity field, and each sample corresponds to a trajectory with its own dynamical effort. Motivated by classical mechanics, we introduce Kinetic Path En",
    "url": "https://arxiv.org/abs/2602.07928",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation",
    "summary": "arXiv:2602.07954v1 Announce Type: cross Abstract: As Large Language Models (LLMs) become increasingly deployed in Polish language applications, the need for efficient and accurate content safety classifiers has become paramount. We present Bielik Guard, a family of compact Polish language safety classifiers comprising two model variants: a 0.1B par",
    "url": "https://arxiv.org/abs/2602.07954",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Accuracy-Delay Trade-Off in LLM Offloading via Token-Level Uncertainty",
    "summary": "arXiv:2602.07958v1 Announce Type: cross Abstract: Large language models (LLMs) offer significant potential for intelligent mobile services but are computationally intensive for resource-constrained devices. Mobile edge computing (MEC) allows such devices to offload inference tasks to edge servers (ESs), yet introduces latency due to communication a",
    "url": "https://arxiv.org/abs/2602.07958",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Lost in Translation? A Comparative Study on the Cross-Lingual Transfer of Composite Harms",
    "summary": "arXiv:2602.07963v1 Announce Type: cross Abstract: Most safety evaluations of large language models (LLMs) remain anchored in English. Translation is often used as a shortcut to probe multilingual behavior, but it rarely captures the full picture, especially when harmful intent or structure morphs across languages. Some types of harm survive transla",
    "url": "https://arxiv.org/abs/2602.07963",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "An Explainable Multi-Task Similarity Measure: Integrating Accumulated Local Effects and Weighted Fr\\'echet Distance",
    "summary": "arXiv:2602.07966v1 Announce Type: cross Abstract: In many machine learning contexts, tasks are often treated as interconnected components with the goal of leveraging knowledge transfer between them, which is the central aim of Multi-Task Learning (MTL). Consequently, this multi-task scenario requires addressing critical questions: which tasks are s",
    "url": "https://arxiv.org/abs/2602.07966",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Learning-guided Kansa collocation for forward and inverse PDEs beyond linearity",
    "summary": "arXiv:2602.07970v1 Announce Type: cross Abstract: Partial Differential Equations are precise in modelling the physical, biological and graphical phenomena. However, the numerical methods suffer from the curse of dimensionality, high computation costs and domain-specific discretization. We aim to explore pros and cons of different PDE solvers, and a",
    "url": "https://arxiv.org/abs/2602.07970",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MCIE: Multimodal LLM-Driven Complex Instruction Image Editing with Spatial Guidance",
    "summary": "arXiv:2602.07993v1 Announce Type: cross Abstract: Recent advances in instruction-based image editing have shown remarkable progress. However, existing methods remain limited to relatively simple editing operations, hindering real-world applications that require complex and compositional instructions. In this work, we address these limitations from ",
    "url": "https://arxiv.org/abs/2602.07993",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Don't Always Pick the Highest-Performing Model: An Information Theoretic View of LLM Ensemble Selection",
    "summary": "arXiv:2602.08003v1 Announce Type: cross Abstract: Large language models (LLMs) are often ensembled together to improve overall reliability and robustness, but in practice models are strongly correlated. This raises a fundamental question: which models should be selected when forming an LLM ensemble? We formulate budgeted ensemble selection as maxim",
    "url": "https://arxiv.org/abs/2602.08003",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "DeltaKV: Residual-Based KV Cache Compression via Long-Range Similarity",
    "summary": "arXiv:2602.08005v1 Announce Type: cross Abstract: The deployment of efficient long-context LLMs in applications like autonomous agents, long-chain reasoning, and creative writing is fundamentally bottlenecked by the linear growth of KV cache memory. Existing compression and eviction methods often struggle to balance accuracy, compression ratio, and",
    "url": "https://arxiv.org/abs/2602.08005",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "ForecastOcc: Vision-based Semantic Occupancy Forecasting",
    "summary": "arXiv:2602.08006v1 Announce Type: cross Abstract: Autonomous driving requires forecasting both geometry and semantics over time to effectively reason about future environment states. Existing vision-based occupancy forecasting methods focus on motion-related categories such as static and dynamic objects, while semantic information remains largely a",
    "url": "https://arxiv.org/abs/2602.08006",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "From $O(mn)$ to $O(r^2)$: Two-Sided Low-Rank Communication for Adam in Distributed Training with Memory Efficiency",
    "summary": "arXiv:2602.08007v1 Announce Type: cross Abstract: As foundation models continue to scale, pretraining increasingly relies on data-parallel distributed optimization, making bandwidth-limited gradient synchronization a key bottleneck. Orthogonally, projection-based low-rank optimizers were mainly designed for memory efficiency, but remain suboptimal ",
    "url": "https://arxiv.org/abs/2602.08007",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "ICBAC: an Intelligent Contract-Based Access Control framework for supply chain management by integrating blockchain and federated learning",
    "summary": "arXiv:2602.08014v1 Announce Type: cross Abstract: This paper addresses the critical challenge of access control in modern supply chains, which operate across multiple independent and competing organizations. Existing access control is static and centralized, unable to adapt to insider threats or evolving contexts. Blockchain improves decentralizati",
    "url": "https://arxiv.org/abs/2602.08014",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "The Rise of Sparse Mixture-of-Experts: A Survey from Algorithmic Foundations to Decentralized Architectures and Vertical Domain Applications",
    "summary": "arXiv:2602.08019v1 Announce Type: cross Abstract: The sparse Mixture of Experts(MoE) architecture has evolved as a powerful approach for scaling deep learning models to more parameters with comparable computation cost. As an important branch of large language model(LLM), MoE model only activate a subset of experts based on a routing network. This s",
    "url": "https://arxiv.org/abs/2602.08019",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment",
    "summary": "arXiv:2602.08023v2 Announce Type: cross Abstract: Real-world offensive security operations are inherently open-ended: attackers explore unknown attack surfaces, revise hypotheses under uncertainty, and operate without guaranteed success. Existing LLM-based offensive agent evaluations rely on closed-world settings with predefined goals and binary su",
    "url": "https://arxiv.org/abs/2602.08023",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging",
    "summary": "arXiv:2602.08024v1 Announce Type: cross Abstract: Although Video Large Language Models (VLLMs) have shown remarkable capabilities in video understanding, they are required to process high volumes of visual tokens, causing significant computational inefficiency. Existing VLLMs acceleration frameworks usually compress spatial and temporal redundancy ",
    "url": "https://arxiv.org/abs/2602.08024",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MIND: Benchmarking Memory Consistency and Action Control in World Models",
    "summary": "arXiv:2602.08025v1 Announce Type: cross Abstract: World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and a",
    "url": "https://arxiv.org/abs/2602.08025",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "FIRE: Frobenius-Isometry Reinitialization for Balancing the Stability-Plasticity Tradeoff",
    "summary": "arXiv:2602.08040v1 Announce Type: cross Abstract: Deep neural networks trained on nonstationary data must balance stability (i.e., retaining prior knowledge) and plasticity (i.e., adapting to new tasks). Standard reinitialization methods, which reinitialize weights toward their original values, are widely used but difficult to tune: conservative re",
    "url": "https://arxiv.org/abs/2602.08040",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Implicit Strategic Optimization: Rethinking Long-Horizon Decision-Making in Adversarial Poker Environments",
    "summary": "arXiv:2602.08041v1 Announce Type: cross Abstract: Training large language model (LLM) agents for adversarial games is often driven by episodic objectives such as win rate. In long-horizon settings, however, payoffs are shaped by latent strategic externalities that evolve over time, so myopic optimization and variation-based regret analyses can beco",
    "url": "https://arxiv.org/abs/2602.08041",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "V-ABFT: Variance-Based Adaptive Threshold for Fault-Tolerant Matrix Multiplication in Mixed-Precision Deep Learning",
    "summary": "arXiv:2602.08043v1 Announce Type: cross Abstract: Algorithm-Based Fault Tolerance (ABFT) is widely adopted to detect silent data corruptions (SDCs) in matrix multiplication, a cornerstone operation in deep learning systems. However, existing threshold determination methods face critical challenges: analytical bounds are overly conservative, while p",
    "url": "https://arxiv.org/abs/2602.08043",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Epigraph-Guided Flow Matching for Safe and Performant Offline Reinforcement Learning",
    "summary": "arXiv:2602.08054v1 Announce Type: cross Abstract: Offline reinforcement learning (RL) provides a compelling paradigm for training autonomous systems without the risks of online exploration, particularly in safety-critical domains. However, jointly achieving strong safety and performance from fixed datasets remains challenging. Existing safe offline",
    "url": "https://arxiv.org/abs/2602.08054",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Weak to Strong: VLM-Based Pseudo-Labeling as a Weakly Supervised Training Strategy in Multimodal Video-based Hidden Emotion Understanding Tasks",
    "summary": "arXiv:2602.08057v1 Announce Type: cross Abstract: To tackle the automatic recognition of \"concealed emotions\" in videos, this paper proposes a multimodal weak-supervision framework and achieves state-of-the-art results on the iMiGUE tennis-interview dataset. First, YOLO 11x detects and crops human portraits frame-by-frame, and DINOv2-Base extracts ",
    "url": "https://arxiv.org/abs/2602.08057",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Picasso: Holistic Scene Reconstruction with Physics-Constrained Sampling",
    "summary": "arXiv:2602.08058v1 Announce Type: cross Abstract: In the presence of occlusions and measurement noise, geometrically accurate scene reconstructions -- which fit the sensor data -- can still be physically incorrect. For instance, when estimating the poses and shapes of objects in the scene and importing the resulting estimates into a simulator, smal",
    "url": "https://arxiv.org/abs/2602.08058",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "DICE: Disentangling Artist Style from Content via Contrastive Subspace Decomposition in Diffusion Models",
    "summary": "arXiv:2602.08059v1 Announce Type: cross Abstract: The recent proliferation of diffusion models has made style mimicry effortless, enabling users to imitate unique artistic styles without authorization. In deployed platforms, this raises copyright and intellectual-property risks and calls for reliable protection. However, existing countermeasures ei",
    "url": "https://arxiv.org/abs/2602.08059",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SiameseNorm: Breaking the Barrier to Reconciling Pre/Post-Norm",
    "summary": "arXiv:2602.08064v1 Announce Type: cross Abstract: Modern Transformers predominantly adopt the Pre-Norm paradigm for its optimization stability, foregoing the superior potential of the unstable Post-Norm architecture. Prior attempts to combine their strengths typically lead to a stability-performance trade-off. We attribute this phenomenon to a stru",
    "url": "https://arxiv.org/abs/2602.08064",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Multimodal normative modeling in Alzheimers Disease with introspective variational autoencoders",
    "summary": "arXiv:2602.08077v1 Announce Type: cross Abstract: Normative modeling learns a healthy reference distribution and quantifies subject-specific deviations to capture heterogeneous disease effects. In Alzheimers disease (AD), multimodal neuroimaging offers complementary signals but VAE-based normative models often (i) fit the healthy reference distribu",
    "url": "https://arxiv.org/abs/2602.08077",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Spectral Guardrails for Agents in the Wild: Detecting Tool Use Hallucinations via Attention Topology",
    "summary": "arXiv:2602.08082v1 Announce Type: cross Abstract: Deploying autonomous agents in the wild requires reliable safeguards against tool use failures. We propose a training free guardrail based on spectral analysis of attention topology that complements supervised approaches. On Llama 3.1 8B, our method achieves 97.7\\% recall with multi-feature detectio",
    "url": "https://arxiv.org/abs/2602.08082",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Large language models for spreading dynamics in complex systems",
    "summary": "arXiv:2602.08085v1 Announce Type: cross Abstract: Spreading dynamics is a central topic in the physics of complex systems and network science, providing a unified framework for understanding how information, behaviors, and diseases propagate through interactions among system units. In many propagation contexts, spreading processes are influenced by",
    "url": "https://arxiv.org/abs/2602.08085",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Online Domain-aware LLM Decoding for Continual Domain Evolution",
    "summary": "arXiv:2602.08088v1 Announce Type: cross Abstract: LLMs are typically fine-tuned offline on domain-specific data, assuming a static domain. In practice, domain knowledge evolves continuously through new regulations, products, services, and interaction patterns. Retraining or fine-tuning LLMs for every new instance is computationally infeasible. Addi",
    "url": "https://arxiv.org/abs/2602.08088",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval",
    "summary": "arXiv:2602.08099v1 Announce Type: cross Abstract: Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we fo",
    "url": "https://arxiv.org/abs/2602.08099",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Emergent Search and Backtracking in Latent Reasoning Models",
    "summary": "arXiv:2602.08100v1 Announce Type: cross Abstract: What happens when a language model thinks without words? Standard reasoning LLMs verbalize intermediate steps as chain-of-thought; latent reasoning transformers (LRTs) instead perform deliberation entirely in continuous hidden space. We investigate an LRT, decoding the model's evolving beliefs at ev",
    "url": "https://arxiv.org/abs/2602.08100",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Constrained Pricing under Finite Mixtures of Logit",
    "summary": "arXiv:2602.08119v1 Announce Type: cross Abstract: The mixed logit model is a flexible and widely used demand model in pricing and revenue management. However, existing work on mixed-logit pricing largely focuses on unconstrained settings, limiting its applicability in practice where prices are subject to business or regulatory constraints. We study",
    "url": "https://arxiv.org/abs/2602.08119",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Gender and Race Bias in Consumer Product Recommendations by Large Language Models",
    "summary": "arXiv:2602.08124v1 Announce Type: cross Abstract: Large Language Models are increasingly employed in generating consumer product recommendations, yet their potential for embedding and amplifying gender and race biases remains underexplored. This paper serves as one of the first attempts to examine these biases within LLM-generated recommendations. ",
    "url": "https://arxiv.org/abs/2602.08124",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Robustness of Vision Language Models Against Split-Image Harmful Input Attacks",
    "summary": "arXiv:2602.08136v1 Announce Type: cross Abstract: Vision-Language Models (VLMs) are now a core part of modern AI. Recent work proposed several visual jailbreak attacks using single/ holistic images. However, contemporary VLMs demonstrate strong robustness against such attacks due to extensive safety alignment through preference optimization (e.g., ",
    "url": "https://arxiv.org/abs/2602.08136",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Reliable and Responsible Foundation Models: A Comprehensive Survey",
    "summary": "arXiv:2602.08145v1 Announce Type: cross Abstract: Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medi",
    "url": "https://arxiv.org/abs/2602.08145",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "DIAL-SUMMER: A Structured Evaluation Framework of Hierarchical Errors in Dialogue Summaries",
    "summary": "arXiv:2602.08149v1 Announce Type: cross Abstract: Dialogues are a predominant mode of communication for humans, and it is immensely helpful to have automatically generated summaries of them (e.g., to revise key points discussed in a meeting, to review conversations between customer agents and product users). Prior works on dialogue summary evaluati",
    "url": "https://arxiv.org/abs/2602.08149",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "The Confidence Manifold: Geometric Structure of Correctness Representations in Language Models",
    "summary": "arXiv:2602.08159v1 Announce Type: cross Abstract: When a language model asserts that \"the capital of Australia is Sydney,\" does it know this is wrong? We characterize the geometry of correctness representations across 9 models from 5 architecture families. The structure is simple: the discriminative signal occupies 3-8 dimensions, performance degra",
    "url": "https://arxiv.org/abs/2602.08159",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Self-Supervised Bootstrapping of Action-Predictive Embodied Reasoning",
    "summary": "arXiv:2602.08167v1 Announce Type: cross Abstract: Embodied Chain-of-Thought (CoT) reasoning has significantly enhanced Vision-Language-Action (VLA) models, yet current methods rely on rigid templates to specify reasoning primitives (e.g., objects in the scene, high-level plans, structural affordances). These templates can force policies to process ",
    "url": "https://arxiv.org/abs/2602.08167",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Nexus: Inferring Join Graphs from Metadata Alone via Iterative Low-Rank Matrix Completion",
    "summary": "arXiv:2602.08186v1 Announce Type: cross Abstract: Automatically inferring join relationships is a critical task for effective data discovery, integration, querying and reuse. However, accurately and efficiently identifying these relationships in large and complex schemas can be challenging, especially in enterprise settings where access to data val",
    "url": "https://arxiv.org/abs/2602.08186",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Large Language Models in Peer-Run Community Behavioral Health Services: Understanding Peer Specialists and Service Users' Perspectives on Opportunities, Risks, and Mitigation Strategies",
    "summary": "arXiv:2602.08187v1 Announce Type: cross Abstract: Peer-run organizations (PROs) provide critical, recovery-based behavioral health support rooted in lived experience. As large language models (LLMs) enter this domain, their scale, conversationality, and opacity introduce new challenges for situatedness, trust, and autonomy. Partnering with Collabor",
    "url": "https://arxiv.org/abs/2602.08187",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Dreaming in Code for Curriculum Learning in Open-Ended Worlds",
    "summary": "arXiv:2602.08194v1 Announce Type: cross Abstract: Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather ",
    "url": "https://arxiv.org/abs/2602.08194",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning",
    "summary": "arXiv:2602.08213v1 Announce Type: cross Abstract: Molecule generation and optimization is a fundamental task in chemical domain. The rapid development of intelligent tools, especially large language models (LLMs) with powerful knowledge reserves and interactive capabilities, has provided new paradigms for it. Nevertheless, the intrinsic challenge f",
    "url": "https://arxiv.org/abs/2602.08213",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Sparsity-Aware Evolution for Model Merging",
    "summary": "arXiv:2602.08218v1 Announce Type: cross Abstract: We propose a sparsity-aware evolutionary (SAE) framework for model merging that involves iterative pruning-merging cycles to act as a novel mutation operator. We incorporate the sparsity constraints into the score function, which steers the evolutionary process to favor more sparse models, in additi",
    "url": "https://arxiv.org/abs/2602.08218",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "CoRect: Context-Aware Logit Contrast for Hidden State Rectification to Resolve Knowledge Conflicts",
    "summary": "arXiv:2602.08221v1 Announce Type: cross Abstract: Retrieval-Augmented Generation (RAG) often struggles with knowledge conflicts, where model-internal parametric knowledge overrides retrieved evidence, leading to unfaithful outputs. Existing approaches are often limited, relying either on superficial decoding adjustments or weight editing that neces",
    "url": "https://arxiv.org/abs/2602.08221",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Investigating Writing Professionals' Relationships with Generative AI: How Combined Perceptions of Rivalry and Collaboration Shape Work Practices and Outcomes",
    "summary": "arXiv:2602.08227v1 Announce Type: cross Abstract: This study investigates how professional writers' complex relationship with GenAI shapes their work practices and outcomes. Through a cross-sectional survey with writing professionals (n=403) in diverse roles, we show that collaboration and rivalry orientation are associated with differences in work",
    "url": "https://arxiv.org/abs/2602.08227",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Generating Adversarial Events: A Motion-Aware Point Cloud Framework",
    "summary": "arXiv:2602.08230v1 Announce Type: cross Abstract: Event cameras have been widely adopted in safety-critical domains such as autonomous driving, robotics, and human-computer interaction. A pressing challenge arises from the vulnerability of deep neural networks to adversarial examples, which poses a significant threat to the reliability of event-bas",
    "url": "https://arxiv.org/abs/2602.08230",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Tutti: Expressive Multi-Singer Synthesis via Structure-Level Timbre Control and Vocal Texture Modeling",
    "summary": "arXiv:2602.08233v1 Announce Type: cross Abstract: While existing Singing Voice Synthesis systems achieve high-fidelity solo performances, they are constrained by global timbre control, failing to address dynamic multi-singer arrangement and vocal texture within a single song. To address this, we propose Tutti, a unified framework designed for struc",
    "url": "https://arxiv.org/abs/2602.08233",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "When Benign Inputs Lead to Severe Harms: Eliciting Unsafe Unintended Behaviors of Computer-Use Agents",
    "summary": "arXiv:2602.08235v1 Announce Type: cross Abstract: Although computer-use agents (CUAs) hold significant potential to automate increasingly complex OS workflows, they can demonstrate unsafe unintended behaviors that deviate from expected outcomes even under benign input contexts. However, exploration of this risk remains largely anecdotal, lacking co",
    "url": "https://arxiv.org/abs/2602.08235",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning",
    "summary": "arXiv:2602.08236v1 Announce Type: cross Abstract: Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination,",
    "url": "https://arxiv.org/abs/2602.08236",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Linearization Explains Fine-Tuning in Large Language Models",
    "summary": "arXiv:2602.08239v1 Announce Type: cross Abstract: Parameter-Efficient Fine-Tuning (PEFT) is a popular class of techniques that strive to adapt large models in a scalable and resource-efficient manner. Yet, the mechanisms underlying their training performance and generalization remain underexplored. In this paper, we provide several insights into su",
    "url": "https://arxiv.org/abs/2602.08239",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Learning in Context, Guided by Choice: A Reward-Free Paradigm for Reinforcement Learning with Transformers",
    "summary": "arXiv:2602.08244v1 Announce Type: cross Abstract: In-context reinforcement learning (ICRL) leverages the in-context learning capabilities of transformer models (TMs) to efficiently generalize to unseen sequential decision-making tasks without parameter updates. However, existing ICRL methods rely on explicit reward signals during pretraining, which",
    "url": "https://arxiv.org/abs/2602.08244",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction",
    "summary": "arXiv:2602.08245v1 Announce Type: cross Abstract: Diffusion policies have recently emerged as a powerful paradigm for visuomotor control in robotic manipulation due to their ability to model the distribution of action sequences and capture multimodality. However, iterative denoising leads to substantial inference latency, limiting control frequency",
    "url": "https://arxiv.org/abs/2602.08245",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Inverting Data Transformations via Diffusion Sampling",
    "summary": "arXiv:2602.08267v1 Announce Type: cross Abstract: We study the problem of transformation inversion on general Lie groups: a datum is transformed by an unknown group element, and the goal is to recover an inverse transformation that maps it back to the original data distribution. Such unknown transformations arise widely in machine learning and scie",
    "url": "https://arxiv.org/abs/2602.08267",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "When Do Multi-Agent Systems Outperform? Analysing the Learning Efficiency of Agentic Systems",
    "summary": "arXiv:2602.08272v1 Announce Type: cross Abstract: Reinforcement Learning (RL) has emerged as a crucial method for training or fine-tuning large language models (LLMs), enabling adaptive, task-specific optimizations through interactive feedback. Multi-Agent Reinforcement Learning (MARL), in particular, offers a promising avenue by decomposing comple",
    "url": "https://arxiv.org/abs/2602.08272",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Language Modeling and Understanding Through Paraphrase Generation and Detection",
    "summary": "arXiv:2602.08274v1 Announce Type: cross Abstract: Language enables humans to share knowledge, reason about the world, and pass on strategies for survival and innovation across generations. At the heart of this process is not just the ability to communicate but also the remarkable flexibility in how we can express ourselves. We can express the same ",
    "url": "https://arxiv.org/abs/2602.08274",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "PISCO: Precise Video Instance Insertion with Sparse Control",
    "summary": "arXiv:2602.08277v1 Announce Type: cross Abstract: The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and \"cherry-picking\" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is c",
    "url": "https://arxiv.org/abs/2602.08277",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Tighnari v2: Mitigating Label Noise and Distribution Shift in Multimodal Plant Distribution Prediction via Mixture of Experts and Weakly Supervised Learning",
    "summary": "arXiv:2602.08282v1 Announce Type: cross Abstract: Large-scale, cross-species plant distribution prediction plays a crucial role in biodiversity conservation, yet modeling efforts in this area still face significant challenges due to the sparsity and bias of observational data. Presence-Absence (PA) data provide accurate and noise-free labels, but a",
    "url": "https://arxiv.org/abs/2602.08282",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Noise Stability of Transformer Models",
    "summary": "arXiv:2602.08287v1 Announce Type: cross Abstract: Understanding simplicity biases in deep learning offers a promising path toward developing reliable AI. A common metric for this, inspired by Boolean function analysis, is average sensitivity, which captures a model's robustness to single-token perturbations. We argue that average sensitivity has tw",
    "url": "https://arxiv.org/abs/2602.08287",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Trust-Based Incentive Mechanisms in Semi-Decentralized Federated Learning Systems",
    "summary": "arXiv:2602.08290v1 Announce Type: cross Abstract: In federated learning (FL), decentralized model training allows multi-ple participants to collaboratively improve a shared machine learning model without exchanging raw data. However, ensuring the integrity and reliability of the system is challenging due to the presence of potentially malicious or ",
    "url": "https://arxiv.org/abs/2602.08290",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Automatic Generation of Polynomial Symmetry Breaking Constraints",
    "summary": "arXiv:2602.08297v1 Announce Type: cross Abstract: Symmetry in integer programming causes redundant search and is often handled with symmetry breaking constraints that remove as many equivalent solutions as possible. We propose an algebraic method which allows to generate a random family of polynomial inequalities which can be used as symmetry break",
    "url": "https://arxiv.org/abs/2602.08297",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Grokking in Linear Models for Logistic Regression",
    "summary": "arXiv:2602.08302v1 Announce Type: cross Abstract: Grokking, the phenomenon of delayed generalization, is often attributed to the depth and compositional structure of deep neural networks. We study grokking in one of the simplest possible settings: the learning of a linear model with logistic loss for binary classification on data that are linearly ",
    "url": "https://arxiv.org/abs/2602.08302",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SWE Context Bench: A Benchmark for Context Learning in Coding",
    "summary": "arXiv:2602.08316v1 Announce Type: cross Abstract: Large language models are increasingly used as programming agents for repository level software engineering tasks. While recent benchmarks evaluate correctness in realistic codebases, they largely treat tasks as independent and do not assess whether agents can reuse experience across related problem",
    "url": "https://arxiv.org/abs/2602.08316",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Near-Oracle KV Selection via Pre-hoc Sparsity for Long-Context Inference",
    "summary": "arXiv:2602.08329v1 Announce Type: cross Abstract: A core bottleneck in large language model (LLM) inference is the cost of attending over the ever-growing key-value (KV) cache. Although near-oracle top-k KV selection can preserve the quality of dense attention while sharply reducing computation and bandwidth, existing sparse methods generally rely ",
    "url": "https://arxiv.org/abs/2602.08329",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Latent Reasoning with Supervised Thinking States",
    "summary": "arXiv:2602.08332v1 Announce Type: cross Abstract: Reasoning with a chain-of-thought (CoT) enables Large Language Models (LLMs) to solve complex tasks but incurs significant inference costs due to the generation of long rationales. We propose Thinking States, a method that performs reasoning {\\em while} the input is processing. Specifically, Thinkin",
    "url": "https://arxiv.org/abs/2602.08332",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Regime Change Hypothesis: Foundations for Decoupled Dynamics in Neural Network Training",
    "summary": "arXiv:2602.08333v1 Announce Type: cross Abstract: Despite the empirical success of DNN, their internal training dynamics remain difficult to characterize. In ReLU-based models, the activation pattern induced by a given input determines the piecewise-linear region in which the network behaves affinely. Motivated by this geometry, we investigate whet",
    "url": "https://arxiv.org/abs/2602.08333",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science",
    "summary": "arXiv:2602.08342v1 Announce Type: cross Abstract: Learning transferable multimodal embeddings for urban environments is challenging because urban understanding is inherently spatial, yet existing datasets and benchmarks lack explicit alignment between street-view images and urban structure. We introduce UGData, a spatially grounded dataset that anc",
    "url": "https://arxiv.org/abs/2602.08342",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "ManifoldKV: Training-Free KV Cache Compression via Euclidean Outlier Detection",
    "summary": "arXiv:2602.08343v1 Announce Type: cross Abstract: Long-context inference is constrained by KV-cache memory, which grows linearly with sequence length; KV-cache compression therefore hinges on reliably selecting which past tokens to retain. Most geometry-based eviction methods score keys by cosine similarity to a global centroid, but cosine is scale",
    "url": "https://arxiv.org/abs/2602.08343",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs",
    "summary": "arXiv:2602.08351v1 Announce Type: cross Abstract: Co-optimizing data and model configurations for training LLMs presents a classic chicken-and-egg dilemma: The best training data configuration (e.g., data mixture) for a downstream task depends on the chosen model configuration (e.g., model architecture), and vice versa. However, jointly optimizing ",
    "url": "https://arxiv.org/abs/2602.08351",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Roadmap to Quantum Aesthetics",
    "summary": "arXiv:2602.08363v1 Announce Type: cross Abstract: Quantum mechanics occupies a central position in contemporary science while remaining largely inaccessible to direct sensory experience. This paper proposes a roadmap to quantum aesthetics that examines how quantum concepts become aesthetic phenomena through artistic mediation rather than direct rep",
    "url": "https://arxiv.org/abs/2602.08363",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Learning Human-Like Badminton Skills for Humanoid Robots",
    "summary": "arXiv:2602.08370v1 Announce Type: cross Abstract: Realizing versatile and human-like performance in high-demand sports like badminton remains a formidable challenge for humanoid robotics. Unlike standard locomotion or static manipulation, this task demands a seamless integration of explosive whole-body coordination and precise, timing-critical inte",
    "url": "https://arxiv.org/abs/2602.08370",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Reinforcement Learning with Backtracking Feedback",
    "summary": "arXiv:2602.08377v1 Announce Type: cross Abstract: Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveragi",
    "url": "https://arxiv.org/abs/2602.08377",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning",
    "summary": "arXiv:2602.08382v1 Announce Type: cross Abstract: Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context",
    "url": "https://arxiv.org/abs/2602.08382",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Altruism and Fair Objective in Mixed-Motive Markov games",
    "summary": "arXiv:2602.08389v1 Announce Type: cross Abstract: Cooperation is fundamental for society's viability, as it enables the emergence of structure within heterogeneous groups that seek collective well-being. However, individuals are inclined to defect in order to benefit from the group's cooperation without contributing the associated costs, thus leadi",
    "url": "https://arxiv.org/abs/2602.08389",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models",
    "summary": "arXiv:2602.08392v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination re",
    "url": "https://arxiv.org/abs/2602.08392",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Intelligent support for Human Oversight: Integrating Reinforcement Learning with Gaze Simulation to Personalize Highlighting",
    "summary": "arXiv:2602.08403v1 Announce Type: cross Abstract: Interfaces for human oversight must effectively support users' situation awareness under time-critical conditions. We explore reinforcement learning (RL)-based UI adaptation to personalize alerting strategies that balance the benefits of highlighting critical events against the cognitive costs of in",
    "url": "https://arxiv.org/abs/2602.08403",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Optimizing Spectral Prediction in MXene-Based Metasurfaces Through Multi-Channel Spectral Refinement and Savitzky-Golay Smoothing",
    "summary": "arXiv:2602.08406v1 Announce Type: cross Abstract: The prediction of electromagnetic spectra for MXene-based solar absorbers is a computationally intensive task, traditionally addressed using full-wave solvers. This study introduces an efficient deep learning framework incorporating transfer learning, multi-channel spectral refinement (MCSR), and Sa",
    "url": "https://arxiv.org/abs/2602.08406",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "LLMs + Security = Trouble",
    "summary": "arXiv:2602.08422v1 Announce Type: cross Abstract: We argue that when it comes to producing secure code with AI, the prevailing \"fighting fire with fire\" approach -- using probabilistic AI-based checkers or attackers to secure probabilistically generated code -- fails to address the long tail of security bugs. As a result, systems may remain exposed",
    "url": "https://arxiv.org/abs/2602.08422",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Prism: Spectral-Aware Block-Sparse Attention",
    "summary": "arXiv:2602.08426v1 Announce Type: cross Abstract: Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level search",
    "url": "https://arxiv.org/abs/2602.08426",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries",
    "summary": "arXiv:2602.08448v1 Announce Type: cross Abstract: Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suff",
    "url": "https://arxiv.org/abs/2602.08448",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Decentralized Spatial Reuse Optimization in Wi-Fi: An Internal Regret Minimization Approach",
    "summary": "arXiv:2602.08456v1 Announce Type: cross Abstract: Spatial Reuse (SR) is a cost-effective technique for improving spectral efficiency in dense IEEE 802.11 deployments by enabling simultaneous transmissions. However, the decentralized optimization of SR parameters -- transmission power and Carrier Sensing Threshold (CST) -- across different Basic Ser",
    "url": "https://arxiv.org/abs/2602.08456",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Gesture Matters: Pedestrian Gesture Recognition for AVs Through Skeleton Pose Evaluation",
    "summary": "arXiv:2602.08479v1 Announce Type: cross Abstract: Gestures are a key component of non-verbal communication in traffic, often helping pedestrian-to-driver interactions when formal traffic rules may be insufficient. This problem becomes more apparent when autonomous vehicles (AVs) struggle to interpret such gestures. In this study, we present a gestu",
    "url": "https://arxiv.org/abs/2602.08479",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "CLEAR: A Knowledge-Centric Vessel Trajectory Analysis Platform",
    "summary": "arXiv:2602.08482v1 Announce Type: cross Abstract: Vessel trajectory data from the Automatic Identification System (AIS) is used widely in maritime analytics. Yet, analysis is difficult for non-expert users due to the incompleteness and complexity of AIS data. We present CLEAR, a knowledge-centric vessel trajectory analysis platform that aims to ove",
    "url": "https://arxiv.org/abs/2602.08482",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Contextual Rollout Bandits for Reinforcement Learning with Verifiable Rewards",
    "summary": "arXiv:2602.08499v1 Announce Type: cross Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is an effective paradigm for improving the reasoning capabilities of large language models. However, existing RLVR methods utilize rollouts in an indiscriminate and short-horizon manner: responses of heterogeneous quality within each prompt are t",
    "url": "https://arxiv.org/abs/2602.08499",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "A General Theory of Proportionality with Additive Utilities",
    "summary": "arXiv:2602.08504v1 Announce Type: cross Abstract: We consider a model where a subset of candidates must be selected based on voter preferences, subject to general constraints that specify which subsets are feasible. This model generalizes committee elections with diversity constraints, participatory budgeting (including constraints specifying how f",
    "url": "https://arxiv.org/abs/2602.08504",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "GISA: A Benchmark for General Information-Seeking Assistant",
    "summary": "arXiv:2602.08543v1 Announce Type: cross Abstract: The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construc",
    "url": "https://arxiv.org/abs/2602.08543",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing",
    "summary": "arXiv:2602.08550v1 Announce Type: cross Abstract: Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geomet",
    "url": "https://arxiv.org/abs/2602.08550",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs",
    "summary": "arXiv:2602.08563v1 Announce Type: cross Abstract: Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independen",
    "url": "https://arxiv.org/abs/2602.08563",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Agent-Supported Foresight for AI Systemic Risks: AI Agents for Breadth, Experts for Judgment",
    "summary": "arXiv:2602.08565v1 Announce Type: cross Abstract: AI impact assessments often stress near-term risks because human judgment degrades over longer horizons, exemplifying the Collingridge dilemma: foresight is most needed when knowledge is scarcest. To address long-term systemic risks, we introduce a scalable approach that simulates in-silico agents u",
    "url": "https://arxiv.org/abs/2602.08565",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Predicting Future Utility: Global Combinatorial Optimization for Task-Agnostic KV Cache Eviction",
    "summary": "arXiv:2602.08585v1 Announce Type: cross Abstract: Given the quadratic complexity of attention, KV cache eviction is vital to accelerate model inference. Current KV cache eviction methods typically rely on instantaneous heuristic metrics, implicitly assuming that score magnitudes are consistent proxies for importance across all heads. However, this ",
    "url": "https://arxiv.org/abs/2602.08585",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Kissan-Dost: Bridging the Last Mile in Smallholder Precision Agriculture with Conversational IoT",
    "summary": "arXiv:2602.08593v1 Announce Type: cross Abstract: We present Kissan-Dost, a multilingual, sensor-grounded conversational system that turns live on-farm measurements and weather into plain-language guidance delivered over WhatsApp text or voice. The system couples commodity soil and climate sensors with retrieval-augmented generation, then enforces ",
    "url": "https://arxiv.org/abs/2602.08593",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces",
    "summary": "arXiv:2602.08616v1 Announce Type: cross Abstract: Reinforcement Learning is increasingly applied to logistics, scheduling, and recommender systems, but standard algorithms struggle with the curse of dimensionality in such large discrete action spaces. Existing algorithms typically rely on restrictive grid-based structures or computationally expensi",
    "url": "https://arxiv.org/abs/2602.08616",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Enhancing Genetic Algorithms with Graph Neural Networks: A Timetabling Case Study",
    "summary": "arXiv:2602.08619v1 Announce Type: cross Abstract: This paper investigates the impact of hybridizing a multi-modal Genetic Algorithm with a Graph Neural Network for timetabling optimization. The Graph Neural Network is designed to encapsulate general domain knowledge to improve schedule quality, while the Genetic Algorithm explores different regions",
    "url": "https://arxiv.org/abs/2602.08619",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs",
    "summary": "arXiv:2602.08621v1 Announce Type: cross Abstract: By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior w",
    "url": "https://arxiv.org/abs/2602.08621",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "CauScale: Neural Causal Discovery at Scale",
    "summary": "arXiv:2602.08629v1 Announce Type: cross Abstract: Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for eff",
    "url": "https://arxiv.org/abs/2602.08629",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "We Should Separate Memorization from Copyright",
    "summary": "arXiv:2602.08632v1 Announce Type: cross Abstract: The widespread use of foundation models has introduced a new risk factor of copyright issue. This issue is leading to an active, lively and on-going debate amongst the data-science community as well as amongst legal scholars. Where claims and results across both sides are often interpreted in differ",
    "url": "https://arxiv.org/abs/2602.08632",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "LEFT: Learnable Fusion of Tri-view Tokens for Unsupervised Time Series Anomaly Detection",
    "summary": "arXiv:2602.08638v1 Announce Type: cross Abstract: As a fundamental data mining task, unsupervised time series anomaly detection (TSAD) aims to build a model for identifying abnormal timestamps without assuming the availability of annotations. A key challenge in unsupervised TSAD is that many anomalies are too subtle to exhibit detectable deviation ",
    "url": "https://arxiv.org/abs/2602.08638",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Equalized Generative Treatment: Matching f-divergences for Fairness in Generative Models",
    "summary": "arXiv:2602.08660v1 Announce Type: cross Abstract: Fairness is a crucial concern for generative models, which not only reflect but can also amplify societal and cultural biases. Existing fairness notions for generative models are largely adapted from classification and focus on balancing the probability of generating samples from each sensitive grou",
    "url": "https://arxiv.org/abs/2602.08660",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "6G-Bench: An Open Benchmark for Semantic Communication and Network-Level Reasoning with Foundation Models in AI-Native 6G Networks",
    "summary": "arXiv:2602.08675v1 Announce Type: cross Abstract: This paper introduces 6G-Bench, an open benchmark for evaluating semantic communication and network-level reasoning in AI-native 6G networks. 6G-Bench defines a taxonomy of 30 decision-making tasks (T1--T30) extracted from ongoing 6G and AI-agent standardization activities in 3GPP, IETF, ETSI, ITU-T",
    "url": "https://arxiv.org/abs/2602.08675",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "LLaDA2.1: Speeding Up Text Diffusion via Token Editing",
    "summary": "arXiv:2602.08676v2 Announce Type: cross Abstract: While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trad",
    "url": "https://arxiv.org/abs/2602.08676",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "CompilerKV: Risk-Adaptive KV Compression via Offline Experience Compilation",
    "summary": "arXiv:2602.08686v1 Announce Type: cross Abstract: Large Language Models (LLMs) in long-context scenarios are severely constrained by the linear growth of Key-Value (KV) cache memory. Existing KV compression methods rely either on static thresholds and attention-only heuristics or on coarse memory budget allocation. Under tight memory budgets, these",
    "url": "https://arxiv.org/abs/2602.08686",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "PBLean: Pseudo-Boolean Proof Certificates for Lean 4",
    "summary": "arXiv:2602.08692v1 Announce Type: cross Abstract: We present PBLean, a method for importing VeriPB pseudo-Boolean (PB) proof certificates into Lean 4. Key to our approach is reflection: a Boolean checker function whose soundness is fully proved in Lean and executed as compiled native code. Our method scales to proofs with tens of thousands of steps",
    "url": "https://arxiv.org/abs/2602.08692",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Technosocial risks of ideal emotion recognition technologies: A defense of the (social) value of emotional expressions",
    "summary": "arXiv:2602.08706v1 Announce Type: cross Abstract: The prospect of AI systems that I call ideal emotion recognition technologies (ERTs) is often defended on the assumption that social life would benefit from increased affective transparency. This paper challenges that assumption by examining the technosocial risks posed by ideal ERTs, understood as ",
    "url": "https://arxiv.org/abs/2602.08706",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images",
    "summary": "arXiv:2602.08717v1 Announce Type: cross Abstract: Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scena",
    "url": "https://arxiv.org/abs/2602.08717",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill",
    "summary": "arXiv:2602.08722v1 Announce Type: cross Abstract: We present QUOKA: Query-oriented KV selection for efficient attention, a training-free and hardware agnostic sparse attention algorithm for accelerating transformer inference under chunked prefill. While many queries focus on a smaller group of keys in the attention operator, we observe that queries",
    "url": "https://arxiv.org/abs/2602.08722",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework",
    "summary": "arXiv:2602.08727v1 Announce Type: cross Abstract: Undersampled CT volumes minimize acquisition time and radiation exposure but introduce artifacts degrading image quality and diagnostic utility. Reducing these artifacts is critical for high-quality imaging. We propose a computationally efficient hybrid deep-learning framework that combines the stre",
    "url": "https://arxiv.org/abs/2602.08727",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "On the Expressive Power of GNNs for Boolean Satisfiability",
    "summary": "arXiv:2602.08745v1 Announce Type: cross Abstract: Machine learning approaches to solving Boolean Satisfiability (SAT) aim to replace handcrafted heuristics with learning-based models. Graph Neural Networks have emerged as the main architecture for SAT solving, due to the natural graph representation of Boolean formulas. We analyze the expressive po",
    "url": "https://arxiv.org/abs/2602.08745",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Efficient Brain Extraction of MRI Scans with Mild to Moderate Neuropathology",
    "summary": "arXiv:2602.08764v1 Announce Type: cross Abstract: Skull stripping magnetic resonance images (MRI) of the human brain is an important process in many image processing techniques, such as automatic segmentation of brain structures. Numerous methods have been developed to perform this task, however, they often fail in the presence of neuropathology an",
    "url": "https://arxiv.org/abs/2602.08764",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Taming Scylla: Understanding the multi-headed agentic daemon of the coding seas",
    "summary": "arXiv:2602.08765v1 Announce Type: cross Abstract: LLM-based tools are automating more software development tasks at a rapid pace, but there is no rigorous way to evaluate how different architectural choices -- prompts, skills, tools, multi-agent setups -- materially affect both capability and cost. This paper introduces Scylla, an evaluation framew",
    "url": "https://arxiv.org/abs/2602.08765",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "FreqLens: Interpretable Frequency Attribution for Time Series Forecasting",
    "summary": "arXiv:2602.08768v1 Announce Type: cross Abstract: Time series forecasting models often lack interpretability, limiting their adoption in domains requiring explainable predictions. We propose \\textsc{FreqLens}, an interpretable forecasting framework that discovers and attributes predictions to learnable frequency components. \\textsc{FreqLens} introd",
    "url": "https://arxiv.org/abs/2602.08768",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Default Machine Learning Hyperparameters Do Not Provide Informative Initialization for Bayesian Optimization",
    "summary": "arXiv:2602.08774v1 Announce Type: cross Abstract: Bayesian Optimization (BO) is a standard tool for hyperparameter tuning thanks to its sample efficiency on expensive black-box functions. While most BO pipelines begin with uniform random initialization, default hyperparameter values shipped with popular ML libraries such as scikit-learn encode impl",
    "url": "https://arxiv.org/abs/2602.08774",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems",
    "summary": "arXiv:2602.08792v1 Announce Type: cross Abstract: The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service di",
    "url": "https://arxiv.org/abs/2602.08792",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework",
    "summary": "arXiv:2602.08797v1 Announce Type: cross Abstract: Accurate brain tumor segmentation from MRI is limited by expensive annotations and data heterogeneity across scanners and sites. We propose a semi-supervised teacher-student framework that combines an uncertainty-aware pseudo-labeling teacher with a progressive, confidence-based curriculum for the s",
    "url": "https://arxiv.org/abs/2602.08797",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "$\\texttt{lrnnx}$: A library for Linear RNNs",
    "summary": "arXiv:2602.08810v1 Announce Type: cross Abstract: Linear recurrent neural networks (LRNNs) provide a structured approach to sequence modeling that bridges classical linear dynamical systems and modern deep learning, offering both expressive power and theoretical guarantees on stability and trainability. In recent years, multiple LRNN-based architec",
    "url": "https://arxiv.org/abs/2602.08810",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Permissive-Washing in the Open AI Supply Chain: A Large-Scale Audit of License Integrity",
    "summary": "arXiv:2602.08816v1 Announce Type: cross Abstract: Permissive licenses like MIT, Apache-2.0, and BSD-3-Clause dominate open-source AI, signaling that artifacts like models, datasets, and code can be freely used, modified, and redistributed. However, these licenses carry mandatory requirements: include the full license text, provide a copyright notic",
    "url": "https://arxiv.org/abs/2602.08816",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Affective Flow Language Model for Emotional Support Conversation",
    "summary": "arXiv:2602.08826v1 Announce Type: cross Abstract: Large language models (LLMs) have been widely applied to emotional support conversation (ESC). However, complex multi-turn support remains challenging.This is because existing alignment schemes rely on sparse outcome-level signals, thus offering limited supervision for intermediate strategy decision",
    "url": "https://arxiv.org/abs/2602.08826",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "WildReward: Learning Reward Models from In-the-Wild Human Interactions",
    "summary": "arXiv:2602.08829v1 Announce Type: cross Abstract: Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the questi",
    "url": "https://arxiv.org/abs/2602.08829",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems",
    "summary": "arXiv:2602.08847v1 Announce Type: cross Abstract: Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agen",
    "url": "https://arxiv.org/abs/2602.08847",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Discovering Interpretable Algorithms by Decompiling Transformers to RASP",
    "summary": "arXiv:2602.08857v1 Announce Type: cross Abstract: Recent work has shown that the computations of Transformers can be simulated in the RASP family of programming languages. These findings have enabled improved understanding of the expressive capacity and generalization abilities of Transformers. In particular, Transformers have been suggested to len",
    "url": "https://arxiv.org/abs/2602.08857",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "FlattenGPT: Depth Compression for Transformer with Layer Flattening",
    "summary": "arXiv:2602.08858v1 Announce Type: cross Abstract: Recent works have indicated redundancy across transformer blocks, prompting the research of depth compression to prune less crucial blocks. However, current ways of entire-block pruning suffer from risks of discarding meaningful cues learned in those blocks, leading to substantial performance degrad",
    "url": "https://arxiv.org/abs/2602.08858",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Understanding Dynamic Compute Allocation in Recurrent Transformers",
    "summary": "arXiv:2602.08864v1 Announce Type: cross Abstract: Token-level adaptive computation seeks to reduce inference cost by allocating more computation to harder tokens and less to easier ones. However, prior work is primarily evaluated on natural-language benchmarks using task-level metrics, where token-level difficulty is unobservable and confounded wit",
    "url": "https://arxiv.org/abs/2602.08864",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "AnomSeer: Reinforcing Multimodal LLMs to Reason for Time-Series Anomaly Detection",
    "summary": "arXiv:2602.08868v1 Announce Type: cross Abstract: Time-series anomaly detection (TSAD) with multimodal large language models (MLLMs) is an emerging area, yet a persistent challenge remains: MLLMs rely on coarse time-series heuristics but struggle with multi-dimensional, detailed reasoning, which is vital for understanding complex time-series data. ",
    "url": "https://arxiv.org/abs/2602.08868",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Whose Name Comes Up? Benchmarking and Intervention-Based Auditing of LLM-Based Scholar Recommendation",
    "summary": "arXiv:2602.08873v1 Announce Type: cross Abstract: Large language models (LLMs) are increasingly used for academic expert recommendation. Existing audits typically evaluate model outputs in isolation, largely ignoring end-user inference-time interventions. As a result, it remains unclear whether failures such as refusals, hallucinations, and uneven ",
    "url": "https://arxiv.org/abs/2602.08873",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Learning Potentials for Dynamic Matching and Application to Heart Transplantation",
    "summary": "arXiv:2602.08878v1 Announce Type: cross Abstract: Each year, thousands of patients in need of heart transplants face life-threatening wait times due to organ scarcity. While allocation policies aim to maximize population-level outcomes, current approaches often fail to account for the dynamic arrival of organs and the composition of waitlisted cand",
    "url": "https://arxiv.org/abs/2602.08878",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression",
    "summary": "arXiv:2602.08885v2 Announce Type: cross Abstract: Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find tha",
    "url": "https://arxiv.org/abs/2602.08885",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "DeepQuali: Initial results of a study on the use of large language models for assessing the quality of user stories",
    "summary": "arXiv:2602.08887v1 Announce Type: cross Abstract: Generative artificial intelligence (GAI), specifically large language models (LLMs), are increasingly used in software engineering, mainly for coding tasks. However, requirements engineering - particularly requirements validation - has seen limited application of GAI. The current focus of using GAI ",
    "url": "https://arxiv.org/abs/2602.08887",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "OmniReview: A Large-scale Benchmark and LLM-enhanced Framework for Realistic Reviewer Recommendation",
    "summary": "arXiv:2602.08896v1 Announce Type: cross Abstract: Academic peer review remains the cornerstone of scholarly validation, yet the field faces some challenges in data and methods. From the data perspective, existing research is hindered by the scarcity of large-scale, verified benchmarks and oversimplified evaluation metrics that fail to reflect real-",
    "url": "https://arxiv.org/abs/2602.08896",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Gesturing Toward Abstraction: Multimodal Convention Formation in Collaborative Physical Tasks",
    "summary": "arXiv:2602.08914v1 Announce Type: cross Abstract: A quintessential feature of human intelligence is the ability to create ad hoc conventions over time to achieve shared goals efficiently. We investigate how communication strategies evolve through repeated collaboration as people coordinate on shared procedural abstractions. To this end, we conducte",
    "url": "https://arxiv.org/abs/2602.08914",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Automatic In-Domain Exemplar Construction and LLM-Based Refinement of Multi-LLM Expansions for Query Expansion",
    "summary": "arXiv:2602.08917v1 Announce Type: cross Abstract: Query expansion with large language models is promising but often relies on hand-crafted prompts, manually chosen exemplars, or a single LLM, making it non-scalable and sensitive to domain shift. We present an automated, domain-adaptive QE framework that builds in-domain exemplar pools by harvesting",
    "url": "https://arxiv.org/abs/2602.08917",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors",
    "summary": "arXiv:2602.08934v1 Announce Type: cross Abstract: AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraph",
    "url": "https://arxiv.org/abs/2602.08934",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "pixelLOG: Logging of Online Gameplay for Cognitive Research",
    "summary": "arXiv:2602.08941v1 Announce Type: cross Abstract: Traditional cognitive assessments often rely on isolated, output-focused measurements that may fail to capture the complexity of human cognition in naturalistic settings. We present pixelLOG, a high-performance data collection framework for Spigot-based Minecraft servers designed specifically for pr",
    "url": "https://arxiv.org/abs/2602.08941",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE",
    "summary": "arXiv:2602.08961v1 Announce Type: cross Abstract: We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to",
    "url": "https://arxiv.org/abs/2602.08961",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents",
    "summary": "arXiv:2602.08964v1 Announce Type: cross Abstract: Understanding an agent's goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of mo",
    "url": "https://arxiv.org/abs/2602.08964",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "StretchTime: Adaptive Time Series Forecasting via Symplectic Attention",
    "summary": "arXiv:2602.08983v1 Announce Type: cross Abstract: Transformer architectures have established strong baselines in time series forecasting, yet they typically rely on positional encodings that assume uniform, index-based temporal progression. However, real-world systems, from shifting financial cycles to elastic biological rhythms, frequently exhibit",
    "url": "https://arxiv.org/abs/2602.08983",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models",
    "summary": "arXiv:2602.08984v1 Announce Type: cross Abstract: We propose Next Concept Prediction (NCP), a generative pretraining paradigm built on top of Next Token Prediction (NTP). NCP predicts discrete concepts that span multiple tokens, thereby forming a more challenging pretraining objective. Our model, ConceptLM, quantizes hidden states using Vector Quan",
    "url": "https://arxiv.org/abs/2602.08984",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Improving Detection of Rare Nodes in Hierarchical Multi-Label Learning",
    "summary": "arXiv:2602.08986v1 Announce Type: cross Abstract: In hierarchical multi-label classification, a persistent challenge is enabling model predictions to reach deeper levels of the hierarchy for more detailed or fine-grained classifications. This difficulty partly arises from the natural rarity of certain classes (or hierarchical nodes) and the hierarc",
    "url": "https://arxiv.org/abs/2602.08986",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "From Obstacles to Etiquette: Robot Social Navigation with VLM-Informed Path Selection",
    "summary": "arXiv:2602.09002v1 Announce Type: cross Abstract: Navigating socially in human environments requires more than satisfying geometric constraints, as collision-free paths may still interfere with ongoing activities or conflict with social norms. Addressing this challenge calls for analyzing interactions between agents and incorporating common-sense r",
    "url": "https://arxiv.org/abs/2602.09002",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "ARO: A New Lens On Matrix Optimization For Large Models",
    "summary": "arXiv:2602.09006v1 Announce Type: cross Abstract: Matrix-based optimizers have attracted growing interest for improving LLM training efficiency, with significant progress centered on orthogonalization/whitening based methods. While yielding substantial performance gains, a fundamental question arises: can we develop new paradigms beyond orthogonali",
    "url": "https://arxiv.org/abs/2602.09006",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "ANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling",
    "summary": "arXiv:2602.09009v1 Announce Type: cross Abstract: Scaling network depth has been a central driver behind the success of modern foundation models, yet recent investigations suggest that deep layers are often underutilized. This paper revisits the default mechanism for deepening neural networks, namely residual connections, from an optimization persp",
    "url": "https://arxiv.org/abs/2602.09009",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense",
    "summary": "arXiv:2602.09012v1 Announce Type: cross Abstract: The rapid evolution of GUI-enabled agents has rendered traditional CAPTCHAs obsolete. While previous benchmarks like OpenCaptchaWorld established a baseline for evaluating multimodal agents, recent advancements in reasoning-heavy models, such as Gemini3-Pro-High and GPT-5.2-Xhigh have effectively co",
    "url": "https://arxiv.org/abs/2602.09012",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation",
    "summary": "arXiv:2602.09014v1 Announce Type: cross Abstract: Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typic",
    "url": "https://arxiv.org/abs/2602.09014",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "CIC-Trap4Phish: A Unified Multi-Format Dataset for Phishing and Quishing Attachment Detection",
    "summary": "arXiv:2602.09015v2 Announce Type: cross Abstract: Phishing attacks represents one of the primary attack methods which is used by cyber attackers. In many cases, attackers use deceptive emails along with malicious attachments to trick users into giving away sensitive information or installing malware while compromising entire systems. The flexibilit",
    "url": "https://arxiv.org/abs/2602.09015",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving",
    "summary": "arXiv:2602.09018v1 Announce Type: cross Abstract: Out of distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled $k$-factor perturbat",
    "url": "https://arxiv.org/abs/2602.09018",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Knowledge-Centric Metacognitive Learning",
    "summary": "arXiv:2402.05346v4 Announce Type: replace Abstract: Interactions are central to intelligent reasoning and learning abilities, with the interpretation of abstract knowledge guiding meaningful interaction with objects in the environment. While humans readily adapt to novel situations by leveraging abstract knowledge acquired over time, artificial int",
    "url": "https://arxiv.org/abs/2402.05346",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Generative AI voting: fair collective choice is resilient to LLM biases and inconsistencies",
    "summary": "arXiv:2406.11871v5 Announce Type: replace Abstract: Recent breakthroughs in generative artificial intelligence (AI) and large language models (LLMs) unravel new capabilities for AI personal assistants to overcome cognitive bandwidth limitations of humans, providing decision support or even direct representation of abstained human voters at large sc",
    "url": "https://arxiv.org/abs/2406.11871",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Tree Search for Language Model Agents",
    "summary": "arXiv:2407.01476v4 Announce Type: replace Abstract: Autonomous agents powered by language models (LMs) have demonstrated promise in their ability to perform decision-making tasks such as web automation. However, a key limitation remains: LMs, primarily optimized for natural language understanding and generation, struggle with multi-step reasoning, ",
    "url": "https://arxiv.org/abs/2407.01476",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Revisiting Privacy, Utility, and Efficiency Trade-offs when Fine-Tuning Large Language Models",
    "summary": "arXiv:2502.13313v2 Announce Type: replace Abstract: We study the inherent trade-offs in minimizing privacy risks and maximizing utility, while maintaining high computational efficiency, when fine-tuning large language models (LLMs). A number of recent works in privacy research have attempted to mitigate privacy risks posed by memorizing fine-tuning",
    "url": "https://arxiv.org/abs/2502.13313",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Research Superalignment Should Advance Now with Alternating Competence and Conformity Optimization",
    "summary": "arXiv:2503.07660v2 Announce Type: replace Abstract: The recent leap in AI capabilities, driven by big generative models, has sparked the possibility of achieving Artificial General Intelligence (AGI) and further triggered discussions on Artificial Superintelligence (ASI)-a system surpassing all humans across measured domains. This gives rise to the",
    "url": "https://arxiv.org/abs/2503.07660",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling Reasoning Benchmark for LLMs",
    "summary": "arXiv:2504.11239v2 Announce Type: replace Abstract: Reasoning is the fundamental capability of large language models (LLMs). Due to the rapid progress of LLMs, there are two main issues of current benchmarks: i) these benchmarks can be crushed in a short time (less than 1 year), and ii) these benchmarks may be easily hacked. To handle these issues,",
    "url": "https://arxiv.org/abs/2504.11239",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "DRAGON: Domain-specific Robust Automatic Data Generation for RAG Optimization",
    "summary": "arXiv:2505.10989v2 Announce Type: replace Abstract: Retrieval-augmented generation (RAG) can substantially enhance the performance of LLMs on knowledge-intensive tasks. Various RAG paradigms - including vanilla, planning-based, and iterative RAG - all depend on a robust retriever, yet existing retrievers rely heavily on public knowledge and often f",
    "url": "https://arxiv.org/abs/2505.10989",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives",
    "summary": "arXiv:2505.15693v2 Announce Type: replace Abstract: Recent advances in reinforcement learning (RL) have renewed interest in reward design for shaping agent behavior, but manually crafting reward functions is tedious and error-prone. A principled alternative is to specify behavioral requirements in a formal, unambiguous language and automatically co",
    "url": "https://arxiv.org/abs/2505.15693",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Capability-Based Scaling Trends for LLM-Based Red-Teaming",
    "summary": "arXiv:2505.20162v2 Announce Type: replace Abstract: As large language models grow in capability and agency, identifying vulnerabilities through red-teaming becomes vital for safe deployment. However, traditional prompt-engineering approaches may prove ineffective once red-teaming turns into a \\emph{weak-to-strong} problem, where target models surpa",
    "url": "https://arxiv.org/abs/2505.20162",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "HouseTS: A Large-Scale, Multimodal Spatiotemporal U.S. Housing Dataset and Benchmark",
    "summary": "arXiv:2506.00765v2 Announce Type: replace Abstract: Accurate long-horizon house-price forecasting requires benchmarks that capture temporal dynamics together with time-varying local context. However, existing public resources remain fragmented: many datasets have limited spatial coverage, temporal depth, or multimodal alignment; the robustness of m",
    "url": "https://arxiv.org/abs/2506.00765",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Guideline Forest: Retrieval-Augmented Reasoning with Branching Experience-Induced Guidelines",
    "summary": "arXiv:2506.07820v3 Announce Type: replace Abstract: Retrieval-augmented generation (RAG) has been widely adopted to ground large language models (LLMs) in external knowledge, yet it remains largely underexplored for improving reasoning. Existing methods either rely on online exploration during inference or heuristic supervision over reasoning traje",
    "url": "https://arxiv.org/abs/2506.07820",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "On Reasoning Strength Planning in Large Reasoning Models",
    "summary": "arXiv:2506.08390v2 Announce Type: replace Abstract: Recent studies empirically reveal that large reasoning models (LRMs) can automatically allocate more reasoning strengths (i.e., the number of reasoning tokens) for harder problems, exhibiting difficulty-awareness for better task performance. While this automatic reasoning strength allocation pheno",
    "url": "https://arxiv.org/abs/2506.08390",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "CID-GraphRAG: Enhancing Multi-Turn Dialogue Systems through Dual-Pathway Retrieval of Conversation Flow and Context Semantics",
    "summary": "arXiv:2506.19385v4 Announce Type: replace Abstract: We present CID-GraphRAG (Conversational Intent-Driven Graph Retrieval-Augmented Generation), a novel framework that addresses the limitations of existing dialogue systems in maintaining both contextual coherence and goal-oriented progression in multi-turn customer service conversations. Unlike tra",
    "url": "https://arxiv.org/abs/2506.19385",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Lyria: A Genetic Algorithm-Driven Neuro-Symbolic Reasoning Framework for LLMs",
    "summary": "arXiv:2507.04034v2 Announce Type: replace Abstract: While LLMs have demonstrated impressive abilities across various domains, they struggle with two major issues. The first is that LLMs trap themselves into local optima and the second is that they lack exhaustive coverage of the solution space. To investigate and improve these two issues, we propos",
    "url": "https://arxiv.org/abs/2507.04034",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Giving AI Agents Access to Cryptocurrency and Smart Contracts Creates New Vectors of AI Harm",
    "summary": "arXiv:2507.08249v2 Announce Type: replace Abstract: There is growing interest in giving AI agents access to cryptocurrencies as well as to the smart contracts that transact them. But doing so, this position paper argues, could lead to formidable new vectors of AI harm. To support this argument, we first examine the unique properties of cryptocurren",
    "url": "https://arxiv.org/abs/2507.08249",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making under Epistemic Uncertainty",
    "summary": "arXiv:2508.08992v2 Announce Type: replace Abstract: Prospect Theory (PT) models human decision-making under uncertainty, while epistemic markers (e.g., maybe) serve to express uncertainty in language. However, it remains largely unexplored whether Prospect Theory applies to contemporary Large Language Models and whether epistemic markers, which exp",
    "url": "https://arxiv.org/abs/2508.08992",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "HiFo-Prompt: Prompting with Hindsight and Foresight for LLM-based Automatic Heuristic Design",
    "summary": "arXiv:2508.13333v2 Announce Type: replace Abstract: LLM-based Automatic Heuristic Design (AHD) within Evolutionary Computation (EC) frameworks has shown promising results. However, its effectiveness is hindered by the use of static operators and the lack of knowledge accumulation mechanisms. We introduce HiFo-Prompt, a framework that guides LLMs wi",
    "url": "https://arxiv.org/abs/2508.13333",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Coarse-to-Fine Grounded Memory for LLM Agent Planning",
    "summary": "arXiv:2508.15305v2 Announce Type: replace Abstract: Recent advancements in Large Language Models (LLMs) have driven growing interest in LLM-based agents for complex planning tasks. To avoid costly agent training, many studies adopted memory mechanism that enhances LLM with offline experiences or online trajectory analysis. However, existing works f",
    "url": "https://arxiv.org/abs/2508.15305",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Certainty-Guided Reasoning in Large Language Models: A Dynamic Thinking Budget Approach",
    "summary": "arXiv:2509.07820v2 Announce Type: replace Abstract: Large reasoning language models are typically run with fixed inference budgets, which can waste computation or terminate reasoning prematurely. We introduce Certainty-Guided Reasoning (CGR), a model-agnostic adaptive inference procedure that periodically probes whether the current reasoning suppor",
    "url": "https://arxiv.org/abs/2509.07820",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Steerable Adversarial Scenario Generation through Test-Time Preference Alignment",
    "summary": "arXiv:2509.20102v2 Announce Type: replace Abstract: Adversarial scenario generation is a cost-effective approach for safety assessment of autonomous driving systems. However, existing methods are often constrained to a single, fixed trade-off between competing objectives such as adversariality and realism. This yields behavior-specific models that ",
    "url": "https://arxiv.org/abs/2509.20102",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Rethinking Explainable Disease Prediction: Synergizing Accuracy and Reliability via Reflective Cognitive Architecture",
    "summary": "arXiv:2509.21266v2 Announce Type: replace Abstract: In clinical decision-making, predictive models face a persistent trade-off: accurate models are often opaque \"black boxes,\" while interpretable methods frequently lack predictive precision or statistical grounding. In this paper, we challenge this dichotomy, positing that high predictive accuracy ",
    "url": "https://arxiv.org/abs/2509.21266",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Correct Reasoning Paths Visit Shared Decision Pivots",
    "summary": "arXiv:2509.21549v3 Announce Type: replace Abstract: Chain-of-thought (CoT) reasoning exposes the intermediate thinking process of large language models (LLMs), yet verifying those traces at scale remains unsolved. In response, we introduce the idea of decision pivots-minimal, verifiable checkpoints that any correct reasoning path must visit. We hyp",
    "url": "https://arxiv.org/abs/2509.21549",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Lifelong Learning with Behavior Consolidation for Vehicle Routing",
    "summary": "arXiv:2509.21765v4 Announce Type: replace Abstract: Recent neural solvers have demonstrated promising performance in learning to solve routing problems. However, existing studies are primarily based on one-off training on one or a set of predefined problem distributions and scales, i.e., tasks. When a new task arises, they typically rely on either ",
    "url": "https://arxiv.org/abs/2509.21765",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "TRACE: Learning to Compute on Circuit Graphs",
    "summary": "arXiv:2509.21886v2 Announce Type: replace Abstract: Learning to compute, the ability to model the functional behavior of a circuit graph, is a fundamental challenge for graph representation learning. Yet, the dominant paradigm is architecturally mismatched for this task. This flawed assumption, central to mainstream message passing neural networks ",
    "url": "https://arxiv.org/abs/2509.21886",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Agentic AI Reasoning for Mobile Edge General Intelligence: Fundamentals, Approaches, and Directions",
    "summary": "arXiv:2509.23248v2 Announce Type: replace Abstract: The rapid advancement of large language models (LLMs) has enabled an emergence of agentic artificial intelligence (AI) with powerful reasoning and autonomous decision-making capabilities. This integration with edge computing has led to the development of Mobile Edge General Intelligence (MEGI), wh",
    "url": "https://arxiv.org/abs/2509.23248",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance",
    "summary": "arXiv:2510.01474v3 Announce Type: replace Abstract: As governments move to regulate AI, there is growing interest in using Large Language Models (LLMs) to assess whether or not an AI system complies with a given AI Regulation (AIR). However, there is presently no way to benchmark the performance of LLMs at this task. To fill this void, we introduce",
    "url": "https://arxiv.org/abs/2510.01474",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SurveyG: A Multi-Agent LLM Framework with Hierarchical Citation Graph for Automated Survey Generation",
    "summary": "arXiv:2510.07733v3 Announce Type: replace Abstract: Large language models (LLMs) are increasingly adopted for automating survey paper generation \\cite{wang2406autosurvey, liang2025surveyx, yan2025surveyforge,su2025benchmarking,wen2025interactivesurvey}. Existing approaches typically extract content from a large collection of related papers and prom",
    "url": "https://arxiv.org/abs/2510.07733",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "The Achilles' Heel of LLMs: How Altering a Handful of Neurons Can Cripple Language Abilities",
    "summary": "arXiv:2510.10238v2 Announce Type: replace Abstract: Large Language Models (LLMs) have become foundational tools in natural language processing, powering a wide range of applications and research. Many studies have shown that LLMs share significant similarities with the human brain. Recent neuroscience research has found that a small subset of biolo",
    "url": "https://arxiv.org/abs/2510.10238",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "OpenPhone: Mobile Agentic Foundation Models",
    "summary": "arXiv:2510.22009v2 Announce Type: replace Abstract: With the advancement of multimodal large language models (MLLMs), building GUI agent systems has become an increasingly promising direction--especially for mobile platforms, given their rich app ecosystems and intuitive touch interactions. Yet mobile GUI agents face a critical dilemma: truly on-de",
    "url": "https://arxiv.org/abs/2510.22009",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "GUI Knowledge Bench: Revealing the Knowledge Gap of VLMs in GUI Tasks",
    "summary": "arXiv:2510.26098v2 Announce Type: replace Abstract: Vision language models (VLMs) have advanced graphical user interface (GUI) task automation but still lag behind humans. We hypothesize this gap stems from missing core GUI knowledge, which existing training schemes (such as supervised fine tuning and reinforcement learning) alone cannot fully addr",
    "url": "https://arxiv.org/abs/2510.26098",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "BEAT: Visual Backdoor Attacks on VLM-based Embodied Agents via Contrastive Trigger Learning",
    "summary": "arXiv:2510.27623v2 Announce Type: replace Abstract: Recent advances in Vision-Language Models (VLMs) have propelled embodied agents by enabling direct perception, reasoning, and planning task-oriented actions from visual inputs. However, such vision-driven embodied agents open a new attack surface: visual backdoor attacks, where the agent behaves n",
    "url": "https://arxiv.org/abs/2510.27623",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Massively Parallel Proof-Number Search for Impartial Games and Beyond",
    "summary": "arXiv:2511.10339v2 Announce Type: replace Abstract: Proof-Number Search is a best-first search algorithm with many successful applications, especially in game solving. As large-scale computing clusters become increasingly accessible, parallelization is a natural way to accelerate computation. However, existing parallel versions of Proof-Number Sear",
    "url": "https://arxiv.org/abs/2511.10339",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Towards Reinforcement Learning from Neural Feedback: Mapping fNIRS Signals to Agent Performance",
    "summary": "arXiv:2511.12844v4 Announce Type: replace Abstract: Reinforcement Learning from Human Feedback (RLHF) is a methodology that aligns agent behavior with human preferences by integrating user feedback into the agent's training process. This paper introduces a framework that guides agent training through implicit neural signals, with a focus on the neu",
    "url": "https://arxiv.org/abs/2511.12844",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "AISAC: An Integrated multi-agent System for Transparent, Retrieval-Grounded Scientific Assistance",
    "summary": "arXiv:2511.14043v2 Announce Type: replace Abstract: AI Scientific Assistant Core (AISAC) is a transparent, modular multi-agent runtime developed at Argonne National Laboratory to support long-horizon, evidence-grounded scientific reasoning. Rather than proposing new agent algorithms or claiming autonomous scientific discovery, AISAC contributes a g",
    "url": "https://arxiv.org/abs/2511.14043",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "HuggingR$^{4}$: A Progressive Reasoning Framework for Discovering Optimal Model Companions",
    "summary": "arXiv:2511.18715v2 Announce Type: replace Abstract: Building effective LLM agents increasingly requires selecting appropriate AI models as tools from large open repositories (e.g., HuggingFace with > 2M models) based on natural language requests. Unlike invoking a fixed set of API tools, repository-scale model selection must handle massive, evolvin",
    "url": "https://arxiv.org/abs/2511.18715",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "UNeMo: Collaborative Visual-Language Reasoning and Navigation via a Multimodal World Model",
    "summary": "arXiv:2511.18845v2 Announce Type: replace Abstract: Vision-and-Language Navigation (VLN) requires agents to autonomously navigate complex environments via visual images and natural language instructions--remains highly challenging. Recent research on enhancing language-guided navigation reasoning using pre-trained large language models (LLMs) has s",
    "url": "https://arxiv.org/abs/2511.18845",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "CostNav: A Navigation Benchmark for Real-World Economic-Cost Evaluation of Physical AI Agents",
    "summary": "arXiv:2511.20216v3 Announce Type: replace Abstract: While current navigation benchmarks prioritize task success in simplified settings, they neglect the multidimensional economic constraints essential for the real-world commercialization of autonomous delivery systems. We introduce CostNav, an Economic Navigation Benchmark that evaluates physical A",
    "url": "https://arxiv.org/abs/2511.20216",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Reasoning With a Star: A Heliophysics Dataset and Benchmark for Agentic Scientific Reasoning",
    "summary": "arXiv:2511.20694v2 Announce Type: replace Abstract: Scientific reasoning through Large Language Models in heliophysics involves more than just recalling facts: it requires incorporating physical assumptions, maintaining consistent units, and providing clear scientific formats through coordinated approaches. To address these challenges, we present R",
    "url": "https://arxiv.org/abs/2511.20694",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Conversational No-code, Multi-agentic Disease Module Identification and Drug Repurposing Prediction with ChatDRex",
    "summary": "arXiv:2511.21438v2 Announce Type: replace Abstract: Repurposing approved drugs offers a time-efficient and cost-effective alternative to traditional drug development. However, in silico prediction of repurposing candidates is challenging and requires the effective collaboration of specialists in various fields, including pharmacology, medicine, bio",
    "url": "https://arxiv.org/abs/2511.21438",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Agent policies from higher-order causal functions",
    "summary": "arXiv:2512.10937v2 Announce Type: replace Abstract: We establish a correspondence between equivalence classes of agent-state policies for deterministic POMDPs and one-input process functions (the classical-deterministic limit of higher-order quantum operations). We use this correspondence to build a bridge between the agent-environment interaction ",
    "url": "https://arxiv.org/abs/2512.10937",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Safety Alignment of LMs via Non-cooperative Games",
    "summary": "arXiv:2512.20806v2 Announce Type: replace Abstract: Ensuring the safety of language models (LMs) while maintaining their usefulness remains a critical challenge in AI alignment. Current approaches rely on sequential adversarial training: generating adversarial prompts and fine-tuning LMs to defend against them. We introduce a different paradigm: fr",
    "url": "https://arxiv.org/abs/2512.20806",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization",
    "summary": "arXiv:2512.23126v3 Announce Type: replace Abstract: Direct Preference Optimization (DPO) and its variants have become standard for aligning Large Language Models due to their simplicity and offline stability. However, we identify two fundamental limitations. First, the optimal policy depends on arbitrary modeling choices (scalarization function, re",
    "url": "https://arxiv.org/abs/2512.23126",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Token-Level LLM Collaboration via FusionRoute",
    "summary": "arXiv:2601.05106v2 Announce Type: replace Abstract: Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-sp",
    "url": "https://arxiv.org/abs/2601.05106",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "VirtualEnv: A Platform for Embodied AI Research",
    "summary": "arXiv:2601.07553v2 Announce Type: replace Abstract: As large language models (LLMs) continue to improve in reasoning and decision-making, there is a growing need for realistic and interactive environments where their abilities can be rigorously evaluated. We present VirtualEnv, a next-generation simulation platform built on Unreal Engine 5 that ena",
    "url": "https://arxiv.org/abs/2601.07553",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Imandra CodeLogician: Neuro-Symbolic Reasoning for Precise Analysis of Software Logic",
    "summary": "arXiv:2601.11840v2 Announce Type: replace Abstract: Large Language Models (LLMs) have shown strong performance on code understanding tasks, yet they fundamentally lack the ability to perform precise, exhaustive mathematical reasoning about program behavior. Existing benchmarks either focus on mathematical proof automation, largely disconnected from",
    "url": "https://arxiv.org/abs/2601.11840",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Stability as a Liability:Systematic Breakdown of Linguistic Structure in LLMs",
    "summary": "arXiv:2601.18588v2 Announce Type: replace Abstract: Training stability is typically regarded as a prerequisite for reliable optimization in large language models. In this work, we analyze how stabilizing training dynamics affects the induced generation distribution. We show that under standard maximum likelihood training, stable parameter trajector",
    "url": "https://arxiv.org/abs/2601.18588",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection",
    "summary": "arXiv:2601.19245v3 Announce Type: replace Abstract: Hallucination detection is critical for deploying large language models (LLMs) in real-world applications. Existing hallucination detection methods achieve strong performance when the training and test data come from the same domain, but they suffer from poor cross-domain generalization. In this p",
    "url": "https://arxiv.org/abs/2601.19245",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence",
    "summary": "arXiv:2601.21083v3 Announce Type: replace Abstract: As large language models (LLMs) improve, so do their offensive applications: frontier agents now generate working exploits for under $50 in compute (Heelan, 2026). Defensive incident response (IR) agents must keep pace, but existing benchmarks conflate action execution with correct execution, hidi",
    "url": "https://arxiv.org/abs/2601.21083",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models",
    "summary": "arXiv:2601.21183v2 Announce Type: replace Abstract: Reasoning models frequently agree with incorrect user suggestions -- a behavior known as sycophancy. However, it is unclear where in the reasoning trace this agreement originates and how strong the commitment is. We introduce \\emph{sycophantic anchors} -- sentences identified via counterfactual an",
    "url": "https://arxiv.org/abs/2601.21183",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design",
    "summary": "arXiv:2601.21239v2 Announce Type: replace Abstract: Although Large Language Models have advanced Automated Heuristic Design, treating algorithm evolution as a monolithic text generation task overlooks the coupling between discrete algorithmic structures and continuous numerical parameters. Consequently, existing methods often discard promising algo",
    "url": "https://arxiv.org/abs/2601.21239",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Zero-Shot Statistical Downscaling via Diffusion Posterior Sampling",
    "summary": "arXiv:2601.21760v2 Announce Type: replace Abstract: Conventional supervised climate downscaling struggles to generalize to Global Climate Models (GCMs) due to the lack of paired training data and inherent domain gaps relative to reanalysis. Meanwhile, current zero-shot methods suffer from physical inconsistencies and vanishing gradient issues under",
    "url": "https://arxiv.org/abs/2601.21760",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling",
    "summary": "arXiv:2601.22636v2 Announce Type: replace Abstract: Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting, which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While rece",
    "url": "https://arxiv.org/abs/2601.22636",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery",
    "summary": "arXiv:2601.22896v2 Announce Type: replace Abstract: Large language models (LLMs) have enabled rapid progress in automatic heuristic discovery (AHD), yet most existing methods are predominantly limited by static evaluation against fixed instance distributions, leading to potential overfitting and poor generalization under distributional shifts. We p",
    "url": "https://arxiv.org/abs/2601.22896",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information Gain Reward",
    "summary": "arXiv:2602.00845v2 Announce Type: replace Abstract: Agentic reasoning enables large reasoning models (LRMs) to dynamically acquire external knowledge, but yet optimizing the retrieval process remains challenging due to the lack of dense, principled reward signals. In this paper, we introduce InfoReasoner, a unified framework that incentivizes effec",
    "url": "https://arxiv.org/abs/2602.00845",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering",
    "summary": "arXiv:2602.01465v2 Announce Type: replace Abstract: Large language models have demonstrated strong capabilities in individual software engineering tasks, yet most autonomous systems still treat issue resolution as a monolithic or pipeline-based process. In contrast, real-world software development is organized as a collaborative activity carried ou",
    "url": "https://arxiv.org/abs/2602.01465",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development",
    "summary": "arXiv:2602.01655v2 Announce Type: replace Abstract: Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulti",
    "url": "https://arxiv.org/abs/2602.01655",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
    "summary": "arXiv:2602.01664v2 Announce Type: replace Abstract: In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signal",
    "url": "https://arxiv.org/abs/2602.01664",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MACD: Model-Aware Contrastive Decoding via Counterfactual Data",
    "summary": "arXiv:2602.01740v2 Announce Type: replace Abstract: Video language models (Video-LLMs) are prone to hallucinations, often generating plausible but ungrounded content when visual evidence is weak, ambiguous, or biased. Existing decoding methods, such as contrastive decoding (CD), rely on random perturbations to construct contrastive data for mitigat",
    "url": "https://arxiv.org/abs/2602.01740",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Beyond Quantity: Trajectory Diversity Scaling for Code Agents",
    "summary": "arXiv:2602.03219v2 Announce Type: replace Abstract: As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck",
    "url": "https://arxiv.org/abs/2602.03219",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration",
    "summary": "arXiv:2602.03786v2 Announce Type: replace Abstract: Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereb",
    "url": "https://arxiv.org/abs/2602.03786",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation",
    "summary": "arXiv:2602.03950v2 Announce Type: replace Abstract: Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems",
    "url": "https://arxiv.org/abs/2602.03950",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Reactive Knowledge Representation and Asynchronous Reasoning",
    "summary": "arXiv:2602.05625v2 Announce Type: replace Abstract: Exact inference in complex probabilistic models often incurs prohibitive computational costs. This challenge is particularly acute for autonomous agents in dynamic environments that require frequent, real-time belief updates. Existing methods are often inefficient for ongoing reasoning, as they re",
    "url": "https://arxiv.org/abs/2602.05625",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Generative Ontology: When Structured Knowledge Learns to Create",
    "summary": "arXiv:2602.05636v2 Announce Type: replace Abstract: Traditional ontologies describe domain structure but cannot generate novel artifacts. Large language models generate fluently but produce outputs lacking structural validity, hallucinating mechanisms without components, goals without end conditions. We introduce Generative Ontology, a framework sy",
    "url": "https://arxiv.org/abs/2602.05636",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning",
    "summary": "arXiv:2602.05818v2 Announce Type: replace Abstract: Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they ",
    "url": "https://arxiv.org/abs/2602.05818",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "From Features to Actions: Explainability in Traditional and Agentic AI Systems",
    "summary": "arXiv:2602.06841v2 Announce Type: replace Abstract: Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour u",
    "url": "https://arxiv.org/abs/2602.06841",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents",
    "summary": "arXiv:2602.06855v2 Announce Type: replace Abstract: LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling,",
    "url": "https://arxiv.org/abs/2602.06855",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Playing 20 Question Game with Policy-Based Reinforcement Learning",
    "summary": "arXiv:1808.07645v4 Announce Type: replace-cross Abstract: The 20 Questions (Q20) game is a well known game which encourages deductive reasoning and creativity. In the game, the answerer first thinks of an object such as a famous person or a kind of animal. Then the questioner tries to guess the object by asking 20 questions. In a Q20 game system, t",
    "url": "https://arxiv.org/abs/1808.07645",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "YaRN: Efficient Context Window Extension of Large Language Models",
    "summary": "arXiv:2309.00071v3 Announce Type: replace-cross Abstract: Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficien",
    "url": "https://arxiv.org/abs/2309.00071",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided Image Editing",
    "summary": "arXiv:2310.08785v3 Announce Type: replace-cross Abstract: Text-guided image editing faces significant challenges when considering training and inference flexibility. Much literature collects large amounts of annotated image-text pairs to train text-conditioned generative models from scratch, which is expensive and not efficient. After that, some ap",
    "url": "https://arxiv.org/abs/2310.08785",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Cognitive Edge Device (CED) for Real-Time Environmental Monitoring in Aquatic Ecosystems",
    "summary": "arXiv:2401.06157v3 Announce Type: replace-cross Abstract: Invasive signal crayfish have a detrimental impact on ecosystems. They spread the fungal-type crayfish plague disease (Aphanomyces astaci) that is lethal to the native white clawed crayfish, the only native crayfish species in Britain. Invasive signal crayfish extensively burrow, causing hab",
    "url": "https://arxiv.org/abs/2401.06157",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Delay-Aware Reinforcement Learning for Highway On-Ramp Merging under Stochastic Communication Latency",
    "summary": "arXiv:2403.11852v4 Announce Type: replace-cross Abstract: Delayed and partially observable state information poses significant challenges for reinforcement learning (RL)-based control in real-world autonomous driving. In highway on-ramp merging, a roadside unit (RSU) can sense nearby traffic, perform edge perception, and transmit state estimates to",
    "url": "https://arxiv.org/abs/2403.11852",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Towards Transparent and Efficient Anomaly Detection in Industrial Processes through ExIFFI",
    "summary": "arXiv:2405.01158v3 Announce Type: replace-cross Abstract: Anomaly Detection (AD) is crucial in industrial settings to streamline operations by detecting underlying issues. Conventional methods merely label observations as normal or anomalous, lacking crucial insights. In Industry 5.0, interpretable outcomes become desirable to enable users to under",
    "url": "https://arxiv.org/abs/2405.01158",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Reproducible Benchmarking for Lung Nodule Detection and Malignancy Classification Across Multiple Low-Dose CT Datasets",
    "summary": "arXiv:2405.04605v5 Announce Type: replace-cross Abstract: Evaluation of artificial intelligence (AI) models for low-dose CT lung cancer screening is limited by heterogeneous datasets, annotation standards, and evaluation protocols, making performance difficult to compare and translate across clinical settings. We establish a public, reproducible mu",
    "url": "https://arxiv.org/abs/2405.04605",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Q-Learning under Finite Model Uncertainty",
    "summary": "arXiv:2407.04259v3 Announce Type: replace-cross Abstract: We propose a robust Q-learning algorithm for Markov decision processes under model uncertainty when each state-action pair is associated with a finite ambiguity set of candidate transition kernels. This finite-measure framework enables highly flexible, user-designed uncertainty models and go",
    "url": "https://arxiv.org/abs/2407.04259",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Why Rectified Power Unit Networks Fail and How to Improve It: An Effective Field Theory Perspective",
    "summary": "arXiv:2408.02697v5 Announce Type: replace-cross Abstract: The Rectified Power Unit (RePU) activation function, a differentiable generalization of the Rectified Linear Unit (ReLU), has shown promise in constructing neural networks due to its smoothness properties. However, deep RePU networks often suffer from critical issues such as vanishing or exp",
    "url": "https://arxiv.org/abs/2408.02697",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Optimizing Automated Picking Systems in Warehouse Robots Using Machine Learning",
    "summary": "arXiv:2408.16633v2 Announce Type: replace-cross Abstract: With the rapid growth of global e-commerce, the demand for automation in the logistics industry is increasing. This study focuses on automated picking systems in warehouses, utilizing deep learning and reinforcement learning technologies to enhance picking efficiency and accuracy while reduc",
    "url": "https://arxiv.org/abs/2408.16633",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems",
    "summary": "arXiv:2409.01392v3 Announce Type: replace-cross Abstract: Much previous AI research has focused on developing monolithic models to maximize their intelligence, with the primary goal of enhancing performance on specific tasks. In contrast, this work attempts to study using LLM-based agents to design collaborative AI systems autonomously. To explore ",
    "url": "https://arxiv.org/abs/2409.01392",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "RARe: Retrieval Augmented Retrieval with In-Context Examples",
    "summary": "arXiv:2410.20088v2 Announce Type: replace-cross Abstract: While in-context learning is well-studied with decoder-only language models (LLMs), its utility for encoder-only models remains underexplored. We study in-context learning for encoder-only models for text retrieval tasks. Can incorporating in-context examples (query-document pairs) to the ta",
    "url": "https://arxiv.org/abs/2410.20088",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Software Performance Engineering for Foundation Model-Powered Software",
    "summary": "arXiv:2411.09580v2 Announce Type: replace-cross Abstract: The rise of Foundation Models (FMs) like Large Language Models (LLMs) is revolutionizing software development. Despite the impressive prototypes, transforming FMware into production-ready products demands complex engineering across various domains. A critical but overlooked aspect is perform",
    "url": "https://arxiv.org/abs/2411.09580",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Disentangled Parameter-Efficient Linear Model for Long-Term Time Series Forecasting",
    "summary": "arXiv:2411.17257v2 Announce Type: replace-cross Abstract: Long-term Time Series Forecasting (LTSF) is crucial across various domains, but complex deep models like Transformers are often prone to overfitting on extended sequences. Linear Fully Connected models have emerged as a powerful alternative, achieving competitive results with fewer parameter",
    "url": "https://arxiv.org/abs/2411.17257",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SoK: Blockchain-Based Decentralized AI (DeAI)",
    "summary": "arXiv:2411.17461v5 Announce Type: replace-cross Abstract: Centralization enhances the efficiency of Artificial Intelligence (AI) but also introduces critical challenges, including single points of failure, inherent biases, data privacy risks, and scalability limitations. To address these issues, blockchain-based Decentralized Artificial Intelligenc",
    "url": "https://arxiv.org/abs/2411.17461",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "DeMo: Decoupled Momentum Optimization",
    "summary": "arXiv:2411.19870v2 Announce Type: replace-cross Abstract: Scaling neural network training increasingly depends on synchronous data-parallelism, yet full-precision gradient all-reduce imposes a severe communication bottleneck. We propose Decoupled Momentum Optimization (DeMo), a drop-in replacement for any momentum-based optimizers that significantl",
    "url": "https://arxiv.org/abs/2411.19870",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Automatic Item Generation for Personality Situational Judgment Tests with Large Language Models",
    "summary": "arXiv:2412.12144v4 Announce Type: replace-cross Abstract: Personality assessment through situational judgment tests (SJTs) offers unique advantages over traditional Likert-type self-report scales, yet their development remains labor-intensive, time-consuming, and heavily dependent on subject matter experts. Recent advances in large language models ",
    "url": "https://arxiv.org/abs/2412.12144",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "AI-Powered Intracranial Hemorrhage Detection: A Co-Scale Convolutional Attention Model with Uncertainty-Based Fuzzy Integral Operator and Feature Screening",
    "summary": "arXiv:2412.14869v2 Announce Type: replace-cross Abstract: Intracranial hemorrhage (ICH) refers to the leakage or accumulation of blood within the skull, which occurs due to the rupture of blood vessels in or around the brain. If this condition is not diagnosed in a timely manner and appropriately treated, it can lead to serious complications such a",
    "url": "https://arxiv.org/abs/2412.14869",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "LEASE: Offline Preference-based Reinforcement Learning with High Sample Efficiency",
    "summary": "arXiv:2412.21001v3 Announce Type: replace-cross Abstract: Offline preference-based reinforcement learning (PbRL) provides an effective way to overcome the challenges of designing reward and the high costs of online interaction. However, since labeling preference needs real-time human feedback, acquiring sufficient preference labels is challenging. ",
    "url": "https://arxiv.org/abs/2412.21001",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Evaluating Sample Utility for Efficient Data Selection by Mimicking Model Weights",
    "summary": "arXiv:2501.06708v4 Announce Type: replace-cross Abstract: Large-scale web-crawled datasets contain noise, bias, and irrelevant information, necessitating data selection techniques. Existing methods depend on hand-crafted heuristics, downstream datasets, or require expensive influence-based computations -- all of which limit scalability and introduc",
    "url": "https://arxiv.org/abs/2501.06708",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Rigor, Reliability, and Reproducibility Matter: A Decade-Scale Survey of 572 Code Benchmarks",
    "summary": "arXiv:2501.10711v4 Announce Type: replace-cross Abstract: Code-related benchmarks play a critical role in evaluating large language models (LLMs), yet their quality fundamentally shapes how the community interprets model capabilities. In the past few years, awareness of benchmark quality has grown. Yet, after a decade-scale (2014-2025) survey over ",
    "url": "https://arxiv.org/abs/2501.10711",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Rethinking Functional Brain Connectome Analysis: Do Graph Deep Learning Models Help",
    "summary": "arXiv:2501.17207v2 Announce Type: replace-cross Abstract: Graph deep learning models, a class of AI-driven approaches employing a message aggregation mechanism, have gained popularity for analyzing the functional brain connectome in neuroimaging. However, their actual effectiveness remains unclear. In this study, we re-examine graph deep learning v",
    "url": "https://arxiv.org/abs/2501.17207",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SafeDialBench: A Fine-Grained Safety Evaluation Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks",
    "summary": "arXiv:2502.11090v4 Announce Type: replace-cross Abstract: With the rapid advancement of Large Language Models (LLMs), the safety of LLMs has been a critical concern requiring precise assessment. Current benchmarks primarily concentrate on single-turn dialogues or a single jailbreak attack method to assess the safety. Additionally, these benchmarks ",
    "url": "https://arxiv.org/abs/2502.11090",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models",
    "summary": "arXiv:2502.15487v4 Announce Type: replace-cross Abstract: Large Language Models (LLMs) are increasingly used in tasks requiring interpretive and inferential accuracy. In this paper, we introduce ExpliCa, a new dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely integrates both causal and temporal relations presented in differ",
    "url": "https://arxiv.org/abs/2502.15487",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence",
    "summary": "arXiv:2502.17420v2 Announce Type: replace-cross Abstract: The safety alignment of large language models (LLMs) can be circumvented through adversarially crafted inputs, yet the mechanisms by which these attacks bypass safety barriers remain poorly understood. Prior work suggests that a single refusal direction in the model's activation space determ",
    "url": "https://arxiv.org/abs/2502.17420",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MAFE: Enabling Equitable Algorithm Design in Multi-Agent Multi-Stage Decision-Making Systems",
    "summary": "arXiv:2502.18534v2 Announce Type: replace-cross Abstract: Algorithmic fairness is often studied in static or single-agent settings, yet many real-world decision-making systems involve multiple interacting entities whose multi-stage actions jointly influence long-term outcomes. Existing fairness methods applied at isolated decision points frequently",
    "url": "https://arxiv.org/abs/2502.18534",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Marking Code Without Breaking It: Code Watermarking for Detecting LLM-Generated Code",
    "summary": "arXiv:2502.18851v4 Announce Type: replace-cross Abstract: Identifying LLM-generated code through watermarking poses a challenge in preserving functional correctness. Previous methods rely on the assumption that watermarking high-entropy tokens effectively maintains output quality. Our analysis reveals a fundamental limitation of this assumption: sy",
    "url": "https://arxiv.org/abs/2502.18851",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Taxation Perspectives from Large Language Models: A Case Study on Additional Tax Penalties",
    "summary": "arXiv:2503.03444v2 Announce Type: replace-cross Abstract: How capable are large language models (LLMs) in the domain of taxation? Although numerous studies have explored the legal domain, research dedicated to taxation remains scarce. Moreover, the datasets used in these studies are either simplified, failing to reflect the real-world complexities,",
    "url": "https://arxiv.org/abs/2503.03444",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "RiskAgent: Synergizing Language Models with Validated Tools for Evidence-Based Risk Prediction",
    "summary": "arXiv:2503.03802v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) achieve competitive results compared to human experts in medical examinations. However, it remains a challenge to apply LLMs to complex clinical decision-making, which requires a deep understanding of medical knowledge and differs from the standardized, exam-styl",
    "url": "https://arxiv.org/abs/2503.03802",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of Experts",
    "summary": "arXiv:2503.05066v4 Announce Type: replace-cross Abstract: The Mixture of Experts (MoE) is an effective architecture for scaling large language models by leveraging sparse expert activation to balance performance and efficiency. However, under expert parallelism, MoE suffers from inference inefficiencies due to imbalanced token-to-expert assignment,",
    "url": "https://arxiv.org/abs/2503.05066",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Right Reward Right Time for Federated Learning",
    "summary": "arXiv:2503.07869v2 Announce Type: replace-cross Abstract: Critical learning periods (CLPs) in federated learning (FL) refer to early stages during which low-quality contributions (e.g., sparse training data availability) can permanently impair the performance of the global model owned by the model owner (i.e., a cloud server). However, existing inc",
    "url": "https://arxiv.org/abs/2503.07869",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Vision Transformer for Intracranial Hemorrhage Classification in CT Scans Using an Entropy-Aware Fuzzy Integral Strategy for Adaptive Scan-Level Decision Fusion",
    "summary": "arXiv:2503.08609v2 Announce Type: replace-cross Abstract: Intracranial hemorrhage (ICH) is a critical medical emergency caused by the rupture of cerebral blood vessels, leading to internal bleeding within the skull. Accurate and timely classification of hemorrhage subtypes is essential for effective clinical decision-making. To address this challen",
    "url": "https://arxiv.org/abs/2503.08609",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "TruthPrInt: Mitigating Large Vision-Language Models Object Hallucination Via Latent Truthful-Guided Pre-Intervention",
    "summary": "arXiv:2503.10602v3 Announce Type: replace-cross Abstract: Object Hallucination (OH) has been acknowledged as one of the major trustworthy challenges in Large Vision-Language Models (LVLMs). Recent advancements in Large Language Models (LLMs) indicate that internal states, such as hidden states, encode the \"overall truthfulness\" of generated respons",
    "url": "https://arxiv.org/abs/2503.10602",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Adversarial Wear and Tear: Exploiting Natural Damage for Generating Physical-World Adversarial Examples",
    "summary": "arXiv:2503.21164v2 Announce Type: replace-cross Abstract: The presence of adversarial examples in the physical world poses significant challenges to the deployment of Deep Neural Networks in safety-critical applications such as autonomous driving. Most existing methods for crafting physical-world adversarial examples are ad-hoc, relying on temporar",
    "url": "https://arxiv.org/abs/2503.21164",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Achieving Unanimous Consensus Through Multi-Agent Deliberation",
    "summary": "arXiv:2504.02128v2 Announce Type: replace-cross Abstract: Blockchain consensus mechanisms have relied on algorithms such as Proof-of-Work (PoW) and Proof-of-Stake (PoS) to ensure network functionality and integrity. However, these approaches struggle with adaptability for decision-making where the opinions of each matter rather than reaching an agr",
    "url": "https://arxiv.org/abs/2504.02128",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Towards an Understanding of Context Utilization in Code Intelligence",
    "summary": "arXiv:2504.08734v2 Announce Type: replace-cross Abstract: Code intelligence is an emerging domain in software engineering, aiming to improve the effectiveness and efficiency of various code-related tasks. Recent research suggests that incorporating contextual information beyond the basic original task inputs (i.e., source code) can substantially en",
    "url": "https://arxiv.org/abs/2504.08734",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex",
    "summary": "arXiv:2504.12474v4 Announce Type: replace-cross Abstract: Text-attributed graphs (TAGs) present unique challenges in representation learning by requiring models to capture both the semantic richness of node-associated texts and the structural dependencies of the graph. While graph neural networks (GNNs) excel at modeling topological information, th",
    "url": "https://arxiv.org/abs/2504.12474",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Toward Efficient Exploration by Large Language Model Agents",
    "summary": "arXiv:2504.20997v2 Announce Type: replace-cross Abstract: A burgeoning area within reinforcement learning (RL) is the design of sequential decision-making agents centered around large language models (LLMs). While autonomous decision-making agents powered by modern LLMs could facilitate numerous real-world applications, such successes demand agents",
    "url": "https://arxiv.org/abs/2504.20997",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Dynamic and Distributed Routing in IoT Networks based on Multi-Objective Q-Learning",
    "summary": "arXiv:2505.00918v5 Announce Type: replace-cross Abstract: IoT networks often face conflicting routing goals such as maximizing packet delivery, minimizing delay, and conserving limited battery energy. These priorities can also change dynamically: for example, an emergency alert requires high reliability, while routine monitoring prioritizes energy ",
    "url": "https://arxiv.org/abs/2505.00918",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Systematic Failures in Collective Reasoning under Distributed Information in Multi-Agent LLMs",
    "summary": "arXiv:2505.11556v3 Announce Type: replace-cross Abstract: Multi-agent systems built on large language models (LLMs) are expected to enhance decision-making by pooling distributed information, yet systematically evaluating this capability has remained challenging. We introduce HiddenBench, a 65-task benchmark grounded in the Hidden Profile paradigm,",
    "url": "https://arxiv.org/abs/2505.11556",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Dist2ill: Distributional Distillation for One-Pass Uncertainty Estimation in Large Language Models",
    "summary": "arXiv:2505.11731v3 Announce Type: replace-cross Abstract: Large Language Models (LLMs) often exhibit misalignment between the quality of their generated responses and the confidence estimates they assign to them. Bayesian treatments, such as marginalizing over a reliable weight posterior or over the space of reasoning traces, provide an effective r",
    "url": "https://arxiv.org/abs/2505.11731",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Safety Subspaces are Not Linearly Distinct: A Fine-Tuning Case Study",
    "summary": "arXiv:2505.14185v3 Announce Type: replace-cross Abstract: Large Language Models (LLMs) rely on safety alignment to produce socially acceptable responses. However, this behavior is known to be brittle: further fine-tuning, even on benign or lightly contaminated data, can degrade safety and reintroduce harmful behaviors. A growing body of work sugges",
    "url": "https://arxiv.org/abs/2505.14185",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "ABBA-Adapters: Efficient and Expressive Fine-Tuning of Foundation Models",
    "summary": "arXiv:2505.14238v4 Announce Type: replace-cross Abstract: Large Language Models have demonstrated strong performance across a wide range of tasks, but adapting them efficiently to new domains remains a key challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by introducing lightweight, trainable modules while keeping most pre-trai",
    "url": "https://arxiv.org/abs/2505.14238",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "PiFlow: Principle-Aware Scientific Discovery with Multi-Agent Collaboration",
    "summary": "arXiv:2505.15047v4 Announce Type: replace-cross Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate remarkable potential for scientific discovery. Existing approaches, however, often automate scientific discovery using predefined workflows that lack rationality constraints. This often leads to aimless hypothesizing and ",
    "url": "https://arxiv.org/abs/2505.15047",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Efficient Policy Optimization in Robust Constrained MDPs with Iteration Complexity Guarantees",
    "summary": "arXiv:2505.19238v3 Announce Type: replace-cross Abstract: Constrained decision-making is essential for designing safe policies in real-world control systems, yet simulated environments often fail to capture real-world adversities. We consider the problem of learning a policy that will maximize the cumulative reward while satisfying a constraint, ev",
    "url": "https://arxiv.org/abs/2505.19238",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Can NeRFs See without Cameras?",
    "summary": "arXiv:2505.22441v3 Announce Type: replace-cross Abstract: Neural Radiance Fields (NeRFs) have been remarkably successful at synthesizing novel views of 3D scenes by optimizing a volumetric scene function. This scene function models how optical rays bring color information from a 3D object to the camera pixels. Radio frequency (RF) or audio signals ",
    "url": "https://arxiv.org/abs/2505.22441",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Who Gets Credit or Blame? Attributing Accountability in Modern AI Systems",
    "summary": "arXiv:2506.00175v4 Announce Type: replace-cross Abstract: Modern AI systems are typically developed through multiple stages-pretraining, fine-tuning rounds, and subsequent adaptation or alignment, where each stage builds on the previous ones and updates the model in distinct ways. This raises a critical question of accountability: when a deployed m",
    "url": "https://arxiv.org/abs/2506.00175",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint",
    "summary": "arXiv:2506.07022v2 Announce Type: replace-cross Abstract: As LLMs are increasingly deployed in real-world applications, ensuring their ability to refuse malicious prompts, especially jailbreak attacks, is essential for safe and reliable use. Recently, activation steering has emerged as an effective approach for enhancing LLM safety by adding a refu",
    "url": "https://arxiv.org/abs/2506.07022",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Task-Conditioned Probing Reveals Brain-Alignment Patterns in Instruction-Tuned Multimodal LLMs",
    "summary": "arXiv:2506.08277v2 Announce Type: replace-cross Abstract: Recent voxel-wise multimodal brain encoding studies have shown that multimodal large language models (MLLMs) exhibit a higher degree of brain alignment compared to unimodal models. More recently, instruction-tuned multimodal (IT) models have been shown to generate task-specific representatio",
    "url": "https://arxiv.org/abs/2506.08277",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Beyond Bias Scores: Unmasking Vacuous Neutrality in Small Language Models",
    "summary": "arXiv:2506.08487v3 Announce Type: replace-cross Abstract: The rapid adoption of Small Language Models (SLMs) for resource constrained applications has outpaced our understanding of their ethical and fairness implications. To address this gap, we introduce the Vacuous Neutrality Framework (VaNeu), a multi-dimensional evaluation paradigm designed to ",
    "url": "https://arxiv.org/abs/2506.08487",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Language Bottleneck Models for Qualitative Knowledge State Modeling",
    "summary": "arXiv:2506.16982v2 Announce Type: replace-cross Abstract: Accurately assessing student knowledge is central to education. Cognitive Diagnosis (CD) models estimate student proficiency at a fixed point in time, while Knowledge Tracing (KT) methods model evolving knowledge states to predict future performance. However, existing approaches either provi",
    "url": "https://arxiv.org/abs/2506.16982",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "These Are Not All the Features You Are Looking For: A Fundamental Bottleneck in Supervised Pretraining",
    "summary": "arXiv:2506.18221v3 Announce Type: replace-cross Abstract: Transfer learning is widely used to adapt large pretrained models to new tasks with only a small amount of new data. However, a challenge persists -- the features from the original task often do not fully cover what is needed for unseen data, especially when the relatedness of tasks is not c",
    "url": "https://arxiv.org/abs/2506.18221",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and Model Selection at Scale",
    "summary": "arXiv:2506.21550v2 Announce Type: replace-cross Abstract: Anomaly detection in multivariate time series is essential across domains such as healthcare, cybersecurity, and industrial monitoring, yet remains fundamentally challenging due to high-dimensional dependencies, the presence of cross-correlations between time-dependent variables, and the sca",
    "url": "https://arxiv.org/abs/2506.21550",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Theoretical Modeling of Large Language Model Self-Improvement Training Dynamics Through Solver-Verifier Gap",
    "summary": "arXiv:2507.00075v4 Announce Type: replace-cross Abstract: Self-improvement is a significant techniques within the realm of large language model (LLM), aiming to enhance the LLM performance without relying on external data. Despite its significance, generally how LLM performances evolve during the self-improvement process remains underexplored. In t",
    "url": "https://arxiv.org/abs/2507.00075",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Optimas: Optimizing Compound AI Systems with Globally Aligned Local Rewards",
    "summary": "arXiv:2507.03041v4 Announce Type: replace-cross Abstract: Compound AI systems integrating multiple components, such as Large Language Models, specialized tools, and traditional machine learning models, are increasingly deployed to solve complex real-world tasks. However, optimizing compound systems remains challenging due to their non-differentiabl",
    "url": "https://arxiv.org/abs/2507.03041",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MedVAL: Toward Expert-Level Medical Text Validation with Language Models",
    "summary": "arXiv:2507.03152v5 Announce Type: replace-cross Abstract: With the growing use of language models (LMs) in clinical environments, there is an immediate need to evaluate the accuracy and safety of LM-generated medical text. Currently, such evaluation relies solely on manual physician review. However, detecting errors in LM-generated text is challeng",
    "url": "https://arxiv.org/abs/2507.03152",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Sequential Attention-based Sampling for Histopathological Analysis",
    "summary": "arXiv:2507.05077v4 Announce Type: replace-cross Abstract: Deep neural networks are increasingly applied in automated histopathology. Yet, whole-slide images (WSIs) are often acquired at gigapixel sizes, rendering them computationally infeasible to analyze entirely at high resolution. Diagnostic labels are largely available only at the slide-level, ",
    "url": "https://arxiv.org/abs/2507.05077",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "DRAGOn: Designing RAG On Periodically Updated Corpus",
    "summary": "arXiv:2507.05713v3 Announce Type: replace-cross Abstract: This paper introduces DRAGOn, method to design a RAG benchmark on a regularly updated corpus. It features recent reference datasets, a question generation framework, an automatic evaluation pipeline, and a public leaderboard. Specified reference datasets allow for uniform comparison of RAG s",
    "url": "https://arxiv.org/abs/2507.05713",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "XiChen: A global weather observation-to-forecast machine learning system via four-dimensional variational gradient-guided flexible assimilation",
    "summary": "arXiv:2507.09202v3 Announce Type: replace-cross Abstract: Machine Learning (ML) has shown great promise in revolutionizing weather forecasting, yet most ML systems still rely on initial conditions generated by Numerical Weather Prediction (NWP) systems. End-to-end ML models aim to eliminate this dependency, but they often rely on observation-specif",
    "url": "https://arxiv.org/abs/2507.09202",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Evolution of Fear and Social Rewards in Prey-Predator Relationship",
    "summary": "arXiv:2507.09992v2 Announce Type: replace-cross Abstract: Fear is a critical brain function that enables us to learn to avoid danger via reinforcement learning (RL). While many researchers have argued that fear has evolved to escape predators, how varying predatory pressures have shaped fear and other rewards, including positive social rewards for ",
    "url": "https://arxiv.org/abs/2507.09992",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal Data",
    "summary": "arXiv:2507.12412v2 Announce Type: replace-cross Abstract: In many critical domains, features are not freely available at inference time: each measurement may come with a cost of time, money, and risk. Longitudinal prediction further complicates this setting because both features and labels evolve over time, and missing measurements at earlier timep",
    "url": "https://arxiv.org/abs/2507.12412",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "\"PhyWorldBench\": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models",
    "summary": "arXiv:2507.13428v2 Announce Type: replace-cross Abstract: Video generation models have achieved remarkable progress in creating high-quality, photorealistic content. However, their ability to accurately simulate physical phenomena remains a critical and unresolved challenge. This paper presents PhyWorldBench, a comprehensive benchmark designed to e",
    "url": "https://arxiv.org/abs/2507.13428",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models",
    "summary": "arXiv:2507.14811v5 Announce Type: replace-cross Abstract: Diffusion models have demonstrated exceptional generative capabilities but are computationally intensive, posing significant challenges for deployment in resource-constrained or latency-sensitive environments. Quantization offers an effective means to reduce model size and computational cost",
    "url": "https://arxiv.org/abs/2507.14811",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Efficient Attention Mechanisms for Large Language Models: A Survey",
    "summary": "arXiv:2507.19595v3 Announce Type: replace-cross Abstract: Transformer-based architectures have become the prevailing backbone of large language models. However, the quadratic time and memory complexity of self-attention remains a fundamental obstacle to efficient long-context modeling. To address this limitation, recent research has introduced two ",
    "url": "https://arxiv.org/abs/2507.19595",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "A Parallel Alternative for Energy-Efficient Neural Network Training and Inferencing",
    "summary": "arXiv:2508.00960v2 Announce Type: replace-cross Abstract: Energy efficiency of training and inferencing with large neural network models is a critical challenge facing the future of sustainable large-scale machine learning workloads. This paper introduces an alternative strategy, called phantom parallelism, to minimize the net energy consumption of",
    "url": "https://arxiv.org/abs/2508.00960",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "TensorHyper-VQC: A Tensor-Train-Guided Hypernetwork for Robust and Scalable Variational Quantum Computing",
    "summary": "arXiv:2508.01116v4 Announce Type: replace-cross Abstract: Variational Quantum Computing (VQC) faces fundamental scalability barriers, primarily due to barren plateaus and sensitivity to quantum noise. To address these challenges, we introduce TensorHyper-VQC, a novel tensor-train (TT)-guided hypernetwork framework that significantly improves the ro",
    "url": "https://arxiv.org/abs/2508.01116",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control",
    "summary": "arXiv:2508.05342v2 Announce Type: replace-cross Abstract: Teaching robots dexterous skills from human videos remains challenging due to the reliance on low-level trajectory imitation, which fails to generalize across object types, spatial layouts, and manipulator configurations. We propose Graph-Fused Vision-Language-Action (GF-VLA), a framework th",
    "url": "https://arxiv.org/abs/2508.05342",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Hyperspectral Imaging",
    "summary": "arXiv:2508.08107v2 Announce Type: replace-cross Abstract: Hyperspectral imaging (HSI) is an advanced sensing modality that simultaneously captures spatial and spectral information, enabling non-invasive, label-free analysis of material, chemical, and biological properties. This Primer presents a comprehensive overview of HSI, from the underlying ph",
    "url": "https://arxiv.org/abs/2508.08107",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Benchmarking Large Language Models for Geolocating Colonial Virginia Land Grants",
    "summary": "arXiv:2508.08266v2 Announce Type: replace-cross Abstract: Virginia's seventeenth- and eighteenth-century land patents survive primarily as narrative metes-and-bounds descriptions, limiting spatial analysis. This study systematically evaluates current-generation large language models (LLMs) in converting these prose abstracts into geographically acc",
    "url": "https://arxiv.org/abs/2508.08266",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "DegDiT: Controllable Audio Generation with Dynamic Event Graph Guided Diffusion Transformer",
    "summary": "arXiv:2508.13786v2 Announce Type: replace-cross Abstract: Controllable text-to-audio generation aims to synthesize audio from textual descriptions while satisfying user-specified constraints, including event types, temporal sequences, and onset and offset timestamps. This enables precise control over both the content and temporal structure of the g",
    "url": "https://arxiv.org/abs/2508.13786",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Beyond the Mean: Fisher-Orthogonal Projection for Natural Gradient Descent in Large Batch Training",
    "summary": "arXiv:2508.13898v3 Announce Type: replace-cross Abstract: Modern GPUs are equipped with large amounts of high-bandwidth memory, enabling them to support mini-batch sizes of up to tens of thousands of training samples. However, most existing optimizers struggle to perform effectively at such a large batch size. As batch size increases, gradient nois",
    "url": "https://arxiv.org/abs/2508.13898",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS",
    "summary": "arXiv:2508.14313v3 Announce Type: replace-cross Abstract: Test-time scaling (TTS) for large language models (LLMs) has thus far fallen into two largely separate paradigms: (1) reinforcement learning (RL) methods that optimize sparse outcome-based rewards, yet suffer from instability and low sample efficiency; and (2) search-based techniques guided ",
    "url": "https://arxiv.org/abs/2508.14313",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Practical Feasibility of Gradient Inversion Attacks in Federated Learning",
    "summary": "arXiv:2508.19819v2 Announce Type: replace-cross Abstract: Gradient inversion attacks are often presented as a serious privacy threat in federated learning, with recent work reporting increasingly strong reconstructions under favorable experimental settings. However, it remains unclear whether such attacks are feasible in modern, performance-optimiz",
    "url": "https://arxiv.org/abs/2508.19819",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis",
    "summary": "arXiv:2508.20033v2 Announce Type: replace-cross Abstract: The ability to research and synthesize knowledge is central to human expertise and progress. A new class of AI systems--designed for generative research synthesis--aims to automate this process by retrieving information from the live web and producing long-form, cited reports. Yet, evaluatin",
    "url": "https://arxiv.org/abs/2508.20033",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning",
    "summary": "arXiv:2509.05614v2 Announce Type: replace-cross Abstract: Pruning is a typical acceleration technique for compute-bound models by removing computation on unimportant values. Recently, it has been applied to accelerate Vision-Language-Action (VLA) model inference. However, existing acceleration methods focus on local information from the current act",
    "url": "https://arxiv.org/abs/2509.05614",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes",
    "summary": "arXiv:2509.10625v2 Announce Type: replace-cross Abstract: Do large language models (LLMs) anticipate when they will answer correctly? To study this, we extract activations after a question is read but before any tokens are generated, and train linear probes to predict whether the model's forthcoming answer will be correct. Across three open-source ",
    "url": "https://arxiv.org/abs/2509.10625",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Vibe Coding for Product Design: Understanding Product Team Members' Perceptions of AI-Assisted Design and Development",
    "summary": "arXiv:2509.10652v2 Announce Type: replace-cross Abstract: Generative AI is reshaping product design practices through \"vibe coding\", where product team members express intent in natural language and AI translates it into functional prototypes and code. Despite rapid adoption, little research has examined how vibe coding reconfigures product develop",
    "url": "https://arxiv.org/abs/2509.10652",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Fed-PISA: Federated Voice Cloning via Personalized Identity-Style Adaptation",
    "summary": "arXiv:2509.16010v2 Announce Type: replace-cross Abstract: Voice cloning for Text-to-Speech (TTS) aims to generate expressive and personalized speech from text using limited data from a target speaker. Federated Learning (FL) offers a collaborative and privacy-preserving framework for this task, but existing approaches suffer from high communication",
    "url": "https://arxiv.org/abs/2509.16010",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation",
    "summary": "arXiv:2509.21265v2 Announce Type: replace-cross Abstract: High-resolution (HR) medical videos are vital for accurate diagnosis, yet are hard to acquire due to hardware limitations and physiological constraints. Clinically, the collected low-resolution (LR) medical videos present unique challenges for video super-resolution (VSR) models, including c",
    "url": "https://arxiv.org/abs/2509.21265",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MIXRAG : Mixture-of-Experts Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering",
    "summary": "arXiv:2509.21391v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have achieved impressive performance across a wide range of applications. However, they often suffer from hallucinations in knowledge-intensive domains due to their reliance on static pretraining corpora. To address this limitation, Retrieval-Augmented Generation",
    "url": "https://arxiv.org/abs/2509.21391",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping",
    "summary": "arXiv:2509.21880v3 Announce Type: replace-cross Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework for improving the reasoning abilities of Large Language Models (LLMs). However, current methods such as GRPO rely only on problems where the model responses to the same input differ in correctness, while ignoring t",
    "url": "https://arxiv.org/abs/2509.21880",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Interpretable Discovery of One-parameter Subgroups: A Modular Framework for Elliptical, Hyperbolic, and Parabolic Symmetries",
    "summary": "arXiv:2509.22219v4 Announce Type: replace-cross Abstract: We propose a modular, data-driven framework for jointly learning unknown functional mappings and discovering the underlying one-parameter symmetry subgroup governing the data. Unlike conventional geometric deep learning methods that assume known symmetries, our approach identifies the releva",
    "url": "https://arxiv.org/abs/2509.22219",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "ASSESS: A Semantic and Structural Evaluation Framework for Statement Similarity",
    "summary": "arXiv:2509.22246v2 Announce Type: replace-cross Abstract: Despite significant strides in statement autoformalization, a critical gap remains in the development of automated evaluation metrics capable of assessing formal translation quality. Existing metrics often fail to balance semantic and structural information: string-based methods neglect sema",
    "url": "https://arxiv.org/abs/2509.22246",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Functional Critics Are Essential for Actor-Critic: From Off-Policy Stability to Efficient Exploration",
    "summary": "arXiv:2509.22964v4 Announce Type: replace-cross Abstract: The actor-critic (AC) framework has achieved strong empirical success in off-policy reinforcement learning but suffers from the \"moving target\" problem, where the evaluated policy changes continually. Functional critics, or policy-conditioned value functions, address this by explicitly inclu",
    "url": "https://arxiv.org/abs/2509.22964",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence",
    "summary": "arXiv:2509.23573v3 Announce Type: replace-cross Abstract: Large language models (LLMs) are increasingly used to help security analysts manage the surge of cyber threats, automating tasks from vulnerability assessment to incident response. Yet in operational CTI workflows, reliability gaps remain substantial. Existing explanations often point to gen",
    "url": "https://arxiv.org/abs/2509.23573",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning",
    "summary": "arXiv:2509.24372v2 Announce Type: replace-cross Abstract: Fine-tuning large language models (LLMs) for downstream tasks is an essential stage of modern AI deployment. Reinforcement learning (RL) has emerged as the dominant fine-tuning paradigm, underpinning many state-of-the-art LLMs. In contrast, evolution strategies (ES) has largely been overlook",
    "url": "https://arxiv.org/abs/2509.24372",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy",
    "summary": "arXiv:2509.24385v3 Announce Type: replace-cross Abstract: Recent developments in Multimodal Large Language Models (MLLMs) have significantly improved Vision-Language (VL) reasoning in 2D domains. However, extending these capabilities to 3D scene understanding remains a major challenge. Existing 3D Multimodal Large Language Models (3D-MLLMs) often d",
    "url": "https://arxiv.org/abs/2509.24385",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models",
    "summary": "arXiv:2509.24510v4 Announce Type: replace-cross Abstract: Recent empirical studies have explored the idea of continuing to train a model at test-time for a given task, known as test-time training (TTT), and have found it to yield significant performance improvements. However, there is limited understanding of why and when TTT is effective. Earlier ",
    "url": "https://arxiv.org/abs/2509.24510",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization",
    "summary": "arXiv:2509.25416v2 Announce Type: replace-cross Abstract: Emotional text-to-speech seeks to convey affect while preserving intelligibility and prosody, yet existing methods rely on coarse labels or proxy classifiers and receive only utterance-level feedback. We introduce Emotion-Aware Stepwise Preference Optimization (EASPO), a post-training framew",
    "url": "https://arxiv.org/abs/2509.25416",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "CHAI: Command Hijacking against embodied AI",
    "summary": "arXiv:2510.00181v2 Announce Type: replace-cross Abstract: Embodied Artificial Intelligence (AI) promises to handle edge cases in robotic vehicle systems where data is scarce by using common-sense reasoning grounded in perception and action to generalize beyond training distributions and adapt to novel real-world situations. These capabilities, howe",
    "url": "https://arxiv.org/abs/2510.00181",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Copy-Paste to Mitigate Large Language Model Hallucinations",
    "summary": "arXiv:2510.00508v2 Announce Type: replace-cross Abstract: While Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to generate contextually grounded responses, contextual faithfulness remains challenging as LLMs may not consistently trust provided context, leading to hallucinations that undermine reliability. We observe an in",
    "url": "https://arxiv.org/abs/2510.00508",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Towards Open-Ended Discovery for Low-Resource NLP",
    "summary": "arXiv:2510.01220v2 Announce Type: replace-cross Abstract: Natural Language Processing (NLP) for low-resource languages remains fundamentally constrained by the lack of textual corpora, standardized orthographies, and scalable annotation pipelines. While recent advances in large language models have improved cross-lingual transfer, they remain inacc",
    "url": "https://arxiv.org/abs/2510.01220",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Trade in Minutes! Rationality-Driven Agentic System for Quantitative Financial Trading",
    "summary": "arXiv:2510.04787v2 Announce Type: replace-cross Abstract: Recent advancements in large language models (LLMs) and agentic systems have shown exceptional decision-making capabilities, revealing significant potential for autonomic finance. Current financial trading agents predominantly simulate anthropomorphic roles that inadvertently introduce emoti",
    "url": "https://arxiv.org/abs/2510.04787",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Deep Generative Model for Human Mobility Behavior",
    "summary": "arXiv:2510.06473v2 Announce Type: replace-cross Abstract: Understanding and modeling human mobility is central to challenges in transport planning, sustainable urban design, and public health. Despite decades of effort, simulating individual mobility remains challenging because of its complex, context-dependent, and exploratory nature. Here, we adv",
    "url": "https://arxiv.org/abs/2510.06473",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "VAO: Validation-Aligned Optimization for Cross-Task Generative Auto-Bidding",
    "summary": "arXiv:2510.07760v3 Announce Type: replace-cross Abstract: Generative auto-bidding has demonstrated strong performance in online advertising, yet it often suffers from data scarcity in small-scale settings with limited advertiser participation. While cross-task data sharing is a natural remedy to mitigate this issue, naive approaches often introduce",
    "url": "https://arxiv.org/abs/2510.07760",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards",
    "summary": "arXiv:2510.08529v2 Announce Type: replace-cross Abstract: Self-evolution is a central research topic in enabling large language model (LLM)-based agents to continually improve their capabilities after pretraining. Recent research has witnessed a transition from reinforcement learning (RL)-free to RL-based methods. Current RL-based methods either re",
    "url": "https://arxiv.org/abs/2510.08529",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SHERLOCK:Towards Dynamic Knowledge Adaptation in LLM-enhanced E-commerce Risk Management",
    "summary": "arXiv:2510.08948v3 Announce Type: replace-cross Abstract: Effective e-commerce risk management requires in-depth case investigations to identify emerging fraud patterns in highly adversarial environments. However, manual investigation typically requires analyzing the associations and couplings among multi-source heterogeneous data, a labor-intensiv",
    "url": "https://arxiv.org/abs/2510.08948",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Diffusion-Inspired Masked Fine-Tuning for Knowledge Injection in Autoregressive LLMs",
    "summary": "arXiv:2510.09885v4 Announce Type: replace-cross Abstract: Large language models (LLMs) are often used in environments where facts evolve, yet factual knowledge updates via fine-tuning on unstructured text often suffers from 1) reliance on compute-heavy paraphrase augmentation and 2) the reversal curse. Recent studies show diffusion large language m",
    "url": "https://arxiv.org/abs/2510.09885",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Probabilistic bias adjustment of seasonal predictions of Arctic Sea Ice Concentration",
    "summary": "arXiv:2510.09891v2 Announce Type: replace-cross Abstract: Seasonal forecast of Arctic sea ice concentration is key to mitigate the negative impact and assess potential opportunities posed by the rapid decline of sea ice coverage. Seasonal prediction systems based on climate models often show systematic biases and complex spatio-temporal errors that",
    "url": "https://arxiv.org/abs/2510.09891",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "PAC-Bayesian Reinforcement Learning Trains Generalizable Policies",
    "summary": "arXiv:2510.10544v2 Announce Type: replace-cross Abstract: We derive a novel PAC-Bayesian generalization bound for reinforcement learning that explicitly accounts for Markov dependencies in the data, through the chain's mixing time. This contributes to overcoming challenges in obtaining generalization guarantees for reinforcement learning, where the",
    "url": "https://arxiv.org/abs/2510.10544",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Transformer-based Learning-to-Optimize Approach for Scalable and Generalizable Beamforming",
    "summary": "arXiv:2510.13077v2 Announce Type: replace-cross Abstract: We develop an unsupervised deep learning framework for downlink beamforming in large-scale MU-MISO channels. The model is trained offline, allowing real-time inference through lightweight feedforward computations in dynamic communication environments. Following the learning-to-optimize (L2O)",
    "url": "https://arxiv.org/abs/2510.13077",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Paper Copilot: Tracking the Evolution of Peer Review in AI Conferences",
    "summary": "arXiv:2510.13201v2 Announce Type: replace-cross Abstract: The rapid growth of AI conferences is straining an already fragile peer-review system, leading to heavy reviewer workloads, expertise mismatches, inconsistent evaluation standards, superficial or templated reviews, and limited accountability under compressed timelines. In response, conferenc",
    "url": "https://arxiv.org/abs/2510.13201",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "What Layers When: Learning to Skip Compute in LLMs with Residual Gates",
    "summary": "arXiv:2510.13876v3 Announce Type: replace-cross Abstract: We introduce GateSkip, a simple residual-stream gating mechanism that enables token-wise layer skipping in decoder-only LMs. Each Attention/MLP branch is equipped with a sigmoid-linear gate that condenses the branch's output before it re-enters the residual stream. During inference we rank t",
    "url": "https://arxiv.org/abs/2510.13876",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "ActivationReasoning: Logical Reasoning in Latent Activation Spaces",
    "summary": "arXiv:2510.18184v2 Announce Type: replace-cross Abstract: Large language models (LLMs) excel at generating fluent text, but their internal reasoning remains opaque and difficult to control. Sparse autoencoders (SAEs) make hidden activations more interpretable by exposing latent features that often align with human concepts. Yet, these features are ",
    "url": "https://arxiv.org/abs/2510.18184",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "The Reasoning Lingua Franca: A Double-Edged Sword for Multilingual AI",
    "summary": "arXiv:2510.20647v3 Announce Type: replace-cross Abstract: Large Reasoning Models (LRMs) achieve strong performance on mathematical, scientific, and other question-answering tasks, but their multilingual reasoning abilities remain underexplored. When presented with non-English questions, LRMs often default to reasoning in English, raising concerns a",
    "url": "https://arxiv.org/abs/2510.20647",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Deep Ensembles for Epistemic Uncertainty: A Frequentist Perspective",
    "summary": "arXiv:2510.22063v3 Announce Type: replace-cross Abstract: Decomposing prediction uncertainty into aleatoric (irreducible) and epistemic (reducible) components is critical for the reliable deployment of machine learning systems. While the mutual information between the response variable and model parameters is a principled measure for epistemic unce",
    "url": "https://arxiv.org/abs/2510.22063",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "GPTOpt: Teaching LLMs to do Interpretable Black-Box Optimization",
    "summary": "arXiv:2510.25404v2 Announce Type: replace-cross Abstract: Global optimization of expensive, derivative-free black-box functions demands extreme sample efficiency and decision interpretability. While Large Language Models (LLMs) have shown broad capabilities, even state-of-the-art models remain limited in solving continuous black-box optimization ta",
    "url": "https://arxiv.org/abs/2510.25404",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models",
    "summary": "arXiv:2511.04919v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) face significant computational and memory constraints when processing long contexts, despite growing demand for applications requiring reasoning over extensive documents, multi-session dialogues, and book length texts. While recent advances have extended context ",
    "url": "https://arxiv.org/abs/2511.04919",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Constructing the Umwelt: Cognitive Planning through Belief-Intent Co-Evolution",
    "summary": "arXiv:2511.05540v3 Announce Type: replace-cross Abstract: This paper challenges a prevailing epistemological assumption in End-to-End Autonomous Driving: that high-performance planning necessitates high-fidelity world reconstruction. Inspired by cognitive science, we propose the Mental Bayesian Causal World Model (MBCWM) and instantiate it as the T",
    "url": "https://arxiv.org/abs/2511.05540",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "IDALC: A Semi-Supervised Framework for Intent Detection and Active Learning based Correction",
    "summary": "arXiv:2511.05921v2 Announce Type: replace-cross Abstract: Voice-controlled dialog systems have become immensely popular due to their ability to perform a wide range of actions in response to diverse user queries. These agents possess a predefined set of skills or intents to fulfill specific user tasks. But every system has its own limitations. Ther",
    "url": "https://arxiv.org/abs/2511.05921",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Explainable Cross-Disease Reasoning for Cardiovascular Risk Assessment from Low-Dose Computed Tomography",
    "summary": "arXiv:2511.06625v4 Announce Type: replace-cross Abstract: Low-dose chest computed tomography (LDCT) inherently captures both pulmonary and cardiac structures, offering a unique opportunity for joint assessment of lung and cardiovascular health. However, most existing approaches treat these domains as independent tasks, overlooking their physiologic",
    "url": "https://arxiv.org/abs/2511.06625",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Training Language Models to Explain Their Own Computations",
    "summary": "arXiv:2511.08579v3 Announce Type: replace-cross Abstract: Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs' privileged access to their own internals can be leveraged to produce new techniques for explaining their behavi",
    "url": "https://arxiv.org/abs/2511.08579",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MTR-DuplexBench: Towards a Comprehensive Evaluation of Multi-Round Conversations for Full-Duplex Speech Language Models",
    "summary": "arXiv:2511.10262v2 Announce Type: replace-cross Abstract: Full-Duplex Speech Language Models (FD-SLMs) enable real-time, overlapping conversational interactions, offering a more dynamic user experience compared to traditional half-duplex models. However, existing benchmarks primarily focus on evaluating single-round interactions, neglecting the com",
    "url": "https://arxiv.org/abs/2511.10262",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Twice Sequential Monte Carlo for Tree Search",
    "summary": "arXiv:2511.14220v2 Announce Type: replace-cross Abstract: Model-based reinforcement learning (RL) methods that leverage search are responsible for many milestone breakthroughs in RL. Sequential Monte Carlo (SMC) recently emerged as an alternative to the Monte Carlo Tree Search (MCTS) algorithm which drove these breakthroughs. SMC is easier to paral",
    "url": "https://arxiv.org/abs/2511.14220",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "MOTION: ML-Assisted On-Device Low-Latency Motion Recognition",
    "summary": "arXiv:2512.00008v3 Announce Type: replace-cross Abstract: The use of tiny devices capable of low-latency gesture recognition is gaining momentum in everyday human-computer interaction and especially in medical monitoring fields. Embedded solutions such as fall detection, rehabilitation tracking, and patient supervision require fast and efficient tr",
    "url": "https://arxiv.org/abs/2512.00008",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Open-Set Domain Adaptation Under Background Distribution Shift: Challenges and A Provably Efficient Solution",
    "summary": "arXiv:2512.01152v3 Announce Type: replace-cross Abstract: As we deploy machine learning systems in the real world, a core challenge is to maintain a model that is performant even as the data shifts. Such shifts can take many forms: new classes may emerge that were absent during training, a problem known as open-set recognition, and the distribution",
    "url": "https://arxiv.org/abs/2512.01152",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment",
    "summary": "arXiv:2512.01952v2 Announce Type: replace-cross Abstract: Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting the",
    "url": "https://arxiv.org/abs/2512.01952",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Breaking Scale Anchoring: Frequency Representation Learning for Accurate High-Resolution Inference from Low-Resolution Training",
    "summary": "arXiv:2512.05132v2 Announce Type: replace-cross Abstract: Zero-Shot Super-Resolution Spatiotemporal Forecasting requires a deep learning model to be trained on low-resolution data and deployed for inference on high-resolution. Existing studies consider maintaining similar error across different resolutions as indicative of successful multi-resoluti",
    "url": "https://arxiv.org/abs/2512.05132",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Fine-tuning an ECG Foundation Model to Predict Coronary CT Angiography Outcomes",
    "summary": "arXiv:2512.05136v2 Announce Type: replace-cross Abstract: Coronary artery disease (CAD) remains a major global public health burden, yet scalable tools for risk screening are limited. Although coronary computed tomography angiography (CCTA) is a first-line non-invasive diagnostic modality, its widespread use is constrained by resource requirements ",
    "url": "https://arxiv.org/abs/2512.05136",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "China Regional 3km Downscaling Based on Residual Corrective Diffusion Model",
    "summary": "arXiv:2512.05377v4 Announce Type: replace-cross Abstract: A fundamental challenge in numerical weather prediction is to efficiently produce high-resolution forecasts. A common solution is applying downscaling methods, which include dynamical downscaling and statistical downscaling, to the outputs of global models. This work focuses on statistical d",
    "url": "https://arxiv.org/abs/2512.05377",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SoK: Trust-Authorization Mismatch in LLM Agent Interactions",
    "summary": "arXiv:2512.06914v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) are evolving into autonomous agents capable of executing complex workflows via standardized protocols (e.g., MCP). However, this paradigm shifts control from deterministic code to probabilistic inference, creating a fundamental Trust-Authorization Mismatch: stati",
    "url": "https://arxiv.org/abs/2512.06914",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Investigating Data Pruning for Pretraining Biological Foundation Models at Scale",
    "summary": "arXiv:2512.12932v2 Announce Type: replace-cross Abstract: Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and b",
    "url": "https://arxiv.org/abs/2512.12932",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Data-Chain Backdoor: Do You Trust Diffusion Models as Generative Data Supplier?",
    "summary": "arXiv:2512.15769v2 Announce Type: replace-cross Abstract: The increasing use of generative models such as diffusion models for synthetic data augmentation has greatly reduced the cost of data collection and labeling in downstream perception tasks. However, this new data source paradigm may introduce important security concerns. Publicly available g",
    "url": "https://arxiv.org/abs/2512.15769",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Bidirectional human-AI collaboration in brain tumour assessments improves both expert human and AI agent performance",
    "summary": "arXiv:2512.19707v2 Announce Type: replace-cross Abstract: The benefits of artificial intelligence (AI) human partnerships-evaluating how AI agents enhance expert human performance-are increasingly studied. Though rarely evaluated in healthcare, an inverse approach is possible: AI benefiting from the support of an expert human agent. Here, we invest",
    "url": "https://arxiv.org/abs/2512.19707",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Block-Recurrent Dynamics in Vision Transformers",
    "summary": "arXiv:2512.19941v3 Announce Type: replace-cross Abstract: As Vision Transformers (ViTs) become standard vision backbones, a mechanistic account of their computational phenomenology is essential. Despite architectural cues that hint at dynamical structure, there is no settled framework that interprets Transformer depth as a well-characterized flow. ",
    "url": "https://arxiv.org/abs/2512.19941",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "TS-Arena -- A Live Forecast Pre-Registration Platform",
    "summary": "arXiv:2512.20761v2 Announce Type: replace-cross Abstract: Time Series Foundation Models (TSFMs) are transforming the field of forecasting. However, evaluating them on historical data is increasingly difficult due to the risks of train-test sample overlaps and temporal overlaps between correlated train and test time series. To address this, we intro",
    "url": "https://arxiv.org/abs/2512.20761",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction in Autonomous Systems",
    "summary": "arXiv:2512.22439v3 Announce Type: replace-cross Abstract: LiDAR-based perception in autonomous systems is constrained by fixed vertical beam resolution and further compromised by beam dropout resulting from environmental occlusions. This paper introduces SuperiorGAT, a graph attention-based framework designed to reconstruct missing elevation inform",
    "url": "https://arxiv.org/abs/2512.22439",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks",
    "summary": "arXiv:2512.22522v3 Announce Type: replace-cross Abstract: Spiking Neural Networks (SNNs) utilize spike-based activations to mimic the brain's energy-efficient information processing. However, the binary and discontinuous nature of spike activations causes vanishing gradients, making adversarial robustness evaluation via gradient descent unreliable.",
    "url": "https://arxiv.org/abs/2512.22522",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Trust Region Masking for Long-Horizon LLM Reinforcement Learning",
    "summary": "arXiv:2512.23075v3 Announce Type: replace-cross Abstract: Policy gradient methods for Large Language Models optimize a policy $\\pi_\\theta$ via a surrogate objective computed from samples of a rollout policy $\\pi_{\\text{roll}}$. However, modern LLM-RL pipelines suffer from unavoidable implementation divergences -- backend discrepancies, Mixture-of-E",
    "url": "https://arxiv.org/abs/2512.23075",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Understanding Emotion in Discourse: Recognition Insights and Linguistic Patterns for Generation",
    "summary": "arXiv:2601.00181v2 Announce Type: replace-cross Abstract: Despite strong recent progress in Emotion Recognition in Conversation (ERC), two gaps remain: we lack clear understanding of which modeling choices materially affect performance, and we have limited linguistic analysis linking recognition findings to actionable generation cues. We address bo",
    "url": "https://arxiv.org/abs/2601.00181",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "The Refutability Gap: Challenges in Validating Reasoning by Large Language Models",
    "summary": "arXiv:2601.02380v3 Announce Type: replace-cross Abstract: Recent reports claim that Large Language Models (LLMs) have achieved the ability to derive new science and exhibit human-level general intelligence. We argue that such claims are not rigorous scientific claims, as they do not satisfy Popper's refutability principle (often termed falsifiabili",
    "url": "https://arxiv.org/abs/2601.02380",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "HEEGNet: Hyperbolic Embeddings for EEG",
    "summary": "arXiv:2601.03322v2 Announce Type: replace-cross Abstract: Electroencephalography (EEG)-based brain-computer interfaces facilitate direct communication with a computer, enabling promising applications in human-computer interactions. However, their utility is currently limited because EEG decoding often suffers from poor generalization due to distrib",
    "url": "https://arxiv.org/abs/2601.03322",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Towards Spatio-Temporal Extrapolation of Phase-Field Simulations with Convolution-Only Neural Networks",
    "summary": "arXiv:2601.04510v2 Announce Type: replace-cross Abstract: Phase-field simulations of liquid metal dealloying (LMD) can capture complex microstructural evolutions but can be prohibitively expensive for large domains and long time horizons. In this paper, we introduce a fully convolutional, conditionally parameterized U-Net surrogate designed to extr",
    "url": "https://arxiv.org/abs/2601.04510",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Challenges and Research Directions for Large Language Model Inference Hardware",
    "summary": "arXiv:2601.05047v3 Announce Type: replace-cross Abstract: Large Language Model (LLM) inference is hard. The autoregressive Decode phase of the underlying Transformer model makes LLM inference fundamentally different from training. Exacerbated by recent AI trends, the primary challenges are memory and interconnect rather than compute. To address the",
    "url": "https://arxiv.org/abs/2601.05047",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "A Review of Online Diffusion Policy RL Algorithms for Scalable Robotic Control",
    "summary": "arXiv:2601.06133v2 Announce Type: replace-cross Abstract: Diffusion policies have emerged as a powerful approach for robotic control, demonstrating superior expressiveness in modeling multimodal action distributions compared to conventional policy networks. However, their integration with online reinforcement learning remains challenging due to fun",
    "url": "https://arxiv.org/abs/2601.06133",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "3D Wavelet-Based Structural Priors for Controlled Diffusion in Whole-Body Low-Dose PET Denoising",
    "summary": "arXiv:2601.07093v3 Announce Type: replace-cross Abstract: Low-dose Positron Emission Tomography (PET) imaging reduces patient radiation exposure but suffers from increased noise that degrades image quality and diagnostic reliability. Although diffusion models have demonstrated strong denoising capability, their stochastic nature makes it challengin",
    "url": "https://arxiv.org/abs/2601.07093",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Moonworks Lunara Aesthetic Dataset",
    "summary": "arXiv:2601.07941v4 Announce Type: replace-cross Abstract: The dataset spans diverse artistic styles, including regionally grounded aesthetics from the Middle East, Northern Europe, East Asia, and South Asia, alongside general categories such as sketch and oil painting. All images are generated using the Moonworks Lunara model and intentionally craf",
    "url": "https://arxiv.org/abs/2601.07941",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "On Evaluation of Unsupervised Feature Selection for Pattern Classification",
    "summary": "arXiv:2601.08257v3 Announce Type: replace-cross Abstract: Unsupervised feature selection aims to identify a compact subset of features that captures the intrinsic structure of data without supervised label. Most existing studies evaluate the performance of methods using the single-label dataset that can be instantiated by selecting a label from mul",
    "url": "https://arxiv.org/abs/2601.08257",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Generating metamers of human scene understanding",
    "summary": "arXiv:2601.11675v2 Announce Type: replace-cross Abstract: Human vision combines low-resolution \"gist\" information from the visual periphery with sparse but high-resolution information from fixated locations to construct a coherent understanding of a visual scene. In this paper, we introduce MetamerGen, a tool for generating scenes that are aligned ",
    "url": "https://arxiv.org/abs/2601.11675",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Report for NSF Workshop on AI for Electronic Design Automation",
    "summary": "arXiv:2601.14541v3 Announce Type: replace-cross Abstract: This report distills the discussions and recommendations from the NSF Workshop on AI for Electronic Design Automation (EDA), held on December 10, 2024 in Vancouver alongside NeurIPS 2024. Bringing together experts across machine learning and EDA, the workshop examined how AI-spanning large l",
    "url": "https://arxiv.org/abs/2601.14541",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Evaluating Semantic and Syntactic Understanding in Large Language Models for Payroll Systems",
    "summary": "arXiv:2601.18012v2 Announce Type: replace-cross Abstract: Large language models are now used daily for writing, search, and analysis, and their natural language understanding continues to improve. However, they remain unreliable on exact numerical calculation and on producing outputs that are straightforward to audit. We study synthetic payroll sys",
    "url": "https://arxiv.org/abs/2601.18012",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Rethinking Cross-Modal Fine-Tuning: Optimizing the Interaction between Feature Alignment and Target Fitting",
    "summary": "arXiv:2601.18231v2 Announce Type: replace-cross Abstract: Adapting pre-trained models to unseen feature modalities has become increasingly important due to the growing need for cross-disciplinary knowledge integration. A key challenge here is how to align the representation of new modalities with the most relevant parts of the pre-trained model's r",
    "url": "https://arxiv.org/abs/2601.18231",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning",
    "summary": "arXiv:2601.18626v2 Announce Type: replace-cross Abstract: Natural gradients have long been studied in deep reinforcement learning due to their fast convergence properties and covariant weight updates. However, computing natural gradients requires inversion of the Fisher Information Matrix (FIM) at each iteration, which is computationally prohibitiv",
    "url": "https://arxiv.org/abs/2601.18626",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "From Fuzzy to Exact: The Halo Architecture for Infinite-Depth Reasoning via Rational Arithmetic",
    "summary": "arXiv:2601.18702v3 Announce Type: replace-cross Abstract: The pursuit of scale in deep learning has entrenched a trade-off: computational throughput is prioritized at the expense of numerical precision. We argue this compromise is fundamentally at odds with the requirements of general intelligence. We propose the \\textit{Exactness Hypothesis}: high",
    "url": "https://arxiv.org/abs/2601.18702",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Bridging Gulfs in UI Generation through Semantic Guidance",
    "summary": "arXiv:2601.19171v2 Announce Type: replace-cross Abstract: While generative AI enables high-fidelity UI generation from text prompts, users struggle to articulate design intent and evaluate or refine results-creating gulfs of execution and evaluation. To understand the information needed for UI generation, we conducted a thematic analysis of UI prom",
    "url": "https://arxiv.org/abs/2601.19171",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Decoupled Split Learning via Auxiliary Loss",
    "summary": "arXiv:2601.19261v2 Announce Type: replace-cross Abstract: Split learning is a distributed training paradigm where a neural network is partitioned between clients and a server, which allows data to remain at the client while only intermediate activations are shared. Traditional split learning relies on end-to-end backpropagation across the client-se",
    "url": "https://arxiv.org/abs/2601.19261",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Safe Exploration via Policy Priors",
    "summary": "arXiv:2601.19612v2 Announce Type: replace-cross Abstract: Safe exploration is a key requirement for reinforcement learning (RL) agents to learn and adapt online, beyond controlled (e.g. simulated) environments. In this work, we tackle this challenge by utilizing suboptimal yet conservative policies (e.g., obtained from offline data or simulators) a",
    "url": "https://arxiv.org/abs/2601.19612",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "NRR-Phi: Text-to-State Mapping for Ambiguity Preservation in LLM Inference",
    "summary": "arXiv:2601.19933v3 Announce Type: replace-cross Abstract: Large language models exhibit a systematic tendency toward early semantic commitment: given ambiguous input, they collapse multiple valid interpretations into a single response before sufficient context is available. We present a formal framework for text-to-state mapping ($\\phi: \\mathcal{T}",
    "url": "https://arxiv.org/abs/2601.19933",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Membership Inference Attacks Against Fine-tuned Diffusion Language Models",
    "summary": "arXiv:2601.20125v3 Announce Type: replace-cross Abstract: Diffusion Language Models (DLMs) represent a promising alternative to autoregressive language models, using bidirectional masked token prediction. Yet their susceptibility to privacy leakage via Membership Inference Attacks (MIA) remains critically underexplored. This paper presents the firs",
    "url": "https://arxiv.org/abs/2601.20125",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Conditional PED-ANOVA: Hyperparameter Importance in Hierarchical & Dynamic Search Spaces",
    "summary": "arXiv:2601.20800v2 Announce Type: replace-cross Abstract: We propose conditional PED-ANOVA (condPED-ANOVA), a principled framework for estimating hyperparameter importance (HPI) in conditional search spaces, where the presence or domain of a hyperparameter can depend on other hyperparameters. Although the original PED-ANOVA provides a fast and effi",
    "url": "https://arxiv.org/abs/2601.20800",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Solver-in-the-Loop: MDP-Based Benchmarks for Self-Correction and Behavioral Rationality in Operations Research",
    "summary": "arXiv:2601.21008v2 Announce Type: replace-cross Abstract: Operations Research practitioners routinely debug infeasible models through an iterative process: analyzing Irreducible Infeasible Subsystems (\\IIS{}), identifying constraint conflicts, and systematically repairing formulations until feasibility is achieved. Yet existing LLM benchmarks evalu",
    "url": "https://arxiv.org/abs/2601.21008",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SAGE: Sequence-level Adaptive Gradient Evolution for Generative Recommendation",
    "summary": "arXiv:2601.21452v2 Announce Type: replace-cross Abstract: While works such as OneRec have validated the scaling laws of Large Language Models (LLMs) in recommender systems, they rely on a cumbersome separate vocabulary. This dependency prevents the model architecture from reusing native LLM vocabularies, resulting in high maintenance costs and poor",
    "url": "https://arxiv.org/abs/2601.21452",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "HER: Human-like Reasoning and Reinforcement Learning for LLM Role-playing",
    "summary": "arXiv:2601.21459v3 Announce Type: replace-cross Abstract: LLM role-playing, i.e., using LLMs to simulate specific personas, has emerged as a key capability in various applications, such as companionship, content creation, and digital games. While current models effectively capture character tones and knowledge, simulating the inner thoughts behind ",
    "url": "https://arxiv.org/abs/2601.21459",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Task-free Adaptive Meta Black-box Optimization",
    "summary": "arXiv:2601.21475v2 Announce Type: replace-cross Abstract: Handcrafted optimizers become prohibitively inefficient for complex black-box optimization (BBO) tasks. MetaBBO addresses this challenge by meta-learning to automatically configure optimizers for low-level BBO tasks, thereby eliminating heuristic dependencies. However, existing methods typic",
    "url": "https://arxiv.org/abs/2601.21475",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Vidmento: Creating Video Stories Through Context-Aware Expansion With Generative Video",
    "summary": "arXiv:2601.22013v2 Announce Type: replace-cross Abstract: Video storytelling is often constrained by available material, limiting creative expression and leaving undesired narrative gaps. Generative video offers a new way to address these limitations by augmenting captured media with tailored visuals. To explore this potential, we interviewed eight",
    "url": "https://arxiv.org/abs/2601.22013",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Degradation-Aware Frequency Regulation of a Heterogeneous Battery Fleet via Reinforcement Learning",
    "summary": "arXiv:2601.22865v2 Announce Type: replace-cross Abstract: Battery energy storage systems are increasingly deployed as fast-responding resources for grid balancing services such as frequency regulation and for mitigating renewable generation uncertainty. However, repeated charging and discharging induces cycling degradation and reduces battery lifet",
    "url": "https://arxiv.org/abs/2601.22865",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Reinforcement Learning-Based Co-Design and Operation of Chiller and Thermal Energy Storage for Cost-Optimal HVAC Systems",
    "summary": "arXiv:2601.22880v2 Announce Type: replace-cross Abstract: We study the joint operation and sizing of cooling infrastructure for commercial HVAC systems using reinforcement learning, with the objective of minimizing life-cycle cost over a 30-year horizon. The cooling system consists of a fixed-capacity electric chiller and a thermal energy storage (",
    "url": "https://arxiv.org/abs/2601.22880",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "How Hyper-Datafication Impacts the Sustainability Costs in Frontier AI",
    "summary": "arXiv:2602.00056v2 Announce Type: replace-cross Abstract: Large-scale data has fuelled the success of frontier artificial intelligence (AI) models over the past decade. This expansion has relied on sustained efforts by large technology corporations to aggregate and curate internet-scale datasets. In this work, we examine the environmental, social, ",
    "url": "https://arxiv.org/abs/2602.00056",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs",
    "summary": "arXiv:2602.00462v2 Announce Type: replace-cross Abstract: Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readi",
    "url": "https://arxiv.org/abs/2602.00462",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Multi-Agent Teams Hold Experts Back",
    "summary": "arXiv:2602.01011v3 Announce Type: replace-cross Abstract: Multi-agent LLM systems are increasingly deployed as autonomous collaborators, where agents interact freely rather than execute fixed, pre-specified workflows. In such settings, effective coordination cannot be fully designed in advance and must instead emerge through interaction. However, m",
    "url": "https://arxiv.org/abs/2602.01011",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "OLion: Approaching the Hadamard Ideal by Intersecting Spectral and $\\ell_{\\infty}$ Implicit Biases",
    "summary": "arXiv:2602.01105v2 Announce Type: replace-cross Abstract: Many optimizers can be interpreted as steepest-descent methods under norm-induced geometries, and thus inherit corresponding implicit biases. We introduce \\nameA{} (\\fullname{}), which combines spectral control from orthogonalized update directions with $\\ell_\\infty$-style coordinate control",
    "url": "https://arxiv.org/abs/2602.01105",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Supervised Fine-Tuning Needs to Unlock the Potential of Token Priority",
    "summary": "arXiv:2602.01227v2 Announce Type: replace-cross Abstract: The transition from fitting empirical data to achieving true human utility is fundamentally constrained by a granularity mismatch, where fine-grained autoregressive generation is often supervised by coarse or uniform signals. This position paper advocates Token Priority as the essential brid",
    "url": "https://arxiv.org/abs/2602.01227",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "TxRay: Agentic Postmortem of Live Blockchain Attacks",
    "summary": "arXiv:2602.01317v4 Announce Type: replace-cross Abstract: Decentralized Finance (DeFi) has turned blockchains into financial infrastructure, allowing anyone to trade, lend, and build protocols without intermediaries, but this openness exposes pools of value controlled by code. Within five years, the DeFi ecosystem has lost over 15.75B USD to report",
    "url": "https://arxiv.org/abs/2602.01317",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "From Pragmas to Partners: A Symbiotic Evolution of Agentic High-Level Synthesis",
    "summary": "arXiv:2602.01401v3 Announce Type: replace-cross Abstract: The rise of large language models has sparked interest in AI-driven hardware design, raising the question: does high-level synthesis (HLS) still matter in the agentic era? We argue that HLS remains essential. While we expect mature agentic hardware systems to leverage both HLS and RTL, this ",
    "url": "https://arxiv.org/abs/2602.01401",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Stein-Rule Shrinkage for Stochastic Gradient Estimation in High Dimensions",
    "summary": "arXiv:2602.01777v2 Announce Type: replace-cross Abstract: Stochastic gradient methods are central to large-scale learning, but they treat mini-batch gradients as unbiased estimators, which classical decision theory shows are inadmissible in high dimensions. We formulate gradient computation as a high-dimensional estimation problem and introduce a f",
    "url": "https://arxiv.org/abs/2602.01777",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "CAM: A Causality-based Analysis Framework for Multi-Agent Code Generation Systems",
    "summary": "arXiv:2602.02138v2 Announce Type: replace-cross Abstract: Despite the remarkable success that Multi-Agent Code Generation Systems (MACGS) have achieved, the inherent complexity of multi-agent architectures produces substantial volumes of intermediate outputs. To date, the individual importance of these intermediate outputs to the system correctness",
    "url": "https://arxiv.org/abs/2602.02138",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "EvoMU: Evolutionary Machine Unlearning",
    "summary": "arXiv:2602.02139v2 Announce Type: replace-cross Abstract: Machine unlearning aims to unlearn specified training data (e.g. sensitive or copyrighted material). A prominent approach is to fine-tune an existing model with an unlearning loss that retains overall utility. The space of suitable unlearning loss functions is vast, making the search for an ",
    "url": "https://arxiv.org/abs/2602.02139",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Decoupling Generalizability and Membership Privacy Risks in Neural Networks",
    "summary": "arXiv:2602.02296v2 Announce Type: replace-cross Abstract: A deep learning model usually has to sacrifice some utilities when it acquires some other abilities or characteristics. Privacy preservation has such trade-off relationships with utilities. The loss disparity between various defense approaches implies the potential to decouple generalizabili",
    "url": "https://arxiv.org/abs/2602.02296",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "ReasonEdit: Editing Vision-Language Models using Human Reasoning",
    "summary": "arXiv:2602.02408v3 Announce Type: replace-cross Abstract: Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images. We the",
    "url": "https://arxiv.org/abs/2602.02408",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
    "summary": "arXiv:2602.02437v3 Announce Type: replace-cross Abstract: Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework tha",
    "url": "https://arxiv.org/abs/2602.02437",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Step-Wise Refusal Dynamics in Autoregressive and Diffusion Language Models",
    "summary": "arXiv:2602.02600v2 Announce Type: replace-cross Abstract: Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) models, offering parallel decoding and controllable sampling dynamics while achieving competitive generation quality at scale. Despite this progress, the role of sampling mechanisms in sh",
    "url": "https://arxiv.org/abs/2602.02600",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Semantics-Aware Generative Latent Data Augmentation for Learning in Low-Resource Domains",
    "summary": "arXiv:2602.02841v2 Announce Type: replace-cross Abstract: Despite strong performance in data-rich regimes, deep learning often underperforms in the data-scarce settings common in practice. While foundation models (FMs) trained on massive datasets demonstrate strong generalization by extracting general-purpose features, they can still suffer from sc",
    "url": "https://arxiv.org/abs/2602.02841",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Learning to Select: Query-Aware Adaptive Dimension Selection for Dense Retrieval",
    "summary": "arXiv:2602.03306v2 Announce Type: replace-cross Abstract: Dense retrieval represents queries and documents as high-dimensional embeddings, but these representations can be redundant at the query level: for a given information need, only a subset of dimensions is consistently helpful for ranking. Prior work addresses this via pseudo-relevance feedba",
    "url": "https://arxiv.org/abs/2602.03306",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "DiGAN: Diffusion-Guided Attention Network for Early Alzheimer's Disease Detection",
    "summary": "arXiv:2602.03881v2 Announce Type: replace-cross Abstract: Early diagnosis of Alzheimer's disease (AD) remains a major challenge due to the subtle and temporally irregular progression of structural brain changes in the prodromal stages. Existing deep learning approaches require large longitudinal datasets and often fail to model the temporal continu",
    "url": "https://arxiv.org/abs/2602.03881",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Structural shifts in institutional participation and collaboration within the AI arXiv preprint research ecosystem",
    "summary": "arXiv:2602.03969v2 Announce Type: replace-cross Abstract: The emergence of large language models (LLMs) represents a significant technological shift within the scientific ecosystem, particularly within the field of artificial intelligence (AI). This paper examines structural changes in the AI research landscape using a dataset of arXiv preprints (c",
    "url": "https://arxiv.org/abs/2602.03969",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Bypassing the Rationale: Causal Auditing of Implicit Reasoning in Language Models",
    "summary": "arXiv:2602.03994v2 Announce Type: replace-cross Abstract: Chain-of-thought (CoT) prompting is widely used as a reasoning aid and is often treated as a transparency mechanism. Yet behavioral gains under CoT do not imply that the model's internal computation causally depends on the emitted reasoning text, i.e., models may produce fluent rationales wh",
    "url": "https://arxiv.org/abs/2602.03994",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Understanding and Guiding Layer Placement in Parameter-Efficient Fine-Tuning of Large Language Models",
    "summary": "arXiv:2602.04019v2 Announce Type: replace-cross Abstract: As large language models (LLMs) continue to grow, the cost of full-parameter fine-tuning has made parameter-efficient fine-tuning (PEFT) the default strategy for downstream adaptation. Constraints from inference latency in scalable serving and fine-tuning cost in edge or rapid-deployment set",
    "url": "https://arxiv.org/abs/2602.04019",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Improving 2D Diffusion Models for 3D Medical Imaging with Inter-Slice Consistent Stochasticity",
    "summary": "arXiv:2602.04162v2 Announce Type: replace-cross Abstract: 3D medical imaging is in high demand and essential for clinical diagnosis and scientific research. Currently, diffusion models (DMs) have become an effective tool for medical imaging reconstruction thanks to their ability to learn rich, high-quality data priors. However, learning the 3D data",
    "url": "https://arxiv.org/abs/2602.04162",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Extracting Recurring Vulnerabilities from Black-Box LLM-Generated Software",
    "summary": "arXiv:2602.04894v2 Announce Type: replace-cross Abstract: LLMs are increasingly used for code generation, but their outputs often follow recurring templates that can induce predictable vulnerabilities. We study \\emph{vulnerability persistence} in LLM-generated software and introduce \\emph{Feature--Security Table (FSTab)} with two components. First,",
    "url": "https://arxiv.org/abs/2602.04894",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Evaluating Kubernetes Performance for GenAI Inference: From Automatic Speech Recognition to LLM Summarization",
    "summary": "arXiv:2602.04900v2 Announce Type: replace-cross Abstract: As Generative AI (GenAI), particularly inference, rapidly emerges as a dominant workload category, the Kubernetes ecosystem is proactively evolving to natively support its unique demands. This industry paper demonstrates how emerging Kubernetes-native projects can be combined to deliver the ",
    "url": "https://arxiv.org/abs/2602.04900",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Momentum Attention: The Physics of In-Context Learning and Spectral Forensics for Mechanistic Interpretability",
    "summary": "arXiv:2602.04902v2 Announce Type: replace-cross Abstract: The Mechanistic Interpretability (MI) program has mapped the Transformer as a precise computational graph. We extend this graph with a conservation law and time-varying AC dynamics, viewing it as a physical circuit. We introduce Momentum Attention, a symplectic augmentation embedding physica",
    "url": "https://arxiv.org/abs/2602.04902",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "SLAY: Geometry-Aware Spherical Linearized Attention with Yat-Kernel",
    "summary": "arXiv:2602.04915v2 Announce Type: replace-cross Abstract: We propose a new class of linear-time attention mechanisms based on a relaxed and computationally efficient formulation of the recently introduced E-Product, often referred to as the Yat-kernel (Bouhsine, 2025). The resulting interactions are geometry-aware and inspired by inverse-square int",
    "url": "https://arxiv.org/abs/2602.04915",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "ASA: Training-Free Representation Engineering for Tool-Calling Agents",
    "summary": "arXiv:2602.04935v2 Announce Type: replace-cross Abstract: Adapting LLM agents to domain-specific tool calling remains notably brittle under evolving interfaces. Prompt and schema engineering is easy to deploy but often fragile under distribution shift and strict parsers, while continual parameter-efficient fine-tuning improves reliability at the co",
    "url": "https://arxiv.org/abs/2602.04935",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Graph-Theoretic Analysis of Phase Optimization Complexity in Variational Wave Functions for Heisenberg Antiferromagnets",
    "summary": "arXiv:2602.04943v2 Announce Type: replace-cross Abstract: Despite extensive study, the phase structure of the wavefunctions in frustrated Heisenberg antiferromagnets (HAF) is not yet systematically characterized. In this work, we represent the Hilbert space of an HAF as a weighted graph, which we term the Hilbert graph (HG), whose vertices are spin",
    "url": "https://arxiv.org/abs/2602.04943",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "EBPO: Empirical Bayes Shrinkage for Stabilizing Group-Relative Policy Optimization",
    "summary": "arXiv:2602.05165v2 Announce Type: replace-cross Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing the reasoning capabilities of Large Language Models (LLMs). However, dominant approaches like Group Relative Policy Optimization (GRPO) face critical stability challenges: they suffer from high estimator",
    "url": "https://arxiv.org/abs/2602.05165",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "A Unified Framework for Rethinking Policy Divergence Measures in GRPO",
    "summary": "arXiv:2602.05494v2 Announce Type: replace-cross Abstract: Reinforcement Learning with Verified Reward (RLVR) has emerged as a critical paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). Most existing RLVR methods, such as GRPO and its variants, ensure stable updates by constraining policy divergence through clipping ",
    "url": "https://arxiv.org/abs/2602.05494",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Alignment Verifiability in Large Language Models: Normative Indistinguishability under Behavioral Evaluation",
    "summary": "arXiv:2602.05656v2 Announce Type: replace-cross Abstract: Behavioral evaluation is the dominant paradigm for assessing alignment in large language models (LLMs). In current practice, observed compliance under finite evaluation protocols is treated as evidence of latent alignment. However, the inference from bounded behavioral evidence to claims abo",
    "url": "https://arxiv.org/abs/2602.05656",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Exploring AI-Augmented Sensemaking of Patient-Generated Health Data: A Mixed-Method Study with Healthcare Professionals in Cardiac Risk Reduction",
    "summary": "arXiv:2602.05687v3 Announce Type: replace-cross Abstract: Individuals are increasingly generating substantial personal health and lifestyle data, e.g. through wearables and smartphones. While such data could transform preventative care, its integration into clinical practice is hindered by its scale, heterogeneity and the time pressure and data lit",
    "url": "https://arxiv.org/abs/2602.05687",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Bagging-Based Model Merging for Robust General Text Embeddings",
    "summary": "arXiv:2602.05787v2 Announce Type: replace-cross Abstract: General-purpose text embedding models underpin a wide range of NLP and information retrieval applications, and are typically trained on large-scale multi-task corpora to encourage broad generalization. However, it remains unclear how different multi-task training strategies compare in practi",
    "url": "https://arxiv.org/abs/2602.05787",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Rethinking Memory Mechanisms of Foundation Agents in the Second Half: A Survey",
    "summary": "arXiv:2602.06052v3 Announce Type: replace-cross Abstract: The research of artificial intelligence is undergoing a paradigm shift from prioritizing model innovations over benchmark scores towards emphasizing problem definition and rigorous real-world evaluation. As the field enters the \"second half,\" the central challenge becomes real utility in lon",
    "url": "https://arxiv.org/abs/2602.06052",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Addressing the Waypoint-Action Gap in End-to-End Autonomous Driving via Vehicle Motion Models",
    "summary": "arXiv:2602.06214v2 Announce Type: replace-cross Abstract: End-to-End Autonomous Driving (E2E-AD) systems are typically grouped by the nature of their outputs: (i) waypoint-based models that predict a future trajectory, and (ii) action-based models that directly output throttle, steer and brake. Most recent benchmark protocols and training pipelines",
    "url": "https://arxiv.org/abs/2602.06214",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Temperature Scaling Attack Disrupting Model Confidence in Federated Learning",
    "summary": "arXiv:2602.06638v2 Announce Type: replace-cross Abstract: Predictive confidence serves as a foundational control signal in mission-critical systems, directly governing risk-aware logic such as escalation, abstention, and conservative fallback. While prior federated learning attacks predominantly target accuracy or implant backdoors, we identify con",
    "url": "https://arxiv.org/abs/2602.06638",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "Zero-shot Generalizable Graph Anomaly Detection with Mixture of Riemannian Experts",
    "summary": "arXiv:2602.06859v2 Announce Type: replace-cross Abstract: Graph Anomaly Detection (GAD) aims to identify irregular patterns in graph data, and recent works have explored zero-shot generalist GAD to enable generalization to unseen graph datasets. However, existing zero-shot GAD methods largely ignore intrinsic geometric differences across diverse an",
    "url": "https://arxiv.org/abs/2602.06859",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  },
  {
    "title": "InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning",
    "summary": "arXiv:2602.06960v2 Announce Type: replace-cross Abstract: Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing",
    "url": "https://arxiv.org/abs/2602.06960",
    "source": "Arxiv AI",
    "published_at": "2026-02-11T05:00:00+00:00"
  }
]