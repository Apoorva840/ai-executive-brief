[
  {
    "title": "AI is already making online crimes easier. It could get much worse.",
    "summary": "Anton Cherepanov is always on the lookout for something interesting. And in late August last year, he spotted just that. It was a file uploaded to VirusTotal, a site cybersecurity researchers like him use to analyze submissions for potential viruses and other types of malicious software, often known as malware. On the surface it seemed&#8230;",
    "url": "https://www.technologyreview.com/2026/02/12/1132386/ai-already-making-online-swindles-easier/",
    "source": "MIT Technology Review AI",
    "published_at": "2026-02-12T11:00:00+00:00"
  },
  {
    "title": "What’s next for Chinese open-source AI",
    "summary": "MIT Technology Review’s What’s Next series looks across industries, trends, and technologies to give you a first look at the future. You can read the rest of them&#160;here. The past year has marked a turning point for Chinese AI. Since DeepSeek released its R1 reasoning model in January 2025, Chinese companies have repeatedly delivered AI&#8230;",
    "url": "https://www.technologyreview.com/2026/02/12/1132811/whats-next-for-chinese-open-source-ai/",
    "source": "MIT Technology Review AI",
    "published_at": "2026-02-12T10:00:00+00:00"
  },
  {
    "title": "‘Uncanny Valley’: ICE’s Secret Expansion Plans, Palantir Workers’ Ethical Concerns, and AI Assistants",
    "summary": "In this episode of Uncanny Valley, our hosts dive into WIRED’s scoop about a secret Trump administration campaign extending right into your backyard.",
    "url": "https://www.wired.com/story/uncanny-valley-podcast-ice-expansion-palantir-workers-ethical-concerns-openclaw-ai-assistants/",
    "source": "Wired AI",
    "published_at": "2026-02-12T22:12:42+00:00"
  },
  {
    "title": "A Wave of Unexplained Bot Traffic Is Sweeping the Web",
    "summary": "From small publishers to US federal agencies, websites are reporting unusual spikes in automated traffic linked to IP addresses in Lanzhou, China.",
    "url": "https://www.wired.com/story/made-in-china-niche-websites-are-seeing-a-surge-of-mysterious-traffic-from-china/",
    "source": "Wired AI",
    "published_at": "2026-02-12T19:50:44+00:00"
  },
  {
    "title": "OpenAI’s President Gave Millions to Trump. He Says It’s for Humanity",
    "summary": "In an interview with WIRED, Greg Brockman says his political donations support OpenAI's mission—even if some employees at the company disagree.",
    "url": "https://www.wired.com/story/openai-president-greg-brockman-political-donations-trump-humanity/",
    "source": "Wired AI",
    "published_at": "2026-02-12T19:00:00+00:00"
  },
  {
    "title": "Crypto-Funded Human Trafficking Is Exploding",
    "summary": "The use of cryptocurrency in sales of human beings for prostitution and scam compounds nearly doubled in 2025, according to a conservative estimate. Many of the deals are happening in plain sight.",
    "url": "https://www.wired.com/story/crypto-funded-human-trafficking-is-exploding/",
    "source": "Wired AI",
    "published_at": "2026-02-12T13:00:00+00:00"
  },
  {
    "title": "I Tried RentAHuman, Where AI Agents Hired Me to Hype Their AI Startups",
    "summary": "Rather than offering a revolutionary new approach to gig work, RentAHuman is filled with bots that just want me to be another cog in the AI hype machine.",
    "url": "https://www.wired.com/story/i-tried-rentahuman-ai-agents-hired-me-to-hype-their-ai-startups/",
    "source": "Wired AI",
    "published_at": "2026-02-12T11:00:00+00:00"
  },
  {
    "title": "Discovering Differences in Strategic Behavior Between Humans and LLMs",
    "summary": "arXiv:2602.10324v1 Announce Type: new Abstract: As Large Language Models (LLMs) are increasingly deployed in social and strategic scenarios, it becomes critical to understand where and why their behavior diverges from that of humans. While behavioral game theory (BGT) provides a framework for analyzing behavior, existing models do not fully capture",
    "url": "https://arxiv.org/abs/2602.10324",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation",
    "summary": "arXiv:2602.10367v1 Announce Type: new Abstract: The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leadi",
    "url": "https://arxiv.org/abs/2602.10367",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Found-RL: foundation model-enhanced reinforcement learning for autonomous driving",
    "summary": "arXiv:2602.10458v1 Announce Type: new Abstract: Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering",
    "url": "https://arxiv.org/abs/2602.10458",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "MERIT Feedback Elicits Better Bargaining in LLM Negotiators",
    "summary": "arXiv:2602.10467v1 Announce Type: new Abstract: Bargaining is often regarded as a logical arena rather than an art or a matter of intuition, yet Large Language Models (LLMs) still struggle to navigate it due to limited strategic depth and difficulty adapting to complex human factors. Current benchmarks rarely capture this limitation. To bridge this",
    "url": "https://arxiv.org/abs/2602.10467",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Abstraction Generation for Generalized Planning with Pretrained Large Language Models",
    "summary": "arXiv:2602.10485v1 Announce Type: new Abstract: Qualitative Numerical Planning (QNP) serves as an important abstraction model for generalized planning (GP), which aims to compute general plans that solve multiple instances at once. Recent works show that large language models (LLMs) can function as generalized planners. This work investigates wheth",
    "url": "https://arxiv.org/abs/2602.10485",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Flow of Spans: Generalizing Language Models to Dynamic Span-Vocabulary via GFlowNets",
    "summary": "arXiv:2602.10583v1 Announce Type: new Abstract: Standard autoregressive language models generate text token-by-token from a fixed vocabulary, inducing a tree-structured state space when viewing token sampling as an action, which limits flexibility and expressiveness. Recent work introduces dynamic vocabulary by sampling retrieved text spans but ove",
    "url": "https://arxiv.org/abs/2602.10583",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Neuro-symbolic Action Masking for Deep Reinforcement Learning",
    "summary": "arXiv:2602.10598v1 Announce Type: new Abstract: Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In th",
    "url": "https://arxiv.org/abs/2602.10598",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks",
    "summary": "arXiv:2602.10625v1 Announce Type: new Abstract: Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still u",
    "url": "https://arxiv.org/abs/2602.10625",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization",
    "summary": "arXiv:2602.10635v1 Announce Type: new Abstract: To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reaso",
    "url": "https://arxiv.org/abs/2602.10635",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation",
    "summary": "arXiv:2602.10699v1 Announce Type: new Abstract: Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decod",
    "url": "https://arxiv.org/abs/2602.10699",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Integrating Generative AI-enhanced Cognitive Systems in Higher Education: From Stakeholder Perceptions to a Conceptual Framework considering the EU AI Act",
    "summary": "arXiv:2602.10802v1 Announce Type: new Abstract: Many staff and students in higher education have adopted generative artificial intelligence (GenAI) tools in their work and study. GenAI is expected to enhance cognitive systems by enabling personalized learning and streamlining educational services. However, stakeholders perceptions of GenAI in highe",
    "url": "https://arxiv.org/abs/2602.10802",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "See, Plan, Snap: Evaluating Multimodal GUI Agents in Scratch",
    "summary": "arXiv:2602.10814v1 Announce Type: new Abstract: Block-based programming environments such as Scratch play a central role in low-code education, yet evaluating the capabilities of AI agents to construct programs through Graphical User Interfaces (GUIs) remains underexplored. We introduce ScratchWorld, a benchmark for evaluating multimodal GUI agents",
    "url": "https://arxiv.org/abs/2602.10814",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "SynergyKGC: Reconciling Topological Heterogeneity in Knowledge Graph Completion via Topology-Aware Synergy",
    "summary": "arXiv:2602.10845v1 Announce Type: new Abstract: Knowledge Graph Completion (KGC) fundamentally hinges on the coherent fusion of pre-trained entity semantics with heterogeneous topological structures to facilitate robust relational reasoning. However, existing paradigms encounter a critical \"structural resolution mismatch,\" failing to reconcile dive",
    "url": "https://arxiv.org/abs/2602.10845",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics",
    "summary": "arXiv:2602.10885v1 Announce Type: new Abstract: Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT re",
    "url": "https://arxiv.org/abs/2602.10885",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Can LLMs Cook Jamaican Couscous? A Study of Cultural Novelty in Recipe Generation",
    "summary": "arXiv:2602.10964v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly used to generate and shape cultural content, ranging from narrative writing to artistic production. While these models demonstrate impressive fluency and generative capacity, prior work has shown that they also exhibit systematic cultural biases, raising c",
    "url": "https://arxiv.org/abs/2602.10964",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion",
    "summary": "arXiv:2602.10999v1 Announce Type: new Abstract: Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to en",
    "url": "https://arxiv.org/abs/2602.10999",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "GameDevBench: Evaluating Agentic Capabilities Through Game Development",
    "summary": "arXiv:2602.11103v1 Announce Type: new Abstract: Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as ",
    "url": "https://arxiv.org/abs/2602.11103",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight",
    "summary": "arXiv:2602.11136v1 Announce Type: new Abstract: As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems w",
    "url": "https://arxiv.org/abs/2602.11136",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Large Language Models Predict Functional Outcomes after Acute Ischemic Stroke",
    "summary": "arXiv:2602.10119v1 Announce Type: cross Abstract: Accurate prediction of functional outcomes after acute ischemic stroke can inform clinical decision-making and resource allocation. Prior work on modified Rankin Scale (mRS) prediction has relied primarily on structured variables (e.g., age, NIHSS) and conventional machine learning. The ability of l",
    "url": "https://arxiv.org/abs/2602.10119",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "A Practical Guide to Agentic AI Transition in Organizations",
    "summary": "arXiv:2602.10122v1 Announce Type: cross Abstract: Agentic AI represents a significant shift in how intelligence is applied within organizations, moving beyond AI-assisted tools toward autonomous systems capable of reasoning, decision-making, and coordinated action across workflows. As these systems mature, they have the potential to automate a subs",
    "url": "https://arxiv.org/abs/2602.10122",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "\"Humans welcome to observe\": A First Look at the Agent Social Network Moltbook",
    "summary": "arXiv:2602.10127v1 Announce Type: cross Abstract: The rapid advancement of artificial intelligence (AI) agents has catalyzed the transition from static language models to autonomous agents capable of tool use, long-term planning, and social interaction. $\\textbf{Moltbook}$, the first social network designed exclusively for AI agents, has experience",
    "url": "https://arxiv.org/abs/2602.10127",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "The Anatomy of the Moltbook Social Graph",
    "summary": "arXiv:2602.10131v1 Announce Type: cross Abstract: I present a descriptive analysis of Moltbook, a social platform populated exclusively by AI agents, using data from the platform's first 3.5 days (6{,}159 agents; 13{,}875 posts; 115{,}031 comments). At the macro level, Moltbook exhibits structural signatures that are familiar from human social netw",
    "url": "https://arxiv.org/abs/2602.10131",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "TokaMark: A Comprehensive Benchmark for MAST Tokamak Plasma Models",
    "summary": "arXiv:2602.10132v1 Announce Type: cross Abstract: Development and operation of commercially viable fusion energy reactors such as tokamaks require accurate predictions of plasma dynamics from sparse, noisy, and incomplete sensors readings. The complexity of the underlying physics and the heterogeneity of experimental data pose formidable challenges",
    "url": "https://arxiv.org/abs/2602.10132",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "AgentTrace: A Structured Logging Framework for Agent System Observability",
    "summary": "arXiv:2602.10133v1 Announce Type: cross Abstract: Despite the growing capabilities of autonomous agents powered by large language models (LLMs), their adoption in high-stakes domains remains limited. A key barrier is security: the inherently nondeterministic behavior of LLM agents defies static auditing approaches that have historically underpinned",
    "url": "https://arxiv.org/abs/2602.10133",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Reverse-Engineering Model Editing on Language Models",
    "summary": "arXiv:2602.10134v1 Announce Type: cross Abstract: Large language models (LLMs) are pretrained on corpora containing trillions of tokens and, therefore, inevitably memorize sensitive information. Locate-then-edit methods, as a mainstream paradigm of model editing, offer a promising solution by modifying model parameters without retraining. However, ",
    "url": "https://arxiv.org/abs/2602.10134",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Multi-encoder ConvNeXt Network with Smooth Attentional Feature Fusion for Multispectral Semantic Segmentation",
    "summary": "arXiv:2602.10137v1 Announce Type: cross Abstract: This work proposes MeCSAFNet, a multi-branch encoder-decoder architecture for land cover segmentation in multispectral imagery. The model separately processes visible and non-visible channels through dual ConvNeXt encoders, followed by individual decoders that reconstruct spatial information. A dedi",
    "url": "https://arxiv.org/abs/2602.10137",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Multimodal Information Fusion for Chart Understanding: A Survey of MLLMs -- Evolution, Limitations, and Cognitive Enhancement",
    "summary": "arXiv:2602.10138v1 Announce Type: cross Abstract: Chart understanding is a quintessential information fusion task, requiring the seamless integration of graphical and textual data to extract meaning. The advent of Multimodal Large Language Models (MLLMs) has revolutionized this domain, yet the landscape of MLLM-based chart analysis remains fragment",
    "url": "https://arxiv.org/abs/2602.10138",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Anonymization-Enhanced Privacy Protection for Mobile GUI Agents: Available but Invisible",
    "summary": "arXiv:2602.10139v1 Announce Type: cross Abstract: Mobile Graphical User Interface (GUI) agents have demonstrated strong capabilities in automating complex smartphone tasks by leveraging multimodal large language models (MLLMs) and system-level control interfaces. However, this paradigm introduces significant privacy risks, as agents typically captu",
    "url": "https://arxiv.org/abs/2602.10139",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Can Large Language Models Implement Agent-Based Models? An ODD-based Replication Study",
    "summary": "arXiv:2602.10140v1 Announce Type: cross Abstract: Large language models (LLMs) can now synthesize non-trivial executable code from textual descriptions, raising an important question: can LLMs reliably implement agent-based models from standardized specifications in a way that supports replication, verification, and validation? We address this ques",
    "url": "https://arxiv.org/abs/2602.10140",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "When LLMs get significantly worse: A statistical approach to detect model degradations",
    "summary": "arXiv:2602.10144v1 Announce Type: cross Abstract: Minimizing the inference cost and latency of foundation models has become a crucial area of research. Optimization approaches include theoretically lossless methods and others without accuracy guarantees like quantization. In all of these cases it is crucial to ensure that the model quality has not ",
    "url": "https://arxiv.org/abs/2602.10144",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Silence Routing: When Not Speaking Improves Collective Judgment",
    "summary": "arXiv:2602.10145v1 Announce Type: cross Abstract: The wisdom of crowds has been shown to operate not only for factual judgments but also in matters of taste, where accuracy is defined relative to an individual's preferences. However, it remains unclear how different types of social signals should be selectively used in such domains. Focusing on a m",
    "url": "https://arxiv.org/abs/2602.10145",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "On the Use of a Large Language Model to Support the Conduction of a Systematic Mapping Study: A Brief Report from a Practitioner's View",
    "summary": "arXiv:2602.10147v1 Announce Type: cross Abstract: The use of Large Language Models (LLMs) has drawn growing interest within the scientific community. LLMs can handle large volumes of textual data and support methods for evidence synthesis. Although recent studies highlight the potential of LLMs to accelerate screening and data extraction steps in s",
    "url": "https://arxiv.org/abs/2602.10147",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Red-teaming the Multimodal Reasoning: Jailbreaking Vision-Language Models via Cross-modal Entanglement Attacks",
    "summary": "arXiv:2602.10148v1 Announce Type: cross Abstract: Vision-Language Models (VLMs) with multimodal reasoning capabilities are high-value attack targets, given their potential for handling complex multimodal harmful tasks. Mainstream black-box jailbreak attacks on VLMs work by distributing malicious clues across modalities to disperse model attention a",
    "url": "https://arxiv.org/abs/2602.10148",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Exploring Semantic Labeling Strategies for Third-Party Cybersecurity Risk Assessment Questionnaires",
    "summary": "arXiv:2602.10149v1 Announce Type: cross Abstract: Third-Party Risk Assessment (TPRA) is a core cybersecurity practice for evaluating suppliers against standards such as ISO/IEC 27001 and NIST. TPRA questionnaires are typically drawn from large repositories of security and compliance questions, yet tailoring assessments to organizational needs remai",
    "url": "https://arxiv.org/abs/2602.10149",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "PEST: Physics-Enhanced Swin Transformer for 3D Turbulence Simulation",
    "summary": "arXiv:2602.10150v1 Announce Type: cross Abstract: Accurate simulation of turbulent flows is fundamental to scientific and engineering applications. Direct numerical simulation (DNS) offers the highest fidelity but is computationally prohibitive, while existing data-driven alternatives struggle with stable long-horizon rollouts, physical consistency",
    "url": "https://arxiv.org/abs/2602.10150",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "PRISM-XR: Empowering Privacy-Aware XR Collaboration with Multimodal Large Language Models",
    "summary": "arXiv:2602.10154v1 Announce Type: cross Abstract: Multimodal Large Language Models (MLLMs) enhance collaboration in Extended Reality (XR) environments by enabling flexible object and animation creation through the combination of natural language and visual inputs. However, visual data captured by XR headsets includes real-world backgrounds that may",
    "url": "https://arxiv.org/abs/2602.10154",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "MalMoE: Mixture-of-Experts Enhanced Encrypted Malicious Traffic Detection Under Graph Drift",
    "summary": "arXiv:2602.10157v1 Announce Type: cross Abstract: Encryption has been commonly used in network traffic to secure transmission, but it also brings challenges for malicious traffic detection, due to the invisibility of the packet payload. Graph-based methods are emerging as promising solutions by leveraging multi-host interactions to promote detectio",
    "url": "https://arxiv.org/abs/2602.10157",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "NMRTrans: Structure Elucidation from Experimental NMR Spectra via Set Transformers",
    "summary": "arXiv:2602.10158v1 Announce Type: cross Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy is fundamental for molecular structure elucidation, yet interpreting spectra at scale remains time-consuming and highly expertise-dependent. While recent spectrum-as-language modeling and retrieval-based methods have shown promise, they rely heavily on l",
    "url": "https://arxiv.org/abs/2602.10158",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "AD$^2$: Analysis and Detection of Adversarial Threats in Visual Perception for End-to-End Autonomous Driving Systems",
    "summary": "arXiv:2602.10160v1 Announce Type: cross Abstract: End-to-end autonomous driving systems have achieved significant progress, yet their adversarial robustness remains largely underexplored. In this work, we conduct a closed-loop evaluation of state-of-the-art autonomous driving agents under black-box adversarial threat models in CARLA. Specifically, ",
    "url": "https://arxiv.org/abs/2602.10160",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Omni-Safety under Cross-Modality Conflict: Vulnerabilities, Dynamics Mechanisms and Efficient Alignment",
    "summary": "arXiv:2602.10161v1 Announce Type: cross Abstract: Omni-modal Large Language Models (OLLMs) greatly expand LLMs' multimodal capabilities but also introduce cross-modal safety risks. However, a systematic understanding of vulnerabilities in omni-modal interactions remains lacking. To bridge this gap, we establish a modality-semantics decoupling princ",
    "url": "https://arxiv.org/abs/2602.10161",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Beyond SMILES: Evaluating Agentic Systems for Drug Discovery",
    "summary": "arXiv:2602.10163v1 Announce Type: cross Abstract: Agentic systems for drug discovery have demonstrated autonomous synthesis planning, literature mining, and molecular design. We ask how well they generalize. Evaluating six frameworks against 15 task classes drawn from peptide therapeutics, in vivo pharmacology, and resource-constrained settings, we",
    "url": "https://arxiv.org/abs/2602.10163",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Anatomy-Preserving Latent Diffusion for Generation of Brain Segmentation Masks with Ischemic Infarct",
    "summary": "arXiv:2602.10167v1 Announce Type: cross Abstract: The scarcity of high-quality segmentation masks remains a major bottleneck for medical image analysis, particularly in non-contrast CT (NCCT) neuroimaging, where manual annotation is costly and variable. To address this limitation, we propose an anatomy-preserving generative framework for the uncond",
    "url": "https://arxiv.org/abs/2602.10167",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "EVA: Towards a universal model of the immune system",
    "summary": "arXiv:2602.10168v1 Announce Type: cross Abstract: The effective application of foundation models to translational research in immune-mediated diseases requires multimodal patient-level representations that can capture complex phenotypes emerging from multicellular interactions. Yet most current biological foundation models focus only on single-cell",
    "url": "https://arxiv.org/abs/2602.10168",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "EvoCodeBench: A Human-Performance Benchmark for Self-Evolving LLM-Driven Coding Systems",
    "summary": "arXiv:2602.10171v1 Announce Type: cross Abstract: As large language models (LLMs) continue to advance in programming tasks, LLM-driven coding systems have evolved from one-shot code generation into complex systems capable of iterative improvement during inference. However, existing code benchmarks primarily emphasize static correctness and implicit",
    "url": "https://arxiv.org/abs/2602.10171",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Cosmo3DFlow: Wavelet Flow Matching for Spatial-to-Spectral Compression in Reconstructing the Early Universe",
    "summary": "arXiv:2602.10172v1 Announce Type: cross Abstract: Reconstructing the early Universe from the evolved present-day Universe is a challenging and computationally demanding problem in modern astrophysics. We devise a novel generative framework, Cosmo3DFlow, designed to address dimensionality and sparsity, the critical bottlenecks inherent in current st",
    "url": "https://arxiv.org/abs/2602.10172",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Towards Autonomous Mathematics Research",
    "summary": "arXiv:2602.10177v1 Announce Type: cross Abstract: Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing l",
    "url": "https://arxiv.org/abs/2602.10177",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models",
    "summary": "arXiv:2602.10179v1 Announce Type: cross Abstract: Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vision-prompt editing, where user intent is inferred directly from visual inputs such as marks, arrows, and visual-text prompts. While this paradigm greatly expands usability, it also introduces ",
    "url": "https://arxiv.org/abs/2602.10179",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Versor: A Geometric Sequence Architecture",
    "summary": "arXiv:2602.10195v1 Announce Type: cross Abstract: A novel sequence architecture design is introduced, Versor, which uses Conformal Geometric Algebra (CGA) in place of the traditional fundamental non-linear operations to achieve structural generalization and significant performance improvements on a variety of tasks, while offering improved interpre",
    "url": "https://arxiv.org/abs/2602.10195",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models",
    "summary": "arXiv:2602.10224v1 Announce Type: cross Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces a meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization i",
    "url": "https://arxiv.org/abs/2602.10224",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Quantum Integrated Sensing and Computation with Indefinite Causal Order",
    "summary": "arXiv:2602.10225v1 Announce Type: cross Abstract: Quantum operations with indefinite causal order (ICO) represent a framework in quantum information processing where the relative order between two events can be indefinite. In this paper, we investigate whether sensing and computation, two canonical tasks in quantum information processing, can be ca",
    "url": "https://arxiv.org/abs/2602.10225",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Self-Evolving Recommendation System: End-To-End Autonomous Model Optimization With LLM Agents",
    "summary": "arXiv:2602.10226v1 Announce Type: cross Abstract: Optimizing large-scale machine learning systems, such as recommendation models for global video platforms, requires navigating a massive hyperparameter search space and, more critically, designing sophisticated optimizers, architectures, and reward functions to capture nuanced user behaviors. Achiev",
    "url": "https://arxiv.org/abs/2602.10226",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards",
    "summary": "arXiv:2602.10231v1 Announce Type: cross Abstract: Group Relative Policy Optimization (GRPO) assigns a single scalar advantage to all tokens in a completion. For structured generations with explicit segments and objectives, this couples unrelated reward signals across segments, leading to objective interference and misattributed credit. We propose B",
    "url": "https://arxiv.org/abs/2602.10231",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "ImprovEvolve: Ask AlphaEvolve to Improve the Input Solution and Then Improvise",
    "summary": "arXiv:2602.10233v1 Announce Type: cross Abstract: Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve, have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. In this article, we present ImprovEvolve, a simple yet effective technique for enhanci",
    "url": "https://arxiv.org/abs/2602.10233",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Transforming Policy-Car Swerving for Mitigating Stop-and-Go Traffic Waves: A Practice-Oriented Jam-Absorption Driving Strategy",
    "summary": "arXiv:2602.10234v1 Announce Type: cross Abstract: Stop-and-go waves, as a major form of freeway traffic congestion, cause severe and long-lasting adverse effects, including reduced traffic efficiency, increased driving risks, and higher vehicle emissions. Amongst the highway traffic management strategies, jam-absorption driving (JAD), in which a de",
    "url": "https://arxiv.org/abs/2602.10234",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "KORAL: Knowledge Graph Guided LLM Reasoning for SSD Operational Analysis",
    "summary": "arXiv:2602.10246v1 Announce Type: cross Abstract: Solid State Drives (SSDs) are critical to datacenters, consumer platforms, and mission-critical systems. Yet diagnosing their performance and reliability is difficult because data are fragmented and time-disjoint, and existing methods demand large datasets and expert input while offering only limite",
    "url": "https://arxiv.org/abs/2602.10246",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "The Complexity of Bayesian Network Learning: Revisiting the Superstructure",
    "summary": "arXiv:2602.10253v1 Announce Type: cross Abstract: We investigate the parameterized complexity of Bayesian Network Structure Learning (BNSL), a classical problem that has received significant attention in empirical but also purely theoretical studies. We follow up on previous works that have analyzed the complexity of BNSL w.r.t. the so-called super",
    "url": "https://arxiv.org/abs/2602.10253",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "From Classical to Topological Neural Networks Under Uncertainty",
    "summary": "arXiv:2602.10266v1 Announce Type: cross Abstract: This chapter explores neural networks, topological data analysis, and topological deep learning techniques, alongside statistical Bayesian methods, for processing images, time series, and graphs to maximize the potential of artificial intelligence in the military domain. Throughout the chapter, we h",
    "url": "https://arxiv.org/abs/2602.10266",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "ERGO: Excess-Risk-Guided Optimization for High-Fidelity Monocular 3D Gaussian Splatting",
    "summary": "arXiv:2602.10278v1 Announce Type: cross Abstract: Generating 3D content from a single image remains a fundamentally challenging and ill-posed problem due to the inherent absence of geometric and textural information in occluded regions. While state-of-the-art generative models can synthesize auxiliary views to provide additional supervision, these ",
    "url": "https://arxiv.org/abs/2602.10278",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "ECHO: An Open Research Platform for Evaluation of Chat, Human Behavior, and Outcomes",
    "summary": "arXiv:2602.10295v1 Announce Type: cross Abstract: ECHO (Evaluation of Chat, Human behavior, and Outcomes) is an open research platform designed to support reproducible, mixed-method studies of human interaction with both conversational AI systems and Web search engines. It enables researchers from varying disciplines to orchestrate end-to-end exper",
    "url": "https://arxiv.org/abs/2602.10295",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Confounding Robust Continuous Control via Automatic Reward Shaping",
    "summary": "arXiv:2602.10305v1 Announce Type: cross Abstract: Reward shaping has been applied widely to accelerate Reinforcement Learning (RL) agents' training. However, a principled way of designing effective reward shaping functions, especially for complex continuous control problems, remains largely under-explained. In this work, we propose to automatically",
    "url": "https://arxiv.org/abs/2602.10305",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Are More Tokens Rational? Inference-Time Scaling in Language Models as Adaptive Resource Rationality",
    "summary": "arXiv:2602.10329v1 Announce Type: cross Abstract: Human reasoning is shaped by resource rationality -- optimizing performance under constraints. Recently, inference-time scaling has emerged as a powerful paradigm to improve the reasoning performance of Large Language Models by expanding test-time computation. Specifically, instruction-tuned (IT) mo",
    "url": "https://arxiv.org/abs/2602.10329",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Learning Self-Interpretation from Interpretability Artifacts: Training Lightweight Adapters on Vector-Label Pairs",
    "summary": "arXiv:2602.10352v1 Announce Type: cross Abstract: Self-interpretation methods prompt language models to describe their own internal states, but remain unreliable due to hyperparameter sensitivity. We show that training lightweight adapters on interpretability artifacts, while keeping the LM entirely frozen, yields reliable self-interpretation acros",
    "url": "https://arxiv.org/abs/2602.10352",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Beyond Calibration: Confounding Pathology Limits Foundation Model Specificity in Abdominal Trauma CT",
    "summary": "arXiv:2602.10359v1 Announce Type: cross Abstract: Purpose: Translating foundation models into clinical practice requires evaluating their performance under compound distribution shift, where severe class imbalance coexists with heterogeneous imaging appearances. This challenge is relevant for traumatic bowel injury, a rare but high-mortality diagno",
    "url": "https://arxiv.org/abs/2602.10359",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "ENIGMA: EEG-to-Image in 15 Minutes Using Less Than 1% of the Parameters",
    "summary": "arXiv:2602.10361v1 Announce Type: cross Abstract: To be practical for real-life applications, models for brain-computer interfaces must be easily and quickly deployable on new subjects, effective on affordable scanning hardware, and small enough to run locally on accessible computing resources. To directly address these current limitations, we intr",
    "url": "https://arxiv.org/abs/2602.10361",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "The Alignment Bottleneck in Decomposition-Based Claim Verification",
    "summary": "arXiv:2602.10380v1 Announce Type: cross Abstract: Structured claim decomposition is often proposed as a solution for verifying complex, multi-faceted claims, yet empirical results have been inconsistent. We argue that these inconsistencies stem from two overlooked bottlenecks: evidence alignment and sub-claim error profiles. To better understand th",
    "url": "https://arxiv.org/abs/2602.10380",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Time-to-Event Transformer to Capture Timing Attention of Events in EHR Time Series",
    "summary": "arXiv:2602.10385v1 Announce Type: cross Abstract: Automatically discovering personalized sequential events from large-scale time-series data is crucial for enabling precision medicine in clinical research, yet it remains a formidable challenge even for contemporary AI models. For example, while transformers capture rich associations, they are mostl",
    "url": "https://arxiv.org/abs/2602.10385",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Making Databases Faster with LLM Evolutionary Sampling",
    "summary": "arXiv:2602.10387v1 Announce Type: cross Abstract: Traditional query optimization relies on cost-based optimizers that estimate execution cost (e.g., runtime, memory, and I/O) using predefined heuristics and statistical models. Improving these heuristics requires substantial engineering effort, and even when implemented, these heuristics often canno",
    "url": "https://arxiv.org/abs/2602.10387",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs",
    "summary": "arXiv:2602.10388v1 Announce Type: cross Abstract: The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals ",
    "url": "https://arxiv.org/abs/2602.10388",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Affordances Enable Partial World Modeling with LLMs",
    "summary": "arXiv:2602.10390v1 Announce Type: cross Abstract: Full models of the world require complex knowledge of immense detail. While pre-trained large models have been hypothesized to contain similar knowledge due to extensive pre-training on vast amounts of internet scale data, using them directly in a search procedure is inefficient and inaccurate. Conv",
    "url": "https://arxiv.org/abs/2602.10390",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Modular Multi-Task Learning for Chemical Reaction Prediction",
    "summary": "arXiv:2602.10404v1 Announce Type: cross Abstract: Adapting large language models (LLMs) trained on broad organic chemistry to smaller, domain-specific reaction datasets is a key challenge in chemical and pharmaceutical R&amp;D. Effective specialisation requires learning new reaction knowledge while preserving general chemical understanding across r",
    "url": "https://arxiv.org/abs/2602.10404",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "AI-rithmetic",
    "summary": "arXiv:2602.10416v1 Announce Type: cross Abstract: Modern AI systems have been successfully deployed to win medals at international math competitions, assist with research workflows, and prove novel technical lemmas. However, despite their progress at advanced levels of mathematics, they remain stubbornly bad at basic arithmetic, consistently failin",
    "url": "https://arxiv.org/abs/2602.10416",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Equivariant Evidential Deep Learning for Interatomic Potentials",
    "summary": "arXiv:2602.10419v1 Announce Type: cross Abstract: Uncertainty quantification (UQ) is critical for assessing the reliability of machine learning interatomic potentials (MLIPs) in molecular dynamics (MD) simulations, identifying extrapolation regimes and enabling uncertainty-aware workflows such as active learning for training dataset construction. E",
    "url": "https://arxiv.org/abs/2602.10419",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "AIvilization v0: Toward Large-Scale Artificial Social Simulation with a Unified Agent Architecture and Adaptive Agent Profiles",
    "summary": "arXiv:2602.10429v1 Announce Type: cross Abstract: AIvilization v0 is a publicly deployed large-scale artificial society that couples a resource-constrained sandbox economy with a unified LLM-agent architecture, aiming to sustain long-horizon autonomy while remaining executable under rapidly changing environment. To mitigate the tension between goal",
    "url": "https://arxiv.org/abs/2602.10429",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Breaking the Curse of Repulsion: Optimistic Distributionally Robust Policy Optimization for Off-Policy Generative Recommendation",
    "summary": "arXiv:2602.10430v1 Announce Type: cross Abstract: Policy-based Reinforcement Learning (RL) has established itself as the dominant paradigm in generative recommendation for optimizing sequential user interactions. However, when applied to offline historical logs, these methods suffer a critical failure: the dominance of low-quality data induces seve",
    "url": "https://arxiv.org/abs/2602.10430",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "A Dual-Stream Physics-Augmented Unsupervised Architecture for Runtime Embedded Vehicle Health Monitoring",
    "summary": "arXiv:2602.10432v1 Announce Type: cross Abstract: Runtime quantification of vehicle operational intensity is essential for predictive maintenance and condition monitoring in commercial and heavy-duty fleets. Traditional metrics like mileage fail to capture mechanical burden, while unsupervised deep learning models detect statistical anomalies, typi",
    "url": "https://arxiv.org/abs/2602.10432",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Control Reinforcement Learning: Token-Level Mechanistic Analysis via Learned SAE Feature Steering",
    "summary": "arXiv:2602.10437v1 Announce Type: cross Abstract: Sparse autoencoders (SAEs) decompose language model activations into interpretable features, but existing methods reveal only which features activate, not which change model outputs when amplified. We introduce Control Reinforcement Learning (CRL), which trains a policy to select SAE features for st",
    "url": "https://arxiv.org/abs/2602.10437",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "AudioRouter: Data Efficient Audio Understanding via RL based Dual Reasoning",
    "summary": "arXiv:2602.10439v1 Announce Type: cross Abstract: Large Audio Language Models (LALMs) have demonstrated strong capabilities in audio understanding and reasoning. However, their performance on fine grained auditory perception remains unreliable, and existing approaches largely rely on data intensive training to internalize perceptual abilities. We p",
    "url": "https://arxiv.org/abs/2602.10439",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "LakeMLB: Data Lake Machine Learning Benchmark",
    "summary": "arXiv:2602.10441v1 Announce Type: cross Abstract: Modern data lakes have emerged as foundational platforms for large-scale machine learning, enabling flexible storage of heterogeneous data and structured analytics through table-oriented abstractions. Despite their growing importance, standardized benchmarks for evaluating machine learning performan",
    "url": "https://arxiv.org/abs/2602.10441",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "A Unified Theory of Random Projection for Influence Functions",
    "summary": "arXiv:2602.10449v1 Announce Type: cross Abstract: Influence functions and related data attribution scores take the form of $g^{\\top}F^{-1}g^{\\prime}$, where $F\\succeq 0$ is a curvature operator. In modern overparameterized models, forming or inverting $F\\in\\mathbb{R}^{d\\times d}$ is prohibitive, motivating scalable influence computation via random ",
    "url": "https://arxiv.org/abs/2602.10449",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Constructing Industrial-Scale Optimization Modeling Benchmark",
    "summary": "arXiv:2602.10450v1 Announce Type: cross Abstract: Optimization modeling underpins decision-making in logistics, manufacturing, energy, and finance, yet translating natural-language requirements into correct optimization formulations and solver-executable code remains labor-intensive. Although large language models (LLMs) have been explored for this",
    "url": "https://arxiv.org/abs/2602.10450",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Authenticated Workflows: A Systems Approach to Protecting Agentic AI",
    "summary": "arXiv:2602.10465v1 Announce Type: cross Abstract: Agentic AI systems automate enterprise workflows but existing defenses--guardrails, semantic filters--are probabilistic and routinely bypassed. We introduce authenticated workflows, the first complete trust layer for enterprise agentic AI. Security reduces to protecting four fundamental boundaries: ",
    "url": "https://arxiv.org/abs/2602.10465",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Why Human Guidance Matters in Collaborative Vibe Coding",
    "summary": "arXiv:2602.10473v1 Announce Type: cross Abstract: Writing code has been one of the most transformative ways for human societies to translate abstract ideas into tangible technologies. Modern AI is transforming this process by enabling experts and non-experts alike to generate code without actually writing code, but instead, through natural language",
    "url": "https://arxiv.org/abs/2602.10473",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Driving Reaction Trajectories via Latent Flow Matching",
    "summary": "arXiv:2602.10476v1 Announce Type: cross Abstract: Recent advances in reaction prediction have achieved near-saturated accuracy on standard benchmarks (e.g., USPTO), yet most state-of-the-art models formulate the task as a one-shot mapping from reactants to products, offering limited insight into the underlying reaction process. Procedural alternati",
    "url": "https://arxiv.org/abs/2602.10476",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Protecting Context and Prompts: Deterministic Security for Non-Deterministic AI",
    "summary": "arXiv:2602.10481v1 Announce Type: cross Abstract: Large Language Model (LLM) applications are vulnerable to prompt injection and context manipulation attacks that traditional security models cannot prevent. We introduce two novel primitives--authenticated prompts and authenticated context--that provide cryptographically verifiable provenance across",
    "url": "https://arxiv.org/abs/2602.10481",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Learning Adaptive Distribution Alignment with Neural Characteristic Function for Graph Domain Adaptation",
    "summary": "arXiv:2602.10489v1 Announce Type: cross Abstract: Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs to unlabeled target graphs but is challenged by complex, multi-faceted distributional shifts. Existing methods attempt to reduce distributional shifts by aligning manually selected graph elements (e.g., node attributes or s",
    "url": "https://arxiv.org/abs/2602.10489",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Low-Dimensional Execution Manifolds in Transformer Learning Dynamics: Evidence from Modular Arithmetic Tasks",
    "summary": "arXiv:2602.10496v1 Announce Type: cross Abstract: We investigate the geometric structure of learning dynamics in overparameterized transformer models through carefully controlled modular arithmetic tasks. Our primary finding is that despite operating in high-dimensional parameter spaces ($d=128$), transformer training trajectories rapidly collapse ",
    "url": "https://arxiv.org/abs/2602.10496",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Learning Structure-Semantic Evolution Trajectories for Graph Domain Adaptation",
    "summary": "arXiv:2602.10506v1 Announce Type: cross Abstract: Graph Domain Adaptation (GDA) aims to bridge distribution shifts between domains by transferring knowledge from well-labeled source graphs to given unlabeled target graphs. One promising recent approach addresses graph transfer by discretizing the adaptation process, typically through the constructi",
    "url": "https://arxiv.org/abs/2602.10506",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "1%>100%: High-Efficiency Visual Adapter with Complex Linear Projection Optimization",
    "summary": "arXiv:2602.10513v1 Announce Type: cross Abstract: Deploying vision foundation models typically relies on efficient adaptation strategies, whereas conventional full fine-tuning suffers from prohibitive costs and low efficiency. While delta-tuning has proven effective in boosting the performance and efficiency of LLMs during adaptation, its advantage",
    "url": "https://arxiv.org/abs/2602.10513",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Co-jump: Cooperative Jumping with Quadrupedal Robots via Multi-Agent Reinforcement Learning",
    "summary": "arXiv:2602.10514v1 Announce Type: cross Abstract: While single-agent legged locomotion has witnessed remarkable progress, individual robots remain fundamentally constrained by physical actuation limits. To transcend these boundaries, we introduce Co-jump, a cooperative task where two quadrupedal robots synchronize to execute jumps far beyond their ",
    "url": "https://arxiv.org/abs/2602.10514",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "LHAW: Controllable Underspecification for Long-Horizon Tasks",
    "summary": "arXiv:2602.10525v1 Announce Type: cross Abstract: Long-horizon workflow agents that operate effectively over extended periods are essential for truly autonomous systems. Their reliable execution critically depends on the ability to reason through ambiguous situations in which clarification seeking is necessary to ensure correct task execution. Howe",
    "url": "https://arxiv.org/abs/2602.10525",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "AI-PACE: A Framework for Integrating AI into Medical Education",
    "summary": "arXiv:2602.10527v1 Announce Type: cross Abstract: The integration of artificial intelligence (AI) into healthcare is accelerating, yet medical education has not kept pace with these technological advancements. This paper synthesizes current knowledge on AI in medical education through a comprehensive analysis of the literature, identifying key comp",
    "url": "https://arxiv.org/abs/2602.10527",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "A Swap-Adversarial Framework for Improving Domain Generalization in Electroencephalography-Based Parkinson's Disease Prediction",
    "summary": "arXiv:2602.10528v1 Announce Type: cross Abstract: Electroencephalography (ECoG) offers a promising alternative to conventional electrocorticography (EEG) for the early prediction of Parkinson's disease (PD), providing higher spatial resolution and a broader frequency range. However, reproducible comparisons has been limited by ethical constraints i",
    "url": "https://arxiv.org/abs/2602.10528",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "$\\mu$pscaling small models: Principled warm starts and hyperparameter transfer",
    "summary": "arXiv:2602.10545v1 Announce Type: cross Abstract: Modern large-scale neural networks are often trained and released in multiple sizes to accommodate diverse inference budgets. To improve efficiency, recent work has explored model upscaling: initializing larger models from trained smaller ones in order to transfer knowledge and accelerate convergenc",
    "url": "https://arxiv.org/abs/2602.10545",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "RealHD: A High-Quality Dataset for Robust Detection of State-of-the-Art AI-Generated Images",
    "summary": "arXiv:2602.10546v1 Announce Type: cross Abstract: The rapid advancement of generative AI has raised concerns about the authenticity of digital images, as highly realistic fake images can now be generated at low cost, potentially increasing societal risks. In response, several datasets have been established to train detection models aimed at disting",
    "url": "https://arxiv.org/abs/2602.10546",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Enhancing Weakly Supervised Multimodal Video Anomaly Detection through Text Guidance",
    "summary": "arXiv:2602.10549v1 Announce Type: cross Abstract: Weakly supervised multimodal video anomaly detection has gained significant attention, yet the potential of the text modality remains under-explored. Text provides explicit semantic information that can enhance anomaly characterization and reduce false alarms. However, extracting effective text feat",
    "url": "https://arxiv.org/abs/2602.10549",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "C^2ROPE: Causal Continuous Rotary Positional Encoding for 3D Large Multimodal-Models Reasoning",
    "summary": "arXiv:2602.10551v1 Announce Type: cross Abstract: Recent advances in 3D Large Multimodal Models (LMMs) built on Large Language Models (LLMs) have established the alignment of 3D visual features with LLM representations as the dominant paradigm. However, the inherited Rotary Position Embedding (RoPE) introduces limitations for multimodal processing.",
    "url": "https://arxiv.org/abs/2602.10551",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Contrastive Learning for Multi Label ECG Classification with Jaccard Score Based Sigmoid Loss",
    "summary": "arXiv:2602.10553v1 Announce Type: cross Abstract: Recent advances in large language models (LLMs) have enabled the development of multimodal medical AI. While models such as MedGemini achieve high accuracy on VQA tasks like USMLE MM, their performance on ECG based tasks remains limited, and some models, such as MedGemma, do not support ECG data at ",
    "url": "https://arxiv.org/abs/2602.10553",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer",
    "summary": "arXiv:2602.10556v1 Announce Type: cross Abstract: A long-standing goal in robotics is a generalist policy that can be deployed zero-shot on new robot embodiments without per-embodiment adaptation. Despite large-scale multi-embodiment pre-training, existing Vision-Language-Action models (VLAs) remain tightly coupled to their training embodiments and",
    "url": "https://arxiv.org/abs/2602.10556",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning",
    "summary": "arXiv:2602.10560v1 Announce Type: cross Abstract: While reasoning over long context is crucial for various real-world applications, it remains challenging for large language models (LLMs) as they suffer from performance degradation as the context length grows. Recent work MemAgent has tried to tackle this by processing context chunk-by-chunk in an ",
    "url": "https://arxiv.org/abs/2602.10560",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning",
    "summary": "arXiv:2602.10575v1 Announce Type: cross Abstract: Metaphorical comprehension in images remains a critical challenge for Nowadays AI systems. While Multimodal Large Language Models (MLLMs) excel at basic Visual Question Answering (VQA), they consistently struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visua",
    "url": "https://arxiv.org/abs/2602.10575",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization",
    "summary": "arXiv:2602.10576v1 Announce Type: cross Abstract: Symbolic regression aims to distill mathematical equations from observational data. Recent approaches have successfully leveraged Large Language Models (LLMs) to generate equation hypotheses, capitalizing on their vast pre-trained scientific priors. However, existing frameworks predominantly treat t",
    "url": "https://arxiv.org/abs/2602.10576",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity",
    "summary": "arXiv:2602.10585v1 Announce Type: cross Abstract: The trade-off between interpretability and accuracy remains a core challenge in machine learning. Standard Generalized Additive Models (GAMs) offer clear feature attributions but are often constrained by their strictly additive nature, which can limit predictive performance. Introducing feature inte",
    "url": "https://arxiv.org/abs/2602.10585",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters",
    "summary": "arXiv:2602.10604v1 Announce Type: cross Abstract: We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with ",
    "url": "https://arxiv.org/abs/2602.10604",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Hierarchical Zero-Order Optimization for Deep Neural Networks",
    "summary": "arXiv:2602.10607v1 Announce Type: cross Abstract: Zeroth-order (ZO) optimization has long been favored for its biological plausibility and its capacity to handle non-differentiable objectives, yet its computational complexity has historically limited its application in deep neural networks. Challenging the conventional paradigm that gradients propa",
    "url": "https://arxiv.org/abs/2602.10607",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Online Causal Kalman Filtering for Stable and Effective Policy Optimization",
    "summary": "arXiv:2602.10609v1 Announce Type: cross Abstract: Reinforcement learning for large language models suffers from high-variance token-level importance sampling (IS) ratios, which would destabilize policy optimization at scale. To improve stability, recent methods typically use a fixed sequence-level IS ratio for all tokens in a sequence or adjust eac",
    "url": "https://arxiv.org/abs/2602.10609",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Mitigating Reward Hacking in RLHF via Bayesian Non-negative Reward Modeling",
    "summary": "arXiv:2602.10623v1 Announce Type: cross Abstract: Reward models learned from human preferences are central to aligning large language models (LLMs) via reinforcement learning from human feedback, yet they are often vulnerable to reward hacking due to noisy annotations and systematic biases such as response length or style. We propose Bayesian Non-N",
    "url": "https://arxiv.org/abs/2602.10623",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "A Vision-Language Foundation Model for Zero-shot Clinical Collaboration and Automated Concept Discovery in Dermatology",
    "summary": "arXiv:2602.10624v1 Announce Type: cross Abstract: Medical foundation models have shown promise in controlled benchmarks, yet widespread deployment remains hindered by reliance on task-specific fine-tuning. Here, we introduce DermFM-Zero, a dermatology vision-language foundation model trained via masked latent modelling and contrastive learning on o",
    "url": "https://arxiv.org/abs/2602.10624",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "The Neurosymbolic Frontier of Nonuniform Ellipticity: Formalizing Sharp Schauder Theory via Topos-Theoretic Reasoning Models",
    "summary": "arXiv:2602.10632v1 Announce Type: cross Abstract: This white paper presents a critical synthesis of the recent breakthrough in nonuniformly elliptic regularity theory and the burgeoning field of neurosymbolic large reasoning models (LRMs). We explore the resolution of the long-standing sharp growth rate conjecture in Schauder theory, achieved by Cr",
    "url": "https://arxiv.org/abs/2602.10632",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "TwiFF (Think With Future Frames): A Large-Scale Dataset for Dynamic Visual Reasoning",
    "summary": "arXiv:2602.10675v1 Announce Type: cross Abstract: Visual Chain-of-Thought (VCoT) has emerged as a promising paradigm for enhancing multimodal reasoning by integrating visual perception into intermediate reasoning steps. However, existing VCoT approaches are largely confined to static scenarios and struggle to capture the temporal dynamics essential",
    "url": "https://arxiv.org/abs/2602.10675",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL",
    "summary": "arXiv:2602.10687v1 Announce Type: cross Abstract: Existing forgery detection methods are often limited to uni-modal or bi-modal settings, failing to handle the interleaved text, images, and videos prevalent in real-world misinformation. To bridge this gap, this paper targets to develop a unified framework for omnibus vision-language forgery detecti",
    "url": "https://arxiv.org/abs/2602.10687",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training",
    "summary": "arXiv:2602.10693v1 Announce Type: cross Abstract: Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. ",
    "url": "https://arxiv.org/abs/2602.10693",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models",
    "summary": "arXiv:2602.10698v1 Announce Type: cross Abstract: Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic perception and control, yet most existing approaches primarily rely on VLM trained using 2D images, which limits their spatial understanding and action grounding in complex 3D environments. To address this limi",
    "url": "https://arxiv.org/abs/2602.10698",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Interpretable Graph-Level Anomaly Detection via Contrast with Normal Prototypes",
    "summary": "arXiv:2602.10708v1 Announce Type: cross Abstract: The task of graph-level anomaly detection (GLAD) is to identify anomalous graphs that deviate significantly from the majority of graphs in a dataset. While deep GLAD methods have shown promising performance, their black-box nature limits their reliability and deployment in real-world applications. A",
    "url": "https://arxiv.org/abs/2602.10708",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Cross-Sectional Asset Retrieval via Future-Aligned Soft Contrastive Learning",
    "summary": "arXiv:2602.10711v1 Announce Type: cross Abstract: Asset retrieval--finding similar assets in a financial universe--is central to quantitative investment decision-making. Existing approaches define similarity through historical price patterns or sector classifications, but such backward-looking criteria provide no guarantee about future behavior. We",
    "url": "https://arxiv.org/abs/2602.10711",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Locomo-Plus: Beyond-Factual Cognitive Memory Evaluation Framework for LLM Agents",
    "summary": "arXiv:2602.10715v1 Announce Type: cross Abstract: Long-term conversational memory is a core capability for LLM-based dialogue systems, yet existing benchmarks and evaluation protocols primarily focus on surface-level factual recall. In realistic interactions, appropriate responses often depend on implicit constraints such as user state, goals, or v",
    "url": "https://arxiv.org/abs/2602.10715",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "A Diffusion-Based Generative Prior Approach to Sparse-view Computed Tomography",
    "summary": "arXiv:2602.10722v1 Announce Type: cross Abstract: The reconstruction of X-rays CT images from sparse or limited-angle geometries is a highly challenging task. The lack of data typically results in artifacts in the reconstructed image and may even lead to object distortions. For this reason, the use of deep generative models in this context has grea",
    "url": "https://arxiv.org/abs/2602.10722",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Calliope: A TTS-based Narrated E-book Creator Ensuring Exact Synchronization, Privacy, and Layout Fidelity",
    "summary": "arXiv:2602.10735v1 Announce Type: cross Abstract: A narrated e-book combines synchronized audio with digital text, highlighting the currently spoken word or sentence during playback. This format supports early literacy and assists individuals with reading challenges, while also allowing general readers to seamlessly switch between reading and liste",
    "url": "https://arxiv.org/abs/2602.10735",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Self-Supervised Image Super-Resolution Quality Assessment based on Content-Free Multi-Model Oriented Representation Learning",
    "summary": "arXiv:2602.10744v1 Announce Type: cross Abstract: Super-resolution (SR) applied to real-world low-resolution (LR) images often results in complex, irregular degradations that stem from the inherent complexity of natural scene acquisition. In contrast to SR artifacts arising from synthetic LR images created under well-defined scenarios, those distor",
    "url": "https://arxiv.org/abs/2602.10744",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "SecureScan: An AI-Driven Multi-Layer Framework for Malware and Phishing Detection Using Logistic Regression and Threat Intelligence Integration",
    "summary": "arXiv:2602.10750v1 Announce Type: cross Abstract: The growing sophistication of modern malware and phishing campaigns has diminished the effectiveness of traditional signature-based intrusion detection systems. This work presents SecureScan, an AI-driven, triple-layer detection framework that integrates logistic regression-based classification, heu",
    "url": "https://arxiv.org/abs/2602.10750",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Exploring the impact of adaptive rewiring in Graph Neural Networks",
    "summary": "arXiv:2602.10754v1 Announce Type: cross Abstract: This paper explores sparsification methods as a form of regularization in Graph Neural Networks (GNNs) to address high memory usage and computational costs in large-scale graph applications. Using techniques from Network Science and Machine Learning, including Erd\\H{o}s-R\\'enyi for model sparsificat",
    "url": "https://arxiv.org/abs/2602.10754",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "LOREN: Low Rank-Based Code-Rate Adaptation in Neural Receivers",
    "summary": "arXiv:2602.10770v1 Announce Type: cross Abstract: Neural network based receivers have recently demonstrated superior system-level performance compared to traditional receivers. However, their practicality is limited by high memory and power requirements, as separate weight sets must be stored for each code rate. To address this challenge, we propos",
    "url": "https://arxiv.org/abs/2602.10770",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Kill it with FIRE: On Leveraging Latent Space Directions for Runtime Backdoor Mitigation in Deep Neural Networks",
    "summary": "arXiv:2602.10780v1 Announce Type: cross Abstract: Machine learning models are increasingly present in our everyday lives; as a result, they become targets of adversarial attackers seeking to manipulate the systems we interact with. A well-known vulnerability is a backdoor introduced into a neural network by poisoned training data or a malicious tra",
    "url": "https://arxiv.org/abs/2602.10780",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "VulReaD: Knowledge-Graph-guided Software Vulnerability Reasoning and Detection",
    "summary": "arXiv:2602.10787v1 Announce Type: cross Abstract: Software vulnerability detection (SVD) is a critical challenge in modern systems. Large language models (LLMs) offer natural-language explanations alongside predictions, but most work focuses on binary evaluation, and explanations often lack semantic consistency with Common Weakness Enumeration (CWE",
    "url": "https://arxiv.org/abs/2602.10787",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Transport, Don't Generate: Deterministic Geometric Flows for Combinatorial Optimization",
    "summary": "arXiv:2602.10794v1 Announce Type: cross Abstract: Recent advances in Neural Combinatorial Optimization (NCO) have been dominated by diffusion models that treat the Euclidean Traveling Salesman Problem (TSP) as a stochastic $N \\times N$ heatmap generation task. In this paper, we propose CycFlow, a framework that replaces iterative edge denoising wit",
    "url": "https://arxiv.org/abs/2602.10794",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "RSHallu: Dual-Mode Hallucination Evaluation for Remote-Sensing Multimodal Large Language Models with Domain-Tailored Mitigation",
    "summary": "arXiv:2602.10799v1 Announce Type: cross Abstract: Multimodal large language models (MLLMs) are increasingly adopted in remote sensing (RS) and have shown strong performance on tasks such as RS visual grounding (RSVG), RS visual question answering (RSVQA), and multimodal dialogue. However, hallucinations, which are responses inconsistent with the in",
    "url": "https://arxiv.org/abs/2602.10799",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "PELLI: Framework to effectively integrate LLMs for quality software generation",
    "summary": "arXiv:2602.10808v1 Announce Type: cross Abstract: Recent studies have revealed that when LLMs are appropriately prompted and configured, they demonstrate mixed results. Such results often meet or exceed the baseline performance. However, these comparisons have two primary issues. First, they mostly considered only reliability as a comparison metric",
    "url": "https://arxiv.org/abs/2602.10808",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Beyond Confidence: The Rhythms of Reasoning in Generative Models",
    "summary": "arXiv:2602.10816v1 Announce Type: cross Abstract: Large Language Models (LLMs) exhibit impressive capabilities yet suffer from sensitivity to slight input context variations, hampering reliability. Conventional metrics like accuracy and perplexity fail to assess local prediction robustness, as normalized output probabilities can obscure the underly",
    "url": "https://arxiv.org/abs/2602.10816",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Flow caching for autoregressive video generation",
    "summary": "arXiv:2602.10825v1 Announce Type: cross Abstract: Autoregressive models, often built on Transformer architectures, represent a powerful paradigm for generating ultra-long videos by synthesizing content in sequential chunks. However, this sequential generation process is notoriously slow. While caching strategies have proven effective for accelerati",
    "url": "https://arxiv.org/abs/2602.10825",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Enhancing Multivariate Time Series Forecasting with Global Temporal Retrieval",
    "summary": "arXiv:2602.10847v1 Announce Type: cross Abstract: Multivariate time series forecasting (MTSF) plays a vital role in numerous real-world applications, yet existing models remain constrained by their reliance on a limited historical context. This limitation prevents them from effectively capturing global periodic patterns that often span cycles signi",
    "url": "https://arxiv.org/abs/2602.10847",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Time Series Foundation Models for Energy Load Forecasting on Consumer Hardware: A Multi-Dimensional Zero-Shot Benchmark",
    "summary": "arXiv:2602.10848v1 Announce Type: cross Abstract: Time Series Foundation Models (TSFMs) have introduced zero-shot prediction capabilities that bypass the need for task-specific training. Whether these capabilities translate to mission-critical applications such as electricity demand forecasting--where accuracy, calibration, and robustness directly ",
    "url": "https://arxiv.org/abs/2602.10848",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "ICA: Information-Aware Credit Assignment for Visually Grounded Long-Horizon Information-Seeking Agents",
    "summary": "arXiv:2602.10863v1 Announce Type: cross Abstract: Despite the strong performance achieved by reinforcement learning-trained information-seeking agents, learning in open-ended web environments remains severely constrained by low signal-to-noise feedback. Text-based parsers often discard layout semantics and introduce unstructured noise, while long-h",
    "url": "https://arxiv.org/abs/2602.10863",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "FedPS: Federated data Preprocessing via aggregated Statistics",
    "summary": "arXiv:2602.10870v1 Announce Type: cross Abstract: Federated Learning (FL) enables multiple parties to collaboratively train machine learning models without sharing raw data. However, before training, data must be preprocessed to address missing values, inconsistent formats, and heterogeneous feature scales. This preprocessing stage is critical for ",
    "url": "https://arxiv.org/abs/2602.10870",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Diagnosing Structural Failures in LLM-Based Evidence Extraction for Meta-Analysis",
    "summary": "arXiv:2602.10881v1 Announce Type: cross Abstract: Systematic reviews and meta-analyses rely on converting narrative articles into structured, numerically grounded study records. Despite rapid advances in large language models (LLMs), it remains unclear whether they can meet the structural requirements of this process, which hinge on preserving role",
    "url": "https://arxiv.org/abs/2602.10881",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "The CLEF-2026 FinMMEval Lab: Multilingual and Multimodal Evaluation of Financial AI Systems",
    "summary": "arXiv:2602.10886v1 Announce Type: cross Abstract: We present the setup and the tasks of the FinMMEval Lab at CLEF 2026, which introduces the first multilingual and multimodal evaluation framework for financial Large Language Models (LLMs). While recent advances in financial natural language processing have enabled automated analysis of market repor",
    "url": "https://arxiv.org/abs/2602.10886",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Interactive LLM-assisted Curriculum Learning for Multi-Task Evolutionary Policy Search",
    "summary": "arXiv:2602.10891v1 Announce Type: cross Abstract: Multi-task policy search is a challenging problem because policies are required to generalize beyond training cases. Curriculum learning has proven to be effective in this setting, as it introduces complexity progressively. However, designing effective curricula is labor-intensive and requires exten",
    "url": "https://arxiv.org/abs/2602.10891",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Resource-Efficient Model-Free Reinforcement Learning for Board Games",
    "summary": "arXiv:2602.10894v1 Announce Type: cross Abstract: Board games have long served as complex decision-making benchmarks in artificial intelligence. In this field, search-based reinforcement learning methods such as AlphaZero have achieved remarkable success. However, their significant computational demands have been pointed out as barriers to their re",
    "url": "https://arxiv.org/abs/2602.10894",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Blind Gods and Broken Screens: Architecting a Secure, Intent-Centric Mobile Agent Operating System",
    "summary": "arXiv:2602.10915v1 Announce Type: cross Abstract: The evolution of Large Language Models (LLMs) has shifted mobile computing from App-centric interactions to system-level autonomous agents. Current implementations predominantly rely on a \"Screen-as-Interface\" paradigm, which inherits structural vulnerabilities and conflicts with the mobile ecosyste",
    "url": "https://arxiv.org/abs/2602.10915",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Traceable, Enforceable, and Compensable Participation: A Participation Ledger for People-Centered AI Governance",
    "summary": "arXiv:2602.10916v1 Announce Type: cross Abstract: Participatory approaches are widely invoked in AI governance, yet participation rarely translates into durable influence. In public sector and civic AI systems, community contributions such as deliberations, annotations, prompts, and incident reports are often recorded informally, weakly linked to s",
    "url": "https://arxiv.org/abs/2602.10916",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "What do people want to fact-check?",
    "summary": "arXiv:2602.10935v1 Announce Type: cross Abstract: Research on misinformation has focused almost exclusively on supply, asking what falsehoods circulate, who produces them, and whether corrections work. A basic demand-side question remains unanswered. When ordinary people can fact-check anything they want, what do they actually ask about? We provide",
    "url": "https://arxiv.org/abs/2602.10935",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Computational Phenomenology of Temporal Experience in Autism: Quantifying the Emotional and Narrative Characteristics of Lived Unpredictability",
    "summary": "arXiv:2602.10947v1 Announce Type: cross Abstract: Disturbances in temporality, such as desynchronization with the social environment and its unpredictability, are considered core features of autism with a deep impact on relationships. However, limitations regarding research on this issue include: 1) the dominance of deficit-based medical models of ",
    "url": "https://arxiv.org/abs/2602.10947",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Search or Accelerate: Confidence-Switched Position Beam Search for Diffusion Language Models",
    "summary": "arXiv:2602.10953v1 Announce Type: cross Abstract: Diffusion Language Models (DLMs) generate text by iteratively denoising a masked sequence, repeatedly deciding which positions to commit at each step. Standard decoding follows a greedy rule: unmask the most confident positions, yet this local choice can lock the model into a suboptimal unmasking or",
    "url": "https://arxiv.org/abs/2602.10953",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Rotary Positional Embeddings as Phase Modulation: Theoretical Bounds on the RoPE Base for Long-Context Transformers",
    "summary": "arXiv:2602.10959v1 Announce Type: cross Abstract: Rotary positional embeddings (RoPE) are widely used in large language models to encode token positions through multiplicative rotations, yet their behavior at long context lengths remains poorly characterized. In this work, we reinterpret RoPE as phase modulation applied to a bank of complex oscilla",
    "url": "https://arxiv.org/abs/2602.10959",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Healthy Harvests: A Comparative Look at Guava Disease Classification Using InceptionV3",
    "summary": "arXiv:2602.10967v1 Announce Type: cross Abstract: Guava fruits often suffer from many diseases. This can harm fruit quality and fruit crop yield. Early identification is important for minimizing damage and ensuring fruit health. This study focuses on 3 different categories for classifying diseases. These are Anthracnose, Fruit flies, and Healthy fr",
    "url": "https://arxiv.org/abs/2602.10967",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "FeatureBench: Benchmarking Agentic Coding for Complex Feature Development",
    "summary": "arXiv:2602.10975v1 Announce Type: cross Abstract: Agents powered by large language models (LLMs) are increasingly adopted in the software industry, contributing code as collaborators or even autonomous developers. As their presence grows, it becomes important to assess the current boundaries of their coding abilities. Existing agentic coding benchm",
    "url": "https://arxiv.org/abs/2602.10975",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "RiemannGL: Riemannian Geometry Changes Graph Deep Learning",
    "summary": "arXiv:2602.10982v1 Announce Type: cross Abstract: Graphs are ubiquitous, and learning on graphs has become a cornerstone in artificial intelligence and data mining communities. Unlike pixel grids in images or sequential structures in language, graphs exhibit a typical non-Euclidean structure with complex interactions among the objects. This paper a",
    "url": "https://arxiv.org/abs/2602.10982",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "LoRA-Squeeze: Simple and Effective Post-Tuning and In-Tuning Compression of LoRA Modules",
    "summary": "arXiv:2602.10993v1 Announce Type: cross Abstract: Despite its huge number of variants, standard Low-Rank Adaptation (LoRA) is still a dominant technique for parameter-efficient fine-tuning (PEFT). Nonetheless, it faces persistent challenges, including the pre-selection of an optimal rank and rank-specific hyper-parameters, as well as the deployment",
    "url": "https://arxiv.org/abs/2602.10993",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Fine-Tuning GPT-5 for GPU Kernel Generation",
    "summary": "arXiv:2602.11000v1 Announce Type: cross Abstract: Developing efficient GPU kernels is essential for scaling modern AI systems, yet it remains a complex task due to intricate hardware architectures and the need for specialized optimization expertise. Although Large Language Models (LLMs) demonstrate strong capabilities in general sequential code gen",
    "url": "https://arxiv.org/abs/2602.11000",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Enhancing Predictability of Multi-Tenant DNN Inference for Autonomous Vehicles' Perception",
    "summary": "arXiv:2602.11004v1 Announce Type: cross Abstract: Autonomous vehicles (AVs) rely on sensors and deep neural networks (DNNs) to perceive their surrounding environment and make maneuver decisions in real time. However, achieving real-time DNN inference in the AV's perception pipeline is challenging due to the large gap between the computation require",
    "url": "https://arxiv.org/abs/2602.11004",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression",
    "summary": "arXiv:2602.11008v1 Announce Type: cross Abstract: We present ROCKET, a training-free model compression method that achieves state-of-the-art performance in comparison with factorization, structured-sparsification and dynamic compression baselines. Operating under a global compression budget, ROCKET comprises two key innovations: First, it formulate",
    "url": "https://arxiv.org/abs/2602.11008",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "CVPL: A Geometric Framework for Post-Hoc Linkage Risk Assessment in Protected Tabular Data",
    "summary": "arXiv:2602.11015v1 Announce Type: cross Abstract: Formal privacy metrics provide compliance-oriented guarantees but often fail to quantify actual linkability in released datasets. We introduce CVPL (Cluster-Vector-Projection Linkage), a geometric framework for post-hoc assessment of linkage risk between original and protected tabular data. CVPL rep",
    "url": "https://arxiv.org/abs/2602.11015",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "From Buffers to Registers: Unlocking Fine-Grained FlashAttention with Hybrid-Bonded 3D NPU Co-Design",
    "summary": "arXiv:2602.11016v1 Announce Type: cross Abstract: Transformer-based models dominate modern AI workloads but exacerbate memory bottlenecks due to their quadratic attention complexity and ever-growing model sizes. Existing accelerators, such as Groq and Cerebras, mitigate off-chip traffic with large on-chip caches, while algorithmic innovations such ",
    "url": "https://arxiv.org/abs/2602.11016",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories",
    "summary": "arXiv:2602.11018v1 Announce Type: cross Abstract: This work addresses the problem of offline safe imitation learning (IL), where the goal is to learn safe and reward-maximizing policies from demonstrations that do not have per-timestep safety cost or reward information. In many real-world domains, online learning in the environment can be risky, an",
    "url": "https://arxiv.org/abs/2602.11018",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "ContactGaussian-WM: Learning Physics-Grounded World Model from Videos",
    "summary": "arXiv:2602.11021v1 Announce Type: cross Abstract: Developing world models that understand complex physical interactions is essential for advancing robotic planning and simulation.However, existing methods often struggle to accurately model the environment under conditions of data scarcity and complex contact-rich dynamic motion.To address these cha",
    "url": "https://arxiv.org/abs/2602.11021",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Chain-of-Look Spatial Reasoning for Dense Surgical Instrument Counting",
    "summary": "arXiv:2602.11024v1 Announce Type: cross Abstract: Accurate counting of surgical instruments in Operating Rooms (OR) is a critical prerequisite for ensuring patient safety during surgery. Despite recent progress of large visual-language models and agentic AI, accurately counting such instruments remains highly challenging, particularly in dense scen",
    "url": "https://arxiv.org/abs/2602.11024",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Linguistic Indicators of Early Cognitive Decline in the DementiaBank Pitt Corpus: A Statistical and Machine Learning Study",
    "summary": "arXiv:2602.11028v1 Announce Type: cross Abstract: Background: Subtle changes in spontaneous language production are among the earliest indicators of cognitive decline. Identifying linguistically interpretable markers of dementia can support transparent and clinically grounded screening approaches. Methods: This study analyzes spontaneous speech tra",
    "url": "https://arxiv.org/abs/2602.11028",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Language Model Inversion through End-to-End Differentiation",
    "summary": "arXiv:2602.11044v1 Announce Type: cross Abstract: Despite emerging research on Language Models (LM), few approaches analyse the invertibility of LMs. That is, given a LM and a desirable target output sequence of tokens, determining what input prompts would yield the target output remains an open problem. We formulate this problem as a classical gra",
    "url": "https://arxiv.org/abs/2602.11044",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "GraphSeek: Next-Generation Graph Analytics with LLMs",
    "summary": "arXiv:2602.11052v1 Announce Type: cross Abstract: Graphs are foundational across domains but remain hard to use without deep expertise. LLMs promise accessible natural language (NL) graph analytics, yet they fail to process industry-scale property graphs effectively and efficiently: such datasets are large, highly heterogeneous, structurally comple",
    "url": "https://arxiv.org/abs/2602.11052",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Conversational Behavior Modeling Foundation Model With Multi-Level Perception",
    "summary": "arXiv:2602.11065v1 Announce Type: cross Abstract: Human conversation is organized by an implicit chain of thoughts that manifests as timed speech acts. Capturing this perceptual pathway is key to building natural full-duplex interactive systems. We introduce a framework that models this process as multi-level perception, and then reasons over conve",
    "url": "https://arxiv.org/abs/2602.11065",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Chatting with Images for Introspective Visual Thinking",
    "summary": "arXiv:2602.11073v1 Announce Type: cross Abstract: Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of ''thinking with images'' attempts to alleviate this limitation by manipulating images via ",
    "url": "https://arxiv.org/abs/2602.11073",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Interpretable Attention-Based Multi-Agent PPO for Latency Spike Resolution in 6G RAN Slicing",
    "summary": "arXiv:2602.11076v1 Announce Type: cross Abstract: Sixth-generation (6G) radio access networks (RANs) must enforce strict service-level agreements (SLAs) for heterogeneous slices, yet sudden latency spikes remain difficult to diagnose and resolve with conventional deep reinforcement learning (DRL) or explainable RL (XRL). We propose \\emph{Attention-",
    "url": "https://arxiv.org/abs/2602.11076",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "In-the-Wild Model Organisms: Mitigating Undesirable Emergent Behaviors in Production LLM Post-Training via Data Attribution",
    "summary": "arXiv:2602.11079v1 Announce Type: cross Abstract: We propose activation-based data attribution, a method that traces behavioral changes in post-trained language models to responsible training datapoints. By computing activation-difference vectors for both test prompts and preference pairs and ranking by cosine similarity, we identify datapoints tha",
    "url": "https://arxiv.org/abs/2602.11079",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "SteuerLLM: Local specialized large language model for German tax law analysis",
    "summary": "arXiv:2602.11081v1 Announce Type: cross Abstract: Large language models (LLMs) demonstrate strong general reasoning and language understanding, yet their performance degrades in domains governed by strict formal rules, precise terminology, and legally binding structure. Tax law exemplifies these challenges, as correct answers require exact statutor",
    "url": "https://arxiv.org/abs/2602.11081",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "GRASP: group-Shapley feature selection for patients",
    "summary": "arXiv:2602.11084v1 Announce Type: cross Abstract: Feature selection remains a major challenge in medical prediction, where existing approaches such as LASSO often lack robustness and interpretability. We introduce GRASP, a novel framework that couples Shapley value driven attribution with group $L_{21}$ regularization to extract compact and non-red",
    "url": "https://arxiv.org/abs/2602.11084",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "General Flexible $f$-divergence for Challenging Offline RL Datasets with Low Stochasticity and Diverse Behavior Policies",
    "summary": "arXiv:2602.11087v1 Announce Type: cross Abstract: Offline RL algorithms aim to improve upon the behavior policy that produces the collected data while constraining the learned policy to be within the support of the dataset. However, practical offline datasets often contain examples with little diversity or limited exploration of the environment, an",
    "url": "https://arxiv.org/abs/2602.11087",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning",
    "summary": "arXiv:2602.11089v1 Announce Type: cross Abstract: In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is a primary driver of model performance. A key lever is the \\emph{data recipe}, which comprises a data processing pipeline to transform raw sources into training corpora. Despite the gr",
    "url": "https://arxiv.org/abs/2602.11089",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Direct Learning of Calibration-Aware Uncertainty for Neural PDE Surrogates",
    "summary": "arXiv:2602.11090v1 Announce Type: cross Abstract: Neural PDE surrogates are often deployed in data-limited or partially observed regimes where downstream decisions depend on calibrated uncertainty in addition to low prediction error. Existing approaches obtain uncertainty through ensemble replication, fixed stochastic noise such as dropout, or post",
    "url": "https://arxiv.org/abs/2602.11090",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away",
    "summary": "arXiv:2602.11096v1 Announce Type: cross Abstract: Reinforcement learning (RL) based post-training for explicit chain-of-thought (e.g., GRPO) improves the reasoning ability of multimodal large-scale reasoning models (MLRMs). But recent evidence shows that it can simultaneously degrade safety alignment and increase jailbreak success rates. We propose",
    "url": "https://arxiv.org/abs/2602.11096",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Learning to Compose for Cross-domain Agentic Workflow Generation",
    "summary": "arXiv:2602.11114v1 Announce Type: cross Abstract: Automatically generating agentic workflows -- executable operator graphs or codes that orchestrate reasoning, verification, and repair -- has become a practical way to solve complex tasks beyond what single-pass LLM generation can reliably handle. Yet what constitutes a good workflow depends heavily",
    "url": "https://arxiv.org/abs/2602.11114",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Weight Decay Improves Language Model Plasticity",
    "summary": "arXiv:2602.11137v1 Announce Type: cross Abstract: The prevailing paradigm in large language model (LLM) development is to pretrain a base model, then perform further training to improve performance and model behavior. However, hyperparameter optimization and scaling laws have been studied primarily from the perspective of the base model's validatio",
    "url": "https://arxiv.org/abs/2602.11137",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Data-Efficient Hierarchical Goal-Conditioned Reinforcement Learning via Normalizing Flows",
    "summary": "arXiv:2602.11142v1 Announce Type: cross Abstract: Hierarchical goal-conditioned reinforcement learning (H-GCRL) provides a powerful framework for tackling complex, long-horizon tasks by decomposing them into structured subgoals. However, its practical adoption is hindered by poor data efficiency and limited policy expressivity, especially in offlin",
    "url": "https://arxiv.org/abs/2602.11142",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "GENIUS: Generative Fluid Intelligence Evaluation Suite",
    "summary": "arXiv:2602.11144v1 Announce Type: cross Abstract: Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess $\\textit{Crystallized Intelligence}$, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks $\\textit{Generative Fluid Intelligence (",
    "url": "https://arxiv.org/abs/2602.11144",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling",
    "summary": "arXiv:2602.11146v1 Announce Type: cross Abstract: Preference optimization for diffusion and flow-matching models relies on reward functions that are both discriminatively robust and computationally efficient. Vision-Language Models (VLMs) have emerged as the primary reward provider, leveraging their rich multimodal priors to guide alignment. Howeve",
    "url": "https://arxiv.org/abs/2602.11146",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Implicit Probabilistic Reasoning Does Not Reflect Explicit Answers in Large Language Models",
    "summary": "arXiv:2406.14986v4 Announce Type: replace Abstract: The handling of probabilities in the form of uncertainty or partial information is an essential task for LLMs in many settings and applications. A common approach to evaluate an LLM's probabilistic reasoning capabilities is to assess its ability to answer questions pertaining to probability throug",
    "url": "https://arxiv.org/abs/2406.14986",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Metareasoning in uncertain environments: a meta-BAMDP framework",
    "summary": "arXiv:2408.01253v3 Announce Type: replace Abstract: \\textit{Reasoning} may be viewed as an algorithm $P$ that makes a choice of an action $a^* \\in \\mathcal{A}$, aiming to optimize some outcome. However, executing $P$ itself bears costs (time, energy, limited capacity, etc.) and needs to be considered alongside explicit utility obtained by making th",
    "url": "https://arxiv.org/abs/2408.01253",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Bridging Explainability and Embeddings: BEE Aware of Spuriousness",
    "summary": "arXiv:2410.18970v5 Announce Type: replace Abstract: Current methods for detecting spurious correlations rely on analyzing dataset statistics or error patterns, leaving many harmful shortcuts invisible when counterexamples are absent. We introduce BEE (Bridging Explainability and Embeddings), a framework that shifts the focus from model predictions ",
    "url": "https://arxiv.org/abs/2410.18970",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Reinforcement Learning in Strategy-Based and Atari Games: A Review of Google DeepMinds Innovations",
    "summary": "arXiv:2502.10303v2 Announce Type: replace Abstract: Reinforcement Learning (RL) has been widely used in many applications, particularly in gaming, which serves as an excellent training ground for AI models. Google DeepMind has pioneered innovations in this field, employing reinforcement learning algorithms, including model-based, model-free, and de",
    "url": "https://arxiv.org/abs/2502.10303",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation",
    "summary": "arXiv:2504.11524v2 Announce Type: replace Abstract: There is growing interest in hypothesis generation with large language models (LLMs). However, fundamental questions remain: what makes a good hypothesis, and how can we systematically evaluate methods for hypothesis generation? To address this, we introduce HypoBench, a novel benchmark designed t",
    "url": "https://arxiv.org/abs/2504.11524",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Is Your LLM Really Mastering the Concept? A Multi-Agent Benchmark",
    "summary": "arXiv:2505.17512v2 Announce Type: replace Abstract: Concepts serve as fundamental abstractions that support human reasoning and categorization. However, it remains unclear whether large language models truly capture such conceptual structures or primarily rely on surface-level pattern memorization. Existing benchmarks are largely static and fact or",
    "url": "https://arxiv.org/abs/2505.17512",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Controllable Logical Hypothesis Generation for Abductive Reasoning in Knowledge Graphs",
    "summary": "arXiv:2505.20948v2 Announce Type: replace Abstract: Abductive reasoning in knowledge graphs aims to generate plausible logical hypotheses from observed entities, with broad applications in areas such as clinical diagnosis and scientific discovery. However, due to a lack of controllability, a single observation may yield numerous plausible but redun",
    "url": "https://arxiv.org/abs/2505.20948",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "PhysUniBench: A Multi-Modal Physics Reasoning Benchmark at Undergraduate Level",
    "summary": "arXiv:2506.17667v4 Announce Type: replace Abstract: Physics problem-solving is a challenging domain for AI models, requiring integration of conceptual understanding, mathematical reasoning, and interpretation of physical diagrams. Existing evaluations fail to capture the full breadth and complexity of undergraduate physics, whereas this level provi",
    "url": "https://arxiv.org/abs/2506.17667",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Scaling Towards the Information Boundary of Instruction Sets: The Infinity Instruct Subject Technical Report",
    "summary": "arXiv:2507.06968v4 Announce Type: replace Abstract: Instruction tuning has become a foundation for unlocking the capabilities of large-scale pretrained models and improving their performance on complex tasks. Thus, the construction of high-quality instruction datasets is crucial for enhancing model performance and generalizability. Although current",
    "url": "https://arxiv.org/abs/2507.06968",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Synthetic Homes: An Accessible Multimodal Pipeline for Producing Residential Building Data with Generative AI",
    "summary": "arXiv:2509.09794v2 Announce Type: replace Abstract: Computational models have emerged as powerful tools for energy modeling research, touting scalability and quantitative results. However, these models require a plethora of data, some of which can be inaccessible, expensive, or can raise privacy concerns. We introduce a modular multimodal framework",
    "url": "https://arxiv.org/abs/2509.09794",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns",
    "summary": "arXiv:2509.21124v3 Announce Type: replace Abstract: Recent progress in large reasoning models for challenging mathematical reasoning has been driven by reinforcement learning (RL). Incorporating long chain-of-thought (CoT) data during mid-training has also been shown to substantially improve reasoning depth. However, current approaches often utiliz",
    "url": "https://arxiv.org/abs/2509.21124",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "AI Driven Discovery of Bio Ecological Mediation in Cascading Heatwave Risks",
    "summary": "arXiv:2509.25112v2 Announce Type: replace Abstract: Compound heatwaves increasingly trigger complex cascading failures that propagate through interconnected physical and human systems, yet the fragmentation of disciplinary knowledge hinders the comprehensive mapping of these systemic risk topologies. This study introduces the Heatwave Discovery Age",
    "url": "https://arxiv.org/abs/2509.25112",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models",
    "summary": "arXiv:2510.01304v3 Announce Type: replace Abstract: Although current large Vision-Language Models (VLMs) have advanced in multimodal understanding and reasoning, their fundamental perceptual and reasoning abilities remain limited. Specifically, even on simple jigsaw tasks, existing VLMs perform near randomly, revealing deficiencies in core percepti",
    "url": "https://arxiv.org/abs/2510.01304",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Measuring What Matters: The AI Pluralism Index",
    "summary": "arXiv:2510.08193v3 Announce Type: replace Abstract: Artificial intelligence systems increasingly mediate knowledge, communication, and decision making. Development and governance remain concentrated within a small set of firms and states, raising concerns that technologies may encode narrow interests and limit public agency. Capability benchmarks f",
    "url": "https://arxiv.org/abs/2510.08193",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Unifying Deductive and Abductive Reasoning in Knowledge Graphs with Masked Diffusion Model",
    "summary": "arXiv:2510.11462v2 Announce Type: replace Abstract: Deductive and abductive reasoning are two critical paradigms for analyzing knowledge graphs, enabling applications from financial query answering to scientific discovery. Deductive reasoning on knowledge graphs usually involves retrieving entities that satisfy a complex logical query, while abduct",
    "url": "https://arxiv.org/abs/2510.11462",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Retrieval- and Argumentation-Enhanced Multi-Agent LLMs for Judgmental Forecasting (Extended Version with Supplementary Material)",
    "summary": "arXiv:2510.24303v3 Announce Type: replace Abstract: Judgmental forecasting is the task of making predictions about future events based on human judgment. This task can be seen as a form of claim verification, where the claim corresponds to a future event and the task is to assess the plausibility of that event. In this paper, we propose a novel mul",
    "url": "https://arxiv.org/abs/2510.24303",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "PreferThinker: Reasoning-based Personalized Image Preference Assessment",
    "summary": "arXiv:2511.00609v3 Announce Type: replace Abstract: Personalized image preference assessment aims to evaluate an individual user's image preferences by relying only on a small set of reference images as prior information. Existing methods mainly focus on general preference assessment, training models with large-scale data to tackle well-defined tas",
    "url": "https://arxiv.org/abs/2511.00609",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "CostNav: A Navigation Benchmark for Real-World Economic-Cost Evaluation of Physical AI Agents",
    "summary": "arXiv:2511.20216v4 Announce Type: replace Abstract: While current navigation benchmarks prioritize task success in simplified settings, they neglect the multidimensional economic constraints essential for the real-world commercialization of autonomous delivery systems. We introduce CostNav, an Economic Navigation Benchmark that evaluates physical A",
    "url": "https://arxiv.org/abs/2511.20216",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "The Specification Trap: Why Content-Based AI Value Alignment Cannot Produce Robust Alignment",
    "summary": "arXiv:2512.03048v2 Announce Type: replace Abstract: I argue that content-based AI value alignment--any approach that treats alignment as optimizing toward a formal value-object (reward function, utility function, constitutional principles, or learned preference representation)--cannot, by itself, produce robust alignment under capability scaling, d",
    "url": "https://arxiv.org/abs/2512.03048",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Beyond Gemini-3-Pro: Revisiting LLM Routing and Aggregation at Scale",
    "summary": "arXiv:2601.01330v2 Announce Type: replace Abstract: Large Language Models (LLMs) have rapidly advanced, with Gemini-3-Pro setting a new performance milestone. In this work, we explore collective intelligence as an alternative to monolithic scaling, and demonstrate that open-source LLMs' collaboration can surpass Gemini-3-Pro. We first revisit LLM r",
    "url": "https://arxiv.org/abs/2601.01330",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Active Evaluation of General Agents: Problem Definition and Comparison of Baseline Algorithms",
    "summary": "arXiv:2601.07651v2 Announce Type: replace Abstract: As intelligent agents become more generally-capable, i.e. able to master a wide variety of tasks, the complexity and cost of properly evaluating them rises significantly. Tasks that assess specific capabilities of the agents can be correlated and stochastic, requiring many samples for accurate com",
    "url": "https://arxiv.org/abs/2601.07651",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Meta Context Engineering via Agentic Skill Evolution",
    "summary": "arXiv:2601.21557v2 Announce Type: replace Abstract: The operational efficacy of large language models relies heavily on their inference-time context. This has established Context Engineering (CE) as a formal discipline for optimizing these inputs. Current CE methods rely on manually crafted harnesses, such as rigid generation-reflection workflows a",
    "url": "https://arxiv.org/abs/2601.21557",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "World of Workflows: A Benchmark for Bringing World Models to Enterprise Systems",
    "summary": "arXiv:2601.22130v2 Announce Type: replace Abstract: Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion si",
    "url": "https://arxiv.org/abs/2601.22130",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "From Abstract to Contextual: What LLMs Still Cannot Do in Mathematics",
    "summary": "arXiv:2601.23048v2 Announce Type: replace Abstract: Large language models now solve many benchmark math problems at near-expert levels, yet this progress has not fully translated into reliable performance in real-world applications. We study this gap through contextual mathematical reasoning, where the mathematical core must be formulated from desc",
    "url": "https://arxiv.org/abs/2601.23048",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility",
    "summary": "arXiv:2602.03402v2 Announce Type: replace Abstract: Vision language models (VLMs) extend the reasoning capabilities of large language models (LLMs) to cross-modal settings, yet remain highly vulnerable to multimodal jailbreak attacks. Existing defenses predominantly rely on safety fine-tuning or aggressive token manipulations, incurring substantial",
    "url": "https://arxiv.org/abs/2602.03402",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Surgery: Mitigating Harmful Fine-Tuning for Large Language Models via Attention Sink",
    "summary": "arXiv:2602.05228v2 Announce Type: replace Abstract: Harmful fine-tuning can invalidate safety alignment of large language models, exposing significant safety risks. In this paper, we utilize the attention sink mechanism to mitigate harmful fine-tuning. Specifically, we first measure a statistic named \\emph{sink divergence} for each attention head a",
    "url": "https://arxiv.org/abs/2602.05228",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "PieArena: Frontier Language Agents Achieve MBA-Level Negotiation Performance and Reveal Novel Behavioral Differences",
    "summary": "arXiv:2602.05302v2 Announce Type: replace Abstract: We present an in-depth evaluation of LLMs' ability to negotiate, a central business task that requires strategic reasoning, theory of mind, and economic value creation. To do so, we introduce PieArena, a large-scale negotiation benchmark grounded in multi-agent interactions over realistic scenario",
    "url": "https://arxiv.org/abs/2602.05302",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Progress Constraints for Reinforcement Learning in Behavior Trees",
    "summary": "arXiv:2602.06525v2 Announce Type: replace Abstract: Behavior Trees (BTs) provide a structured and reactive framework for decision-making, commonly used to switch between sub-controllers based on environmental conditions. Reinforcement Learning (RL), on the other hand, can learn near-optimal controllers but sometimes struggles with sparse rewards, s",
    "url": "https://arxiv.org/abs/2602.06525",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "EventCast: Hybrid Demand Forecasting in E-Commerce with LLM-Based Event Knowledge",
    "summary": "arXiv:2602.07695v2 Announce Type: replace Abstract: Demand forecasting is a cornerstone of e-commerce operations, directly impacting inventory planning and fulfillment scheduling. However, existing forecasting systems often fail during high-impact periods such as flash sales, holiday campaigns, and sudden policy interventions, where demand patterns",
    "url": "https://arxiv.org/abs/2602.07695",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "MePo: Meta Post-Refinement for Rehearsal-Free General Continual Learning",
    "summary": "arXiv:2602.07940v2 Announce Type: replace Abstract: To cope with uncertain changes of the external world, intelligent systems must continually learn from complex, evolving environments and respond in real time. This ability, collectively known as general continual learning (GCL), encapsulates practical challenges such as online datastreams and blur",
    "url": "https://arxiv.org/abs/2602.07940",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent",
    "summary": "arXiv:2602.08412v2 Announce Type: replace Abstract: Although large language model (LLM)-based agents, exemplified by OpenClaw, are increasingly evolving from task-oriented systems into personalized AI assistants for solving complex real-world tasks, their practical deployment also introduces severe security risks. However, existing agent security r",
    "url": "https://arxiv.org/abs/2602.08412",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning",
    "summary": "arXiv:2602.08520v2 Announce Type: replace Abstract: Modern large language models (LLMs) are often evaluated and deployed under a one-shot, greedy inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missi",
    "url": "https://arxiv.org/abs/2602.08520",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Why do we Trust Chatbots? From Normative Principles to Behavioral Drivers",
    "summary": "arXiv:2602.08707v2 Announce Type: replace Abstract: As chatbots increasingly blur the boundary between automated systems and human conversation, the foundations of trust in these systems warrant closer examination. While regulatory and policy frameworks tend to define trust in normative terms, the trust users place in chatbots often emerges from be",
    "url": "https://arxiv.org/abs/2602.08707",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning",
    "summary": "arXiv:2602.08835v2 Announce Type: replace Abstract: Value-aware AI should recognise human values and adapt to the value systems (value-based preferences) of different users. This requires operationalization of values, which can be prone to misspecification. The social nature of values demands their representation to adhere to multiple users while v",
    "url": "https://arxiv.org/abs/2602.08835",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "SpotAgent: Grounding Visual Geo-localization in Large Vision-Language Models through Agentic Reasoning",
    "summary": "arXiv:2602.09463v2 Announce Type: replace Abstract: Large Vision-Language Models (LVLMs) have demonstrated strong reasoning capabilities in geo-localization, yet they often struggle in real-world scenarios where visual cues are sparse, long-tailed, and highly ambiguous. Previous approaches, bound by internal knowledge, often fail to provide verifia",
    "url": "https://arxiv.org/abs/2602.09463",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "ClinAlign: Scaling Healthcare Alignment from Clinician Preference",
    "summary": "arXiv:2602.09653v2 Announce Type: replace Abstract: Although large language models (LLMs) demonstrate expert-level medical knowledge, aligning their open-ended outputs with fine-grained clinician preferences remains challenging. Existing methods often rely on coarse objectives or unreliable automated judges that are weakly grounded in professional ",
    "url": "https://arxiv.org/abs/2602.09653",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs",
    "summary": "arXiv:2602.10085v2 Announce Type: replace Abstract: Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is inf",
    "url": "https://arxiv.org/abs/2602.10085",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning",
    "summary": "arXiv:2602.10090v2 Announce Type: replace Abstract: Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agen",
    "url": "https://arxiv.org/abs/2602.10090",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Structured Sentiment Analysis as Transition-based Dependency Graph Parsing",
    "summary": "arXiv:2305.05311v2 Announce Type: replace-cross Abstract: Structured sentiment analysis (SSA) aims to automatically extract people's opinions from a text in natural language and adequately represent that information in a graph structure. One of the most accurate methods for performing SSA was recently proposed and consists of approaching it as a de",
    "url": "https://arxiv.org/abs/2305.05311",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Games with Payments between Learning Agents",
    "summary": "arXiv:2405.20880v3 Announce Type: replace-cross Abstract: In repeated games, such as auctions, players rely on autonomous learning agents to choose their actions. We study settings in which players have their agents make monetary transfers to other agents during play at their own expense, in order to influence learning dynamics in their favor. Our ",
    "url": "https://arxiv.org/abs/2405.20880",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Tensor learning with orthogonal, Lorentz, and symplectic symmetries",
    "summary": "arXiv:2406.01552v2 Announce Type: replace-cross Abstract: Tensors are a fundamental data structure for many scientific contexts, such as time series analysis, materials science, and physics, among many others. Improving our ability to produce and handle tensors is essential to efficiently address problems in these domains. In this paper, we show ho",
    "url": "https://arxiv.org/abs/2406.01552",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Towards Better Code Understanding in Decoder-Only Models with Contrastive Learning",
    "summary": "arXiv:2406.12326v2 Announce Type: replace-cross Abstract: Recent advances in large-scale code generation models have led to remarkable progress in producing high-quality code. These models are trained in a self-supervised manner on extensive unlabeled code corpora using a decoder-only architecture. However, despite their generative strength, decode",
    "url": "https://arxiv.org/abs/2406.12326",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Proficient Graph Neural Network Design by Accumulating Knowledge on Large Language Models",
    "summary": "arXiv:2408.06717v3 Announce Type: replace-cross Abstract: High-level automation is increasingly critical in AI, driven by rapid advances in large language models (LLMs) and AI agents. However, LLMs, despite their general reasoning power, struggle significantly in specialized, data-sensitive tasks such as designing Graph Neural Networks (GNNs). This",
    "url": "https://arxiv.org/abs/2408.06717",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "ZebraPose: Zebra Detection and Pose Estimation using only Synthetic Data",
    "summary": "arXiv:2408.10831v2 Announce Type: replace-cross Abstract: Collecting and labeling large real-world wild animal datasets is impractical, costly, error-prone, and labor-intensive. For animal monitoring tasks, as detection, tracking, and pose estimation, out-of-distribution viewpoints (e.g. aerial) are also typically needed but rarely found in publicl",
    "url": "https://arxiv.org/abs/2408.10831",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Enhancing Inverse Reinforcement Learning through Encoding Dynamic Information in Reward Shaping",
    "summary": "arXiv:2410.03847v3 Announce Type: replace-cross Abstract: In this paper, we aim to tackle the limitation of the Adversarial Inverse Reinforcement Learning (AIRL) method in stochastic environments where theoretical results cannot hold and performance is degraded. To address this issue, we propose a novel method which infuses the dynamics information",
    "url": "https://arxiv.org/abs/2410.03847",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "When Speculation Spills Secrets: Side Channels via Speculative Decoding In LLMs",
    "summary": "arXiv:2411.01076v4 Announce Type: replace-cross Abstract: Deployed large language models (LLMs) often rely on speculative decoding, a technique that generates and verifies multiple candidate tokens in parallel, to improve throughput and latency. In this work, we reveal a new side-channel whereby input-dependent patterns of correct and incorrect spe",
    "url": "https://arxiv.org/abs/2411.01076",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Symmetrization Weighted Binary Cross-Entropy: Modeling Perceptual Asymmetry for Human-Consistent Neural Edge Detection",
    "summary": "arXiv:2501.13365v4 Announce Type: replace-cross Abstract: Edge detection (ED) is a fundamental perceptual process in computer vision, forming the structural basis for high-level reasoning tasks such as segmentation, recognition, and scene understanding. Despite substantial progress achieved by deep neural networks, most ED models attain high numeri",
    "url": "https://arxiv.org/abs/2501.13365",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Multi-Objective Bayesian Optimization for Networked Black-Box Systems: A Path to Greener Profits and Smarter Designs",
    "summary": "arXiv:2502.14121v2 Announce Type: replace-cross Abstract: Designing modern industrial systems requires balancing several competing objectives, such as profitability, resilience, and sustainability, while accounting for complex interactions between technological, economic, and environmental factors. Multi-objective optimization (MOO) methods are com",
    "url": "https://arxiv.org/abs/2502.14121",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "from Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors",
    "summary": "arXiv:2503.00038v5 Announce Type: replace-cross Abstract: Current studies have exposed the risk of Large Language Models (LLMs) generating harmful content by jailbreak attacks. However, they overlook that the direct generation of harmful content from scratch is more difficult than inducing LLM to calibrate benign content into harmful forms. In our ",
    "url": "https://arxiv.org/abs/2503.00038",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Fixing the Broken Compass: Diagnosing and Improving Inference-Time Reward Modeling",
    "summary": "arXiv:2503.05188v2 Announce Type: replace-cross Abstract: Inference-time scaling techniques have shown promise in enhancing the reasoning capabilities of large language models (LLMs). While recent research has primarily focused on training-time optimization, our work highlights inference-time reward model (RM)-based reasoning as a critical yet over",
    "url": "https://arxiv.org/abs/2503.05188",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "GenDR: Lighten Generative Detail Restoration",
    "summary": "arXiv:2503.06790v3 Announce Type: replace-cross Abstract: Although recent research applying text-to-image (T2I) diffusion models to real-world super-resolution (SR) has achieved remarkable progress, the misalignment of their targets leads to a suboptimal trade-off between inference speed and detail fidelity. Specifically, the T2I task requires mult",
    "url": "https://arxiv.org/abs/2503.06790",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Convergence and Connectivity: Dynamics of Multi-Agent Q-Learning in Random Networks",
    "summary": "arXiv:2503.10186v2 Announce Type: replace-cross Abstract: Beyond specific settings, many multi-agent learning algorithms fail to converge to an equilibrium solution, instead displaying complex, non-stationary behaviours such as recurrent or chaotic orbits. In fact, recent literature suggests that such complex behaviours are likely to occur when the",
    "url": "https://arxiv.org/abs/2503.10186",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "LLM-Mediated Guidance of MARL Systems",
    "summary": "arXiv:2503.13553v2 Announce Type: replace-cross Abstract: In complex multi-agent environments, achieving efficient learning and desirable behaviours is a significant challenge for Multi-Agent Reinforcement Learning (MARL) systems. This work explores the potential of combining MARL with Large Language Model (LLM)-mediated interventions to guide agen",
    "url": "https://arxiv.org/abs/2503.13553",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "MTBench: A Multimodal Time Series Benchmark for Temporal Reasoning and Question Answering",
    "summary": "arXiv:2503.16858v2 Announce Type: replace-cross Abstract: Understanding the relationship between textual news and time-series evolution is a critical yet under-explored challenge in applied data science. While multimodal learning has gained traction, existing multimodal time-series datasets fall short in evaluating cross-modal reasoning and complex",
    "url": "https://arxiv.org/abs/2503.16858",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Efficient IoT Intrusion Detection with an Improved Attention-Based CNN-BiLSTM Architecture",
    "summary": "arXiv:2503.19339v4 Announce Type: replace-cross Abstract: The ever-increasing security vulnerabilities in the Internet-of-Things (IoT) systems require improved threat detection approaches. This paper presents a compact and efficient approach to detect botnet attacks by employing an integrated approach that consists of traffic pattern analysis, temp",
    "url": "https://arxiv.org/abs/2503.19339",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Localized Graph-Based Neural Dynamics Models for Terrain Manipulation",
    "summary": "arXiv:2503.23270v3 Announce Type: replace-cross Abstract: Predictive models can be particularly helpful for robots to effectively manipulate terrains in construction sites and extraterrestrial surfaces. However, terrain state representations become extremely high-dimensional especially to capture fine-resolution details and when depth is unknown or",
    "url": "https://arxiv.org/abs/2503.23270",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Geospatial Representation Learning: A Survey from Deep Learning to The LLM Era",
    "summary": "arXiv:2505.09651v2 Announce Type: replace-cross Abstract: The ability to transform location-centric geospatial data into meaningful computational representations has become fundamental to modern spatial analysis and decision-making. Geospatial Representation Learning (GRL), the process of automatically extracting latent structures and semantic patt",
    "url": "https://arxiv.org/abs/2505.09651",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Spatiotemporal Field Generation Based on Hybrid Mamba-Transformer with Physics-informed Fine-tuning",
    "summary": "arXiv:2505.11578v5 Announce Type: replace-cross Abstract: This research confronts the challenge of substantial physical equation discrepancies encountered in the generation of spatiotemporal physical fields through data-driven trained models. A spatiotemporal physical field generation model, named HMT-PF, is developed based on the hybrid Mamba-Tran",
    "url": "https://arxiv.org/abs/2505.11578",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training",
    "summary": "arXiv:2505.11739v3 Announce Type: replace-cross Abstract: Token-level attention tuning, a class of training-free methods including Post-hoc Attention Steering (PASTA) and Attention Calibration (ACT), has emerged as a promising approach for improving frozen LLMs via interpretable interventions. However, these methods rely on auxiliary heuristics to ",
    "url": "https://arxiv.org/abs/2505.11739",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Intrinsic Self-Correction in LLMs: Towards Explainable Prompting via Mechanistic Interpretability",
    "summary": "arXiv:2505.11924v3 Announce Type: replace-cross Abstract: Intrinsic self-correction refers to the phenomenon where a language model refines its own outputs purely through prompting, without external feedback or parameter updates. While this approach improves performance across diverse tasks, its mechanism remains unclear. We show that intrinsic sel",
    "url": "https://arxiv.org/abs/2505.11924",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation",
    "summary": "arXiv:2505.16415v5 Announce Type: replace-cross Abstract: Retrieval-Augmented Generation (RAG) leverages large language models (LLMs) combined with external contexts to enhance the accuracy and reliability of generated responses. However, reliably attributing generated content to specific context segments, context attribution, remains challenging d",
    "url": "https://arxiv.org/abs/2505.16415",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Unveiling the \"Fairness Seesaw\": Discovering and Mitigating Gender and Race Bias in Vision-Language Models",
    "summary": "arXiv:2505.23798v2 Announce Type: replace-cross Abstract: Although Vision-Language Models (VLMs) have achieved remarkable success, the knowledge mechanisms underlying their social biases remain a black box, where fairness- and ethics-related problems harm certain groups of people in society. It is unknown to what extent VLMs yield gender and race b",
    "url": "https://arxiv.org/abs/2505.23798",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Can LLMs Reason Structurally? Benchmarking via the Lens of Data Structures",
    "summary": "arXiv:2505.24069v3 Announce Type: replace-cross Abstract: Large language models (LLMs) are deployed on increasingly complex tasks that require multi-step decision-making. Understanding their algorithmic reasoning abilities is therefore crucial. However, we lack a diagnostic benchmark for evaluating this capability. We propose data structures as a p",
    "url": "https://arxiv.org/abs/2505.24069",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Cross-Attention Speculative Decoding",
    "summary": "arXiv:2505.24544v4 Announce Type: replace-cross Abstract: Speculative decoding (SD) is a widely adopted approach for accelerating inference in large language models (LLMs), particularly when the draft and target models are well aligned. However, state-of-the-art SD methods typically rely on tightly coupled, self-attention-based Transformer decoders",
    "url": "https://arxiv.org/abs/2505.24544",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Belief-Based Offline Reinforcement Learning for Delay-Robust Policy Optimization",
    "summary": "arXiv:2506.00131v2 Announce Type: replace-cross Abstract: Offline-to-online deployment of reinforcement-learning (RL) agents must bridge two gaps: (1) the sim-to-real gap, where real systems add latency and other imperfections not present in simulation, and (2) the interaction gap, where policies trained purely offline face out-of-distribution stat",
    "url": "https://arxiv.org/abs/2506.00131",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Algorithmically Establishing Trust in Evaluators",
    "summary": "arXiv:2506.03083v4 Announce Type: replace-cross Abstract: An evaluator, such as an LLM-as-a-judge, is trustworthy when there exists some agreed-upon way to measure its performance as a labeller. Traditional approaches either rely on testing the evaluator against references or assume that it `knows' somehow the correct labelling. Both approaches fai",
    "url": "https://arxiv.org/abs/2506.03083",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Uni-DPO: A Unified Paradigm for Dynamic Preference Optimization of LLMs",
    "summary": "arXiv:2506.10054v3 Announce Type: replace-cross Abstract: Direct Preference Optimization (DPO) has emerged as a cornerstone of reinforcement learning from human feedback (RLHF) due to its simplicity and efficiency. However, existing DPO-based methods typically treat all preference pairs equally, overlooking substantial variations in data quality an",
    "url": "https://arxiv.org/abs/2506.10054",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Complexity of normalized stochastic first-order methods with momentum under heavy-tailed noise",
    "summary": "arXiv:2506.11214v2 Announce Type: replace-cross Abstract: In this paper, we propose practical normalized stochastic first-order methods with Polyak momentum, multi-extrapolated momentum, and recursive momentum for solving unconstrained optimization problems. These methods employ dynamically updated algorithmic parameters and do not require explicit",
    "url": "https://arxiv.org/abs/2506.11214",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for Panorama-Style Mobile Captures",
    "summary": "arXiv:2507.06109v2 Announce Type: replace-cross Abstract: We introduce LighthouseGS, a practical novel view synthesis framework based on 3D Gaussian Splatting that utilizes simple panorama-style captures from a single mobile device. While convenient, this rotation-dominant motion and narrow baseline make accurate camera pose and 3D point estimation",
    "url": "https://arxiv.org/abs/2507.06109",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Computers",
    "summary": "arXiv:2507.10775v2 Announce Type: replace-cross Abstract: Spacecraft deployed in outer space are routinely subjected to various forms of damage due to exposure to hazardous environments. In addition, there are significant risks to the subsequent process of in-space repairs through human extravehicular activity or robotic manipulation, incurring sub",
    "url": "https://arxiv.org/abs/2507.10775",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle",
    "summary": "arXiv:2508.05612v4 Announce Type: replace-cross Abstract: Reinforcement learning (RL) has emerged as an effective post-training paradigm for enhancing the reasoning capabilities of multimodal large language model (MLLM). However, current RL pipelines often suffer from training inefficiencies caused by two underexplored issues: Advantage Collapsing,",
    "url": "https://arxiv.org/abs/2508.05612",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models",
    "summary": "arXiv:2508.09210v2 Announce Type: replace-cross Abstract: Recent advances in multimodal large language models (MLLMs) have catalyzed transformative progress in affective computing, enabling models to exhibit emergent emotional intelligence. Despite substantial methodological progress, current emotional benchmarks remain limited, as it is still unkn",
    "url": "https://arxiv.org/abs/2508.09210",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning",
    "summary": "arXiv:2508.20866v4 Announce Type: replace-cross Abstract: The increasing complexity of software systems and the sophistication of cyber-attacks have underscored the need for reliable automated software vulnerability detection. Data-driven approaches using deep learning models show promise but critically depend on the availability of large, accurate",
    "url": "https://arxiv.org/abs/2508.20866",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "AUDETER: A Large-scale Dataset for Deepfake Audio Detection in Open Worlds",
    "summary": "arXiv:2509.04345v2 Announce Type: replace-cross Abstract: Speech synthesis systems can now produce highly realistic vocalisations that pose significant authenticity challenges. Despite substantial progress in deepfake detection models, their real-world effectiveness is often undermined by evolving distribution shifts between training and test data,",
    "url": "https://arxiv.org/abs/2509.04345",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward",
    "summary": "arXiv:2509.07430v3 Announce Type: replace-cross Abstract: A central paradox in fine-tuning Large Language Models (LLMs) with Reinforcement Learning with Verifiable Reward (RLVR) is the frequent degradation of multi-attempt performance (Pass@k) despite improvements in single-attempt accuracy (Pass@1). This is often accompanied by catastrophic forget",
    "url": "https://arxiv.org/abs/2509.07430",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms",
    "summary": "arXiv:2509.09679v3 Announce Type: replace-cross Abstract: Large language models require massive memory footprints, severely limiting deployment on consumer hardware. Quantization reduces memory through lower numerical precision, but extreme 2-bit quantization suffers from catastrophic performance loss due to outliers in activations. Rotation-based ",
    "url": "https://arxiv.org/abs/2509.09679",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision",
    "summary": "arXiv:2509.09893v2 Announce Type: replace-cross Abstract: Imitation learning is a promising paradigm for training robot agents; however, standard approaches typically require substantial data acquisition -- via numerous demonstrations or random exploration -- to ensure reliable performance. Although exploration reduces human effort, it lacks safety",
    "url": "https://arxiv.org/abs/2509.09893",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Is In-Context Learning Learning?",
    "summary": "arXiv:2509.10414v4 Announce Type: replace-cross Abstract: In-context learning (ICL) allows some autoregressive models to solve tasks via next-token prediction and without needing further training. This has led to claims about these model's ability to solve (learn) unseen tasks with only a few shots (exemplars) in the prompt. However, deduction does",
    "url": "https://arxiv.org/abs/2509.10414",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "TableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding",
    "summary": "arXiv:2509.14671v2 Announce Type: replace-cross Abstract: Modeling semantic and structural information from tabular data remains a core challenge for effective table understanding. Existing Table-as-Text approaches flatten tables for large language models (LLMs), but lose crucial structural cues, while Table-as-Image methods preserve structure yet ",
    "url": "https://arxiv.org/abs/2509.14671",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "MaskVCT: Masked Voice Codec Transformer for Zero-Shot Voice Conversion With Increased Controllability via Multiple Guidances",
    "summary": "arXiv:2509.17143v2 Announce Type: replace-cross Abstract: We introduce MaskVCT, a zero-shot voice conversion (VC) model that offers multi-factor controllability through multiple classifier-free guidances (CFGs). While previous VC models rely on a fixed conditioning scheme, MaskVCT integrates diverse conditions in a single model. To further enhance ",
    "url": "https://arxiv.org/abs/2509.17143",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "HuMam: Humanoid Motion Control via End-to-End Deep Reinforcement Learning with Mamba",
    "summary": "arXiv:2509.18046v2 Announce Type: replace-cross Abstract: End-to-end reinforcement learning (RL) for humanoid locomotion is appealing for its compact perception-action mapping, yet practical policies often suffer from training instability, inefficient feature fusion, and high actuation cost. We present HuMam, a state-centric end-to-end RL framework",
    "url": "https://arxiv.org/abs/2509.18046",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Bridging Fairness and Explainability: Can Input-Based Explanations Promote Fairness in Hate Speech Detection?",
    "summary": "arXiv:2509.22291v2 Announce Type: replace-cross Abstract: Natural language processing (NLP) models often replicate or amplify social bias from training data, raising concerns about fairness. At the same time, their black-box nature makes it difficult for users to recognize biased predictions and for developers to effectively mitigate them. While so",
    "url": "https://arxiv.org/abs/2509.22291",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Beyond Aggregation: Guiding Clients in Heterogeneous Federated Learning",
    "summary": "arXiv:2509.23049v2 Announce Type: replace-cross Abstract: Federated learning (FL) is increasingly adopted in domains like healthcare, where data privacy is paramount. A fundamental challenge in these systems is statistical heterogeneity-the fact that data distributions vary significantly across clients (e.g., different hospitals may treat distinct ",
    "url": "https://arxiv.org/abs/2509.23049",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding",
    "summary": "arXiv:2509.23050v3 Announce Type: replace-cross Abstract: Large vision-language models (LVLMs) achieve strong performance on multimodal tasks, yet they often default to their language prior (LP) -- memorized textual patterns from pre-training while under-utilizing visual evidence. Prior analyses of LP mostly rely on input-output probing, which fail",
    "url": "https://arxiv.org/abs/2509.23050",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment",
    "summary": "arXiv:2509.24384v2 Announce Type: replace-cross Abstract: The potential for large language models (LLMs) to generate harmful content poses a significant safety risk in their deployment. To address and assess this risk, the community has developed numerous harmfulness evaluation metrics and judges. However, the lack of a systematic benchmark for eva",
    "url": "https://arxiv.org/abs/2509.24384",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Discrete Variational Autoencoding via Policy Search",
    "summary": "arXiv:2509.24716v3 Announce Type: replace-cross Abstract: Discrete latent bottlenecks in variational autoencoders (VAEs) offer high bit efficiency and can be modeled with autoregressive discrete distributions, enabling parameter-efficient multimodal search with transformers. However, discrete random variables do not allow for exact differentiable p",
    "url": "https://arxiv.org/abs/2509.24716",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "GLASS Flows: Transition Sampling for Alignment of Flow and Diffusion Models",
    "summary": "arXiv:2509.25170v3 Announce Type: replace-cross Abstract: The performance of flow matching and diffusion models can be greatly improved at inference time using reward alignment algorithms, yet efficiency remains a major limitation. While several algorithms were proposed, we demonstrate that a common bottleneck is the sampling method these algorithm",
    "url": "https://arxiv.org/abs/2509.25170",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "VoiceBridge: Designing Latent Bridge Models for General Speech Restoration at Scale",
    "summary": "arXiv:2509.25275v2 Announce Type: replace-cross Abstract: Bridge models have recently been explored for speech enhancement tasks such as denoising, dereverberation, and super-resolution, while these efforts are typically confined to a single task or small-scale datasets, with constrained general speech restoration (GSR) capability at scale. In this",
    "url": "https://arxiv.org/abs/2509.25275",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "ACT: Agentic Classification Tree",
    "summary": "arXiv:2509.26433v3 Announce Type: replace-cross Abstract: When used in high-stakes settings, AI systems are expected to produce decisions that are transparent, interpretable and auditable, a requirement increasingly expected by regulations. Decision trees such as CART provide clear and verifiable rules, but they are restricted to structured tabular",
    "url": "https://arxiv.org/abs/2509.26433",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Data Provenance Auditing of Fine-Tuned Large Language Models with a Text-Preserving Technique",
    "summary": "arXiv:2510.09655v2 Announce Type: replace-cross Abstract: We propose a system for marking sensitive or copyrighted texts to detect their use in fine-tuning large language models under black-box access with statistical guarantees. Our method builds digital ``marks'' using invisible Unicode characters organized into (``cue'', ``reply'') pairs. During",
    "url": "https://arxiv.org/abs/2510.09655",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Learning under Quantization for High-Dimensional Linear Regression",
    "summary": "arXiv:2510.18259v2 Announce Type: replace-cross Abstract: The use of low-bit quantization has emerged as an indispensable technique for enabling the efficient training of large-scale models. Despite its widespread empirical success, a rigorous theoretical understanding of its impact on learning performance remains notably absent, even in the simple",
    "url": "https://arxiv.org/abs/2510.18259",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Context-level Language Modeling by Learning Predictive Context Embeddings",
    "summary": "arXiv:2510.20280v3 Announce Type: replace-cross Abstract: We propose ContextLM, a framework that implicitly learns multi-token prediction by augmenting standard pretraining with an intrinsic next-context prediction objective. ContextLM builds a language model on top of context embeddings that span multiple tokens, enabling better next-token predict",
    "url": "https://arxiv.org/abs/2510.20280",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "RELOOP: Recursive Retrieval with Multi-Hop Reasoner and Planners for Heterogeneous QA",
    "summary": "arXiv:2510.20505v2 Announce Type: replace-cross Abstract: Retrieval-augmented generation (RAG) remains brittle on multi-step questions and heterogeneous evidence sources, trading accuracy against latency and token/tool budgets. This paper introduces RELOOP, a structure aware framework using Hierarchical Sequence (HSEQ) that (i) linearize documents,",
    "url": "https://arxiv.org/abs/2510.20505",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "An Indoor Radio Mapping Dataset Combining 3D Point Clouds and RSSI",
    "summary": "arXiv:2511.00494v3 Announce Type: replace-cross Abstract: The growing number of smart devices supporting bandwidth-intensive and latency-sensitive applications, such as real-time video analytics, smart sensing, Extended Reality (XR), etc., necessitates reliable wireless connectivity in indoor environments. In such environments, accurate design of R",
    "url": "https://arxiv.org/abs/2511.00494",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Designing Beyond Language: Sociotechnical Barriers in AI Health Technologies for Limited English Proficiency",
    "summary": "arXiv:2511.07277v2 Announce Type: replace-cross Abstract: Limited English proficiency (LEP) patients in the U.S. face systemic barriers to healthcare beyond language and interpreter access, encompassing procedural and institutional constraints. AI advances may support communication and care through on-demand translation and visit preparation, but a",
    "url": "https://arxiv.org/abs/2511.07277",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard",
    "summary": "arXiv:2511.10222v3 Announce Type: replace-cross Abstract: Recent progress in LLMs has enabled understanding of audio signals, but has also exposed new safety risks arising from complex audio inputs that are inadequately handled by current safeguards. We introduce SACRED-Bench (Speech-Audio Composition for RED-teaming) to evaluate the robustness of ",
    "url": "https://arxiv.org/abs/2511.10222",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "A Multimodal Manufacturing Safety Chatbot: Knowledge Base Design, Benchmark Development, and Evaluation of Multiple RAG Approaches",
    "summary": "arXiv:2511.11847v2 Announce Type: replace-cross Abstract: Ensuring worker safety remains a critical challenge in modern manufacturing environments. Industry 5.0 reorients the prevailing manufacturing paradigm toward more human-centric operations. Using a design science research methodology, we identify three essential requirements for next-generati",
    "url": "https://arxiv.org/abs/2511.11847",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Finding Kissing Numbers with Game-theoretic Reinforcement Learning",
    "summary": "arXiv:2511.13391v3 Announce Type: replace-cross Abstract: Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem is the local analogue of Hilbert's 18th problem, bridging geometry, number theory, and inf",
    "url": "https://arxiv.org/abs/2511.13391",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "KernelBand: Steering LLM-based Kernel Optimization via Hardware-Aware Multi-Armed Bandits",
    "summary": "arXiv:2511.18868v2 Announce Type: replace-cross Abstract: High-performance GPU kernels are critical for efficient LLM serving, yet their optimization remains a bottleneck requiring deep system expertise. While code LLMs show promise in generating functionally correct code, kernel optimization is intrinsically a search problem over a vast optimizati",
    "url": "https://arxiv.org/abs/2511.18868",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "WaymoQA: A Multi-View Visual Question Answering Dataset for Safety-Critical Reasoning in Autonomous Driving",
    "summary": "arXiv:2511.20022v2 Announce Type: replace-cross Abstract: Recent advancements in multimodal large language models (MLLMs) have shown strong understanding of driving scenes, drawing interest in their application to autonomous driving. However, high-level reasoning in safety-critical scenarios, where avoiding one traffic risk can create another, rema",
    "url": "https://arxiv.org/abs/2511.20022",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Constructing and Benchmarking: a Labeled Email Dataset for Text-Based Phishing and Spam Detection Framework",
    "summary": "arXiv:2511.21448v4 Announce Type: replace-cross Abstract: Phishing and spam emails remain a major cybersecurity threat, with attackers increasingly leveraging Large Language Models (LLMs) to craft highly deceptive content. This study presents a comprehensive email dataset containing phishing, spam, and legitimate messages, explicitly distinguishing",
    "url": "https://arxiv.org/abs/2511.21448",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Orion-Bix: Bi-Axial Attention for Tabular In-Context Learning",
    "summary": "arXiv:2512.00181v2 Announce Type: replace-cross Abstract: Tabular data drive most real-world machine learning applications, yet building general-purpose models for them remains difficult. Mixed numeric and categorical fields, weak feature structure, and limited labeled data make scaling and generalization challenging. To this end, we introduce Orio",
    "url": "https://arxiv.org/abs/2512.00181",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders",
    "summary": "arXiv:2512.08892v2 Announce Type: replace-cross Abstract: Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detectio",
    "url": "https://arxiv.org/abs/2512.08892",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence",
    "summary": "arXiv:2512.12768v2 Announce Type: replace-cross Abstract: Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their exten",
    "url": "https://arxiv.org/abs/2512.12768",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "AOI: Context-Aware Multi-Agent Operations via Dynamic Scheduling and Hierarchical Memory Compression",
    "summary": "arXiv:2512.13956v2 Announce Type: replace-cross Abstract: The proliferation of cloud-native architectures, characterized by microservices and dynamic orchestration, has rendered modern IT infrastructures exceedingly complex and volatile. This complexity generates overwhelming volumes of operational data, leading to critical bottlenecks in conventio",
    "url": "https://arxiv.org/abs/2512.13956",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "SecureCode: A Production-Grade Multi-Turn Dataset for Training Security-Aware Code Generation Models",
    "summary": "arXiv:2512.18542v2 Announce Type: replace-cross Abstract: AI coding assistants produce vulnerable code in 45\\% of security-relevant scenarios~\\cite{veracode2025}, yet no public training dataset teaches both traditional web security and AI/ML-specific defenses in a format suitable for instruction tuning. We present SecureCode, a production-grade dat",
    "url": "https://arxiv.org/abs/2512.18542",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "EvoXplain: When Machine Learning Models Agree on Predictions but Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs",
    "summary": "arXiv:2512.22240v4 Announce Type: replace-cross Abstract: Machine learning models are primarily judged by predictive performance, especially in applied settings. Once a model reaches high accuracy, its explanation is often assumed to be correct and trustworthy. This assumption raises an overlooked question: when two models achieve high accuracy, do",
    "url": "https://arxiv.org/abs/2512.22240",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "SegNSP: Revisiting Next Sentence Prediction for Linear Text Segmentation",
    "summary": "arXiv:2601.03474v2 Announce Type: replace-cross Abstract: Linear text segmentation is a long-standing problem in natural language processing (NLP), focused on dividing continuous text into coherent and semantically meaningful units. Despite its importance, the task remains challenging due to the complexity of defining topic boundaries, the variabil",
    "url": "https://arxiv.org/abs/2601.03474",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Autoregressive Ranking: Bridging the Gap Between Dual and Cross Encoders",
    "summary": "arXiv:2601.05588v4 Announce Type: replace-cross Abstract: The success of Large Language Models (LLMs) has motivated a shift toward generative approaches to retrieval and ranking, aiming to supersede classical Dual Encoders (DEs) and Cross Encoders (CEs). A prominent paradigm is pointwise Autoregressive Ranking (ARR), where an LLM generates document",
    "url": "https://arxiv.org/abs/2601.05588",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "A.X K1 Technical Report",
    "summary": "arXiv:2601.09200v5 Announce Type: replace-cross Abstract: We introduce A.X K1, a 519B-parameter Mixture-of-Experts (MoE) language model trained from scratch. Our design leverages scaling laws to optimize training configurations and vocabulary size under fixed computational budgets. A.X K1 is pre-trained on a corpus of approximately 10T tokens, cura",
    "url": "https://arxiv.org/abs/2601.09200",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets",
    "summary": "arXiv:2601.09605v2 Announce Type: replace-cross Abstract: Vision-based policies for robot manipulation have achieved significant recent success, but are still brittle to distribution shifts such as camera viewpoint variations. Robot demonstration data is scarce and often lacks appropriate variation in camera viewpoints. Simulation offers a way to c",
    "url": "https://arxiv.org/abs/2601.09605",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "StatLLaMA: Multi-Stage training for domain-optimized statistical large language models",
    "summary": "arXiv:2601.09718v2 Announce Type: replace-cross Abstract: This study investigates how to efficiently build a domain-specialized large language model (LLM) for statistics using the lightweight LLaMA-3.2-3B family as the foundation model (FM). We systematically compare three multi-stage training pipelines--starting from a base FM with no instruction-",
    "url": "https://arxiv.org/abs/2601.09718",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Building Production-Ready Probes For Gemini",
    "summary": "arXiv:2601.11516v4 Announce Type: replace-cross Abstract: Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: prob",
    "url": "https://arxiv.org/abs/2601.11516",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "MRAG: Benchmarking Retrieval-Augmented Generation for Bio-medicine",
    "summary": "arXiv:2601.16503v2 Announce Type: replace-cross Abstract: While Retrieval-Augmented Generation (RAG) has been swiftly adopted in scientific and clinical QA systems, a comprehensive evaluation benchmark in the medical domain is lacking. To address this gap, we introduce the Medical Retrieval-Augmented Generation (MRAG) benchmark, covering various ta",
    "url": "https://arxiv.org/abs/2601.16503",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Provably Robust Bayesian Counterfactual Explanations under Model Changes",
    "summary": "arXiv:2601.16659v2 Announce Type: replace-cross Abstract: Counterfactual explanations (CEs) offer interpretable insights into machine learning predictions by answering ``what if?\" questions. However, in real-world settings where models are frequently updated, existing counterfactual explanations can quickly become invalid or unreliable. In this pap",
    "url": "https://arxiv.org/abs/2601.16659",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Adapter Merging Reactivates Latent Reasoning Traces: A Mechanism Analysis",
    "summary": "arXiv:2601.18350v4 Announce Type: replace-cross Abstract: Large language models fine-tuned via a two-stage pipeline (domain adaptation followed by instruction alignment) can exhibit non-trivial interference after adapter merging, including the re-emergence of explicit reasoning traces under strict decoding. We study this phenomenon in medical LLM s",
    "url": "https://arxiv.org/abs/2601.18350",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning",
    "summary": "arXiv:2601.18626v3 Announce Type: replace-cross Abstract: Natural gradients have long been studied in deep reinforcement learning due to their fast convergence properties and covariant weight updates. However, computing natural gradients requires inversion of the Fisher Information Matrix (FIM) at each iteration, which is computationally prohibitiv",
    "url": "https://arxiv.org/abs/2601.18626",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
    "summary": "arXiv:2601.21204v2 Announce Type: replace-cross Abstract: While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Thr",
    "url": "https://arxiv.org/abs/2601.21204",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Industrialized Deception: The Collateral Effects of LLM-Generated Misinformation on Digital Ecosystems",
    "summary": "arXiv:2601.21963v2 Announce Type: replace-cross Abstract: Generative AI and misinformation research has evolved since our 2024 survey. This paper presents an updated perspective, transitioning from literature review to practical countermeasures. We report on changes in the threat landscape, including improved AI-generated content through Large Lang",
    "url": "https://arxiv.org/abs/2601.21963",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Bayesian Interpolating Neural Network (B-INN): a scalable and reliable Bayesian model for large-scale physical systems",
    "summary": "arXiv:2601.22860v2 Announce Type: replace-cross Abstract: Neural networks and machine learning models for uncertainty quantification suffer from limited scalability and poor reliability compared to their deterministic counterparts. In industry-scale active learning settings, where generating a single high-fidelity simulation may require days or wee",
    "url": "https://arxiv.org/abs/2601.22860",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Bias Beyond Borders: Political Ideology Evaluation and Steering in Multilingual LLMs",
    "summary": "arXiv:2601.23001v3 Announce Type: replace-cross Abstract: Large Language Models (LLMs) increasingly shape global discourse, making fairness and ideological neutrality essential for responsible AI deployment. Despite growing attention to political bias in LLMs, prior work largely focuses on high-resource, Western languages or narrow multilingual set",
    "url": "https://arxiv.org/abs/2601.23001",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "LOGOS-CA: A Cellular Automaton Using Natural Language as State and Rule",
    "summary": "arXiv:2602.00036v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs), trained solely on massive text data, have achieved high performance on the Winograd Schema Challenge (WSC), a benchmark proposed to measure commonsense knowledge and reasoning abilities about the real world. This suggests that the language produced by humanity d",
    "url": "https://arxiv.org/abs/2602.00036",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning",
    "summary": "arXiv:2602.00181v2 Announce Type: replace-cross Abstract: Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cu",
    "url": "https://arxiv.org/abs/2602.00181",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "TABES: Trajectory-Aware Backward-on-Entropy Steering for Masked Diffusion Models",
    "summary": "arXiv:2602.00250v2 Announce Type: replace-cross Abstract: Masked Diffusion Models (MDMs) have emerged as a promising non-autoregressive paradigm for generative tasks, offering parallel decoding and bidirectional context utilization. However, current sampling methods rely on simple confidence-based heuristics that ignore the long-term impact of loca",
    "url": "https://arxiv.org/abs/2602.00250",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "A Conditional Companion: Lived Experiences of People with Mental Health Disorders Using LLMs",
    "summary": "arXiv:2602.00402v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) are increasingly used for mental health support, yet little is known about how people with mental health challenges engage with them, how they evaluate their usefulness, and what design opportunities they envision. We conducted 20 semi-structured interviews with ",
    "url": "https://arxiv.org/abs/2602.00402",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "From Junior to Senior: Allocating Agency and Navigating Professional Growth in Agentic AI-Mediated Software Engineering",
    "summary": "arXiv:2602.00496v2 Announce Type: replace-cross Abstract: Juniors enter as AI-natives, seniors adapted mid-career. AI is not just changing how engineers code-it is reshaping who holds agency across work and professional growth. We contribute junior-senior accounts on their usage of agentic AI through a three-phase mixed-methods study: ACTA combined",
    "url": "https://arxiv.org/abs/2602.00496",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Non-Contrastive Vision-Language Learning with Predictive Embedding Alignment",
    "summary": "arXiv:2602.00653v2 Announce Type: replace-cross Abstract: Vision-language models have transformed multimodal representation learning, yet dominant contrastive approaches like CLIP require large batch sizes, careful negative sampling, and extensive hyperparameter tuning. We introduce NOVA, a NOn-contrastive Vision-language Alignment framework based ",
    "url": "https://arxiv.org/abs/2602.00653",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics",
    "summary": "arXiv:2602.02128v2 Announce Type: replace-cross Abstract: Molecular dynamics (MD) simulations remain the gold standard for studying protein dynamics, but their computational cost limits access to biologically relevant timescales. Recent generative models have shown promise in accelerating simulations, yet they struggle with long-horizon generation ",
    "url": "https://arxiv.org/abs/2602.02128",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "FragmentFlow: Scalable Transition State Generation for Large Molecules",
    "summary": "arXiv:2602.02310v2 Announce Type: replace-cross Abstract: Transition states (TSs) are central to understanding and quantitatively predicting chemical reactivity and reaction mechanisms. Although traditional TS generation methods are computationally expensive, recent generative modeling approaches have enabled chemically meaningful TS prediction for",
    "url": "https://arxiv.org/abs/2602.02310",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Learning to Explore with Parameter-Space Noise: A Deep Dive into Parameter-Space Noise for Reinforcement Learning with Verifiable Rewards",
    "summary": "arXiv:2602.02555v2 Announce Type: replace-cross Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) improves LLM reasoning, yet growing evidence indicates an exploration ceiling: it often reweights existing solution traces rather than discovering new strategies, limiting gains under large sampling budgets (e.g., pass-at-256). We address",
    "url": "https://arxiv.org/abs/2602.02555",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Entropy-Guided Dynamic Tokens for Graph-LLM Alignment in Molecular Understanding",
    "summary": "arXiv:2602.02742v2 Announce Type: replace-cross Abstract: Molecular understanding is central to advancing areas such as scientific discovery, yet Large Language Models (LLMs) struggle to understand molecular graphs effectively. Existing graph-LLM bridges often adapt the Q-Former-style connector with fixed-length static tokens, which is originally d",
    "url": "https://arxiv.org/abs/2602.02742",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Live or Lie: Action-Aware Capsule Multiple Instance Learning for Risk Assessment in Live Streaming Platforms",
    "summary": "arXiv:2602.03520v2 Announce Type: replace-cross Abstract: Live streaming has become a cornerstone of today's internet, enabling massive real-time social interactions. However, it faces severe risks arising from sparse, coordinated malicious behaviors among multiple participants, which are often concealed within normal activities and challenging to ",
    "url": "https://arxiv.org/abs/2602.03520",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "PaperX: A Unified Framework for Multimodal Academic Presentation Generation with Scholar DAG",
    "summary": "arXiv:2602.03866v3 Announce Type: replace-cross Abstract: Transforming scientific papers into multimodal presentation content is essential for research dissemination but remains labor intensive. Existing automated solutions typically treat each format as an isolated downstream task, leading to redundant processing and semantic inconsistency. We int",
    "url": "https://arxiv.org/abs/2602.03866",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "OAT: Ordered Action Tokenization",
    "summary": "arXiv:2602.04215v2 Announce Type: replace-cross Abstract: Autoregressive policies offer a compelling foundation for scalable robot learning by enabling discrete abstraction, token-level reasoning, and flexible inference. However, applying autoregressive modeling to continuous robot actions requires an effective action tokenization scheme. Existing ",
    "url": "https://arxiv.org/abs/2602.04215",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Evaluating Kubernetes Performance for GenAI Inference: From Automatic Speech Recognition to LLM Summarization",
    "summary": "arXiv:2602.04900v3 Announce Type: replace-cross Abstract: As Generative AI (GenAI), particularly inference, rapidly emerges as a dominant workload category, the Kubernetes ecosystem is proactively evolving to natively support its unique demands. This industry paper demonstrates how emerging Kubernetes-native projects can be combined to deliver the ",
    "url": "https://arxiv.org/abs/2602.04900",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "From Fragmentation to Integration: Exploring the Design Space of AI Agents for Human-as-the-Unit Privacy Management",
    "summary": "arXiv:2602.05016v2 Announce Type: replace-cross Abstract: Managing one's digital footprint is overwhelming, as it spans multiple platforms and involves countless context-dependent decisions. Recent advances in agentic AI offer ways forward by enabling holistic, contextual privacy-enhancing solutions. Building on this potential, we adopted a ''human",
    "url": "https://arxiv.org/abs/2602.05016",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Variational Speculative Decoding: Rethinking Draft Training from Token Likelihood to Sequence Acceptance",
    "summary": "arXiv:2602.05774v2 Announce Type: replace-cross Abstract: Speculative decoding accelerates inference for (M)LLMs, yet a training-decoding discrepancy persists: while existing methods optimize single greedy trajectories, decoding involves verifying and ranking multiple sampled draft paths. We propose Variational Speculative Decoding (VSD), formulati",
    "url": "https://arxiv.org/abs/2602.05774",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Implementing Grassroots Logic Programs with Multiagent Transition Systems and AI",
    "summary": "arXiv:2602.06934v2 Announce Type: replace-cross Abstract: Grassroots Logic Programs (GLP) is a concurrent logic programming language with variables partitioned into paired \\emph{readers} and \\emph{writers}, conjuring both linear logic and futures/promises: an assignment is produced at most once via the sole occurrence of a writer (promise) and cons",
    "url": "https://arxiv.org/abs/2602.06934",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation",
    "summary": "arXiv:2602.07954v2 Announce Type: replace-cross Abstract: As Large Language Models (LLMs) become increasingly deployed in Polish language applications, the need for efficient and accurate content safety classifiers has become paramount. We present Bielik Guard, a family of compact Polish language safety classifiers comprising two model variants: a ",
    "url": "https://arxiv.org/abs/2602.07954",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "MIND: Benchmarking Memory Consistency and Action Control in World Models",
    "summary": "arXiv:2602.08025v2 Announce Type: replace-cross Abstract: World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIsten",
    "url": "https://arxiv.org/abs/2602.08025",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression",
    "summary": "arXiv:2602.08885v3 Announce Type: replace-cross Abstract: Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We ",
    "url": "https://arxiv.org/abs/2602.08885",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "CIC-Trap4Phish: A Unified Multi-Format Dataset for Phishing and Quishing Attachment Detection",
    "summary": "arXiv:2602.09015v3 Announce Type: replace-cross Abstract: Phishing attacks represents one of the primary attack methods which is used by cyber attackers. In many cases, attackers use deceptive emails along with malicious attachments to trick users into giving away sensitive information or installing malware while compromising entire systems. The fl",
    "url": "https://arxiv.org/abs/2602.09015",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Learning to Remember, Learn, and Forget in Attention-Based Models",
    "summary": "arXiv:2602.09075v2 Announce Type: replace-cross Abstract: In-Context Learning (ICL) in transformers acts as an online associative memory and is believed to underpin their high performance on complex sequence processing tasks. However, in gated linear attention models, this memory has a fixed capacity and is prone to interference, especially for lon",
    "url": "https://arxiv.org/abs/2602.09075",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "SWE-AGI: Benchmarking Specification-Driven Software Construction with MoonBit in the Era of Autonomous Agents",
    "summary": "arXiv:2602.09447v2 Announce Type: replace-cross Abstract: Although large language models (LLMs) have demonstrated impressive coding capabilities, their ability to autonomously build production-scale software from explicit specifications remains an open question. We introduce SWE-AGI, an open-source benchmark for evaluating end-to-end, specification",
    "url": "https://arxiv.org/abs/2602.09447",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies",
    "summary": "arXiv:2602.09514v2 Announce Type: replace-cross Abstract: Long-horizon planning is widely recognized as a core capability of autonomous LLM-based agents; however, current evaluation frameworks suffer from being largely episodic, domain-specific, or insufficiently grounded in persistent economic dynamics. We introduce EcoGym, a generalizable benchma",
    "url": "https://arxiv.org/abs/2602.09514",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "On the Optimal Reasoning Length for RL-Trained Language Models",
    "summary": "arXiv:2602.09591v2 Announce Type: replace-cross Abstract: Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal o",
    "url": "https://arxiv.org/abs/2602.09591",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "A Controlled Study of Double DQN and Dueling DQN Under Cross-Environment Transfer",
    "summary": "arXiv:2602.09810v2 Announce Type: replace-cross Abstract: Transfer learning in deep reinforcement learning is often motivated by improved stability and reduced training cost, but it can also fail under substantial domain shift. This paper presents a controlled empirical study examining how architectural differences between Double Deep Q-Networks (D",
    "url": "https://arxiv.org/abs/2602.09810",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Text summarization via global structure awareness",
    "summary": "arXiv:2602.09821v2 Announce Type: replace-cross Abstract: Text summarization is a fundamental task in natural language processing (NLP), and the information explosion has made long-document processing increasingly demanding, making summarization essential. Existing research mainly focuses on model improvements and sentence-level pruning, but often ",
    "url": "https://arxiv.org/abs/2602.09821",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Monocular Normal Estimation via Shading Sequence Estimation",
    "summary": "arXiv:2602.09929v2 Announce Type: replace-cross Abstract: Monocular normal estimation aims to estimate the normal map from a single RGB image of an object under arbitrary lights. Existing methods rely on deep models to directly predict normal maps. However, they often suffer from 3D misalignment: while the estimated normal maps may appear to have a",
    "url": "https://arxiv.org/abs/2602.09929",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Infusion: Shaping Model Behavior by Editing Training Data via Influence Functions",
    "summary": "arXiv:2602.09987v2 Announce Type: replace-cross Abstract: Influence functions are commonly used to attribute model behavior to training documents. We explore the reverse: crafting training data that induces model behavior. Our framework, Infusion, uses scalable influence-function approximations to compute small perturbations to training documents t",
    "url": "https://arxiv.org/abs/2602.09987",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments",
    "summary": "arXiv:2602.10015v2 Announce Type: replace-cross Abstract: Temporally locating and classifying fine-grained sub-task segments in long, untrimmed videos is crucial to safe human-robot collaboration. Unlike generic activity recognition, collaborative manipulation requires sub-task labels that are directly robot-executable. We present RoboSubtaskNet, a",
    "url": "https://arxiv.org/abs/2602.10015",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  },
  {
    "title": "Fake-HR1: Rethinking Reasoning of Vision Language Model for Synthetic Image Detection",
    "summary": "arXiv:2602.10042v2 Announce Type: replace-cross Abstract: Recent studies have demonstrated that incorporating Chain-of-Thought (CoT) reasoning into the detection process can enhance a model's ability to detect synthetic images. However, excessively lengthy reasoning incurs substantial resource overhead, including token consumption and latency, whic",
    "url": "https://arxiv.org/abs/2602.10042",
    "source": "Arxiv AI",
    "published_at": "2026-02-12T05:00:00+00:00"
  }
]