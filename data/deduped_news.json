[
  {
    "title": "Converge Bio raises $25M, backed by Bessemer and execs from Meta, OpenAI, Wiz",
    "summary": "AI drug discovery startup Converge Bio raised $25 million in a Series A led by Bessemer Venture Partners, with additional backing from executives at Meta, OpenAI, and Wiz.",
    "url": "https://techcrunch.com/2026/01/13/ai-drug-discovery-startup-converge-bio-pulls-in-25m-from-bessemer-and-execs-from-meta-openai-and-wiz/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Meta bought 1 GW of solar this week",
    "summary": "The social media company inked three deals in the U.S. to power its data centers and offset its carbon footprint.",
    "url": "https://techcrunch.com/2025/10/31/meta-bought-1-gw-of-solar-this-week/",
    "source": "TechCrunch AI"
  },
  {
    "title": "How one AI startup is helping rice farmers battle climate change",
    "summary": "Mitti Labs is working with The Nature Conservancy to expand the use of climate-friendly rice farming practices in India. The startup uses its AI to verify reductions in methane emissions.",
    "url": "https://techcrunch.com/2025/08/26/how-one-ai-startup-is-helping-rice-farmers-battle-climate-change/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Harvard dropouts to launch ‘always on’ AI smart glasses that listen and record every conversation",
    "summary": "After developing a facial-recognition app for Meta’s Ray-Ban glasses and doxing random people, two former Harvard students are now launching a startup that makes smart glasses with an always-on microphone.",
    "url": "https://techcrunch.com/2025/08/20/harvard-dropouts-to-launch-always-on-ai-smart-glasses-that-listen-and-record-every-conversation/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Meta to add 100MW of solar power from US gear",
    "summary": "The social media company is adding another tranche of solar to power a new AI data center in South Carolina.",
    "url": "https://techcrunch.com/2025/08/20/meta-to-add-100-mw-of-solar-power-from-u-s-gear/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Perplexity accused of scraping websites that explicitly blocked AI scraping",
    "summary": "Internet giant Cloudflare says it detected Perplexity crawling and scraping websites, even after customers had added technical blocks telling Perplexity not to scrape their pages.",
    "url": "https://techcrunch.com/2025/08/04/perplexity-accused-of-scraping-websites-that-explicitly-blocked-ai-scraping/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Obvio’s stop sign cameras use AI to root out unsafe drivers",
    "summary": "American streets are incredibly dangerous for pedestrians. A San Carlos, California-based startup called Obvio thinks it can change that by installing cameras at stop signs -- a solution the founders also say won’t create a panopticon.",
    "url": "https://techcrunch.com/2025/06/04/obvios-stop-sign-cameras-use-ai-to-root-out-unsafe-drivers/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Breakneck data center growth challenges Microsoft’s sustainability goals",
    "summary": "Microsoft's sustainability goals are imperiled by its push into AI and cloud services.",
    "url": "https://techcrunch.com/2025/06/02/breakneck-data-center-growth-challenges-microsofts-sustainability-goals/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Gridcare thinks more than 100 GW of data center capacity is hiding in the grid",
    "summary": "Gridcare raised $13.3 million for its data platform that finds underutilized capacity on the electrical grid.",
    "url": "https://techcrunch.com/2025/05/27/gridcare-thinks-more-than-100-gw-of-data-center-capacity-is-hiding-in-the-grid/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Meta adds another 650 MW of solar power to its AI push",
    "summary": "The company already has more than 12 gigawatts of capacity in its renewable power portfolio.",
    "url": "https://techcrunch.com/2025/05/22/meta-adds-another-650-mw-of-solar-power-to-its-ai-push/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Who are climate-conscious consumers? Not who you’d expect, says Northwind Climate",
    "summary": "Rather than divide people into demographic buckets, Northwind Climate analyzes survey responses for behavioral clues.",
    "url": "https://techcrunch.com/2025/04/01/who-are-climate-conscious-consumers-not-who-youd-expect-says-northwind-climate/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Data centers love solar: Here’s a comprehensive guide to deals over 100 megawatts",
    "summary": "New and expanded data centers are expected to double the sector’s power demand by 2029 as tech companies rush to capitalize on AI.",
    "url": "https://techcrunch.com/2025/03/30/data-centers-love-solar-heres-a-comprehensive-guide-to-deals-over-100-megawatts/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Nvidia thinks AI can solve electrical grid problems caused by AI",
    "summary": "The Open Power AI Consortium says it will use domain-specific AI models to tackle problems in the power industry.",
    "url": "https://techcrunch.com/2025/03/20/nvidia-thinks-ai-can-solve-electrical-grid-problems-caused-by-ai/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Solar notches another win as Microsoft adds 475 MW to power its AI data centers",
    "summary": "The company recently signed a deal with energy provider AES for three solar projects across the Midwest.",
    "url": "https://techcrunch.com/2025/03/20/solar-notches-another-win-as-microsoft-adds-475-mw-to-power-its-ai-data-centers/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Geothermal could power nearly all new data centers through 2030",
    "summary": "Geothermal resources have enormous potential to provide the sort of consistent power that data centers crave.",
    "url": "https://techcrunch.com/2025/03/11/geothermal-could-power-nearly-all-new-data-centers-through-2030/",
    "source": "TechCrunch AI"
  },
  {
    "title": "ElevenLabs now lets authors create and publish audiobooks on its own platform",
    "summary": "Voice AI company ElevenLabs is now letting authors publish AI-generated audiobooks on its own Reader app, TechCrunch has learned and the company confirmed. The announcement comes days after the company partnered with Spotify for AI-narrated audiobooks. ElevenLabs, which raised a $180 million mega-round last month, started inviting authors to try ou",
    "url": "https://techcrunch.com/2025/02/25/elevenlabs-is-now-letting-authors-create-and-publish-audiobooks-on-its-own-platform/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Data center tweaks could unlock 76 GW of new power capacity in the US",
    "summary": "A new study argues that data centers could be ideal demand-response participants because they have the potential to be flexible.",
    "url": "https://techcrunch.com/2025/02/13/data-center-tweaks-could-unlock-76-gw-of-new-power-capacity-in-the-u-s/",
    "source": "TechCrunch AI"
  },
  {
    "title": "YouTube AI updates include auto dubbing expansion, age ID tech, and more",
    "summary": "In his annual letter, YouTube CEO Neal Mohan dubbed AI one of the company&#8217;s four &#8220;big bets&#8221; for 2025. The executive pointed to the company&#8217;s investments in AI tools for creators, including ones for video ideas, thumbnails, and language translation. The latter feature will roll out to all creators in YouTube&#8217;s Partner P",
    "url": "https://techcrunch.com/2025/02/11/youtube-ai-updates-to-include-expansion-of-auto-dubbing-age-identifying-tech-and-more/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Self Inspection raises $3M for its AI-powered vehicle inspections",
    "summary": "A number of startups are racing to make vehicle inspections faster, easier, and cheaper. Self Inspection, a startup based in San Diego, thinks it has them all beat with its AI-powered service &#8212; and now it has convinced outside investors. Self Inspection, founded in 2021, is set to announce Thursday it&#8217;s raised $3 million in [&#8230;]",
    "url": "https://techcrunch.com/2025/02/07/self-inspection-raises-3m-for-its-ai-powered-vehicle-inspections/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Meta turns to solar — again — in its data center-building boom",
    "summary": "The announcement comes as Meta CEO Mark Zuckerberg maintains the company’s ambitious AI strategy, which will require hefty capital investments in data centers.",
    "url": "https://techcrunch.com/2025/01/31/meta-turns-to-solar-again-in-its-data-center-building-boom/",
    "source": "TechCrunch AI"
  },
  {
    "title": "Inside the marketplace powering bespoke AI deepfakes of real women",
    "summary": "Civitai—an online marketplace for buying and selling AI-generated content, backed by the venture capital firm Andreessen Horowitz—is letting users buy custom instruction files for generating celebrity deepfakes. Some of these files were specifically designed to make pornographic images banned by the site, a new analysis has found. The study, from r",
    "url": "https://www.technologyreview.com/2026/01/30/1131945/inside-the-marketplace-powering-bespoke-ai-deepfakes-of-real-women/",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "The AI Hype Index: Grok makes porn, and Claude Code nails your job",
    "summary": "Everyone is panicking because AI is very bad; everyone is panicking because AI is very good. It’s just that you never know which one you’re going to get. Grok is a pornography machine. Claude Code can do anything from building websites to reading your MRI. So of course Gen Z is spooked by what this&#8230;",
    "url": "https://www.technologyreview.com/2026/01/29/1131787/the-ai-hype-index-grok-makes-porn-claude-code-nails-your-job/",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "DHS is using Google and Adobe AI to make videos",
    "summary": "The US Department of Homeland Security is using AI video generators from Google and Adobe to make and edit content shared with the public, a new document reveals. It comes as immigration agencies have flooded social media with content to support President Trump&#8217;s mass deportation agenda—some of which appears to be made with AI—and as&#8230;",
    "url": "https://www.technologyreview.com/2026/01/29/1131938/dhs-is-using-google-and-adobe-ai-to-make-videos/",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "What AI “remembers” about you is privacy’s next frontier",
    "summary": "The ability to remember you and your preferences is rapidly becoming a big selling point for AI chatbots and agents.&#160; Earlier this month, Google announced Personal Intelligence, a new way for people to interact with the company’s Gemini chatbot that draws on their Gmail, photos, search, and YouTube histories to make Gemini “more personal, proa",
    "url": "https://www.technologyreview.com/2026/01/28/1131835/what-ai-remembers-about-you-is-privacys-next-frontier/",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "Rules fail at the prompt, succeed at the boundary",
    "summary": "From the Gemini Calendar prompt-injection attack of 2026 to the September 2025 state-sponsored hack using Anthropic’s Claude code as an automated intrusion engine, the coercion of human-in-the-loop agentic actions and fully autonomous agentic workflows are the new attack vector for hackers. In the Anthropic case, roughly 30 organizations across tec",
    "url": "https://www.technologyreview.com/2026/01/28/1131003/rules-fail-at-the-prompt-succeed-at-the-boundary/",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "OpenAI’s latest product lets you vibe code science",
    "summary": "OpenAI just revealed what its new in-house team, OpenAI for Science, has been up to. The firm has released a free LLM-powered tool for scientists called Prism, which embeds ChatGPT in a text editor for writing scientific papers. The idea is to put ChatGPT front and center inside software that scientists use to write up&#8230;",
    "url": "https://www.technologyreview.com/2026/01/27/1131793/openais-latest-product-lets-you-vibe-code-science/",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "Inside OpenAI’s big play for science",
    "summary": "In the three years since ChatGPT’s explosive debut, OpenAI’s technology has upended a remarkable range of everyday activities at home, at work, in schools—anywhere people have a browser open or a phone out, which is everywhere. Now OpenAI is making an explicit play for scientists. In October, the firm announced that it had launched a&#8230;",
    "url": "https://www.technologyreview.com/2026/01/26/1131728/inside-openais-big-play-for-science/",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "Why chatbots are starting to check your age",
    "summary": "This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here. How do tech companies check if their users are kids? This question has taken on new urgency recently thanks to growing concern about the dangers that can arise when children talk to&#8230;",
    "url": "https://www.technologyreview.com/2026/01/26/1131726/why-chatbots-are-starting-to-check-your-age/",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "America’s coming war over AI regulation",
    "summary": "MIT Technology Review’s What’s Next series looks across industries, trends, and technologies to give you a first look at the future. You can read the rest of them here. In the final weeks of 2025, the battle over regulating artificial intelligence in the US reached a boiling point. On December 11, after Congress failed twice&#8230;",
    "url": "https://www.technologyreview.com/2026/01/23/1131559/americas-coming-war-over-ai-regulation/",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "“Dr. Google” had its issues. Can ChatGPT Health do better?",
    "summary": "For the past two decades, there’s been a clear first step for anyone who starts experiencing new medical symptoms: Look them up online. The practice was so common that it gained the pejorative moniker “Dr. Google.” But times are changing, and many medical-information seekers are now using LLMs. According to OpenAI, 230 million people ask&#8230;",
    "url": "https://www.technologyreview.com/2026/01/22/1131692/dr-google-had-its-issues-can-chatgpt-health-do-better/",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "Jeffrey Epstein Had a ‘Personal Hacker,’ Informant Claims",
    "summary": "Plus: AI agent OpenClaw gives cybersecurity experts the willies, China executes 11 scam compound bosses, a $40 million crypto theft has an unexpected alleged culprit, and more.",
    "url": "https://www.wired.com/story/security-news-this-week-jeffrey-epstein-had-a-personal-hacker-informant-claims/",
    "source": "Wired AI"
  },
  {
    "title": "I Let Google’s ‘Auto Browse’ AI Agent Take Over Chrome. It Didn’t Quite Click",
    "summary": "Auto Browse can shop for clothes, plan a trip, and buy tickets for you. Or at least, that’s the idea.",
    "url": "https://www.wired.com/story/google-chrome-auto-browse-hands-on/",
    "source": "Wired AI"
  },
  {
    "title": "‘Uncanny Valley’: Minneapolis Misinformation, TikTok’s New Owners, and Moltbot Hype",
    "summary": "On this episode of Uncanny Valley, we dive into the news that's held our attention this week: ICE activity as it's been unfolding in Minnesota.",
    "url": "https://www.wired.com/story/uncanny-valley-podcast-ice-minneapolis-tiktok-moltbot/",
    "source": "Wired AI"
  },
  {
    "title": "AI-Generated Anti-ICE Videos Are Getting the Fanfic Treatment",
    "summary": "Across Instagram and Facebook, AI-generated videos show people of color putting ICE agents in their place. Are they cathartic or just adding to a stew of misinformation?",
    "url": "https://www.wired.com/story/anti-ice-videos-are-getting-the-ai-fanfic-treatment-online/",
    "source": "Wired AI"
  },
  {
    "title": "A Yann LeCun–Linked Startup Charts a New Path to AGI",
    "summary": "As the world’s largest companies pour hundreds of billions of dollars into large language models, San Francisco-based Logical Intelligence is trying something different in pursuit of AI that can mimic the human brain.",
    "url": "https://www.wired.com/story/logical-intelligence-yann-lecun-startup-chart-new-course-agi/",
    "source": "Wired AI"
  },
  {
    "title": "An AI Toy Exposed 50,000 Logs of Its Chats With Kids to Anyone With a Gmail Account",
    "summary": "AI chat toy company Bondu left its web console almost entirely unprotected. Researchers who accessed it found nearly all the conversations children had with the company’s stuffed animals.",
    "url": "https://www.wired.com/story/an-ai-toy-exposed-50000-logs-of-its-chats-with-kids-to-anyone-with-a-gmail-account/",
    "source": "Wired AI"
  },
  {
    "title": "Data Centers Are Driving a US Gas Boom",
    "summary": "Gas projects in the US pipeline explicitly linked to data centers increased by almost 25 times over the past two years, according to new research from Global Energy Monitor.",
    "url": "https://www.wired.com/story/data-centers-are-driving-a-us-gas-boom/",
    "source": "Wired AI"
  },
  {
    "title": "ICE Is Using Palantir’s AI Tools to Sort Through Tips",
    "summary": "ICE has been using an AI-powered Palantir system to summarize tips sent to its tip line since last spring, according to a newly released Homeland Security document.",
    "url": "https://www.wired.com/story/ice-is-using-palantirs-ai-tools-to-sort-through-tips/",
    "source": "Wired AI"
  },
  {
    "title": "The Doomsday Clock Is Now 85 Seconds to Midnight. Here’s What That Means",
    "summary": "Catastrophic risks are increasing, cooperation is declining, and swift action is needed from global leaders to correct course.",
    "url": "https://www.wired.com/story/the-doomsday-clock-is-now-85-seconds-to-midnight-heres-what-that-means/",
    "source": "Wired AI"
  },
  {
    "title": "Moltbot Is Taking Over Silicon Valley",
    "summary": "People are letting the viral AI assistant formerly known as Clawdbot run their lives, regardless of the privacy concerns.",
    "url": "https://www.wired.com/story/clawdbot-moltbot-viral-ai-assistant/",
    "source": "Wired AI"
  },
  {
    "title": "JAF: Judge Agent Forest",
    "summary": "arXiv:2601.22269v1 Announce Type: new Abstract: Judge agents are fundamental to agentic AI frameworks: they provide automated evaluation, and enable iterative self-refinement of reasoning processes. We introduce JAF: Judge Agent Forest, a framework in which the judge agent conducts joint inference across a cohort of query--response pairs generated ",
    "url": "https://arxiv.org/abs/2601.22269",
    "source": "Arxiv AI"
  },
  {
    "title": "The Six Sigma Agent: Achieving Enterprise-Grade Reliability in LLM Systems Through Consensus-Driven Decomposed Execution",
    "summary": "arXiv:2601.22290v1 Announce Type: new Abstract: Large Language Models demonstrate remarkable capabilities yet remain fundamentally probabilistic, presenting critical reliability challenges for enterprise deployment. We introduce the Six Sigma Agent, a novel architecture that achieves enterprise-grade reliability through three synergistic components",
    "url": "https://arxiv.org/abs/2601.22290",
    "source": "Arxiv AI"
  },
  {
    "title": "Why Reasoning Fails to Plan: A Planning-Centric Analysis of Long-Horizon Decision Making in LLM Agents",
    "summary": "arXiv:2601.22311v1 Announce Type: new Abstract: Large language model (LLM)-based agents exhibit strong step-by-step reasoning capabilities over short horizons, yet often fail to sustain coherent behavior over long planning horizons. We argue that this failure reflects a fundamental mismatch: step-wise reasoning induces a form of step-wise greedy po",
    "url": "https://arxiv.org/abs/2601.22311",
    "source": "Arxiv AI"
  },
  {
    "title": "Sparks of Rationality: Do Reasoning LLMs Align with Human Judgment and Choice?",
    "summary": "arXiv:2601.22329v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly positioned as decision engines for hiring, healthcare, and economic judgment, yet real-world human judgment reflects a balance between rational deliberation and emotion-driven bias. If LLMs are to participate in high-stakes decisions or serve as models of ",
    "url": "https://arxiv.org/abs/2601.22329",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning Provably Correct Distributed Protocols Without Human Knowledge",
    "summary": "arXiv:2601.22369v1 Announce Type: new Abstract: Provably correct distributed protocols, which are a critical component of modern distributed systems, are highly challenging to design and have often required decades of human effort. These protocols allow multiple agents to coordinate to come to a common agreement in an environment with uncertainty a",
    "url": "https://arxiv.org/abs/2601.22369",
    "source": "Arxiv AI"
  },
  {
    "title": "Semi-Autonomous Mathematics Discovery with Gemini: A Case Study on the Erd\\H{o}s Problems",
    "summary": "arXiv:2601.22401v1 Announce Type: new Abstract: We present a case study in semi-autonomous mathematics discovery, using Gemini to systematically evaluate 700 conjectures labeled 'Open' in Bloom's Erd\\H{o}s Problems database. We employ a hybrid methodology: AI-driven natural language verification to narrow the search space, followed by human expert ",
    "url": "https://arxiv.org/abs/2601.22401",
    "source": "Arxiv AI"
  },
  {
    "title": "AI-Enabled Waste Classification as a Data-Driven Decision Support Tool for Circular Economy and Urban Sustainability",
    "summary": "arXiv:2601.22418v1 Announce Type: new Abstract: Efficient waste sorting is crucial for enabling circular-economy practices and resource recovery in smart cities. This paper evaluates both traditional machine-learning (Random Forest, SVM, AdaBoost) and deep-learning techniques including custom CNNs, VGG16, ResNet50, and three transfer-learning model",
    "url": "https://arxiv.org/abs/2601.22418",
    "source": "Arxiv AI"
  },
  {
    "title": "When LLM meets Fuzzy-TOPSIS for Personnel Selection through Automated Profile Analysis",
    "summary": "arXiv:2601.22433v1 Announce Type: new Abstract: In this highly competitive employment environment, the selection of suitable personnel is essential for organizational success. This study presents an automated personnel selection system that utilizes sophisticated natural language processing (NLP) methods to assess and rank software engineering appl",
    "url": "https://arxiv.org/abs/2601.22433",
    "source": "Arxiv AI"
  },
  {
    "title": "Anytime Safe PAC Efficient Reasoning",
    "summary": "arXiv:2601.22446v1 Announce Type: new Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks but suffer from high computational costs and latency. While selective thinking strategies improve efficiency by routing easy queries to non-thinking models, existing approaches often incur uncontrollable errors, es",
    "url": "https://arxiv.org/abs/2601.22446",
    "source": "Arxiv AI"
  },
  {
    "title": "Controllable Information Production",
    "summary": "arXiv:2601.22449v1 Announce Type: new Abstract: Intrinsic Motivation (IM) is a paradigm for generating intelligent behavior without external utilities. The existing information-theoretic methods for IM are predominantly based on information transmission, which explicitly depends on the designer's choice of which random variables engage in transmiss",
    "url": "https://arxiv.org/abs/2601.22449",
    "source": "Arxiv AI"
  },
  {
    "title": "Why Self-Rewarding Works: Theoretical Guarantees for Iterative Alignment of Language Models",
    "summary": "arXiv:2601.22513v1 Announce Type: new Abstract: Self-Rewarding Language Models (SRLMs) achieve notable success in iteratively improving alignment without external feedback. Yet, despite their striking empirical progress, the core mechanisms driving their capabilities remain unelucidated, leaving a critical gap in theoretical understanding. This pap",
    "url": "https://arxiv.org/abs/2601.22513",
    "source": "Arxiv AI"
  },
  {
    "title": "Darwinian Memory: A Training-Free Self-Regulating Memory System for GUI Agent Evolution",
    "summary": "arXiv:2601.22528v1 Announce Type: new Abstract: Multimodal Large Language Model (MLLM) agents facilitate Graphical User Interface (GUI) automation but struggle with long-horizon, cross-application tasks due to limited context windows. While memory systems provide a viable solution, existing paradigms struggle to adapt to dynamic GUI environments, s",
    "url": "https://arxiv.org/abs/2601.22528",
    "source": "Arxiv AI"
  },
  {
    "title": "Enhancing TableQA through Verifiable Reasoning Trace Reward",
    "summary": "arXiv:2601.22530v1 Announce Type: new Abstract: A major challenge in training TableQA agents, compared to standard text- and image-based agents, is that answers cannot be inferred from a static input but must be reasoned through stepwise transformations of the table state, introducing multi-step reasoning complexity and environmental interaction. T",
    "url": "https://arxiv.org/abs/2601.22530",
    "source": "Arxiv AI"
  },
  {
    "title": "Decoding in Geometry: Alleviating Embedding-Space Crowding for Complex Reasoning",
    "summary": "arXiv:2601.22536v1 Announce Type: new Abstract: Sampling-based decoding underlies complex reasoning in large language models (LLMs), where decoding strategies critically shape model behavior. Temperature- and truncation-based methods reshape the next-token distribution through global probability reweighting or thresholding to balance the quality-di",
    "url": "https://arxiv.org/abs/2601.22536",
    "source": "Arxiv AI"
  },
  {
    "title": "PerfGuard: A Performance-Aware Agent for Visual Content Generation",
    "summary": "arXiv:2601.22571v1 Announce Type: new Abstract: The advancement of Large Language Model (LLM)-powered agents has enabled automated task processing through reasoning and tool invocation capabilities. However, existing frameworks often operate under the idealized assumption that tool executions are invariably successful, relying solely on textual des",
    "url": "https://arxiv.org/abs/2601.22571",
    "source": "Arxiv AI"
  },
  {
    "title": "WED-Net: A Weather-Effect Disentanglement Network with Causal Augmentation for Urban Flow Prediction",
    "summary": "arXiv:2601.22586v1 Announce Type: new Abstract: Urban spatio-temporal prediction under extreme conditions (e.g., heavy rain) is challenging due to event rarity and dynamics. Existing data-driven approaches that incorporate weather as auxiliary input often rely on coarse-grained descriptors and lack dedicated mechanisms to capture fine-grained spati",
    "url": "https://arxiv.org/abs/2601.22586",
    "source": "Arxiv AI"
  },
  {
    "title": "Learn More with Less: Uncertainty Consistency Guided Query Selection for RLVR",
    "summary": "arXiv:2601.22595v1 Announce Type: new Abstract: Large Language Models (LLMs) have recently improved mathematical reasoning through Reinforcement Learning with Verifiable Reward (RLVR). However, existing RLVR algorithms require large query budgets, making annotation costly. We investigate whether fewer but more informative queries can yield similar ",
    "url": "https://arxiv.org/abs/2601.22595",
    "source": "Arxiv AI"
  },
  {
    "title": "From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents",
    "summary": "arXiv:2601.22607v1 Announce Type: new Abstract: Interactive tool-using agents must solve real-world tasks via multi-turn interaction with both humans and external environments, requiring dialogue state tracking, multi-step tool execution, while following complex instructions. Post-training such agents is challenging because synthesis for high-quali",
    "url": "https://arxiv.org/abs/2601.22607",
    "source": "Arxiv AI"
  },
  {
    "title": "EntroCut: Entropy-Guided Adaptive Truncation for Efficient Chain-of-Thought Reasoning in Small-scale Large Reasoning Models",
    "summary": "arXiv:2601.22617v1 Announce Type: new Abstract: Large Reasoning Models (LRMs) excel at complex reasoning tasks through extended chain-of-thought generation, but their reliance on lengthy intermediate steps incurs substantial computational cost. We find that the entropy of the model's output distribution in early reasoning steps reliably distinguish",
    "url": "https://arxiv.org/abs/2601.22617",
    "source": "Arxiv AI"
  },
  {
    "title": "SYMPHONY: Synergistic Multi-agent Planning with Heterogeneous Language Model Assembly",
    "summary": "arXiv:2601.22623v1 Announce Type: new Abstract: Recent advancements have increasingly focused on leveraging large language models (LLMs) to construct autonomous agents for complex problem-solving tasks. However, existing approaches predominantly employ a single-agent framework to generate search branches and estimate rewards during Monte Carlo Tree",
    "url": "https://arxiv.org/abs/2601.22623",
    "source": "Arxiv AI"
  },
  {
    "title": "Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling",
    "summary": "arXiv:2601.22636v1 Announce Type: new Abstract: Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting, which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While recent w",
    "url": "https://arxiv.org/abs/2601.22636",
    "source": "Arxiv AI"
  },
  {
    "title": "Beyond Medical Chatbots: Meddollina and the Rise of Continuous Clinical Intelligence",
    "summary": "arXiv:2601.22645v1 Announce Type: new Abstract: Generative medical AI now appears fluent and knowledgeable enough to resemble clinical intelligence, encouraging the belief that scaling will make it safe. But clinical reasoning is not text generation. It is a responsibility-bound process under ambiguity, incomplete evidence, and longitudinal context",
    "url": "https://arxiv.org/abs/2601.22645",
    "source": "Arxiv AI"
  },
  {
    "title": "Test-Time Mixture of World Models for Embodied Agents in Dynamic Environments",
    "summary": "arXiv:2601.22647v1 Announce Type: new Abstract: Language model (LM)-based embodied agents are increasingly deployed in real-world settings. Yet, their adaptability remains limited in dynamic environments, where constructing accurate and flexible world models is crucial for effective reasoning and decision-making. To address this challenge, we exten",
    "url": "https://arxiv.org/abs/2601.22647",
    "source": "Arxiv AI"
  },
  {
    "title": "UCPO: Uncertainty-Aware Policy Optimization",
    "summary": "arXiv:2601.22648v1 Announce Type: new Abstract: The key to building trustworthy Large Language Models (LLMs) lies in endowing them with inherent uncertainty expression capabilities to mitigate the hallucinations that restrict their high-stakes applications. However, existing RL paradigms such as GRPO often suffer from Advantage Bias due to binary d",
    "url": "https://arxiv.org/abs/2601.22648",
    "source": "Arxiv AI"
  },
  {
    "title": "Task-Aware LLM Council with Adaptive Decision Pathways for Decision Support",
    "summary": "arXiv:2601.22662v1 Announce Type: new Abstract: Large language models (LLMs) have shown strong capabilities across diverse decision-making tasks. However, existing approaches often overlook the specialization differences among available models, treating all LLMs as uniformly applicable regardless of task characteristics. This limits their ability t",
    "url": "https://arxiv.org/abs/2601.22662",
    "source": "Arxiv AI"
  },
  {
    "title": "Real-Time Aligned Reward Model beyond Semantics",
    "summary": "arXiv:2601.22664v1 Announce Type: new Abstract: Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully captur",
    "url": "https://arxiv.org/abs/2601.22664",
    "source": "Arxiv AI"
  },
  {
    "title": "Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference",
    "summary": "arXiv:2601.22701v1 Announce Type: new Abstract: Vision-Language Models (VLMs) have become powerful backbones for agents to autonomously operate in digital environments like the web and operating systems. However, these models suffer from inadaptability to fast-changing environments like the web, which can be alleviated by fine-tuning requiring expa",
    "url": "https://arxiv.org/abs/2601.22701",
    "source": "Arxiv AI"
  },
  {
    "title": "A Step Back: Prefix Importance Ratio Stabilizes Policy Optimization",
    "summary": "arXiv:2601.22718v1 Announce Type: new Abstract: Reinforcement learning (RL) post-training has increasingly demonstrated strong ability to elicit reasoning behaviors in large language models (LLMs). For training efficiency, rollouts are typically generated in an off-policy manner using an older sampling policy and then used to update the current tar",
    "url": "https://arxiv.org/abs/2601.22718",
    "source": "Arxiv AI"
  },
  {
    "title": "AutoRefine: From Trajectories to Reusable Expertise for Continual LLM Agent Refinement",
    "summary": "arXiv:2601.22758v1 Announce Type: new Abstract: Large language model agents often fail to accumulate knowledge from experience, treating each task as an independent challenge. Recent methods extract experience as flattened textual knowledge, which cannot capture procedural logic of complex subtasks. They also lack maintenance mechanisms, causing re",
    "url": "https://arxiv.org/abs/2601.22758",
    "source": "Arxiv AI"
  },
  {
    "title": "TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy Optimization",
    "summary": "arXiv:2601.22776v1 Announce Type: new Abstract: Multi-turn tool-integrated reasoning enables Large Language Models (LLMs) to solve complex tasks through iterative information retrieval. However, current reinforcement learning (RL) frameworks for search-augmented reasoning predominantly rely on sparse outcome-level rewards, leading to a \"Double Homo",
    "url": "https://arxiv.org/abs/2601.22776",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning with Challenges: Adaptive Difficulty-Aware Data Generation for Mobile GUI Agent Training",
    "summary": "arXiv:2601.22781v1 Announce Type: new Abstract: Large-scale, high-quality interaction trajectories are essential for advancing mobile Graphical User Interface (GUI) agents. While existing methods typically rely on labor-intensive human demonstrations or automated model exploration to generate GUI trajectories, they lack fine-grained control over ta",
    "url": "https://arxiv.org/abs/2601.22781",
    "source": "Arxiv AI"
  },
  {
    "title": "Toward IIT-Inspired Consciousness in LLMs: A Reward-Based Learning Framework",
    "summary": "arXiv:2601.22786v1 Announce Type: new Abstract: The pursuit of Artificial General Intelligence (AGI) is a central goal in language model development, in which consciousness-like processing could serve as a key facilitator. While current language models are not conscious, they exhibit behaviors analogous to certain aspects of consciousness. This pap",
    "url": "https://arxiv.org/abs/2601.22786",
    "source": "Arxiv AI"
  },
  {
    "title": "Conditional Performance Guarantee for Large Reasoning Models",
    "summary": "arXiv:2601.22790v1 Announce Type: new Abstract: Large reasoning models have shown strong performance through extended chain-of-thought reasoning, yet their computational cost remains significant. Probably approximately correct (PAC) reasoning provides statistical guarantees for efficient reasoning by adaptively switching between thinking and non-th",
    "url": "https://arxiv.org/abs/2601.22790",
    "source": "Arxiv AI"
  },
  {
    "title": "CVeDRL: An Efficient Code Verifier via Difficulty-aware Reinforcement Learning",
    "summary": "arXiv:2601.22803v1 Announce Type: new Abstract: Code verifiers play a critical role in post-verification for LLM-based code generation, yet existing supervised fine-tuning methods suffer from data scarcity, high failure rates, and poor inference efficiency. While reinforcement learning (RL) offers a promising alternative by optimizing models throug",
    "url": "https://arxiv.org/abs/2601.22803",
    "source": "Arxiv AI"
  },
  {
    "title": "Aligning the Unseen in Attributed Graphs: Interplay between Graph Geometry and Node Attributes Manifold",
    "summary": "arXiv:2601.22806v1 Announce Type: new Abstract: The standard approach to representation learning on attributed graphs -- i.e., simultaneously reconstructing node attributes and graph structure -- is geometrically flawed, as it merges two potentially incompatible metric spaces. This forces a destructive alignment that erodes information about the gr",
    "url": "https://arxiv.org/abs/2601.22806",
    "source": "Arxiv AI"
  },
  {
    "title": "Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery",
    "summary": "arXiv:2601.22896v1 Announce Type: new Abstract: Large language models (LLMs) have enabled rapid progress in automatic heuristic discovery (AHD), yet most existing methods are predominantly limited by static evaluation against fixed instance distributions, leading to potential overfitting and poor generalization under distributional shifts. We propo",
    "url": "https://arxiv.org/abs/2601.22896",
    "source": "Arxiv AI"
  },
  {
    "title": "MulFeRL: Enhancing Reinforcement Learning with Verbal Feedback in a Multi-turn Loop",
    "summary": "arXiv:2601.22900v1 Announce Type: new Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is widely used to improve reasoning in multiple domains, yet outcome-only scalar rewards are often sparse and uninformative, especially on failed samples, where they merely indicate failure and provide no insight into why the reasoning fails. In th",
    "url": "https://arxiv.org/abs/2601.22900",
    "source": "Arxiv AI"
  },
  {
    "title": "Alignment among Language, Vision and Action Representations",
    "summary": "arXiv:2601.22948v1 Announce Type: new Abstract: A fundamental question in cognitive science and AI concerns whether different learning modalities: language, vision, and action, give rise to distinct or shared internal representations. Traditional views assume that models trained on different data types develop specialized, non-transferable represen",
    "url": "https://arxiv.org/abs/2601.22948",
    "source": "Arxiv AI"
  },
  {
    "title": "EvoClinician: A Self-Evolving Agent for Multi-Turn Medical Diagnosis via Test-Time Evolutionary Learning",
    "summary": "arXiv:2601.22964v1 Announce Type: new Abstract: Prevailing medical AI operates on an unrealistic ''one-shot'' model, diagnosing from a complete patient file. However, real-world diagnosis is an iterative inquiry where Clinicians sequentially ask questions and order tests to strategically gather information while managing cost and time. To address t",
    "url": "https://arxiv.org/abs/2601.22964",
    "source": "Arxiv AI"
  },
  {
    "title": "Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text",
    "summary": "arXiv:2601.22975v1 Announce Type: new Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a cornerstone for unlocking complex reasoning in Large Language Models (LLMs). Yet, scaling up RL is bottlenecked by limited existing verifiable data, where improvements increasingly saturate over prolonged training. To overcome this, we",
    "url": "https://arxiv.org/abs/2601.22975",
    "source": "Arxiv AI"
  },
  {
    "title": "Quantifying Model Uniqueness in Heterogeneous AI Ecosystems",
    "summary": "arXiv:2601.22977v1 Announce Type: new Abstract: As AI systems evolve from isolated predictors into complex, heterogeneous ecosystems of foundation models and specialized adapters, distinguishing genuine behavioral novelty from functional redundancy becomes a critical governance challenge. Here, we introduce a statistical framework for auditing mode",
    "url": "https://arxiv.org/abs/2601.22977",
    "source": "Arxiv AI"
  },
  {
    "title": "Why Your Deep Research Agent Fails? On Hallucination Evaluation in Full Research Trajectory",
    "summary": "arXiv:2601.22984v1 Announce Type: new Abstract: Diagnosing the failure mechanisms of Deep Research Agents (DRAs) remains a critical challenge. Existing benchmarks predominantly rely on end-to-end evaluation, obscuring critical intermediate hallucinations, such as flawed planning, that accumulate throughout the research trajectory. To bridge this ga",
    "url": "https://arxiv.org/abs/2601.22984",
    "source": "Arxiv AI"
  },
  {
    "title": "TriCEGAR: A Trace-Driven Abstraction Mechanism for Agentic AI",
    "summary": "arXiv:2601.22997v1 Announce Type: new Abstract: Agentic AI systems act through tools and evolve their behavior over long, stochastic interaction traces. This setting complicates assurance, because behavior depends on nondeterministic environments and probabilistic model outputs. Prior work introduced runtime verification for agentic AI via Dynamic ",
    "url": "https://arxiv.org/abs/2601.22997",
    "source": "Arxiv AI"
  },
  {
    "title": "Guided by Trajectories: Repairing and Rewarding Tool-Use Trajectories for Tool-Integrated Reasoning",
    "summary": "arXiv:2601.23032v1 Announce Type: new Abstract: Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to solve complex tasks by interacting with external tools, yet existing approaches depend on high-quality synthesized trajectories selected by scoring functions and sparse outcome-based rewards, providing limited and biased supervisi",
    "url": "https://arxiv.org/abs/2601.23032",
    "source": "Arxiv AI"
  },
  {
    "title": "The Hot Mess of AI: How Does Misalignment Scale With Model Intelligence and Task Complexity?",
    "summary": "arXiv:2601.23045v1 Announce Type: new Abstract: As AI becomes more capable, we entrust it with more general and consequential tasks. The risks from failure grow more severe with increasing task scope. It is therefore important to understand how extremely capable AI models will fail: Will they fail by systematically pursuing goals we do not intend? ",
    "url": "https://arxiv.org/abs/2601.23045",
    "source": "Arxiv AI"
  },
  {
    "title": "From Abstract to Contextual: What LLMs Still Cannot Do in Mathematics",
    "summary": "arXiv:2601.23048v1 Announce Type: new Abstract: Large language models now solve many benchmark math problems at near-expert levels, yet this progress has not fully translated into reliable performance in real-world applications. We study this gap through contextual mathematical reasoning, where the mathematical core must be formulated from descript",
    "url": "https://arxiv.org/abs/2601.23048",
    "source": "Arxiv AI"
  },
  {
    "title": "MedMCP-Calc: Benchmarking LLMs for Realistic Medical Calculator Scenarios via MCP Integration",
    "summary": "arXiv:2601.23049v1 Announce Type: new Abstract: Medical calculators are fundamental to quantitative, evidence-based clinical practice. However, their real-world use is an adaptive, multi-stage process, requiring proactive EHR data acquisition, scenario-dependent calculator selection, and multi-step computation, whereas current benchmarks focus only",
    "url": "https://arxiv.org/abs/2601.23049",
    "source": "Arxiv AI"
  },
  {
    "title": "Chain-of-thought obfuscation learned from output supervision can generalise to unseen tasks",
    "summary": "arXiv:2601.23086v1 Announce Type: new Abstract: Chain-of-thought (CoT) reasoning provides a significant performance uplift to LLMs by enabling planning, exploration, and deliberation of their actions. CoT is also a powerful tool for monitoring the behaviours of these agents: when faithful, they offer interpretations of the model's decision making p",
    "url": "https://arxiv.org/abs/2601.23086",
    "source": "Arxiv AI"
  },
  {
    "title": "RAudit: A Blind Auditing Protocol for Large Language Model Reasoning",
    "summary": "arXiv:2601.23133v1 Announce Type: new Abstract: Inference-time scaling can amplify reasoning pathologies: sycophancy, rung collapse, and premature certainty. We present RAudit, a diagnostic protocol for auditing LLM reasoning without ground truth access. The key constraint is blindness: the auditor evaluates only whether derivation steps support co",
    "url": "https://arxiv.org/abs/2601.23133",
    "source": "Arxiv AI"
  },
  {
    "title": "THINKSAFE: Self-Generated Safety Alignment for Reasoning Models",
    "summary": "arXiv:2601.23143v1 Announce Type: new Abstract: Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safet",
    "url": "https://arxiv.org/abs/2601.23143",
    "source": "Arxiv AI"
  },
  {
    "title": "Make Anything Match Your Target: Universal Adversarial Perturbations against Closed-Source MLLMs via Multi-Crop Routed Meta Optimization",
    "summary": "arXiv:2601.23179v1 Announce Type: new Abstract: Targeted adversarial attacks on closed-source multimodal large language models (MLLMs) have been increasingly explored under black-box transfer, yet prior methods are predominantly sample-specific and offer limited reusability across inputs. We instead study a more stringent setting, Universal Targete",
    "url": "https://arxiv.org/abs/2601.23179",
    "source": "Arxiv AI"
  },
  {
    "title": "TSAQA: Time Series Analysis Question And Answering Benchmark",
    "summary": "arXiv:2601.23204v1 Announce Type: new Abstract: Time series data are integral to critical applications across domains such as finance, healthcare, transportation, and environmental science. While recent work has begun to explore multi-task time series question answering (QA), current benchmarks remain limited to forecasting and anomaly detection ta",
    "url": "https://arxiv.org/abs/2601.23204",
    "source": "Arxiv AI"
  },
  {
    "title": "High-quality generation of dynamic game content via small language models: A proof of concept",
    "summary": "arXiv:2601.23206v1 Announce Type: new Abstract: Large language models (LLMs) offer promise for dynamic game content generation, but they face critical barriers, including narrative incoherence and high operational costs. Due to their large size, they are often accessed in the cloud, limiting their application in offline games. Many of these practic",
    "url": "https://arxiv.org/abs/2601.23206",
    "source": "Arxiv AI"
  },
  {
    "title": "Scaling Multiagent Systems with Process Rewards",
    "summary": "arXiv:2601.23228v1 Announce Type: new Abstract: While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent",
    "url": "https://arxiv.org/abs/2601.23228",
    "source": "Arxiv AI"
  },
  {
    "title": "Strongly Polynomial Time Complexity of Policy Iteration for $L_\\infty$ Robust MDPs",
    "summary": "arXiv:2601.23229v1 Announce Type: new Abstract: Markov decision processes (MDPs) are a fundamental model in sequential decision making. Robust MDPs (RMDPs) extend this framework by allowing uncertainty in transition probabilities and optimizing against the worst-case realization of that uncertainty. In particular, $(s, a)$-rectangular RMDPs with $L",
    "url": "https://arxiv.org/abs/2601.23229",
    "source": "Arxiv AI"
  },
  {
    "title": "Smart Routing with Precise Link Estimation: DSEE-Based Anypath Routing for Reliable Wireless Networking",
    "summary": "arXiv:2405.10377v1 Announce Type: cross Abstract: In dynamic and resource-constrained environments, such as multi-hop wireless mesh networks, traditional routing protocols often falter by relying on predetermined paths that prove ineffective in unpredictable link conditions. Shortest Anypath routing offers a solution by adapting routing decisions b",
    "url": "https://arxiv.org/abs/2405.10377",
    "source": "Arxiv AI"
  },
  {
    "title": "Screen, Match, and Cache: A Training-Free Causality-Consistent Reference Frame Framework for Human Animation",
    "summary": "arXiv:2601.22160v1 Announce Type: cross Abstract: Human animation aims to generate temporally coherent and visually consistent videos over long sequences, yet modeling long-range dependencies while preserving frame quality remains challenging. Inspired by the human ability to leverage past observations for interpreting ongoing actions, we propose F",
    "url": "https://arxiv.org/abs/2601.22160",
    "source": "Arxiv AI"
  },
  {
    "title": "UniFinEval: Towards Unified Evaluation of Financial Multimodal Models across Text, Images and Videos",
    "summary": "arXiv:2601.22162v1 Announce Type: cross Abstract: Multimodal large language models are playing an increasingly significant role in empowering the financial domain, however, the challenges they face, such as multimodal and high-density information and cross-modal multi-hop reasoning, go beyond the evaluation scope of existing multimodal benchmarks. ",
    "url": "https://arxiv.org/abs/2601.22162",
    "source": "Arxiv AI"
  },
  {
    "title": "Stablecoin Design with Adversarial-Robust Multi-Agent Systems via Trust-Weighted Signal Aggregation",
    "summary": "arXiv:2601.22168v1 Announce Type: cross Abstract: Algorithmic stablecoins promise decentralized monetary stability by maintaining a target peg through programmatic reserve management. Yet, their reserve controllers remain vulnerable to regime-blind optimization, calibrating risk parameters on fair-weather data while ignoring tail events that precip",
    "url": "https://arxiv.org/abs/2601.22168",
    "source": "Arxiv AI"
  },
  {
    "title": "In Vino Veritas and Vulnerabilities: Examining LLM Safety via Drunk Language Inducement",
    "summary": "arXiv:2601.22169v1 Announce Type: cross Abstract: Humans are susceptible to undesirable behaviours and privacy leaks under the influence of alcohol. This paper investigates drunk language, i.e., text written under the influence of alcohol, as a driver for safety failures in large language models (LLMs). We investigate three mechanisms for inducing ",
    "url": "https://arxiv.org/abs/2601.22169",
    "source": "Arxiv AI"
  },
  {
    "title": "ShellForge: Adversarial Co-Evolution of Webshell Generation and Multi-View Detection for Robust Webshell Defense",
    "summary": "arXiv:2601.22182v1 Announce Type: cross Abstract: Webshells remain a primary foothold for attackers to compromise servers, particularly within PHP ecosystems. However, existing detection mechanisms often struggle to keep pace with rapid variant evolution and sophisticated obfuscation techniques that camouflage malicious intent. Furthermore, many cu",
    "url": "https://arxiv.org/abs/2601.22182",
    "source": "Arxiv AI"
  },
  {
    "title": "COL-Trees: Efficient Hierarchical Object Search in Road Networks",
    "summary": "arXiv:2601.22183v1 Announce Type: cross Abstract: Location-based services rely heavily on efficient methods that search for relevant points-of-interest (POIs) near a given location. A k Nearest Neighbor (kNN) query is one such example that finds the k closest POIs from an agent's location. While most existing techniques focus on retrieving nearby P",
    "url": "https://arxiv.org/abs/2601.22183",
    "source": "Arxiv AI"
  },
  {
    "title": "Practical Evaluation of Quantum Kernel Methods for Radar Micro-Doppler Classification on Noisy Intermediate-Scale Quantum (NISQ) Hardware",
    "summary": "arXiv:2601.22194v1 Announce Type: cross Abstract: This paper examines the application of a Quantum Support Vector Machine (QSVM) for radarbased aerial target classification using micro-Doppler signatures. Classical features are extracted and reduced via Principal Component Analysis (PCA) to enable efficient quantum encoding. The reduced feature vec",
    "url": "https://arxiv.org/abs/2601.22194",
    "source": "Arxiv AI"
  },
  {
    "title": "Multitask Learning for Earth Observation Data Classification with Hybrid Quantum Network",
    "summary": "arXiv:2601.22195v1 Announce Type: cross Abstract: Quantum machine learning (QML) has gained increasing attention as a potential solution to address the challenges of computation requirements in the future. Earth observation (EO) has entered the era of Big Data, and the computational demands for effectively analyzing large EO data with complex deep ",
    "url": "https://arxiv.org/abs/2601.22195",
    "source": "Arxiv AI"
  },
  {
    "title": "Neural Signals Generate Clinical Notes in the Wild",
    "summary": "arXiv:2601.22197v1 Announce Type: cross Abstract: Generating clinical reports that summarize abnormal patterns, diagnostic findings, and clinical interpretations from long-term EEG recordings remains labor-intensive. We curate a large-scale clinical EEG dataset with $9{,}922$ reports paired with approximately $11{,}000$ hours of EEG recordings from",
    "url": "https://arxiv.org/abs/2601.22197",
    "source": "Arxiv AI"
  },
  {
    "title": "Advanced techniques and applications of LiDAR Place Recognition in Agricultural Environments: A Comprehensive Survey",
    "summary": "arXiv:2601.22198v1 Announce Type: cross Abstract: An optimal solution to the localization problem is essential for developing autonomous robotic systems. Apart from autonomous vehicles, precision agriculture is one of the elds that can bene t most from these systems. Although LiDAR place recognition is a widely used technique in recent years to ach",
    "url": "https://arxiv.org/abs/2601.22198",
    "source": "Arxiv AI"
  },
  {
    "title": "Beyond Conditional Computation: Retrieval-Augmented Genomic Foundation Models with Gengram",
    "summary": "arXiv:2601.22203v1 Announce Type: cross Abstract: Current genomic foundation models (GFMs) rely on extensive neural computation to implicitly approximate conserved biological motifs from single-nucleotide inputs. We propose Gengram, a conditional memory module that introduces an explicit and highly efficient lookup primitive for multi-base motifs v",
    "url": "https://arxiv.org/abs/2601.22203",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning to Recommend Multi-Agent Subgraphs from Calling Trees",
    "summary": "arXiv:2601.22209v1 Announce Type: cross Abstract: Multi-agent systems (MAS) increasingly solve complex tasks by orchestrating agents and tools selected from rapidly growing marketplaces. As these marketplaces expand, many candidates become functionally overlapping, making selection not just a retrieval problem: beyond filtering relevant agents, an ",
    "url": "https://arxiv.org/abs/2601.22209",
    "source": "Arxiv AI"
  },
  {
    "title": "Lost in Space? Vision-Language Models Struggle with Relative Camera Pose Estimation",
    "summary": "arXiv:2601.22228v1 Announce Type: cross Abstract: Vision-Language Models (VLMs) perform well in 2D perception and semantic reasoning compared to their limited understanding of 3D spatial structure. We investigate this gap using relative camera pose estimation (RCPE), a fundamental vision task that requires inferring relative camera translation and ",
    "url": "https://arxiv.org/abs/2601.22228",
    "source": "Arxiv AI"
  },
  {
    "title": "A Systematic Literature Review on LLM Defenses Against Prompt Injection and Jailbreaking: Expanding NIST Taxonomy",
    "summary": "arXiv:2601.22240v1 Announce Type: cross Abstract: The rapid advancement and widespread adoption of generative artificial intelligence (GenAI) and large language models (LLMs) has been accompanied by the emergence of new security vulnerabilities and challenges, such as jailbreaking and other prompt injection attacks. These maliciously crafted inputs",
    "url": "https://arxiv.org/abs/2601.22240",
    "source": "Arxiv AI"
  },
  {
    "title": "MirrorMark: A Distortion-Free Multi-Bit Watermark for Large Language Models",
    "summary": "arXiv:2601.22246v1 Announce Type: cross Abstract: As large language models (LLMs) become integral to applications such as question answering and content creation, reliable content attribution has become increasingly important. Watermarking is a promising approach, but existing methods either provide only binary signals or distort the sampling distr",
    "url": "https://arxiv.org/abs/2601.22246",
    "source": "Arxiv AI"
  },
  {
    "title": "AI Narrative Breakdown. A Critical Assessment of Power and Promise",
    "summary": "arXiv:2601.22255v1 Announce Type: cross Abstract: This article sets off for an exploration of the still evolving discourse surrounding artificial intelligence (AI) in the wake of the release of ChatGPT. It scrutinizes the pervasive narratives that are shaping the societal engagement with AI, spotlighting key themes such as agency and decision-makin",
    "url": "https://arxiv.org/abs/2601.22255",
    "source": "Arxiv AI"
  },
  {
    "title": "Predicting Intermittent Job Failure Categories for Diagnosis Using Few-Shot Fine-Tuned Language Models",
    "summary": "arXiv:2601.22264v1 Announce Type: cross Abstract: In principle, Continuous Integration (CI) pipeline failures provide valuable feedback to developers on code-related errors. In practice, however, pipeline jobs often fail intermittently due to non-deterministic tests, network outages, infrastructure failures, resource exhaustion, and other reliabili",
    "url": "https://arxiv.org/abs/2601.22264",
    "source": "Arxiv AI"
  },
  {
    "title": "VMonarch: Efficient Video Diffusion Transformers with Structured Attention",
    "summary": "arXiv:2601.22275v1 Announce Type: cross Abstract: The quadratic complexity of the attention mechanism severely limits the context scalability of Video Diffusion Transformers (DiTs). We find that the highly sparse spatio-temporal attention patterns exhibited in Video DiTs can be naturally represented by the Monarch matrix. It is a class of structure",
    "url": "https://arxiv.org/abs/2601.22275",
    "source": "Arxiv AI"
  },
  {
    "title": "PersonaCite: VoC-Grounded Interviewable Agentic Synthetic AI Personas for Verifiable User and Design Research",
    "summary": "arXiv:2601.22288v1 Announce Type: cross Abstract: LLM-based and agent-based synthetic personas are increasingly used in design and product decision-making, yet prior work shows that prompt-based personas often produce persuasive but unverifiable responses that obscure their evidentiary basis. We present PersonaCite, an agentic system that reframes ",
    "url": "https://arxiv.org/abs/2601.22288",
    "source": "Arxiv AI"
  },
  {
    "title": "ParalESN: Enabling parallel information processing in Reservoir Computing",
    "summary": "arXiv:2601.22296v1 Announce Type: cross Abstract: Reservoir Computing (RC) has established itself as an efficient paradigm for temporal processing. However, its scalability remains severely constrained by (i) the necessity of processing temporal data sequentially and (ii) the prohibitive memory footprint of high-dimensional reservoirs. In this work",
    "url": "https://arxiv.org/abs/2601.22296",
    "source": "Arxiv AI"
  },
  {
    "title": "Conformal Prediction for Generative Models via Adaptive Cluster-Based Density Estimation",
    "summary": "arXiv:2601.22298v1 Announce Type: cross Abstract: Conditional generative models map input variables to complex, high-dimensional distributions, enabling realistic sample generation in a diverse set of domains. A critical challenge with these models is the absence of calibrated uncertainty, which undermines trust in individual outputs for high-stake",
    "url": "https://arxiv.org/abs/2601.22298",
    "source": "Arxiv AI"
  },
  {
    "title": "Stealthy Poisoning Attacks Bypass Defenses in Regression Settings",
    "summary": "arXiv:2601.22308v1 Announce Type: cross Abstract: Regression models are widely used in industrial processes, engineering and in natural and physical sciences, yet their robustness to poisoning has received less attention. When it has, studies often assume unrealistic threat models and are thus less useful in practice. In this paper, we propose a no",
    "url": "https://arxiv.org/abs/2601.22308",
    "source": "Arxiv AI"
  },
  {
    "title": "From Retrieving Information to Reasoning with AI: Exploring Different Interaction Modalities to Support Human-AI Coordination in Clinical Decision-Making",
    "summary": "arXiv:2601.22338v1 Announce Type: cross Abstract: LLMs are popular among clinicians for decision-support because of simple text-based interaction. However, their impact on clinicians' performance is ambiguous. Not knowing how clinicians use this new technology and how they compare it to traditional clinical decision-support systems (CDSS) restricts",
    "url": "https://arxiv.org/abs/2601.22338",
    "source": "Arxiv AI"
  },
  {
    "title": "MixQuant: Pushing the Limits of Block Rotations in Post-Training Quantization",
    "summary": "arXiv:2601.22347v1 Announce Type: cross Abstract: Recent post-training quantization (PTQ) methods have adopted block rotations to diffuse outliers prior to rounding. While this reduces the overhead of full-vector rotations, the effect of block structure on outlier suppression remains poorly understood. To fill this gap, we present the first systema",
    "url": "https://arxiv.org/abs/2601.22347",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning Policy Representations for Steerable Behavior Synthesis",
    "summary": "arXiv:2601.22350v1 Announce Type: cross Abstract: Given a Markov decision process (MDP), we seek to learn representations for a range of policies to facilitate behavior steering at test time. As policies of an MDP are uniquely determined by their occupancy measures, we propose modeling policy representations as expectations of state-action feature ",
    "url": "https://arxiv.org/abs/2601.22350",
    "source": "Arxiv AI"
  },
  {
    "title": "Recoverability Has a Law: The ERR Measure for Tool-Augmented Agents",
    "summary": "arXiv:2601.22352v1 Announce Type: cross Abstract: Language model agents often appear capable of self-recovery after failing tool call executions, yet this behavior lacks a formal explanation. We present a predictive theory that resolves this gap by showing that recoverability follows a measurable law. To elaborate, we formalize recoverability throu",
    "url": "https://arxiv.org/abs/2601.22352",
    "source": "Arxiv AI"
  },
  {
    "title": "The Unseen Threat: Residual Knowledge in Machine Unlearning under Perturbed Samples",
    "summary": "arXiv:2601.22359v1 Announce Type: cross Abstract: Machine unlearning offers a practical alternative to avoid full model re-training by approximately removing the influence of specific user data. While existing methods certify unlearning via statistical indistinguishability from re-trained models, these guarantees do not naturally extend to model ou",
    "url": "https://arxiv.org/abs/2601.22359",
    "source": "Arxiv AI"
  },
  {
    "title": "MERMAID: Memory-Enhanced Retrieval and Reasoning with Multi-Agent Iterative Knowledge Grounding for Veracity Assessment",
    "summary": "arXiv:2601.22361v1 Announce Type: cross Abstract: Assessing the veracity of online content has become increasingly critical. Large language models (LLMs) have recently enabled substantial progress in automated veracity assessment, including automated fact-checking and claim verification systems. Typical veracity assessment pipelines break down comp",
    "url": "https://arxiv.org/abs/2601.22361",
    "source": "Arxiv AI"
  },
  {
    "title": "Context Structure Reshapes the Representational Geometry of Language Models",
    "summary": "arXiv:2601.22364v1 Announce Type: cross Abstract: Large Language Models (LLMs) have been shown to organize the representations of input sequences into straighter neural trajectories in their deep layers, which has been hypothesized to facilitate next-token prediction via linear extrapolation. Language models can also adapt to diverse tasks and lear",
    "url": "https://arxiv.org/abs/2601.22364",
    "source": "Arxiv AI"
  },
  {
    "title": "Graph is a Substrate Across Data Modalities",
    "summary": "arXiv:2601.22384v1 Announce Type: cross Abstract: Graphs provide a natural representation of relational structure that arises across diverse domains. Despite this ubiquity, graph structure is typically learned in a modality- and task-isolated manner, where graph representations are constructed within individual task contexts and discarded thereafte",
    "url": "https://arxiv.org/abs/2601.22384",
    "source": "Arxiv AI"
  },
  {
    "title": "SP^2DPO: An LLM-assisted Semantic Per-Pair DPO Generalization",
    "summary": "arXiv:2601.22385v1 Announce Type: cross Abstract: Direct Preference Optimization (DPO) controls the trade-off between fitting preference labels and staying close to a reference model using a single global temperature beta, implicitly treating all preference pairs as equally informative. Real-world preference corpora are heterogeneous: they mix high",
    "url": "https://arxiv.org/abs/2601.22385",
    "source": "Arxiv AI"
  },
  {
    "title": "Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks",
    "summary": "arXiv:2601.22396v1 Announce Type: cross Abstract: Despite the growing utility of Large Language Models (LLMs) for simulating human behavior, the extent to which these synthetic personas accurately reflect world and moral value systems across different cultural conditionings remains uncertain. This paper investigates the alignment of synthetic, cult",
    "url": "https://arxiv.org/abs/2601.22396",
    "source": "Arxiv AI"
  },
  {
    "title": "Jailbreaks on Vision Language Model via Multimodal Reasoning",
    "summary": "arXiv:2601.22398v1 Announce Type: cross Abstract: Vision-language models (VLMs) have become central to tasks such as visual question answering, image captioning, and text-to-image generation. However, their outputs are highly sensitive to prompt variations, which can reveal vulnerabilities in safety alignment. In this work, we present a jailbreak f",
    "url": "https://arxiv.org/abs/2601.22398",
    "source": "Arxiv AI"
  },
  {
    "title": "Score-based Integrated Gradient for Root Cause Explanations of Outliers",
    "summary": "arXiv:2601.22399v1 Announce Type: cross Abstract: Identifying the root causes of outliers is a fundamental problem in causal inference and anomaly detection. Traditional approaches based on heuristics or counterfactual reasoning often struggle under uncertainty and high-dimensional dependencies. We introduce SIREN, a novel and scalable method that ",
    "url": "https://arxiv.org/abs/2601.22399",
    "source": "Arxiv AI"
  },
  {
    "title": "Spectral Filtering for Learning Quantum Dynamics",
    "summary": "arXiv:2601.22400v1 Announce Type: cross Abstract: Learning high-dimensional quantum systems is a fundamental challenge that notoriously suffers from the curse of dimensionality. We formulate the task of predicting quantum evolution in the linear response regime as a specific instance of learning a Complex-Valued Linear Dynamical System (CLDS) with ",
    "url": "https://arxiv.org/abs/2601.22400",
    "source": "Arxiv AI"
  },
  {
    "title": "Optimization, Generalization and Differential Privacy Bounds for Gradient Descent on Kolmogorov-Arnold Networks",
    "summary": "arXiv:2601.22409v1 Announce Type: cross Abstract: Kolmogorov--Arnold Networks (KANs) have recently emerged as a structured alternative to standard MLPs, yet a principled theory for their training dynamics, generalization, and privacy properties remains limited. In this paper, we analyze gradient descent (GD) for training two-layer KANs and derive g",
    "url": "https://arxiv.org/abs/2601.22409",
    "source": "Arxiv AI"
  },
  {
    "title": "Dynamic Welfare-Maximizing Pooled Testing",
    "summary": "arXiv:2601.22419v1 Announce Type: cross Abstract: Pooled testing is a common strategy for public health disease screening under limited testing resources, allowing multiple biological samples to be tested together with the resources of a single test, at the cost of reduced individual resolution. While dynamic and adaptive strategies have been exten",
    "url": "https://arxiv.org/abs/2601.22419",
    "source": "Arxiv AI"
  },
  {
    "title": "MetaLead: A Comprehensive Human-Curated Leaderboard Dataset for Transparent Reporting of Machine Learning Experiments",
    "summary": "arXiv:2601.22420v1 Announce Type: cross Abstract: Leaderboards are crucial in the machine learning (ML) domain for benchmarking and tracking progress. However, creating leaderboards traditionally demands significant manual effort. In recent years, efforts have been made to automate leaderboard generation, but existing datasets for this purpose are ",
    "url": "https://arxiv.org/abs/2601.22420",
    "source": "Arxiv AI"
  },
  {
    "title": "CoDCL: Counterfactual Data Augmentation Contrastive Learning for Continuous-Time Dynamic Network Link Prediction",
    "summary": "arXiv:2601.22427v1 Announce Type: cross Abstract: The rapid growth and continuous structural evolution of dynamic networks make effective predictions increasingly challenging. To enable prediction models to adapt to complex temporal environments, they need to be robust to emerging structural changes. We propose a dynamic network learning framework ",
    "url": "https://arxiv.org/abs/2601.22427",
    "source": "Arxiv AI"
  },
  {
    "title": "AI and My Values: User Perceptions of LLMs' Ability to Extract, Embody, and Explain Human Values from Casual Conversations",
    "summary": "arXiv:2601.22440v1 Announce Type: cross Abstract: Does AI understand human values? While this remains an open philosophical question, we take a pragmatic stance by introducing VAPT, the Value-Alignment Perception Toolkit, for studying how LLMs reflect people's values and how people judge those reflections. 20 participants texted a human-like chatbo",
    "url": "https://arxiv.org/abs/2601.22440",
    "source": "Arxiv AI"
  },
  {
    "title": "Automating Forecasting Question Generation and Resolution for AI Evaluation",
    "summary": "arXiv:2601.22444v1 Announce Type: cross Abstract: Forecasting future events is highly valuable in decision-making and is a robust measure of general intelligence. As forecasting is probabilistic, developing and evaluating AI forecasters requires generating large numbers of diverse and difficult questions, and accurately resolving them. Previous eff",
    "url": "https://arxiv.org/abs/2601.22444",
    "source": "Arxiv AI"
  },
  {
    "title": "Tuning the Implicit Regularizer of Masked Diffusion Language Models: Enhancing Generalization via Insights from $k$-Parity",
    "summary": "arXiv:2601.22450v1 Announce Type: cross Abstract: Masked Diffusion Language Models have recently emerged as a powerful generative paradigm, yet their generalization properties remain understudied compared to their auto-regressive counterparts. In this work, we investigate these properties within the setting of the $k$-parity problem (computing the ",
    "url": "https://arxiv.org/abs/2601.22450",
    "source": "Arxiv AI"
  },
  {
    "title": "Countering the Over-Reliance Trap: Mitigating Object Hallucination for LVLMs via a Self-Validation Framework",
    "summary": "arXiv:2601.22451v1 Announce Type: cross Abstract: Despite progress in Large Vision Language Models (LVLMs), object hallucination remains a critical issue in image captioning task, where models generate descriptions of non-existent objects, compromising their reliability. Previous work attributes this to LVLMs' over-reliance on language priors and a",
    "url": "https://arxiv.org/abs/2601.22451",
    "source": "Arxiv AI"
  },
  {
    "title": "Does My Chatbot Have an Agenda? Understanding Human and AI Agency in Human-Human-like Chatbot Interaction",
    "summary": "arXiv:2601.22452v1 Announce Type: cross Abstract: AI chatbots are shifting from tools to companions. This raises critical questions about agency: who drives conversations and sets boundaries in human-AI chatrooms? We report a month-long longitudinal study with 22 adults who chatted with Day, an LLM companion we built, followed by a semi-structured ",
    "url": "https://arxiv.org/abs/2601.22452",
    "source": "Arxiv AI"
  },
  {
    "title": "Temporal Graph Pattern Machine",
    "summary": "arXiv:2601.22454v1 Announce Type: cross Abstract: Temporal graph learning is pivotal for deciphering dynamic systems, where the core challenge lies in explicitly modeling the underlying evolving patterns that govern network transformation. However, prevailing methods are predominantly task-centric and rely on restrictive assumptions -- such as shor",
    "url": "https://arxiv.org/abs/2601.22454",
    "source": "Arxiv AI"
  },
  {
    "title": "Machine Unlearning in Low-Dimensional Feature Subspace",
    "summary": "arXiv:2601.22456v1 Announce Type: cross Abstract: Machine Unlearning (MU) aims at removing the influence of specific data from a pretrained model while preserving performance on the remaining data. In this work, a novel perspective for MU is presented upon low-dimensional feature subspaces, which gives rise to the potentials of separating the remai",
    "url": "https://arxiv.org/abs/2601.22456",
    "source": "Arxiv AI"
  },
  {
    "title": "AI Decodes Historical Chinese Archives to Reveal Lost Climate History",
    "summary": "arXiv:2601.22458v1 Announce Type: cross Abstract: Historical archives contain qualitative descriptions of climate events, yet converting these into quantitative records has remained a fundamental challenge. Here we introduce a paradigm shift: a generative AI framework that inverts the logic of historical chroniclers by inferring the quantitative cl",
    "url": "https://arxiv.org/abs/2601.22458",
    "source": "Arxiv AI"
  },
  {
    "title": "Training-Free Representation Guidance for Diffusion Models with a Representation Alignment Projector",
    "summary": "arXiv:2601.22468v1 Announce Type: cross Abstract: Recent progress in generative modeling has enabled high-quality visual synthesis with diffusion-based frameworks, supporting controllable sampling and large-scale training. Inference-time guidance methods such as classifier-free and representative guidance enhance semantic alignment by modifying sam",
    "url": "https://arxiv.org/abs/2601.22468",
    "source": "Arxiv AI"
  },
  {
    "title": "RulePlanner: All-in-One Reinforcement Learner for Unifying Design Rules in 3D Floorplanning",
    "summary": "arXiv:2601.22476v1 Announce Type: cross Abstract: Floorplanning determines the coordinate and shape of each module in Integrated Circuits. With the scaling of technology nodes, in floorplanning stage especially 3D scenarios with multiple stacked layers, it has become increasingly challenging to adhere to complex hardware design rules. Current metho",
    "url": "https://arxiv.org/abs/2601.22476",
    "source": "Arxiv AI"
  },
  {
    "title": "FraudShield: Knowledge Graph Empowered Defense for LLMs against Fraud Attacks",
    "summary": "arXiv:2601.22485v1 Announce Type: cross Abstract: Large language models (LLMs) have been widely integrated into critical automated workflows, including contract review and job application processes. However, LLMs are susceptible to manipulation by fraudulent information, which can lead to harmful outcomes. Although advanced defense methods have bee",
    "url": "https://arxiv.org/abs/2601.22485",
    "source": "Arxiv AI"
  },
  {
    "title": "AI Literacy, Safety Awareness, and STEM Career Aspirations of Australian Secondary Students: Evaluating the Impact of Workshop Interventions",
    "summary": "arXiv:2601.22486v1 Announce Type: cross Abstract: Deepfakes and other forms of synthetic media pose growing safety risks for adolescents, yet evidence on students' exposure and related behaviours remains limited. This study evaluates the impact of Day of AI Australia's workshop-based intervention designed to improve AI literacy and conceptual under",
    "url": "https://arxiv.org/abs/2601.22486",
    "source": "Arxiv AI"
  },
  {
    "title": "Action-Sufficient Goal Representations",
    "summary": "arXiv:2601.22496v1 Announce Type: cross Abstract: Hierarchical policies in offline goal-conditioned reinforcement learning (GCRL) addresses long-horizon tasks by decomposing control into high-level subgoal planning and low-level action execution. A critical design choice in such architectures is the goal representation-the compressed encoding of go",
    "url": "https://arxiv.org/abs/2601.22496",
    "source": "Arxiv AI"
  },
  {
    "title": "Keep Rehearsing and Refining: Lifelong Learning Vehicle Routing under Continually Drifting Tasks",
    "summary": "arXiv:2601.22509v1 Announce Type: cross Abstract: Existing neural solvers for vehicle routing problems (VRPs) are typically trained either in a one-off manner on a fixed set of pre-defined tasks or in a lifelong manner on several tasks arriving sequentially, assuming sufficient training on each task. Both settings overlook a common real-world prope",
    "url": "https://arxiv.org/abs/2601.22509",
    "source": "Arxiv AI"
  },
  {
    "title": "Shattered Compositionality: Counterintuitive Learning Dynamics of Transformers for Arithmetic",
    "summary": "arXiv:2601.22510v1 Announce Type: cross Abstract: Large language models (LLMs) often exhibit unexpected errors or unintended behavior, even at scale. While recent work reveals the discrepancy between LLMs and humans in skill compositions, the learning dynamics of skill compositions and the underlying cause of non-human behavior remain elusive. In t",
    "url": "https://arxiv.org/abs/2601.22510",
    "source": "Arxiv AI"
  },
  {
    "title": "SCOPE-PD: Explainable AI on Subjective and Clinical Objective Measurements of Parkinson's Disease for Precision Decision-Making",
    "summary": "arXiv:2601.22516v1 Announce Type: cross Abstract: Parkinson's disease (PD) is a chronic and complex neurodegenerative disorder influenced by genetic, clinical, and lifestyle factors. Predicting this disease early is challenging because it depends on traditional diagnostic methods that face issues of subjectivity, which commonly delay diagnosis. Sev",
    "url": "https://arxiv.org/abs/2601.22516",
    "source": "Arxiv AI"
  },
  {
    "title": "Learn from A Rationalist: Distilling Intermediate Interpretable Rationales",
    "summary": "arXiv:2601.22531v1 Announce Type: cross Abstract: Because of the pervasive use of deep neural networks (DNNs), especially in high-stakes domains, the interpretability of DNNs has received increased attention. The general idea of rationale extraction (RE) is to provide an interpretable-by-design framework for DNNs via a select-predict architecture w",
    "url": "https://arxiv.org/abs/2601.22531",
    "source": "Arxiv AI"
  },
  {
    "title": "Demystifying Design Choices of Reinforcement Fine-tuning: A Batched Contextual Bandit Learning Perspective",
    "summary": "arXiv:2601.22532v1 Announce Type: cross Abstract: The reinforcement fine-tuning area is undergoing an explosion papers largely on optimizing design choices. Though performance gains are often claimed, inconsistent conclusions also arise from time to time, making the progress illusive. Reflecting on this illusion, we still lack principled answers to",
    "url": "https://arxiv.org/abs/2601.22532",
    "source": "Arxiv AI"
  },
  {
    "title": "Adapting Reinforcement Learning for Path Planning in Constrained Parking Scenarios",
    "summary": "arXiv:2601.22545v1 Announce Type: cross Abstract: Real-time path planning in constrained environments remains a fundamental challenge for autonomous systems. Traditional classical planners, while effective under perfect perception assumptions, are often sensitive to real-world perception constraints and rely on online search procedures that incur h",
    "url": "https://arxiv.org/abs/2601.22545",
    "source": "Arxiv AI"
  },
  {
    "title": "Towards the Holographic Characteristic of LLMs for Efficient Short-text Generation",
    "summary": "arXiv:2601.22546v1 Announce Type: cross Abstract: The recent advancements in Large Language Models (LLMs) have attracted interest in exploring their in-context learning abilities and chain-of-thought capabilities. However, there are few studies investigating the specific traits related to the powerful generation capacity of LLMs. This paper aims to",
    "url": "https://arxiv.org/abs/2601.22546",
    "source": "Arxiv AI"
  },
  {
    "title": "Are LLM Evaluators Really Narcissists? Sanity Checking Self-Preference Evaluations",
    "summary": "arXiv:2601.22548v1 Announce Type: cross Abstract: Recent research has shown that large language models (LLM) favor own outputs when acting as judges, undermining the integrity of automated post-training and evaluation workflows. However, it is difficult to disentangle which evaluation biases are explained by narcissism versus general experimental c",
    "url": "https://arxiv.org/abs/2601.22548",
    "source": "Arxiv AI"
  },
  {
    "title": "EUGens: Efficient, Unified, and General Dense Layers",
    "summary": "arXiv:2601.22563v1 Announce Type: cross Abstract: Efficient neural networks are essential for scaling machine learning models to real-time applications and resource-constrained environments. Fully-connected feedforward layers (FFLs) introduce computation and parameter count bottlenecks within neural network architectures. To address this challenge,",
    "url": "https://arxiv.org/abs/2601.22563",
    "source": "Arxiv AI"
  },
  {
    "title": "Whispers of Wealth: Red-Teaming Google's Agent Payments Protocol via Prompt Injection",
    "summary": "arXiv:2601.22569v1 Announce Type: cross Abstract: Large language model (LLM) based agents are increasingly used to automate financial transactions, yet their reliance on contextual reasoning exposes payment systems to prompt-driven manipulation. The Agent Payments Protocol (AP2) aims to secure agent-led purchases through cryptographically verifiabl",
    "url": "https://arxiv.org/abs/2601.22569",
    "source": "Arxiv AI"
  },
  {
    "title": "Mitigating Hallucinations in Video Large Language Models via Spatiotemporal-Semantic Contrastive Decoding",
    "summary": "arXiv:2601.22574v1 Announce Type: cross Abstract: Although Video Large Language Models perform remarkably well across tasks such as video understanding, question answering, and reasoning, they still suffer from the problem of hallucination, which refers to generating outputs that are inconsistent with explicit video content or factual evidence. How",
    "url": "https://arxiv.org/abs/2601.22574",
    "source": "Arxiv AI"
  },
  {
    "title": "FedDis: A Causal Disentanglement Framework for Federated Traffic Prediction",
    "summary": "arXiv:2601.22578v1 Announce Type: cross Abstract: Federated learning offers a promising paradigm for privacy-preserving traffic prediction, yet its performance is often challenged by the non-identically and independently distributed (non-IID) nature of decentralized traffic data. Existing federated methods frequently struggle with this data heterog",
    "url": "https://arxiv.org/abs/2601.22578",
    "source": "Arxiv AI"
  },
  {
    "title": "SpanNorm: Reconciling Training Stability and Performance in Deep Transformers",
    "summary": "arXiv:2601.22580v1 Announce Type: cross Abstract: The success of Large Language Models (LLMs) hinges on the stable training of deep Transformer architectures. A critical design choice is the placement of normalization layers, leading to a fundamental trade-off: the ``PreNorm'' architecture ensures training stability at the cost of potential perform",
    "url": "https://arxiv.org/abs/2601.22580",
    "source": "Arxiv AI"
  },
  {
    "title": "Cross-Domain Few-Shot Learning for Hyperspectral Image Classification Based on Mixup Foundation Model",
    "summary": "arXiv:2601.22581v1 Announce Type: cross Abstract: Although cross-domain few-shot learning (CDFSL) for hyper-spectral image (HSI) classification has attracted significant research interest, existing works often rely on an unrealistic data augmentation procedure in the form of external noise to enlarge the sample size, thus greatly simplifying the is",
    "url": "https://arxiv.org/abs/2601.22581",
    "source": "Arxiv AI"
  },
  {
    "title": "MC-GRPO: Median-Centered Group Relative Policy Optimization for Small-Rollout Reinforcement Learning",
    "summary": "arXiv:2601.22582v1 Announce Type: cross Abstract: Group-relative policy optimization methods train language models by generating multiple rollouts per prompt and normalizing rewards with a shared mean reward baseline. In resource-constrained settings where the rollout budget is small, accuracy often degrades. We find that noise in the shared baseli",
    "url": "https://arxiv.org/abs/2601.22582",
    "source": "Arxiv AI"
  },
  {
    "title": "Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry",
    "summary": "arXiv:2601.22588v1 Announce Type: cross Abstract: Large language models (LLMs) are widely used as reference-free evaluators via prompting, but this \"LLM-as-a-Judge\" paradigm is costly, opaque, and sensitive to prompt design. In this work, we investigate whether smaller models can serve as efficient evaluators by leveraging internal representations ",
    "url": "https://arxiv.org/abs/2601.22588",
    "source": "Arxiv AI"
  },
  {
    "title": "FedCARE: Federated Unlearning with Conflict-Aware Projection and Relearning-Resistant Recovery",
    "summary": "arXiv:2601.22589v1 Announce Type: cross Abstract: Federated learning (FL) enables collaborative model training without centralizing raw data, but privacy regulations such as the right to be forgotten require FL systems to remove the influence of previously used training data upon request. Retraining a federated model from scratch is prohibitively e",
    "url": "https://arxiv.org/abs/2601.22589",
    "source": "Arxiv AI"
  },
  {
    "title": "Language Model Circuits Are Sparse in the Neuron Basis",
    "summary": "arXiv:2601.22594v1 Announce Type: cross Abstract: The high-level concepts that a neural network uses to perform computation need not be aligned to individual neurons (Smolensky, 1986). Language model interpretability research has thus turned to techniques such as \\textit{sparse autoencoders} (SAEs) to decompose the neuron basis into more interpreta",
    "url": "https://arxiv.org/abs/2601.22594",
    "source": "Arxiv AI"
  },
  {
    "title": "Local-Global Multimodal Contrastive Learning for Molecular Property Prediction",
    "summary": "arXiv:2601.22610v1 Announce Type: cross Abstract: Accurate molecular property prediction requires integrating complementary information from molecular structure and chemical semantics. In this work, we propose LGM-CL, a local-global multimodal contrastive learning framework that jointly models molecular graphs and textual representations derived fr",
    "url": "https://arxiv.org/abs/2601.22610",
    "source": "Arxiv AI"
  },
  {
    "title": "TTCS: Test-Time Curriculum Synthesis for Self-Evolving",
    "summary": "arXiv:2601.22628v1 Announce Type: cross Abstract: Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield hi",
    "url": "https://arxiv.org/abs/2601.22628",
    "source": "Arxiv AI"
  },
  {
    "title": "Time-Annealed Perturbation Sampling: Diverse Generation for Diffusion Language Models",
    "summary": "arXiv:2601.22629v1 Announce Type: cross Abstract: Diffusion language models (Diffusion-LMs) introduce an explicit temporal dimension into text generation, yet how this structure can be leveraged to control generation diversity for exploring multiple valid semantic or reasoning paths remains underexplored. In this paper, we show that Diffusion-LMs, ",
    "url": "https://arxiv.org/abs/2601.22629",
    "source": "Arxiv AI"
  },
  {
    "title": "PEFT-MuTS: A Multivariate Parameter-Efficient Fine-Tuning Framework for Remaining Useful Life Prediction based on Cross-domain Time Series Representation Model",
    "summary": "arXiv:2601.22631v1 Announce Type: cross Abstract: The application of data-driven remaining useful life (RUL) prediction has long been constrained by the availability of large amount of degradation data. Mainstream solutions such as domain adaptation and meta-learning still rely on large amounts of historical degradation data from equipment that is ",
    "url": "https://arxiv.org/abs/2601.22631",
    "source": "Arxiv AI"
  },
  {
    "title": "MCP-Diag: A Deterministic, Protocol-Driven Architecture for AI-Native Network Diagnostics",
    "summary": "arXiv:2601.22633v1 Announce Type: cross Abstract: The integration of Large Language Models (LLMs) into network operations (AIOps) is hindered by two fundamental challenges: the stochastic grounding problem, where LLMs struggle to reliably parse unstructured, vendor-specific CLI output, and the security gap of granting autonomous agents shell access",
    "url": "https://arxiv.org/abs/2601.22633",
    "source": "Arxiv AI"
  },
  {
    "title": "What can Computer Vision learn from Ranganathan?",
    "summary": "arXiv:2601.22634v1 Announce Type: cross Abstract: The Semantic Gap Problem (SGP) in Computer Vision (CV) arises from the misalignment between visual and lexical semantics leading to flawed CV dataset design and CV benchmarks. This paper proposes that classification principles of S.R. Ranganathan can offer a principled starting point to address SGP ",
    "url": "https://arxiv.org/abs/2601.22634",
    "source": "Arxiv AI"
  },
  {
    "title": "Training Beyond Convergence: Grokking nnU-Net for Glioma Segmentation in Sub-Saharan MRI",
    "summary": "arXiv:2601.22637v1 Announce Type: cross Abstract: Gliomas are placing an increasingly clinical burden on Sub-Saharan Africa (SSA). In the region, the median survival for patients remains under two years, and access to diagnostic imaging is extremely limited. These constraints highlight an urgent need for automated tools that can extract the maximum",
    "url": "https://arxiv.org/abs/2601.22637",
    "source": "Arxiv AI"
  },
  {
    "title": "ScholarPeer: A Context-Aware Multi-Agent Framework for Automated Peer Review",
    "summary": "arXiv:2601.22638v1 Announce Type: cross Abstract: Automated peer review has evolved from simple text classification to structured feedback generation. However, current state-of-the-art systems still struggle with \"surface-level\" critiques: they excel at summarizing content but often fail to accurately assess novelty and significance or identify dee",
    "url": "https://arxiv.org/abs/2601.22638",
    "source": "Arxiv AI"
  },
  {
    "title": "GUDA: Counterfactual Group-wise Training Data Attribution for Diffusion Models via Unlearning",
    "summary": "arXiv:2601.22651v1 Announce Type: cross Abstract: Training-data attribution for vision generative models aims to identify which training data influenced a given output. While most methods score individual examples, practitioners often need group-level answers (e.g., artistic styles or object classes). Group-wise attribution is counterfactual: how w",
    "url": "https://arxiv.org/abs/2601.22651",
    "source": "Arxiv AI"
  },
  {
    "title": "Human-Centered Explainability in AI-Enhanced UI Security Interfaces: Designing Trustworthy Copilots for Cybersecurity Analysts",
    "summary": "arXiv:2601.22653v1 Announce Type: cross Abstract: Artificial intelligence (AI) copilots are increasingly integrated into enterprise cybersecurity platforms to assist analysts in threat detection, triage, and remediation. However, the effectiveness of these systems depends not only on the accuracy of underlying models but also on the degree to which",
    "url": "https://arxiv.org/abs/2601.22653",
    "source": "Arxiv AI"
  },
  {
    "title": "NAG: A Unified Native Architecture for Encoder-free Text-Graph Modeling in Language Models",
    "summary": "arXiv:2601.22657v1 Announce Type: cross Abstract: Prevailing methods for integrating graphs into Language Models (LMs) typically rely on a segregated architecture: external Graph Neural Networks (GNNs) encode structural topology, while LMs process textual semantics. We argue this approach is suboptimal for text-graphs: it creates a conceptually dis",
    "url": "https://arxiv.org/abs/2601.22657",
    "source": "Arxiv AI"
  },
  {
    "title": "Unsupervised Synthetic Image Attribution: Alignment and Disentanglement",
    "summary": "arXiv:2601.22663v1 Announce Type: cross Abstract: As the quality of synthetic images improves, identifying the underlying concepts of model-generated images is becoming increasingly crucial for copyright protection and ensuring model transparency. Existing methods achieve this attribution goal by training models using annotated pairs of synthetic i",
    "url": "https://arxiv.org/abs/2601.22663",
    "source": "Arxiv AI"
  },
  {
    "title": "From Horizontal Layering to Vertical Integration: A Comparative Study of the AI-Driven Software Development Paradigm",
    "summary": "arXiv:2601.22667v1 Announce Type: cross Abstract: This paper examines the organizational implications of Generative AI adoption in software engineering through a multiple-case comparative study. We contrast two development environments: a traditional enterprise (brownfield) and an AI-native startup (greenfield). Our analysis reveals that transition",
    "url": "https://arxiv.org/abs/2601.22667",
    "source": "Arxiv AI"
  },
  {
    "title": "Fire on Motion: Optimizing Video Pass-bands for Efficient Spiking Action Recognition",
    "summary": "arXiv:2601.22675v1 Announce Type: cross Abstract: Spiking neural networks (SNNs) have gained traction in vision due to their energy efficiency, bio-plausibility, and inherent temporal processing. Yet, despite this temporal capacity, most progress concentrates on static image benchmarks, and SNNs still underperform on dynamic video tasks compared to",
    "url": "https://arxiv.org/abs/2601.22675",
    "source": "Arxiv AI"
  },
  {
    "title": "Do Transformers Have the Ability for Periodicity Generalization?",
    "summary": "arXiv:2601.22690v1 Announce Type: cross Abstract: Large language models (LLMs) based on the Transformer have demonstrated strong performance across diverse tasks. However, current models still exhibit substantial limitations in out-of-distribution (OOD) generalization compared with humans. We investigate this gap through periodicity, one of the bas",
    "url": "https://arxiv.org/abs/2601.22690",
    "source": "Arxiv AI"
  },
  {
    "title": "FNF: Functional Network Fingerprint for Large Language Models",
    "summary": "arXiv:2601.22692v1 Announce Type: cross Abstract: The development of large language models (LLMs) is costly and has significant commercial value. Consequently, preventing unauthorized appropriation of open-source LLMs and protecting developers' intellectual property rights have become critical challenges. In this work, we propose the Functional Net",
    "url": "https://arxiv.org/abs/2601.22692",
    "source": "Arxiv AI"
  },
  {
    "title": "PEAR: Pixel-aligned Expressive humAn mesh Recovery",
    "summary": "arXiv:2601.22693v1 Announce Type: cross Abstract: Reconstructing detailed 3D human meshes from a single in-the-wild image remains a fundamental challenge in computer vision. Existing SMPLX-based methods often suffer from slow inference, produce only coarse body poses, and exhibit misalignments or unnatural artifacts in fine-grained regions such as ",
    "url": "https://arxiv.org/abs/2601.22693",
    "source": "Arxiv AI"
  },
  {
    "title": "Deep Learning-Based Early-Stage IR-Drop Estimation via CNN Surrogate Modeling",
    "summary": "arXiv:2601.22707v1 Announce Type: cross Abstract: IR-drop is a critical power integrity challenge in modern VLSI designs that can cause timing degradation, reliability issues, and functional failures if not detected early in the design flow. Conventional IR-drop analysis relies on physics-based signoff tools, which provide high accuracy but incur s",
    "url": "https://arxiv.org/abs/2601.22707",
    "source": "Arxiv AI"
  },
  {
    "title": "Gated Relational Alignment via Confidence-based Distillation for Efficient VLMs",
    "summary": "arXiv:2601.22709v1 Announce Type: cross Abstract: Vision-Language Models (VLMs) achieve strong multimodal performance but are costly to deploy, and post-training quantization often causes significant accuracy loss. Despite its potential, quantization-aware training for VLMs remains underexplored. We propose GRACE, a framework unifying knowledge dis",
    "url": "https://arxiv.org/abs/2601.22709",
    "source": "Arxiv AI"
  },
  {
    "title": "Vision-Language Models Unlock Task-Centric Latent Actions",
    "summary": "arXiv:2601.22714v1 Announce Type: cross Abstract: Latent Action Models (LAMs) have rapidly gained traction as an important component in the pre-training pipelines of leading Vision-Language-Action models. However, they fail when observations contain action-correlated distractors, often encoding noise instead of meaningful latent actions. Humans, on",
    "url": "https://arxiv.org/abs/2601.22714",
    "source": "Arxiv AI"
  },
  {
    "title": "Breaking the Blocks: Continuous Low-Rank Decomposed Scaling for Unified LLM Quantization and Adaptation",
    "summary": "arXiv:2601.22716v1 Announce Type: cross Abstract: Current quantization methods for LLMs predominantly rely on block-wise structures to maintain efficiency, often at the cost of representational flexibility. In this work, we demonstrate that element-wise quantization can be made as efficient as block-wise scaling while providing strictly superior ex",
    "url": "https://arxiv.org/abs/2601.22716",
    "source": "Arxiv AI"
  },
  {
    "title": "AEGIS: White-Box Attack Path Generation using LLMs and Training Effectiveness Evaluation for Large-Scale Cyber Defence Exercises",
    "summary": "arXiv:2601.22720v1 Announce Type: cross Abstract: Creating attack paths for cyber defence exercises requires substantial expert effort. Existing automation requires vulnerability graphs or exploit sets curated in advance, limiting where it can be applied. We present AEGIS, a system that generates attack paths using LLMs, white-box access, and Monte",
    "url": "https://arxiv.org/abs/2601.22720",
    "source": "Arxiv AI"
  },
  {
    "title": "A Cross-Domain Graph Learning Protocol for Single-Step Molecular Geometry Refinement",
    "summary": "arXiv:2601.22723v1 Announce Type: cross Abstract: Accurate molecular geometries are a prerequisite for reliable quantum-chemical predictions, yet density functional theory (DFT) optimization remains a major bottleneck for high-throughput molecular screening. Here we present GeoOpt-Net, a multi-branch SE(3)-equivariant geometry refinement network th",
    "url": "https://arxiv.org/abs/2601.22723",
    "source": "Arxiv AI"
  },
  {
    "title": "OpenVTON-Bench: A Large-Scale High-Resolution Benchmark for Controllable Virtual Try-On Evaluation",
    "summary": "arXiv:2601.22725v1 Announce Type: cross Abstract: Recent advances in diffusion models have significantly elevated the visual fidelity of Virtual Try-On (VTON) systems, yet reliable evaluation remains a persistent bottleneck. Traditional metrics struggle to quantify fine-grained texture details and semantic consistency, while existing datasets fail ",
    "url": "https://arxiv.org/abs/2601.22725",
    "source": "Arxiv AI"
  },
  {
    "title": "ImgCoT: Compressing Long Chain of Thought into Compact Visual Tokens for Efficient Reasoning of Large Language Model",
    "summary": "arXiv:2601.22730v1 Announce Type: cross Abstract: Compressing long chains of thought (CoT) into compact latent tokens is crucial for efficient reasoning with large language models (LLMs). Recent studies employ autoencoders to achieve this by reconstructing textual CoT from latent tokens, thus encoding CoT semantics. However, treating textual CoT as",
    "url": "https://arxiv.org/abs/2601.22730",
    "source": "Arxiv AI"
  },
  {
    "title": "Decomposing Epistemic Uncertainty for Causal Decision Making",
    "summary": "arXiv:2601.22736v1 Announce Type: cross Abstract: Causal inference from observational data provides strong evidence for the best action in decision-making without performing expensive randomized trials. The effect of an action is usually not identifiable under unobserved confounding, even with an infinite amount of data. Recent work uses neural net",
    "url": "https://arxiv.org/abs/2601.22736",
    "source": "Arxiv AI"
  },
  {
    "title": "UrbanMoE: A Sparse Multi-Modal Mixture-of-Experts Framework for Multi-Task Urban Region Profiling",
    "summary": "arXiv:2601.22746v1 Announce Type: cross Abstract: Urban region profiling, the task of characterizing geographical areas, is crucial for urban planning and resource allocation. However, existing research in this domain faces two significant limitations. First, most methods are confined to single-task prediction, failing to capture the interconnected",
    "url": "https://arxiv.org/abs/2601.22746",
    "source": "Arxiv AI"
  },
  {
    "title": "Procedural Knowledge Extraction from Industrial Troubleshooting Guides Using Vision Language Models",
    "summary": "arXiv:2601.22754v1 Announce Type: cross Abstract: Industrial troubleshooting guides encode diagnostic procedures in flowchart-like diagrams where spatial layout and technical language jointly convey meaning. To integrate this knowledge into operator support systems, which assist shop-floor personnel in diagnosing and resolving equipment issues, the",
    "url": "https://arxiv.org/abs/2601.22754",
    "source": "Arxiv AI"
  },
  {
    "title": "Qualitative Evaluation of LLM-Designed GUI",
    "summary": "arXiv:2601.22759v1 Announce Type: cross Abstract: As generative artificial intelligence advances, Large Language Models (LLMs) are being explored for automated graphical user interface (GUI) design. This study investigates the usability and adaptability of LLM-generated interfaces by analysing their ability to meet diverse user needs. The experimen",
    "url": "https://arxiv.org/abs/2601.22759",
    "source": "Arxiv AI"
  },
  {
    "title": "How Far Can Pretrained LLMs Go in Symbolic Music? Controlled Comparisons of Supervised and Preference-based Adaptation",
    "summary": "arXiv:2601.22764v1 Announce Type: cross Abstract: Music often shares notable parallels with language, motivating the use of pretrained large language models (LLMs) for symbolic music understanding and generation. Despite growing interest, the practical effectiveness of adapting instruction-tuned LLMs to symbolic music remains insufficiently charact",
    "url": "https://arxiv.org/abs/2601.22764",
    "source": "Arxiv AI"
  },
  {
    "title": "Beyond Abstract Compliance: Operationalising trust in AI as a moral relationship",
    "summary": "arXiv:2601.22769v1 Announce Type: cross Abstract: Dominant approaches, e.g. the EU's \"Trustworthy AI framework\", treat trust as a property that can be designed for, evaluated, and governed according to normative and technical criteria. They do not address how trust is subjectively cultivated and experienced, culturally embedded, and inherently rela",
    "url": "https://arxiv.org/abs/2601.22769",
    "source": "Arxiv AI"
  },
  {
    "title": "SOMBRERO: Measuring and Steering Boundary Placement in End-to-End Hierarchical Sequence Models",
    "summary": "arXiv:2601.22805v1 Announce Type: cross Abstract: Hierarchical sequence models replace fixed tokenization with learned segmentations that compress long byte sequences for efficient autoregressive modeling. While recent end-to-end methods can learn meaningful boundaries from the language-modeling objective alone, it remains difficult to quantitative",
    "url": "https://arxiv.org/abs/2601.22805",
    "source": "Arxiv AI"
  },
  {
    "title": "Hide and Seek in Embedding Space: Geometry-based Steganography and Detection in Large Language Models",
    "summary": "arXiv:2601.22818v1 Announce Type: cross Abstract: Fine-tuned LLMs can covertly encode prompt secrets into outputs via steganographic channels. Prior work demonstrated this threat but relied on trivially recoverable encodings. We formalize payload recoverability via classifier accuracy and show previous schemes achieve 100\\% recoverability. In respo",
    "url": "https://arxiv.org/abs/2601.22818",
    "source": "Arxiv AI"
  },
  {
    "title": "User-Adaptive Meta-Learning for Cold-Start Medication Recommendation with Uncertainty Filtering",
    "summary": "arXiv:2601.22820v1 Announce Type: cross Abstract: Large-scale Electronic Health Record (EHR) databases have become indispensable in supporting clinical decision-making through data-driven treatment recommendations. However, existing medication recommender methods often struggle with a user (i.e., patient) cold-start problem, where recommendations f",
    "url": "https://arxiv.org/abs/2601.22820",
    "source": "Arxiv AI"
  },
  {
    "title": "Offline Reinforcement Learning of High-Quality Behaviors Under Robust Style Alignment",
    "summary": "arXiv:2601.22823v1 Announce Type: cross Abstract: We study offline reinforcement learning of style-conditioned policies using explicit style supervision via subtrajectory labeling functions. In this setting, aligning style with high task performance is particularly challenging due to distribution shift and inherent conflicts between style and rewar",
    "url": "https://arxiv.org/abs/2601.22823",
    "source": "Arxiv AI"
  },
  {
    "title": "Just-in-Time Catching Test Generation at Meta",
    "summary": "arXiv:2601.22832v1 Announce Type: cross Abstract: We report on Just-in-Time catching test generation at Meta, designed to prevent bugs in large scale backend systems of hundreds of millions of line of code. Unlike traditional hardening tests, which pass at generation time, catching tests are meant to fail, surfacing bugs before code lands. The prim",
    "url": "https://arxiv.org/abs/2601.22832",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning to Build Shapes by Extrusion",
    "summary": "arXiv:2601.22858v1 Announce Type: cross Abstract: We introduce Text Encoded Extrusion (TEE), a text-based representation that expresses mesh construction as sequences of face extrusions rather than polygon lists, and a method for generating 3D meshes from TEE using a large language model (LLM). By learning extrusion sequences that assemble a mesh, ",
    "url": "https://arxiv.org/abs/2601.22858",
    "source": "Arxiv AI"
  },
  {
    "title": "MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering",
    "summary": "arXiv:2601.22859v1 Announce Type: cross Abstract: The evolution of Large Language Model (LLM) agents for software engineering (SWE) is constrained by the scarcity of verifiable datasets, a bottleneck stemming from the complexity of constructing executable environments across diverse languages. To address this, we introduce MEnvAgent, a Multi-langua",
    "url": "https://arxiv.org/abs/2601.22859",
    "source": "Arxiv AI"
  },
  {
    "title": "Bayesian Interpolating Neural Network (B-INN): a scalable and reliable Bayesian model for large-scale physical systems",
    "summary": "arXiv:2601.22860v1 Announce Type: cross Abstract: Neural networks and machine learning models for uncertainty quantification suffer from limited scalability and poor reliability compared to their deterministic counterparts. In industry-scale active learning settings, where generating a single high-fidelity simulation may require days or weeks of co",
    "url": "https://arxiv.org/abs/2601.22860",
    "source": "Arxiv AI"
  },
  {
    "title": "Degradation-Aware Frequency Regulation of a Heterogeneous Battery Fleet via Reinforcement Learning",
    "summary": "arXiv:2601.22865v1 Announce Type: cross Abstract: Battery energy storage systems are increasingly deployed as fast-responding resources for grid balancing services such as frequency regulation and for mitigating renewable generation uncertainty. However, repeated charging and discharging induces cycling degradation and reduces battery lifetime. Thi",
    "url": "https://arxiv.org/abs/2601.22865",
    "source": "Arxiv AI"
  },
  {
    "title": "Eroding the Truth-Default: A Causal Analysis of Human Susceptibility to Foundation Model Hallucinations and Disinformation in the Wild",
    "summary": "arXiv:2601.22871v1 Announce Type: cross Abstract: As foundation models (FMs) approach human-level fluency, distinguishing synthetic from organic content has become a key challenge for Trustworthy Web Intelligence. This paper presents JudgeGPT and RogueGPT, a dual-axis framework that decouples \"authenticity\" from \"attribution\" to investigate the mec",
    "url": "https://arxiv.org/abs/2601.22871",
    "source": "Arxiv AI"
  },
  {
    "title": "EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis",
    "summary": "arXiv:2601.22873v1 Announce Type: cross Abstract: Achieving precise and controllable emotional expression is crucial for producing natural and context-appropriate speech in text-to-speech (TTS) synthesis. However, many emotion-aware TTS systems, including large language model (LLM)-based designs, rely on scaling fixed emotion embeddings or external",
    "url": "https://arxiv.org/abs/2601.22873",
    "source": "Arxiv AI"
  },
  {
    "title": "Reinforcement Learning-Based Co-Design and Operation of Chiller and Thermal Energy Storage for Cost-Optimal HVAC Systems",
    "summary": "arXiv:2601.22880v1 Announce Type: cross Abstract: We study the joint operation and sizing of cooling infrastructure for commercial HVAC systems using reinforcement learning, with the objective of minimizing life-cycle cost over a 30-year horizon. The cooling system consists of a fixed-capacity electric chiller and a thermal energy storage (TES) uni",
    "url": "https://arxiv.org/abs/2601.22880",
    "source": "Arxiv AI"
  },
  {
    "title": "MoVE: Mixture of Value Embeddings -- A New Axis for Scaling Parametric Memory in Autoregressive Models",
    "summary": "arXiv:2601.22887v1 Announce Type: cross Abstract: Autoregressive sequence modeling stands as the cornerstone of modern Generative AI, powering results across diverse modalities ranging from text generation to image generation. However, a fundamental limitation of this paradigm is the rigid structural coupling of model capacity to computational cost",
    "url": "https://arxiv.org/abs/2601.22887",
    "source": "Arxiv AI"
  },
  {
    "title": "Should LLMs, $\\textit{like}$, Generate How Users Talk? Building Dialect-Accurate Dialog[ue]s Beyond the American Default with MDial",
    "summary": "arXiv:2601.22888v1 Announce Type: cross Abstract: More than 80% of the 1.6 billion English speakers do not use Standard American English (SAE) and experience higher failure rates and stereotyped responses when interacting with LLMs as a result. Yet multi-dialectal performance remains underexplored. We introduce $\\textbf{MDial}$, the first large-sca",
    "url": "https://arxiv.org/abs/2601.22888",
    "source": "Arxiv AI"
  },
  {
    "title": "DiffuSpeech: Silent Thought, Spoken Answer via Unified Speech-Text Diffusion",
    "summary": "arXiv:2601.22889v1 Announce Type: cross Abstract: Current speech language models generate responses directly without explicit reasoning, leading to errors that cannot be corrected once audio is produced. We introduce \\textbf{``Silent Thought, Spoken Answer''} -- a paradigm where speech LLMs generate internal text reasoning alongside spoken response",
    "url": "https://arxiv.org/abs/2601.22889",
    "source": "Arxiv AI"
  },
  {
    "title": "DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation",
    "summary": "arXiv:2601.22904v1 Announce Type: cross Abstract: Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work,",
    "url": "https://arxiv.org/abs/2601.22904",
    "source": "Arxiv AI"
  },
  {
    "title": "Evaluating Large Language Models for Security Bug Report Prediction",
    "summary": "arXiv:2601.22921v1 Announce Type: cross Abstract: Early detection of security bug reports (SBRs) is critical for timely vulnerability mitigation. We present an evaluation of prompt-based engineering and fine-tuning approaches for predicting SBRs using Large Language Models (LLMs). Our findings reveal a distinct trade-off between the two approaches.",
    "url": "https://arxiv.org/abs/2601.22921",
    "source": "Arxiv AI"
  },
  {
    "title": "BEAR: Towards Beam-Search-Aware Optimization for Recommendation with Large Language Models",
    "summary": "arXiv:2601.22925v1 Announce Type: cross Abstract: Recent years have witnessed a rapid surge in research leveraging Large Language Models (LLMs) for recommendation. These methods typically employ supervised fine-tuning (SFT) to adapt LLMs to recommendation scenarios, and utilize beam search during inference to efficiently retrieve $B$ top-ranked rec",
    "url": "https://arxiv.org/abs/2601.22925",
    "source": "Arxiv AI"
  },
  {
    "title": "MTDrive: Multi-turn Interactive Reinforcement Learning for Autonomous Driving",
    "summary": "arXiv:2601.22930v1 Announce Type: cross Abstract: Trajectory planning is a core task in autonomous driving, requiring the prediction of safe and comfortable paths across diverse scenarios. Integrating Multi-modal Large Language Models (MLLMs) with Reinforcement Learning (RL) has shown promise in addressing \"long-tail\" scenarios. However, existing m",
    "url": "https://arxiv.org/abs/2601.22930",
    "source": "Arxiv AI"
  },
  {
    "title": "Protecting Private Code in IDE Autocomplete using Differential Privacy",
    "summary": "arXiv:2601.22935v1 Announce Type: cross Abstract: Modern Integrated Development Environments (IDEs) increasingly leverage Large Language Models (LLMs) to provide advanced features like code autocomplete. While powerful, training these models on user-written code introduces significant privacy risks, making the models themselves a new type of data v",
    "url": "https://arxiv.org/abs/2601.22935",
    "source": "Arxiv AI"
  },
  {
    "title": "A Real-Time Privacy-Preserving Behavior Recognition System via Edge-Cloud Collaboration",
    "summary": "arXiv:2601.22938v1 Announce Type: cross Abstract: As intelligent sensing expands into high-privacy environments such as restrooms and changing rooms, the field faces a critical privacy-security paradox. Traditional RGB surveillance raises significant concerns regarding visual recording and storage, while existing privacy-preserving methods-ranging ",
    "url": "https://arxiv.org/abs/2601.22938",
    "source": "Arxiv AI"
  },
  {
    "title": "From Data Leak to Secret Misses: The Impact of Data Leakage on Secret Detection Models",
    "summary": "arXiv:2601.22946v1 Announce Type: cross Abstract: Machine learning models are increasingly used for software security tasks. These models are commonly trained and evaluated on large Internet-derived datasets, which often contain duplicated or highly similar samples. When such samples are split across training and test sets, data leakage may occur, ",
    "url": "https://arxiv.org/abs/2601.22946",
    "source": "Arxiv AI"
  },
  {
    "title": "Perplexity Cannot Always Tell Right from Wrong",
    "summary": "arXiv:2601.22950v1 Announce Type: cross Abstract: Perplexity -- a function measuring a model's overall level of \"surprise\" when encountering a particular output -- has gained significant traction in recent years, both as a loss function and as a simple-to-compute metric of model quality. Prior studies have pointed out several limitations of perplex",
    "url": "https://arxiv.org/abs/2601.22950",
    "source": "Arxiv AI"
  },
  {
    "title": "Residual Context Diffusion Language Models",
    "summary": "arXiv:2601.22954v1 Announce Type: cross Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to purely autoregressive language models because they can decode multiple tokens in parallel. However, state-of-the-art block-wise dLLMs rely on a \"remasking\" mechanism that decodes only the most confident tokens and dis",
    "url": "https://arxiv.org/abs/2601.22954",
    "source": "Arxiv AI"
  },
  {
    "title": "Stabilizing the Q-Gradient Field for Policy Smoothness in Actor-Critic",
    "summary": "arXiv:2601.22970v1 Announce Type: cross Abstract: Policies learned via continuous actor-critic methods often exhibit erratic, high-frequency oscillations, making them unsuitable for physical deployment. Current approaches attempt to enforce smoothness by directly regularizing the policy's output. We argue that this approach treats the symptom rathe",
    "url": "https://arxiv.org/abs/2601.22970",
    "source": "Arxiv AI"
  },
  {
    "title": "About an Automating Annotation Method for Robot Markers",
    "summary": "arXiv:2601.22982v1 Announce Type: cross Abstract: Factory automation has become increasingly important due to labor shortages, leading to the introduction of autonomous mobile robots for tasks such as material transportation. Markers are commonly used for robot self-localization and object identification. In the RoboCup Logistics League (RCLL), ArU",
    "url": "https://arxiv.org/abs/2601.22982",
    "source": "Arxiv AI"
  },
  {
    "title": "Self-Supervised Slice-to-Volume Reconstruction with Gaussian Representations for Fetal MRI",
    "summary": "arXiv:2601.22990v1 Announce Type: cross Abstract: Reconstructing 3D fetal MR volumes from motion-corrupted stacks of 2D slices is a crucial and challenging task. Conventional slice-to-volume reconstruction (SVR) methods are time-consuming and require multiple orthogonal stacks for reconstruction. While learning-based SVR approaches have significant",
    "url": "https://arxiv.org/abs/2601.22990",
    "source": "Arxiv AI"
  },
  {
    "title": "Mano: Restriking Manifold Optimization for LLM Training",
    "summary": "arXiv:2601.23000v1 Announce Type: cross Abstract: While large language models (LLMs) have emerged as a significant advancement in artificial intelligence, the hardware and computational costs for training LLMs are also significantly burdensome. Among the state-of-the-art optimizers, AdamW relies on diagonal curvature estimates and ignores structura",
    "url": "https://arxiv.org/abs/2601.23000",
    "source": "Arxiv AI"
  },
  {
    "title": "Bias Beyond Borders: Political Ideology Evaluation and Steering in Multilingual LLMs",
    "summary": "arXiv:2601.23001v1 Announce Type: cross Abstract: Large Language Models (LLMs) increasingly shape global discourse, making fairness and ideological neutrality essential for responsible AI deployment. Despite growing attention to political bias in LLMs, prior work largely focuses on high-resource, Western languages or narrow multilingual settings, l",
    "url": "https://arxiv.org/abs/2601.23001",
    "source": "Arxiv AI"
  },
  {
    "title": "Automatic Constraint Policy Optimization based on Continuous Constraint Interpolation Framework for Offline Reinforcement Learning",
    "summary": "arXiv:2601.23010v1 Announce Type: cross Abstract: Offline Reinforcement Learning (RL) relies on policy constraints to mitigate extrapolation error, where both the constraint form and constraint strength critically shape performance. However, most existing methods commit to a single constraint family: weighted behavior cloning, density regularizatio",
    "url": "https://arxiv.org/abs/2601.23010",
    "source": "Arxiv AI"
  },
  {
    "title": "Leveraging Convolutional Sparse Autoencoders for Robust Movement Classification from Low-Density sEMG",
    "summary": "arXiv:2601.23011v1 Announce Type: cross Abstract: Reliable control of myoelectric prostheses is often hindered by high inter-subject variability and the clinical impracticality of high-density sensor arrays. This study proposes a deep learning framework for accurate gesture recognition using only two surface electromyography (sEMG) channels. The me",
    "url": "https://arxiv.org/abs/2601.23011",
    "source": "Arxiv AI"
  },
  {
    "title": "Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference",
    "summary": "arXiv:2601.23039v1 Announce Type: cross Abstract: Differentiable matching layers, often implemented via entropy-regularized Optimal Transport, serve as a critical approximate inference mechanism in structural prediction. However, recovering discrete permutations via annealing $\\epsilon \\to 0$ is notoriously unstable. We identify a fundamental mecha",
    "url": "https://arxiv.org/abs/2601.23039",
    "source": "Arxiv AI"
  },
  {
    "title": "Adaptive Edge Learning for Density-Aware Graph Generation",
    "summary": "arXiv:2601.23052v1 Announce Type: cross Abstract: Generating realistic graph-structured data is challenging due to discrete structures, variable sizes, and class-specific connectivity patterns that resist conventional generative modelling. While recent graph generation methods employ generative adversarial network (GAN) frameworks to handle permuta",
    "url": "https://arxiv.org/abs/2601.23052",
    "source": "Arxiv AI"
  },
  {
    "title": "On the Impact of Code Comments for Automated Bug-Fixing: An Empirical Study",
    "summary": "arXiv:2601.23059v1 Announce Type: cross Abstract: Large Language Models (LLMs) are increasingly relevant in Software Engineering research and practice, with Automated Bug Fixing (ABF) being one of their key applications. ABF involves transforming a buggy method into its fixed equivalent. A common preprocessing step in ABF involves removing comments",
    "url": "https://arxiv.org/abs/2601.23059",
    "source": "Arxiv AI"
  },
  {
    "title": "HierLoc: Hyperbolic Entity Embeddings for Hierarchical Visual Geolocation",
    "summary": "arXiv:2601.23064v1 Announce Type: cross Abstract: Visual geolocalization, the task of predicting where an image was taken, remains challenging due to global scale, visual ambiguity, and the inherently hierarchical structure of geography. Existing paradigms rely on either large-scale retrieval, which requires storing a large number of image embeddin",
    "url": "https://arxiv.org/abs/2601.23064",
    "source": "Arxiv AI"
  },
  {
    "title": "Towards Explicit Acoustic Evidence Perception in Audio LLMs for Speech Deepfake Detection",
    "summary": "arXiv:2601.23066v1 Announce Type: cross Abstract: Speech deepfake detection (SDD) focuses on identifying whether a given speech signal is genuine or has been synthetically generated. Existing audio large language model (LLM)-based methods excel in content understanding; however, their predictions are often biased toward semantically correlated cues",
    "url": "https://arxiv.org/abs/2601.23066",
    "source": "Arxiv AI"
  },
  {
    "title": "ExplainerPFN: Towards tabular foundation models for model-free zero-shot feature importance estimations",
    "summary": "arXiv:2601.23068v1 Announce Type: cross Abstract: Computing the importance of features in supervised classification tasks is critical for model interpretability. Shapley values are a widely used approach for explaining model predictions, but require direct access to the underlying model, an assumption frequently violated in real-world deployments. ",
    "url": "https://arxiv.org/abs/2601.23068",
    "source": "Arxiv AI"
  },
  {
    "title": "Character as a Latent Variable in Large Language Models: A Mechanistic Account of Emergent Misalignment and Conditional Safety Failures",
    "summary": "arXiv:2601.23081v1 Announce Type: cross Abstract: Emergent Misalignment refers to a failure mode in which fine-tuning large language models (LLMs) on narrowly scoped data induces broadly misaligned behavior. Prior explanations mainly attribute this phenomenon to the generalization of erroneous or unsafe content. In this work, we show that this view",
    "url": "https://arxiv.org/abs/2601.23081",
    "source": "Arxiv AI"
  },
  {
    "title": "OrLog: Resolving Complex Queries with LLMs and Probabilistic Reasoning",
    "summary": "arXiv:2601.23085v1 Announce Type: cross Abstract: Resolving complex information needs that come with multiple constraints should consider enforcing the logical operators encoded in the query (i.e., conjunction, disjunction, negation) on the candidate answer set. Current retrieval systems either ignore these constraints in neural embeddings or appro",
    "url": "https://arxiv.org/abs/2601.23085",
    "source": "Arxiv AI"
  },
  {
    "title": "From Similarity to Vulnerability: Key Collision Attack on LLM Semantic Caching",
    "summary": "arXiv:2601.23088v1 Announce Type: cross Abstract: Semantic caching has emerged as a pivotal technique for scaling LLM applications, widely adopted by major providers including AWS and Microsoft. By utilizing semantic embedding vectors as cache keys, this mechanism effectively minimizes latency and redundant computation for semantically similar quer",
    "url": "https://arxiv.org/abs/2601.23088",
    "source": "Arxiv AI"
  },
  {
    "title": "WiFiPenTester: Advancing Wireless Ethical Hacking with Governed GenAI",
    "summary": "arXiv:2601.23092v1 Announce Type: cross Abstract: Wireless ethical hacking relies heavily on skilled practitioners manually interpreting reconnaissance results and executing complex, time-sensitive sequences of commands to identify vulnerable targets, capture authentication handshakes, and assess password resilience; a process that is inherently la",
    "url": "https://arxiv.org/abs/2601.23092",
    "source": "Arxiv AI"
  },
  {
    "title": "To See Far, Look Close: Evolutionary Forecasting for Long-term Time Series",
    "summary": "arXiv:2601.23114v1 Announce Type: cross Abstract: The prevailing Direct Forecasting (DF) paradigm dominates Long-term Time Series Forecasting (LTSF) by forcing models to predict the entire future horizon in a single forward pass. While efficient, this rigid coupling of output and evaluation horizons necessitates computationally prohibitive re-train",
    "url": "https://arxiv.org/abs/2601.23114",
    "source": "Arxiv AI"
  },
  {
    "title": "Regularisation in neural networks: a survey and empirical analysis of approaches",
    "summary": "arXiv:2601.23131v1 Announce Type: cross Abstract: Despite huge successes on a wide range of tasks, neural networks are known to sometimes struggle to generalise to unseen data. Many approaches have been proposed over the years to promote the generalisation ability of neural networks, collectively known as regularisation techniques. These are used a",
    "url": "https://arxiv.org/abs/2601.23131",
    "source": "Arxiv AI"
  },
  {
    "title": "Secure Tool Manifest and Digital Signing Solution for Verifiable MCP and LLM Pipelines",
    "summary": "arXiv:2601.23132v1 Announce Type: cross Abstract: Large Language Models (LLMs) are increasingly adopted in sensitive domains such as healthcare and financial institutions' data analytics; however, their execution pipelines remain vulnerable to manipulation and unverifiable behavior. Existing control mechanisms, such as the Model Context Protocol (M",
    "url": "https://arxiv.org/abs/2601.23132",
    "source": "Arxiv AI"
  },
  {
    "title": "Machine Learning for Energy-Performance-aware Scheduling",
    "summary": "arXiv:2601.23134v1 Announce Type: cross Abstract: In the post-Dennard era, optimizing embedded systems requires navigating complex trade-offs between energy efficiency and latency. Traditional heuristic tuning is often inefficient in such high-dimensional, non-smooth landscapes. In this work, we propose a Bayesian Optimization framework using Gauss",
    "url": "https://arxiv.org/abs/2601.23134",
    "source": "Arxiv AI"
  },
  {
    "title": "Securing Time in Energy IoT: A Clock-Dynamics-Aware Spatio-Temporal Graph Attention Network for Clock Drift Attacks and Y2K38 Failures",
    "summary": "arXiv:2601.23147v1 Announce Type: cross Abstract: The integrity of time in distributed Internet of Things (IoT) devices is crucial for reliable operation in energy cyber-physical systems, such as smart grids and microgrids. However, IoT systems are vulnerable to clock drift, time-synchronization manipulation, and timestamp discontinuities, such as ",
    "url": "https://arxiv.org/abs/2601.23147",
    "source": "Arxiv AI"
  },
  {
    "title": "On Safer Reinforcement Learning Policies for Sedation and Analgesia in Intensive Care",
    "summary": "arXiv:2601.23154v1 Announce Type: cross Abstract: Pain management in intensive care usually involves complex trade-offs between therapeutic goals and patient safety, since both inadequate and excessive treatment may induce serious sequelae. Reinforcement learning can help address this challenge by learning medication dosing policies from retrospect",
    "url": "https://arxiv.org/abs/2601.23154",
    "source": "Arxiv AI"
  },
  {
    "title": "SPICE: Submodular Penalized Information-Conflict Selection for Efficient Large Language Model Training",
    "summary": "arXiv:2601.23155v1 Announce Type: cross Abstract: Information-based data selection for instruction tuning is compelling: maximizing the log-determinant of the Fisher information yields a monotone submodular objective, enabling greedy algorithms to achieve a $(1-1/e)$ approximation under a cardinality budget. In practice, however, we identify allevi",
    "url": "https://arxiv.org/abs/2601.23155",
    "source": "Arxiv AI"
  },
  {
    "title": "Probing the Trajectories of Reasoning Traces in Large Language Models",
    "summary": "arXiv:2601.23163v1 Announce Type: cross Abstract: Large language models (LLMs) increasingly solve difficult problems by producing \"reasoning traces\" before emitting a final response. However, it remains unclear how accuracy and decision commitment evolve along a reasoning trajectory, and whether intermediate trace segments provide answer-relevant i",
    "url": "https://arxiv.org/abs/2601.23163",
    "source": "Arxiv AI"
  },
  {
    "title": "Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization",
    "summary": "arXiv:2601.23174v1 Announce Type: cross Abstract: Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessa",
    "url": "https://arxiv.org/abs/2601.23174",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning to Execute Graph Algorithms Exactly with Graph Neural Networks",
    "summary": "arXiv:2601.23207v1 Announce Type: cross Abstract: Understanding what graph neural networks can learn, especially their ability to learn to execute algorithms, remains a central theoretical challenge. In this work, we prove exact learnability results for graph algorithms under bounded-degree and finite-precision constraints. Our approach follows a t",
    "url": "https://arxiv.org/abs/2601.23207",
    "source": "Arxiv AI"
  },
  {
    "title": "Disentangling multispecific antibody function with graph neural networks",
    "summary": "arXiv:2601.23212v1 Announce Type: cross Abstract: Multispecific antibodies offer transformative therapeutic potential by engaging multiple epitopes simultaneously, yet their efficacy is an emergent property governed by complex molecular architectures. Rational design is often bottlenecked by the inability to predict how subtle changes in domain top",
    "url": "https://arxiv.org/abs/2601.23212",
    "source": "Arxiv AI"
  },
  {
    "title": "MonoScale: Scaling Multi-Agent System with Monotonic Improvement",
    "summary": "arXiv:2601.23219v1 Announce Type: cross Abstract: In recent years, LLM-based multi-agent systems (MAS) have advanced rapidly, using a router to decompose tasks and delegate subtasks to specialized agents. A natural way to expand capability is to scale up the agent pool by continually integrating new functional agents or tool interfaces, but naive e",
    "url": "https://arxiv.org/abs/2601.23219",
    "source": "Arxiv AI"
  },
  {
    "title": "Med-Scout: Curing MLLMs' Geometric Blindness in Medical Perception via Geometry-Aware RL Post-Training",
    "summary": "arXiv:2601.23220v1 Announce Type: cross Abstract: Despite recent Multimodal Large Language Models (MLLMs)' linguistic prowess in medical diagnosis, we find even state-of-the-art MLLMs suffer from a critical perceptual deficit: geometric blindness. This failure to ground outputs in objective geometric constraints leads to plausible yet factually inc",
    "url": "https://arxiv.org/abs/2601.23220",
    "source": "Arxiv AI"
  },
  {
    "title": "Agile Reinforcement Learning through Separable Neural Architecture",
    "summary": "arXiv:2601.23225v1 Announce Type: cross Abstract: Deep reinforcement learning (RL) is increasingly deployed in resource-constrained environments, yet the go-to function approximators - multilayer perceptrons (MLPs) - are often parameter-inefficient due to an imperfect inductive bias for the smooth structure of many value functions. This mismatch ca",
    "url": "https://arxiv.org/abs/2601.23225",
    "source": "Arxiv AI"
  },
  {
    "title": "ShotFinder: Imagination-Driven Open-Domain Video Shot Retrieval via Web Search",
    "summary": "arXiv:2601.23232v1 Announce Type: cross Abstract: In recent years, large language models (LLMs) have made rapid progress in information retrieval, yet existing research has mainly focused on text or static multimodal settings. Open-domain video shot retrieval, which involves richer temporal structure and more complex semantics, still lacks systemat",
    "url": "https://arxiv.org/abs/2601.23232",
    "source": "Arxiv AI"
  },
  {
    "title": "YuriiFormer: A Suite of Nesterov-Accelerated Transformers",
    "summary": "arXiv:2601.23236v1 Announce Type: cross Abstract: We propose a variational framework that interprets transformer layers as iterations of an optimization algorithm acting on token embeddings. In this view, self-attention implements a gradient step of an interaction energy, while MLP layers correspond to gradient updates of a potential energy. Standa",
    "url": "https://arxiv.org/abs/2601.23236",
    "source": "Arxiv AI"
  },
  {
    "title": "Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models",
    "summary": "arXiv:2601.23255v1 Announce Type: cross Abstract: Large audio-language models increasingly operate on raw speech inputs, enabling more seamless integration across domains such as voice assistants, education, and clinical triage. This transition, however, introduces a distinct class of vulnerabilities that remain largely uncharacterized. We examine ",
    "url": "https://arxiv.org/abs/2601.23255",
    "source": "Arxiv AI"
  },
  {
    "title": "Agnostic Language Identification and Generation",
    "summary": "arXiv:2601.23258v1 Announce Type: cross Abstract: Recent works on language identification and generation have established tight statistical rates at which these tasks can be achieved. These works typically operate under a strong realizability assumption: that the input data is drawn from an unknown distribution necessarily supported on some languag",
    "url": "https://arxiv.org/abs/2601.23258",
    "source": "Arxiv AI"
  },
  {
    "title": "TEON: Tensorized Orthonormalization Beyond Layer-Wise Muon for Large Language Model Pre-Training",
    "summary": "arXiv:2601.23261v1 Announce Type: cross Abstract: The Muon optimizer has demonstrated strong empirical performance in pre-training large language models by performing matrix-level gradient (or momentum) orthogonalization in each layer independently. In this work, we propose TEON, a principled generalization of Muon that extends orthogonalization be",
    "url": "https://arxiv.org/abs/2601.23261",
    "source": "Arxiv AI"
  },
  {
    "title": "IRL-DAL: Safe and Adaptive Trajectory Planning for Autonomous Driving via Energy-Guided Diffusion Models",
    "summary": "arXiv:2601.23266v1 Announce Type: cross Abstract: This paper proposes a novel inverse reinforcement learning framework using a diffusion-based adaptive lookahead planner (IRL-DAL) for autonomous vehicles. Training begins with imitation from an expert finite state machine (FSM) controller to provide a stable initialization. Environment terms are com",
    "url": "https://arxiv.org/abs/2601.23266",
    "source": "Arxiv AI"
  },
  {
    "title": "End-to-end Optimization of Belief and Policy Learning in Shared Autonomy Paradigms",
    "summary": "arXiv:2601.23285v1 Announce Type: cross Abstract: Shared autonomy systems require principled methods for inferring user intent and determining appropriate assistance levels. This is a central challenge in human-robot interaction, where systems must be successful while being mindful of user agency. Previous approaches relied on static blending ratio",
    "url": "https://arxiv.org/abs/2601.23285",
    "source": "Arxiv AI"
  },
  {
    "title": "VideoGPA: Distilling Geometry Priors for 3D-Consistent Video Generation",
    "summary": "arXiv:2601.23286v1 Announce Type: cross Abstract: While recent video diffusion models (VDMs) produce visually impressive results, they fundamentally struggle to maintain 3D structural consistency, often resulting in object deformation or spatial drift. We hypothesize that these failures arise because standard denoising objectives lack explicit ince",
    "url": "https://arxiv.org/abs/2601.23286",
    "source": "Arxiv AI"
  },
  {
    "title": "Ambig-SWE: Interactive Agents to Overcome Underspecificity in Software Engineering",
    "summary": "arXiv:2502.13069v2 Announce Type: replace Abstract: AI agents are increasingly being deployed to automate tasks, often based on underspecified user instructions. Making unwarranted assumptions to compensate for the missing information and failing to ask clarifying questions can lead to suboptimal outcomes, safety risks due to tool misuse, and waste",
    "url": "https://arxiv.org/abs/2502.13069",
    "source": "Arxiv AI"
  },
  {
    "title": "Lost in Transmission: When and Why LLMs Fail to Reason Globally",
    "summary": "arXiv:2505.08140v5 Announce Type: replace Abstract: Despite their many successes, transformer-based large language models (LLMs) continue to struggle with tasks that require complex reasoning over large parts of their input. We argue that these failures arise due to capacity limits on the accurate flow of information within LLMs. To formalize this ",
    "url": "https://arxiv.org/abs/2505.08140",
    "source": "Arxiv AI"
  },
  {
    "title": "Language Models That Walk the Talk: A Framework for Formal Fairness Certificates",
    "summary": "arXiv:2505.12767v2 Announce Type: replace Abstract: As large language models become integral to high-stakes applications, ensuring their robustness and fairness is critical. Despite their success, large language models remain vulnerable to adversarial attacks, where small perturbations, such as synonym substitutions, can alter model predictions, po",
    "url": "https://arxiv.org/abs/2505.12767",
    "source": "Arxiv AI"
  },
  {
    "title": "Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings",
    "summary": "arXiv:2505.13718v3 Announce Type: replace Abstract: Designing effective reasoning-capable LLMs typically requires training using Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on extensive training data. This creates a major challenge when the a",
    "url": "https://arxiv.org/abs/2505.13718",
    "source": "Arxiv AI"
  },
  {
    "title": "Identification of Probabilities of Causation: from Recursive to Closed-Form Bounds",
    "summary": "arXiv:2505.15274v3 Announce Type: replace Abstract: Probabilities of causation (PoCs) are fundamental quantities for counterfactual analysis and personalized decision making. However, existing analytical results are largely confined to binary settings. This paper extends PoCs to multi-valued treatments and outcomes by deriving closed form bounds fo",
    "url": "https://arxiv.org/abs/2505.15274",
    "source": "Arxiv AI"
  },
  {
    "title": "FloorplanQA: A Benchmark for Spatial Reasoning in LLMs using Structured Representations",
    "summary": "arXiv:2507.07644v3 Announce Type: replace Abstract: We introduce FloorplanQA, a diagnostic benchmark for evaluating spatial reasoning in large-language models (LLMs). FloorplanQA is grounded in structured representations of indoor scenes, such as (e.g., kitchens, living rooms, bedrooms, bathrooms, and others), encoded symbolically in JSON or XML la",
    "url": "https://arxiv.org/abs/2507.07644",
    "source": "Arxiv AI"
  },
  {
    "title": "Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces",
    "summary": "arXiv:2507.11352v2 Announce Type: replace Abstract: Logistics operators, from battlefield coordinators re-routing airlifts ahead of a storm to warehouse managers juggling late trucks, need to make mission-critical decisions. Prevailing methods for logistics planning such as integer programming yield plans that satisfy user-defined logical constrain",
    "url": "https://arxiv.org/abs/2507.11352",
    "source": "Arxiv AI"
  },
  {
    "title": "Thinking Machines: Mathematical Reasoning in the Age of LLMs",
    "summary": "arXiv:2508.00459v2 Announce Type: replace Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in structured reasoning and symbolic tasks, with coding emerging as a particularly successful application. This progress has naturally motivated efforts to extend these models to mathematics, both in its traditional form, expre",
    "url": "https://arxiv.org/abs/2508.00459",
    "source": "Arxiv AI"
  },
  {
    "title": "Social World Models",
    "summary": "arXiv:2509.00559v2 Announce Type: replace Abstract: Humans intuitively navigate social interactions by simulating unspoken dynamics and reasoning about others' perspectives, even with limited information. In contrast, AI systems struggle to structure and reason about implicit social contexts, as they lack explicit representations for unobserved dyn",
    "url": "https://arxiv.org/abs/2509.00559",
    "source": "Arxiv AI"
  },
  {
    "title": "RAFFLES: Reasoning-based Attribution of Faults for LLM Systems",
    "summary": "arXiv:2509.06822v3 Announce Type: replace Abstract: The advent of complex, interconnected long-horizon LLM systems has made it incredibly tricky to identify where and when these systems break down. Evaluation capabilities that currently exist today are limited in that they often focus on simple metrics, end-to-end outcomes, and are dependent on the",
    "url": "https://arxiv.org/abs/2509.06822",
    "source": "Arxiv AI"
  },
  {
    "title": "Leveraging AI Agents for Autonomous Networks: A Reference Architecture and Empirical Studies",
    "summary": "arXiv:2509.08312v2 Announce Type: replace Abstract: The evolution toward Level 4 (L4) Autonomous Networks (AN) represents a strategic inflection point in telecommunications, where networks must transcend reactive automation to achieve genuine cognitive capabilities--fulfilling TM Forum's vision of self-configuring, self-healing, and self-optimizing",
    "url": "https://arxiv.org/abs/2509.08312",
    "source": "Arxiv AI"
  },
  {
    "title": "EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models",
    "summary": "arXiv:2509.11914v2 Announce Type: replace Abstract: We introduce EgoMem, the first lifelong memory agent tailored for full-duplex models that process real-time omnimodal streams. EgoMem enables real-time models to recognize multiple users directly from raw audiovisual streams, to provide personalized response, and to maintain long-term knowledge of",
    "url": "https://arxiv.org/abs/2509.11914",
    "source": "Arxiv AI"
  },
  {
    "title": "Self-Improvement of Language Models by Post-Training on Multi-Agent Debate",
    "summary": "arXiv:2509.15172v3 Announce Type: replace Abstract: Self-improvement, where models improve beyond their current performance without external supervision, remains a challenge. The core difficulty is sourcing a training signal stronger than what the model itself can currently produce. Majority voting has been shown to provide such a signal by aggrega",
    "url": "https://arxiv.org/abs/2509.15172",
    "source": "Arxiv AI"
  },
  {
    "title": "FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs",
    "summary": "arXiv:2509.16648v4 Announce Type: replace Abstract: The accurate trust assessment of multimodal large language models (MLLMs) generated predictions, which can enable selective prediction and improve user confidence, is challenging due to the diverse multi-modal input paradigms. We propose Functionally Equivalent Sampling for Trust Assessment (FESTA",
    "url": "https://arxiv.org/abs/2509.16648",
    "source": "Arxiv AI"
  },
  {
    "title": "Lifelong Learning with Behavior Consolidation for Vehicle Routing",
    "summary": "arXiv:2509.21765v3 Announce Type: replace Abstract: Recent neural solvers have demonstrated promising performance in learning to solve routing problems. However, existing studies are primarily based on one-off training on one or a set of predefined problem distributions and scales, i.e., tasks. When a new task arises, they typically rely on either ",
    "url": "https://arxiv.org/abs/2509.21765",
    "source": "Arxiv AI"
  },
  {
    "title": "Collaborative Belief Reasoning with LLMs for Efficient Multi-Agent Collaboration",
    "summary": "arXiv:2509.21981v3 Announce Type: replace Abstract: Effective real-world multi-agent collaboration requires not only accurate planning but also the ability to reason about collaborators' intents--a crucial capability for avoiding miscoordination and redundant communication under partial observable environments. Due to their strong planning and reas",
    "url": "https://arxiv.org/abs/2509.21981",
    "source": "Arxiv AI"
  },
  {
    "title": "IRIS: Intrinsic Reward Image Synthesis",
    "summary": "arXiv:2509.25562v2 Announce Type: replace Abstract: Despite the success of Reinforcement Learning from Human Feedback (RLHF) in language reasoning, its application to autoregressive Text-to-Image (T2I) generation is often constrained by the limited availability of human preference data. This paper explores how an autoregressive T2I model can learn ",
    "url": "https://arxiv.org/abs/2509.25562",
    "source": "Arxiv AI"
  },
  {
    "title": "InvThink: Towards AI Safety via Inverse Reasoning",
    "summary": "arXiv:2510.01569v2 Announce Type: replace Abstract: We present InvThink, a simple yet powerful approach that gives language models the capability of inverse thinking: reasoning through failure modes before generating responses. Unlike existing safety alignment methods that optimize directly for safe response, InvThink instructs models to 1) enumera",
    "url": "https://arxiv.org/abs/2510.01569",
    "source": "Arxiv AI"
  },
  {
    "title": "On the Provable Performance Guarantee of Efficient Reasoning Models",
    "summary": "arXiv:2510.09133v2 Announce Type: replace Abstract: Large reasoning models (LRMs) have achieved remarkable progress in complex problem-solving tasks. Despite this success, LRMs typically suffer from high computational costs during deployment, highlighting a need for efficient inference. A practical direction of efficiency improvement is to switch t",
    "url": "https://arxiv.org/abs/2510.09133",
    "source": "Arxiv AI"
  },
  {
    "title": "Don't Just Fine-tune the Agent, Tune the Environment",
    "summary": "arXiv:2510.10197v2 Announce Type: replace Abstract: Large Language Model (LLM) agents show great promise for complex, multi-turn tool-use tasks, but their development is often hampered by the extreme scarcity of high-quality training data. Supervised fine-tuning (SFT) on synthetic data leads to overfitting, whereas standard reinforcement learning (",
    "url": "https://arxiv.org/abs/2510.10197",
    "source": "Arxiv AI"
  },
  {
    "title": "PaperArena: An Evaluation Benchmark for Tool-Augmented Agentic Reasoning on Scientific Literature",
    "summary": "arXiv:2510.10909v4 Announce Type: replace Abstract: Understanding and reasoning on the large-scale scientific literature is a crucial touchstone for large language model (LLM) based agents. However, existing works are mainly restricted to tool-free tasks within single papers, largely due to the lack of a benchmark that evaluates cross-paper reasoni",
    "url": "https://arxiv.org/abs/2510.10909",
    "source": "Arxiv AI"
  },
  {
    "title": "Are Agents Probabilistic Automata? A Trace-Based, Memory-Constrained Theory of Agentic AI",
    "summary": "arXiv:2510.23487v2 Announce Type: replace Abstract: This paper studies standard controller architectures for agentic AI and derives automata-theoretic models of their interaction behavior via trace semantics and abstraction. We model an agent implementation as a finite control program augmented with explicit memory primitives (bounded buffers, a ca",
    "url": "https://arxiv.org/abs/2510.23487",
    "source": "Arxiv AI"
  },
  {
    "title": "An Aristotelian ontology of instrumental goals: Structural features to be managed and not failures to be eliminated",
    "summary": "arXiv:2510.25471v2 Announce Type: replace Abstract: Instrumental goals such as resource acquisition, power-seeking, and self-preservation are key to contemporary AI alignment research, yet the phenomenon's ontology remains under-theorised. This article develops an ontological account of instrumental goals and draws out governance-relevant distincti",
    "url": "https://arxiv.org/abs/2510.25471",
    "source": "Arxiv AI"
  },
  {
    "title": "BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning",
    "summary": "arXiv:2510.26374v3 Announce Type: replace Abstract: Reinforcement finetuning (RFT) is a key technique for aligning Large Language Models (LLMs) with human preferences and enhancing reasoning, yet its effectiveness is highly sensitive to which tasks are explored during training. Uniform task sampling is inefficient, wasting computation on tasks that",
    "url": "https://arxiv.org/abs/2510.26374",
    "source": "Arxiv AI"
  },
  {
    "title": "CATArena: Evaluating Evolutionary Capabilities of Code Agents via Iterative Tournaments",
    "summary": "arXiv:2510.26852v2 Announce Type: replace Abstract: Current evaluation for Large Language Model (LLM) code agents predominantly focus on generating functional code in single-turn scenarios, which fails to evaluate the agent's capability for continuous code optimization and multi-turn iterative development. To bridge this gap, we introduce CATArena,",
    "url": "https://arxiv.org/abs/2510.26852",
    "source": "Arxiv AI"
  },
  {
    "title": "Closing the Expression Gap in LLM Instructions via Socratic Questioning",
    "summary": "arXiv:2510.27410v3 Announce Type: replace Abstract: A fundamental bottleneck in human-AI collaboration is the ``intention expression gap,\" the difficulty for humans to effectively convey complex, high-dimensional thoughts to AI. This challenge often traps users in inefficient trial-and-error loops and is exacerbated by the diverse expertise levels ",
    "url": "https://arxiv.org/abs/2510.27410",
    "source": "Arxiv AI"
  },
  {
    "title": "Enhancing Logical Expressiveness in Graph Neural Networks via Path-Neighbor Aggregation",
    "summary": "arXiv:2511.07994v3 Announce Type: replace Abstract: Graph neural networks (GNNs) can effectively model structural information of graphs, making them widely used in knowledge graph (KG) reasoning. However, existing studies on the expressive power of GNNs mainly focuses on simple single-relation graphs, and there is still insufficient discussion on t",
    "url": "https://arxiv.org/abs/2511.07994",
    "source": "Arxiv AI"
  },
  {
    "title": "Learning to Pose Problems: Reasoning-Driven and Solver-Adaptive Data Synthesis for Large Reasoning Models",
    "summary": "arXiv:2511.09907v4 Announce Type: replace Abstract: Data synthesis for training large reasoning models offers a scalable alternative to limited, human-curated datasets, enabling the creation of high-quality data. However, existing approaches face several challenges: (i) indiscriminate generation that ignores the solver's ability and yields low-valu",
    "url": "https://arxiv.org/abs/2511.09907",
    "source": "Arxiv AI"
  },
  {
    "title": "ChartAnchor: Chart Grounding with Structural-Semantic Fidelity",
    "summary": "arXiv:2512.01017v3 Announce Type: replace Abstract: Recent advances in multimodal large language models (MLLMs) highlight the need for benchmarks that rigorously evaluate structured chart comprehension. Chart grounding refers to the bidirectional alignment between a chart's visual appearance and its structured semantics. This task requires models t",
    "url": "https://arxiv.org/abs/2512.01017",
    "source": "Arxiv AI"
  },
  {
    "title": "Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application",
    "summary": "arXiv:2512.19299v2 Announce Type: replace Abstract: In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awaren",
    "url": "https://arxiv.org/abs/2512.19299",
    "source": "Arxiv AI"
  },
  {
    "title": "Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing",
    "summary": "arXiv:2601.10543v2 Announce Type: replace Abstract: Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks",
    "url": "https://arxiv.org/abs/2601.10543",
    "source": "Arxiv AI"
  },
  {
    "title": "AgenticRed: Optimizing Agentic Systems for Automated Red-teaming",
    "summary": "arXiv:2601.13518v2 Announce Type: replace Abstract: While recent automated red-teaming methods show promise for systematically exposing model vulnerabilities, most existing approaches rely on human-specified workflows. This dependence on manually designed workflows suffers from human biases and makes exploring the broader design space expensive. We",
    "url": "https://arxiv.org/abs/2601.13518",
    "source": "Arxiv AI"
  },
  {
    "title": "Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety Testing",
    "summary": "arXiv:2601.18061v2 Announce Type: replace Abstract: Learning from human feedback~(LHF) assumes that expert judgments, appropriately aggregated, yield valid ground truth for training and evaluating AI systems. We tested this assumption in mental health, where high safety stakes make expert consensus essential. Three certified psychiatrists independe",
    "url": "https://arxiv.org/abs/2601.18061",
    "source": "Arxiv AI"
  },
  {
    "title": "RPO:Reinforcement Fine-Tuning with Partial Reasoning Optimization",
    "summary": "arXiv:2601.19404v2 Announce Type: replace Abstract: Within the domain of large language models, reinforcement fine-tuning algorithms necessitate the generation of a complete reasoning trajectory beginning from the input query, which incurs significant computational overhead during the rollout phase of training. To address this issue, we analyze the",
    "url": "https://arxiv.org/abs/2601.19404",
    "source": "Arxiv AI"
  },
  {
    "title": "Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities",
    "summary": "arXiv:2601.21937v2 Announce Type: replace Abstract: Despite strong performance on existing benchmarks, it remains unclear whether large language models can reason over genuinely novel scientific information. Most evaluations score end-to-end RAG pipelines, where reasoning is confounded with retrieval and toolchain choices, and the signal is further",
    "url": "https://arxiv.org/abs/2601.21937",
    "source": "Arxiv AI"
  },
  {
    "title": "Generative quantum machine learning via denoising diffusion probabilistic models",
    "summary": "arXiv:2310.05866v5 Announce Type: replace-cross Abstract: Deep generative models are key-enabling technology to computer vision, text generation, and large language models. Denoising diffusion probabilistic models (DDPMs) have recently gained much attention due to their ability to generate diverse and high-quality samples in many computer vision ta",
    "url": "https://arxiv.org/abs/2310.05866",
    "source": "Arxiv AI"
  },
  {
    "title": "XAI-CF -- Examining the Role of Explainable Artificial Intelligence in Cyber Forensics",
    "summary": "arXiv:2402.02452v3 Announce Type: replace-cross Abstract: With the rise of complex cyber devices Cyber Forensics (CF) is facing many new challenges. For example, there are dozens of systems running on smartphones, each with more than millions of downloadable applications. Sifting through this large amount of data and making sense requires new techn",
    "url": "https://arxiv.org/abs/2402.02452",
    "source": "Arxiv AI"
  },
  {
    "title": "Posterior Label Smoothing for Node Classification",
    "summary": "arXiv:2406.00410v3 Announce Type: replace-cross Abstract: Label smoothing is a widely studied regularization technique in machine learning. However, its potential for node classification in graph-structured data, spanning homophilic to heterophilic graphs, remains largely unexplored. We introduce posterior label smoothing, a novel method for transd",
    "url": "https://arxiv.org/abs/2406.00410",
    "source": "Arxiv AI"
  },
  {
    "title": "Unveiling and Mitigating Bias in Large Language Model Recommendations: A Path to Fairness",
    "summary": "arXiv:2409.10825v5 Announce Type: replace-cross Abstract: Large Language Model (LLM)-based recommendation systems excel in delivering comprehensive suggestions by deeply analyzing content and user behavior. However, they often inherit biases from skewed training data, favoring mainstream content while underrepresenting diverse or non-traditional op",
    "url": "https://arxiv.org/abs/2409.10825",
    "source": "Arxiv AI"
  },
  {
    "title": "ARB-LLM: Alternating Refined Binarizations for Large Language Models",
    "summary": "arXiv:2410.03129v3 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have greatly pushed forward advancements in natural language processing, yet their high memory and computational demands hinder practical deployment. Binarization, as an effective compression technique, can shrink model weights to just 1 bit, significantly reduci",
    "url": "https://arxiv.org/abs/2410.03129",
    "source": "Arxiv AI"
  },
  {
    "title": "A spatiotemporal fused network considering electrode spatial topology and time-window transition for MDD detection",
    "summary": "arXiv:2411.08521v4 Announce Type: replace-cross Abstract: Recently, researchers have begun to experiment with deep learning-based methods for detecting major depressive disor-der (MDD) using electroencephalogram (EEG) signals in search of a more objective means of diagnosis. However, exist-ing spatiotemporal feature extraction methods only consider",
    "url": "https://arxiv.org/abs/2411.08521",
    "source": "Arxiv AI"
  },
  {
    "title": "A Library for Learning Neural Operators",
    "summary": "arXiv:2412.10354v5 Announce Type: replace-cross Abstract: We present NeuralOperator, an open-source Python library for operator learning. Neural operators generalize neural networks to maps between function spaces instead of finite-dimensional Euclidean spaces. They can be trained and inferenced on input and output functions given at various discre",
    "url": "https://arxiv.org/abs/2412.10354",
    "source": "Arxiv AI"
  },
  {
    "title": "Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models",
    "summary": "arXiv:2501.13428v5 Announce Type: replace-cross Abstract: Large language models have achieved remarkable success in recent years, primarily due to self-attention. However, traditional Softmax attention suffers from numerical instability and reduced performance as the number of inference tokens increases. This work addresses these issues by proposin",
    "url": "https://arxiv.org/abs/2501.13428",
    "source": "Arxiv AI"
  },
  {
    "title": "Understanding Transformer Optimization via Gradient Heterogeneity",
    "summary": "arXiv:2502.00213v3 Announce Type: replace-cross Abstract: Transformers are difficult to optimize with stochastic gradient descent (SGD) and largely rely on adaptive optimizers such as Adam. Despite their empirical success, the reasons behind Adam's superior performance over SGD remain poorly understood. In this study, we analyze the optimization of",
    "url": "https://arxiv.org/abs/2502.00213",
    "source": "Arxiv AI"
  },
  {
    "title": "Causal Imitation Learning under Expert-Observable and Expert-Unobservable Confounding",
    "summary": "arXiv:2502.07656v2 Announce Type: replace-cross Abstract: We propose a general framework for causal Imitation Learning (IL) with hidden confounders, which subsumes several existing settings. Our framework accounts for two types of hidden confounders: (a) variables observed by the expert but not by the imitator, and (b) confounding noise hidden from",
    "url": "https://arxiv.org/abs/2502.07656",
    "source": "Arxiv AI"
  },
  {
    "title": "Ethical AI for Young Digital Citizens: A Call to Action on Privacy Governance",
    "summary": "arXiv:2503.11947v4 Announce Type: replace-cross Abstract: The rapid expansion of Artificial Intelligence (AI) in digital platforms used by youth has created significant challenges related to privacy, autonomy, and data protection. While AI-driven personalization offers enhanced user experiences, it often operates without clear ethical boundaries, l",
    "url": "https://arxiv.org/abs/2503.11947",
    "source": "Arxiv AI"
  },
  {
    "title": "FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs",
    "summary": "arXiv:2503.17229v3 Announce Type: replace-cross Abstract: Large Language Models (LLMs) frequently generate hallucinated content, posing significant challenges for applications where factuality is crucial. While existing hallucination detection methods typically operate at the sentence level or passage level, we propose FactSelfCheck, a novel zero-r",
    "url": "https://arxiv.org/abs/2503.17229",
    "source": "Arxiv AI"
  },
  {
    "title": "AccidentSim: Generating Vehicle Collision Videos with Physically Realistic Collision Trajectories from Real-World Accident Reports",
    "summary": "arXiv:2503.20654v2 Announce Type: replace-cross Abstract: Collecting real-world vehicle accident videos for autonomous driving research is challenging due to their rarity and complexity. While existing driving video generation methods may produce visually realistic videos, they often fail to deliver physically realistic simulations because they lac",
    "url": "https://arxiv.org/abs/2503.20654",
    "source": "Arxiv AI"
  },
  {
    "title": "Decentralized Domain Generalization with Style Sharing: Formal Model and Convergence Analysis",
    "summary": "arXiv:2504.06235v4 Announce Type: replace-cross Abstract: Much of federated learning (FL) focuses on settings where local dataset statistics remain the same between training and testing. However, this assumption often does not hold in practice due to distribution shifts, motivating the development of domain generalization (DG) approaches that lever",
    "url": "https://arxiv.org/abs/2504.06235",
    "source": "Arxiv AI"
  },
  {
    "title": "What Matters in Linearizing Language Models? A Comparative Study of Architecture, Scale, and Task Adaptation",
    "summary": "arXiv:2504.14366v3 Announce Type: replace-cross Abstract: Linearization has emerged as a strategy for developing efficient language models (LMs). Starting from an existing Transformer-based LM, linearization replaces the attention component with computationally efficient subquadratic \\textit{token mixers}. However, as an increasing number of mixers",
    "url": "https://arxiv.org/abs/2504.14366",
    "source": "Arxiv AI"
  },
  {
    "title": "Uncertainty Quantification for Prior-Data Fitted Networks using Martingale Posteriors",
    "summary": "arXiv:2505.11325v3 Announce Type: replace-cross Abstract: Prior-data fitted networks (PFNs) have emerged as promising foundation models for prediction from tabular data sets, achieving state-of-the-art performance on small to moderate data sizes without tuning. While PFNs are motivated by Bayesian ideas, they do not provide any uncertainty quantifi",
    "url": "https://arxiv.org/abs/2505.11325",
    "source": "Arxiv AI"
  },
  {
    "title": "SuperCoder: Assembly Program Superoptimization with Large Language Models",
    "summary": "arXiv:2505.11480v3 Announce Type: replace-cross Abstract: Superoptimization is the task of transforming a program into a faster one while preserving its input-output behavior. In this work, we investigate whether large language models (LLMs) can serve as superoptimizers, generating assembly programs that outperform code already optimized by industr",
    "url": "https://arxiv.org/abs/2505.11480",
    "source": "Arxiv AI"
  },
  {
    "title": "SAINT: Attention-Based Policies for Discrete Combinatorial Action Spaces",
    "summary": "arXiv:2505.12109v3 Announce Type: replace-cross Abstract: The combinatorial structure of many real-world action spaces leads to exponential growth in the number of possible actions, limiting the effectiveness of conventional reinforcement learning algorithms. Recent approaches for combinatorial action spaces impose factorized or sequential structur",
    "url": "https://arxiv.org/abs/2505.12109",
    "source": "Arxiv AI"
  },
  {
    "title": "LightRetriever: A LLM-based Text Retrieval Architecture with Extremely Faster Query Inference",
    "summary": "arXiv:2505.12260v5 Announce Type: replace-cross Abstract: Large Language Models (LLMs)-based text retrieval retrieves documents relevant to search queries based on vector similarities. Documents are pre-encoded offline, while queries arrive in real-time, necessitating an efficient online query encoder. Although LLMs significantly enhance retrieval ",
    "url": "https://arxiv.org/abs/2505.12260",
    "source": "Arxiv AI"
  },
  {
    "title": "Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning",
    "summary": "arXiv:2505.13709v3 Announce Type: replace-cross Abstract: Offline reinforcement learning (RL) offers a powerful paradigm for data-driven control. Compared to model-free approaches, offline model-based RL (MBRL) explicitly learns a world model from a static dataset and uses it as a surrogate simulator, improving data efficiency and enabling potentia",
    "url": "https://arxiv.org/abs/2505.13709",
    "source": "Arxiv AI"
  },
  {
    "title": "Mechanistic evaluation of Transformers and state space models",
    "summary": "arXiv:2505.15105v3 Announce Type: replace-cross Abstract: State space models (SSMs) for language modelling promise an efficient and performant alternative to quadratic-attention Transformers, yet show variable performance on recalling basic information from the context. While performance on synthetic tasks like Associative Recall (AR) can point to ",
    "url": "https://arxiv.org/abs/2505.15105",
    "source": "Arxiv AI"
  },
  {
    "title": "An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating the Impact of Noisy Annotations",
    "summary": "arXiv:2505.16705v3 Announce Type: replace-cross Abstract: Concept bottleneck models (CBMs) ensure interpretability by decomposing predictions into human interpretable concepts. Yet the annotations used for training CBMs that enable this transparency are often noisy, and the impact of such corruption is not well understood. In this study, we present",
    "url": "https://arxiv.org/abs/2505.16705",
    "source": "Arxiv AI"
  },
  {
    "title": "Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective",
    "summary": "arXiv:2505.17652v3 Announce Type: replace-cross Abstract: Reinforcement learning exhibits potential in enhancing the reasoning abilities of large language models, yet it is hard to scale for the low sample efficiency during the rollout phase. Existing methods attempt to improve efficiency by scheduling problems based on problem difficulties. Howeve",
    "url": "https://arxiv.org/abs/2505.17652",
    "source": "Arxiv AI"
  },
  {
    "title": "Reinforcement Learning for Ballbot Navigation in Uneven Terrain",
    "summary": "arXiv:2505.18417v2 Announce Type: replace-cross Abstract: Ballbot (i.e. Ball balancing robot) navigation usually relies on methods rooted in control theory (CT), and works that apply Reinforcement learning (RL) to the problem remain rare while generally being limited to specific subtasks (e.g. balance recovery). Unlike CT based methods, RL does not",
    "url": "https://arxiv.org/abs/2505.18417",
    "source": "Arxiv AI"
  },
  {
    "title": "Framing Political Bias in Multilingual LLMs Across Pakistani Languages",
    "summary": "arXiv:2506.00068v3 Announce Type: replace-cross Abstract: Large Language Models (LLMs) increasingly shape public discourse, yet most evaluations of political and economic bias have focused on high-resource, Western languages and contexts. This leaves critical blind spots in low-resource, multilingual regions such as Pakistan, where linguistic ident",
    "url": "https://arxiv.org/abs/2506.00068",
    "source": "Arxiv AI"
  },
  {
    "title": "Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack",
    "summary": "arXiv:2506.01318v3 Announce Type: replace-cross Abstract: Machine unlearning (MU) aims to expunge a designated forget set from a trained model without costly retraining, yet the existing techniques overlook two critical blind spots: \"over-unlearning\" that deteriorates retained data near the forget set, and post-hoc \"relearning\" attacks that aim to ",
    "url": "https://arxiv.org/abs/2506.01318",
    "source": "Arxiv AI"
  },
  {
    "title": "Influence Functions for Edge Edits in Non-Convex Graph Neural Networks",
    "summary": "arXiv:2506.04694v2 Announce Type: replace-cross Abstract: Understanding how individual edges influence the behavior of graph neural networks (GNNs) is essential for improving their interpretability and robustness. Graph influence functions have emerged as promising tools to efficiently estimate the effects of edge deletions without retraining. Howe",
    "url": "https://arxiv.org/abs/2506.04694",
    "source": "Arxiv AI"
  },
  {
    "title": "Ravan: Multi-Head Low-Rank Adaptation for Federated Fine-Tuning",
    "summary": "arXiv:2506.05568v2 Announce Type: replace-cross Abstract: Large language models (LLMs) have not yet effectively leveraged the vast amounts of edge-device data, and federated learning (FL) offers a promising paradigm to collaboratively fine-tune LLMs without transferring private edge data to the cloud. To operate within the computation and communica",
    "url": "https://arxiv.org/abs/2506.05568",
    "source": "Arxiv AI"
  },
  {
    "title": "Tokenization Multiplicity Leads to Arbitrary Price Variation in LLM-as-a-service",
    "summary": "arXiv:2506.06446v2 Announce Type: replace-cross Abstract: Providers of LLM-as-a-service have predominantly adopted a simple pricing model: users pay a fixed price per token. Consequently, one may think that the price two different users would pay for the same output string under the same input prompt is the same. In our work, we show that, surprisi",
    "url": "https://arxiv.org/abs/2506.06446",
    "source": "Arxiv AI"
  },
  {
    "title": "Symmetrical Flow Matching: Unified Image Generation, Segmentation, and Classification with Score-Based Generative Models",
    "summary": "arXiv:2506.10634v3 Announce Type: replace-cross Abstract: Flow Matching has emerged as a powerful framework for learning continuous transformations between distributions, enabling high-fidelity generative modeling. This work introduces Symmetrical Flow Matching (SymmFlow), a new formulation that unifies semantic segmentation, classification, and im",
    "url": "https://arxiv.org/abs/2506.10634",
    "source": "Arxiv AI"
  },
  {
    "title": "BNMusic: Blending Environmental Noises into Personalized Music",
    "summary": "arXiv:2506.10754v3 Announce Type: replace-cross Abstract: While being disturbed by environmental noises, the acoustic masking technique is a conventional way to reduce the annoyance in audio engineering that seeks to cover up the noises with other dominant yet less intrusive sounds. However, misalignment between the dominant sound and the noise-suc",
    "url": "https://arxiv.org/abs/2506.10754",
    "source": "Arxiv AI"
  },
  {
    "title": "Direct Reasoning Optimization: Constrained RL with Token-Level Dense Reward and Rubric-Gated Constraints for Open-ended Tasks",
    "summary": "arXiv:2506.13351v2 Announce Type: replace-cross Abstract: RL training of LLMs on open-ended tasks is challenging due to the lack of direct verifiability. In this paper, we frame such training as constrained RL that (i) optimizes a token-level dense Reasoning Reflection Reward (R3) aligned with reasoning quality, and (ii) enforces rubric-gating as f",
    "url": "https://arxiv.org/abs/2506.13351",
    "source": "Arxiv AI"
  },
  {
    "title": "SAFER: Probing Safety in Reward Models with Sparse Autoencoder",
    "summary": "arXiv:2507.00665v3 Announce Type: replace-cross Abstract: Reinforcement learning from human feedback (RLHF) is a key paradigm for aligning large language models (LLMs) with human values, yet the reward models at its core remain largely opaque. In this work, we present Sparse Autoencoder For Enhanced Reward model (\\textbf{SAFER}), a novel framework ",
    "url": "https://arxiv.org/abs/2507.00665",
    "source": "Arxiv AI"
  },
  {
    "title": "Spattack: Subgroup Poisoning Attacks on Federated Recommender Systems",
    "summary": "arXiv:2507.06258v2 Announce Type: replace-cross Abstract: Federated recommender systems (FedRec) have emerged as a promising approach to provide personalized recommendations while protecting user privacy. However, recent studies have shown their vulnerability to poisoning attacks, where malicious clients inject crafted gradients to promote target i",
    "url": "https://arxiv.org/abs/2507.06258",
    "source": "Arxiv AI"
  },
  {
    "title": "A Pre-training Framework for Relational Data with Information-theoretic Principles",
    "summary": "arXiv:2507.09837v2 Announce Type: replace-cross Abstract: Relational databases underpin critical infrastructure across a wide range of domains, yet the design of generalizable pre-training strategies for learning from relational databases remains an open challenge due to task heterogeneity. Specifically, there exist many possible downstream tasks, ",
    "url": "https://arxiv.org/abs/2507.09837",
    "source": "Arxiv AI"
  },
  {
    "title": "PICACO: Pluralistic In-Context Value Alignment of LLMs via Total Correlation Optimization",
    "summary": "arXiv:2507.16679v2 Announce Type: replace-cross Abstract: In-Context Learning has shown great potential for aligning Large Language Models (LLMs) with human values, helping reduce harmful outputs and accommodate diverse preferences without costly post-training, known as In-Context Alignment (ICA). However, LLMs' comprehension of input prompts remai",
    "url": "https://arxiv.org/abs/2507.16679",
    "source": "Arxiv AI"
  },
  {
    "title": "ElectriQ: A Benchmark for Assessing the Response Capability of Large Language Models in Power Marketing",
    "summary": "arXiv:2507.22911v2 Announce Type: replace-cross Abstract: As power systems decarbonise and digitalise, high penetrations of distributed energy resources and flexible tariffs make electric power marketing (EPM) a key interface between regulation, system operation and sustainable-energy deployment. Many utilities still rely on human agents and rule- ",
    "url": "https://arxiv.org/abs/2507.22911",
    "source": "Arxiv AI"
  },
  {
    "title": "BiasGym: Fantastic LLM Biases and How to Find (and Remove) Them",
    "summary": "arXiv:2508.08855v3 Announce Type: replace-cross Abstract: Understanding biases and stereotypes encoded in the weights of Large Language Models (LLMs) is crucial for developing effective mitigation strategies. However, biased behaviour is often subtle and non-trivial to isolate, even when deliberately elicited, making systematic analysis and debiasi",
    "url": "https://arxiv.org/abs/2508.08855",
    "source": "Arxiv AI"
  },
  {
    "title": "Multi-Level Safety Continual Projection for Fine-Tuned Large Language Models without Retraining",
    "summary": "arXiv:2508.09190v4 Announce Type: replace-cross Abstract: While fine-tuning services drive the rapid expansion of task capabilities in large language models (LLMs), they are often accompanied by the degradation and reorganization of safety-aligned representations, making models more prone to deviating from human preferences and exposing them to eme",
    "url": "https://arxiv.org/abs/2508.09190",
    "source": "Arxiv AI"
  },
  {
    "title": "FLM-Audio: Natural Monologues Improves Native Full-Duplex Chatbots via Dual Training",
    "summary": "arXiv:2509.02521v3 Announce Type: replace-cross Abstract: Full-duplex dialog models aim to listen and speak simultaneously, delivering rapid responses to dynamic user input. Among different solutions to full-duplexity, a native solution merges multiple channels in each time step, achieving the lowest latency. However, prevailing designs break down ",
    "url": "https://arxiv.org/abs/2509.02521",
    "source": "Arxiv AI"
  },
  {
    "title": "SpiderNets: Vision Models Predict Human Fear From Aversive Images",
    "summary": "arXiv:2509.04889v2 Announce Type: replace-cross Abstract: Phobias are common and impairing, and exposure therapy, which involves confronting patients with fear-provoking visual stimuli, is the most effective treatment. Scalable computerized exposure therapy requires automated prediction of fear directly from image content to adapt stimulus selectio",
    "url": "https://arxiv.org/abs/2509.04889",
    "source": "Arxiv AI"
  },
  {
    "title": "Feature Space Topology Control via Hopkins Loss",
    "summary": "arXiv:2509.11154v2 Announce Type: replace-cross Abstract: Feature space topology refers to the organization of samples within the feature space. Modifying this topology can be beneficial in machine learning applications, including dimensionality reduction, generative modeling, transfer learning, and robustness to adversarial attacks. This paper int",
    "url": "https://arxiv.org/abs/2509.11154",
    "source": "Arxiv AI"
  },
  {
    "title": "Impact of Phonetics on Speaker Identity in Adversarial Voice Attack",
    "summary": "arXiv:2509.15437v2 Announce Type: replace-cross Abstract: Adversarial perturbations in speech pose a serious threat to automatic speech recognition (ASR) and speaker verification by introducing subtle waveform modifications that remain imperceptible to humans but can significantly alter system outputs. While targeted attacks on end-to-end ASR model",
    "url": "https://arxiv.org/abs/2509.15437",
    "source": "Arxiv AI"
  },
  {
    "title": "Accurate and Efficient Low-Rank Model Merging in Core Space",
    "summary": "arXiv:2509.17786v4 Announce Type: replace-cross Abstract: In this paper, we address the challenges associated with merging low-rank adaptations of large neural networks. With the rise of parameter-efficient adaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning has become more accessible. While fine-tuning models with LoRA is ",
    "url": "https://arxiv.org/abs/2509.17786",
    "source": "Arxiv AI"
  },
  {
    "title": "Towards Atoms of Large Language Models",
    "summary": "arXiv:2509.20784v2 Announce Type: replace-cross Abstract: The fundamental representational units (FRUs) of large language models (LLMs) remain undefined, limiting further understanding of their underlying mechanisms. In this paper, we introduce Atom Theory to systematically define, evaluate, and identify such FRUs, which we term atoms. Building on ",
    "url": "https://arxiv.org/abs/2509.20784",
    "source": "Arxiv AI"
  },
  {
    "title": "LAVA: Explainability for Unsupervised Latent Embeddings",
    "summary": "arXiv:2509.21149v2 Announce Type: replace-cross Abstract: Unsupervised black-box models are drivers of scientific discovery, yet are difficult to interpret, as their output is often a multidimensional embedding rather than a well-defined target. While explainability for supervised learning uncovers how input features contribute to predictions, its ",
    "url": "https://arxiv.org/abs/2509.21149",
    "source": "Arxiv AI"
  },
  {
    "title": "It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL",
    "summary": "arXiv:2509.21282v2 Announce Type: replace-cross Abstract: Training large language models (LLMs) with reinforcement learning (RL) methods such as PPO and GRPO commonly relies on ratio clipping to stabilise updates. While effective at preventing instability, clipping discards information, introduces gradient discontinuities and can prevent exploratio",
    "url": "https://arxiv.org/abs/2509.21282",
    "source": "Arxiv AI"
  },
  {
    "title": "On the Separability of Information in Diffusion Models",
    "summary": "arXiv:2509.23937v4 Announce Type: replace-cross Abstract: Diffusion models transform noise into data by injecting information that was captured in their neural network during the training phase. In this paper, we ask: \\textit{what} is this information? We find that, in pixel-space diffusion models, (1) a large fraction of the total information in t",
    "url": "https://arxiv.org/abs/2509.23937",
    "source": "Arxiv AI"
  },
  {
    "title": "Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in Large Language Models",
    "summary": "arXiv:2509.24319v3 Announce Type: replace-cross Abstract: Large language models can express values in two main ways: (1) intrinsic expression, reflecting the model's inherent values learned during training, and (2) prompted expression, elicited by explicit prompts. Given their widespread use in value alignment, it is paramount to clearly understand",
    "url": "https://arxiv.org/abs/2509.24319",
    "source": "Arxiv AI"
  },
  {
    "title": "Causal-Adapter: Taming Text-to-Image Diffusion for Faithful Counterfactual Generation",
    "summary": "arXiv:2509.24798v4 Announce Type: replace-cross Abstract: We present Causal-Adapter, a modular framework that adapts frozen text-to-image diffusion backbones for counterfactual image generation. Our method enables causal interventions on target attributes, consistently propagating their effects to causal dependents without altering the core identit",
    "url": "https://arxiv.org/abs/2509.24798",
    "source": "Arxiv AI"
  },
  {
    "title": "Think Less, Label Better: Multi-Stage Domain-Grounded Synthetic Data Generation for Fine-Tuning Large Language Models in Telecommunications",
    "summary": "arXiv:2509.25736v2 Announce Type: replace-cross Abstract: The success of large language models (LLMs) depends heavily on large-scale, high-quality instruction-following and reinforcement datasets. However, generating such data through human annotation is prohibitively time-consuming particularly for domain-specific tasks like telecom network troubl",
    "url": "https://arxiv.org/abs/2509.25736",
    "source": "Arxiv AI"
  },
  {
    "title": "TAP: Two-Stage Adaptive Personalization of Multi-Task and Multi-Modal Foundation Models in Federated Learning",
    "summary": "arXiv:2509.26524v2 Announce Type: replace-cross Abstract: In federated learning (FL), local personalization of models has received significant attention, yet personalized fine-tuning of foundation models remains a significant challenge. In particular, there is a lack of understanding in the literature on how to fine-tune and personalize foundation ",
    "url": "https://arxiv.org/abs/2509.26524",
    "source": "Arxiv AI"
  },
  {
    "title": "Thoughtbubbles: an Unsupervised Method for Parallel Thinking in Latent Space",
    "summary": "arXiv:2510.00219v2 Announce Type: replace-cross Abstract: Current approaches for scaling inference-time compute in transformers train them to emit explicit chain-of-thought tokens before producing an answer. While these methods are powerful, they are limited because they cannot be applied during pretraining and rely solely on serially-generated, na",
    "url": "https://arxiv.org/abs/2510.00219",
    "source": "Arxiv AI"
  },
  {
    "title": "VideoNSA: Native Sparse Attention Scales Video Understanding",
    "summary": "arXiv:2510.02295v2 Announce Type: replace-cross Abstract: Video understanding in multimodal language models remains limited by context length: models often miss key transition frames and struggle to maintain coherence across long time scales. To address this, we adapt Native Sparse Attention (NSA) to video-language models. Our method, VideoNSA, ada",
    "url": "https://arxiv.org/abs/2510.02295",
    "source": "Arxiv AI"
  },
  {
    "title": "PT$^2$-LLM: Post-Training Ternarization for Large Language Models",
    "summary": "arXiv:2510.03267v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have shown impressive capabilities across diverse tasks, but their large memory and compute demands hinder deployment. Ternarization has gained attention as a promising compression technique, delivering substantial size reduction and high computational efficiency",
    "url": "https://arxiv.org/abs/2510.03267",
    "source": "Arxiv AI"
  },
  {
    "title": "InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions",
    "summary": "arXiv:2510.03370v3 Announce Type: replace-cross Abstract: Multimodal protein language models deliver strong performance on mutation-effect prediction, but training such models from scratch demands substantial computational resources. In this paper, we propose a fine-tuning framework called InstructPLM-mu and try to answer a question: \\textit{Can mu",
    "url": "https://arxiv.org/abs/2510.03370",
    "source": "Arxiv AI"
  },
  {
    "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models",
    "summary": "arXiv:2510.04618v2 Announce Type: replace-cross Abstract: Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation -- modifying inputs with instructions, strategies, or evidence, rather than weight updates. Prior approaches improve usability but often suffer from brevity bias, whic",
    "url": "https://arxiv.org/abs/2510.04618",
    "source": "Arxiv AI"
  },
  {
    "title": "Post-Norm can Resharpen Attention",
    "summary": "arXiv:2510.08341v2 Announce Type: replace-cross Abstract: Length Generalization is the essential capacity of autonomous agents to perform tasks in longer contexts than those encountered during training. To systematically study this feat, we test how well models can approximate the next token distributions in algorithmic tasks. This is to take into ",
    "url": "https://arxiv.org/abs/2510.08341",
    "source": "Arxiv AI"
  },
  {
    "title": "Herb.jl: A Unifying Program Synthesis Library",
    "summary": "arXiv:2510.09726v2 Announce Type: replace-cross Abstract: Program synthesis -- the automatic generation of code given a specification -- is one of the most fundamental tasks in artificial intelligence (AI) and the dream of many programmers. Numerous synthesizers have been developed for program synthesis, offering different approaches to the exponen",
    "url": "https://arxiv.org/abs/2510.09726",
    "source": "Arxiv AI"
  },
  {
    "title": "Understanding and Bridging the Planner-Coder Gap: A Systematic Study on the Robustness of Multi-Agent Systems for Code Generation",
    "summary": "arXiv:2510.10460v2 Announce Type: replace-cross Abstract: Multi-agent systems (MASs) have emerged as a promising paradigm for automated code generation, demonstrating impressive performance on established benchmarks. Despite their prosperous development, the fundamental mechanisms underlying their robustness remain poorly understood, raising critic",
    "url": "https://arxiv.org/abs/2510.10460",
    "source": "Arxiv AI"
  },
  {
    "title": "Thompson Sampling via Fine-Tuning of LLMs",
    "summary": "arXiv:2510.13328v3 Announce Type: replace-cross Abstract: Bayesian optimization in large unstructured discrete spaces is often hindered by the computational cost of maximizing acquisition functions due to the absence of gradients. We propose a scalable alternative based on Thompson sampling that eliminates the need for acquisition function maximiza",
    "url": "https://arxiv.org/abs/2510.13328",
    "source": "Arxiv AI"
  },
  {
    "title": "Open Shouldn't Mean Exempt: Open-Source Exceptionalism and Generative AI",
    "summary": "arXiv:2510.16048v2 Announce Type: replace-cross Abstract: Open-source status should not shield generative artificial intelligence systems from ethical or legal accountability. Through a rigorous analysis of regulatory, legal, and policy frameworks, this Article contends that open-source GenAI must be held to the same standards as proprietary system",
    "url": "https://arxiv.org/abs/2510.16048",
    "source": "Arxiv AI"
  },
  {
    "title": "In the Mood to Exclude: Revitalizing Trespass to Chattels in the Era of GenAI Scraping",
    "summary": "arXiv:2510.16049v2 Announce Type: replace-cross Abstract: GenAI companies are strip-mining the web. Their scraping bots harvest content at an unprecedented scale, circumventing technical barriers to fuel billion-dollar models while creators receive nothing. Courts have enabled this exploitation by misunderstanding what property rights protect onlin",
    "url": "https://arxiv.org/abs/2510.16049",
    "source": "Arxiv AI"
  },
  {
    "title": "DDSC: Dynamic Dual-Signal Curriculum for Data-Efficient Acoustic Scene Classification under Domain Shift",
    "summary": "arXiv:2510.17345v2 Announce Type: replace-cross Abstract: Acoustic scene classification (ASC) suffers from device-induced domain shift, especially when labels are limited. Prior work focuses on curriculum-based training schedules that structure data presentation by ordering or reweighting training examples from easy-to-hard to facilitate learning; ",
    "url": "https://arxiv.org/abs/2510.17345",
    "source": "Arxiv AI"
  },
  {
    "title": "TopSeg: A Multi-Scale Topological Framework for Data-Efficient Heart Sound Segmentation",
    "summary": "arXiv:2510.17346v2 Announce Type: replace-cross Abstract: Deep learning approaches for heart-sound (PCG) segmentation built on time-frequency features can be accurate but often rely on large expert-labeled datasets, limiting robustness and deployment. We present TopSeg, a topological representation-centric framework that encodes PCG dynamics with m",
    "url": "https://arxiv.org/abs/2510.17346",
    "source": "Arxiv AI"
  },
  {
    "title": "Evaluating LLMs for Career Guidance: Comparative Analysis of Computing Competency Recommendations Across Ten African Countries",
    "summary": "arXiv:2510.18902v2 Announce Type: replace-cross Abstract: Employers increasingly expect graduates to utilize large language models (LLMs) in the workplace, yet the competencies needed for computing roles across Africa remain unclear given varying national contexts. This study examined how six LLMs, namely ChatGPT 4, DeepSeek, Gemini, Claude 3.5, Ll",
    "url": "https://arxiv.org/abs/2510.18902",
    "source": "Arxiv AI"
  },
  {
    "title": "Context-aware Fairness Evaluation and Mitigation in LLMs",
    "summary": "arXiv:2510.18914v2 Announce Type: replace-cross Abstract: Large language models often display undesirable behaviors embedded in their internal representations, undermining fairness, inconsistency drift, amplification of harmful content, and the propagation of unwanted patterns during extended dialogue and conversations. Although training-time or da",
    "url": "https://arxiv.org/abs/2510.18914",
    "source": "Arxiv AI"
  },
  {
    "title": "Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation",
    "summary": "arXiv:2510.19689v2 Announce Type: replace-cross Abstract: Industrial and government organizations increasingly depend on data-driven analytics for workforce, finance, and regulated decision processes, where timeliness, cost efficiency, and compliance are critical. Distributed frameworks such as Spark and Flink remain effective for massive-scale bat",
    "url": "https://arxiv.org/abs/2510.19689",
    "source": "Arxiv AI"
  },
  {
    "title": "TOM-SWE: User Mental Modeling For Software Engineering Agents",
    "summary": "arXiv:2510.21903v2 Announce Type: replace-cross Abstract: Recent advances in coding agents have made them capable of planning, editing, running, and testing complex code bases. Despite their growing ability in coding tasks, these systems still struggle to infer and track user intent, especially when instructions are underspecified or context-depend",
    "url": "https://arxiv.org/abs/2510.21903",
    "source": "Arxiv AI"
  },
  {
    "title": "Emotions Where Art Thou: Understanding and Characterizing the Emotional Latent Space of Large Language Models",
    "summary": "arXiv:2510.22042v2 Announce Type: replace-cross Abstract: This work investigates how large language models (LLMs) internally represent emotion by analyzing the geometry of their hidden-state space. The paper identifies a low-dimensional emotional manifold and shows that emotional representations are directionally encoded, distributed across layers,",
    "url": "https://arxiv.org/abs/2510.22042",
    "source": "Arxiv AI"
  },
  {
    "title": "Deep Ensembles for Epistemic Uncertainty: A Frequentist Perspective",
    "summary": "arXiv:2510.22063v2 Announce Type: replace-cross Abstract: Decomposing prediction uncertainty into aleatoric (irreducible) and epistemic (reducible) components is critical for the reliable deployment of machine learning systems. While the mutual information between the response variable and model parameters is a principled measure for epistemic unce",
    "url": "https://arxiv.org/abs/2510.22063",
    "source": "Arxiv AI"
  },
  {
    "title": "Batch Speculative Decoding Done Right",
    "summary": "arXiv:2510.22876v2 Announce Type: replace-cross Abstract: Speculative decoding must produce outputs distribution identical to standard autoregressive generation-this output equivalence is not an optimization target but the defining criterion of valid speculative decoding. We demonstrate that all existing batch speculative decoding implementations v",
    "url": "https://arxiv.org/abs/2510.22876",
    "source": "Arxiv AI"
  },
  {
    "title": "Key and Value Weights Are Probably All You Need: On the Necessity of the Query, Key, Value weight Triplet in Encoder-Only and Decoder-Only Transformers",
    "summary": "arXiv:2510.23912v4 Announce Type: replace-cross Abstract: We theoretically investigate whether the Query, Key, Value weight triplet can be reduced in encoder-only and decoder-only transformers. Under mild assumptions, we prove that Query weights are redundant and can be replaced with the identity matrix, reducing attention parameters by $25\\%$. Thi",
    "url": "https://arxiv.org/abs/2510.23912",
    "source": "Arxiv AI"
  },
  {
    "title": "Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start",
    "summary": "arXiv:2510.25801v3 Announce Type: replace-cross Abstract: Reinforcement learning (RL) with verifiable rewards has recently catalyzed a wave of \"MLLM-r1\" approaches that bring RL to vision language models. Most representative paradigms begin with a cold start, typically employing supervised fine-tuning (SFT), to initialize the policy before RL. Howe",
    "url": "https://arxiv.org/abs/2510.25801",
    "source": "Arxiv AI"
  },
  {
    "title": "LoCoT2V-Bench: Benchmarking Long-Form and Complex Text-to-Video Generation",
    "summary": "arXiv:2510.26412v2 Announce Type: replace-cross Abstract: Recent advances in text-to-video generation have achieved impressive performance on short clips, yet evaluating long-form generation under complex textual inputs remains a significant challenge. In response to this challenge, we present LoCoT2V-Bench, a benchmark for long video generation (L",
    "url": "https://arxiv.org/abs/2510.26412",
    "source": "Arxiv AI"
  },
  {
    "title": "Latent Domain Prompt Learning for Vision-Language Models",
    "summary": "arXiv:2511.00067v2 Announce Type: replace-cross Abstract: The objective of domain generalization (DG) is to enable models to be robust against domain shift. DG is crucial for deploying vision-language models (VLMs) in real-world applications, yet most existing methods rely on domain labels that may not be available and often ambiguous. We instead s",
    "url": "https://arxiv.org/abs/2511.00067",
    "source": "Arxiv AI"
  },
  {
    "title": "Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement",
    "summary": "arXiv:2511.01706v2 Announce Type: replace-cross Abstract: Natural Language Explanations (NLEs) describe how Large Language Models (LLMs) make decisions by drawing on external Context Knowledge (CK) and Parametric Knowledge (PK). Understanding the interaction between these sources is key to assessing NLE grounding, yet these dynamics remain underexp",
    "url": "https://arxiv.org/abs/2511.01706",
    "source": "Arxiv AI"
  },
  {
    "title": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live",
    "summary": "arXiv:2511.02230v3 Announce Type: replace-cross Abstract: KV cache management is essential for efficient LLM inference. To maximize utilization, existing inference engines evict finished requests' KV cache if new requests are waiting. This policy breaks for agentic workloads, which interleave LLM calls with tools, introducing pauses that prevent ef",
    "url": "https://arxiv.org/abs/2511.02230",
    "source": "Arxiv AI"
  },
  {
    "title": "Multi-agent Coordination via Flow Matching",
    "summary": "arXiv:2511.05005v2 Announce Type: replace-cross Abstract: This work presents MAC-Flow, a simple yet expressive framework for multi-agent coordination. We argue that requirements of effective coordination are twofold: (i) a rich representation of the diverse joint behaviors present in offline data and (ii) the ability to act efficiently in real time",
    "url": "https://arxiv.org/abs/2511.05005",
    "source": "Arxiv AI"
  },
  {
    "title": "Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models",
    "summary": "arXiv:2511.10691v2 Announce Type: replace-cross Abstract: The potential data contamination issue in contemporary large language models (LLMs) benchmarks presents a fundamental challenge to establishing trustworthy evaluation frameworks. Meanwhile, they predominantly assume benign, resource-rich settings, leaving the behavior of LLMs under pressure ",
    "url": "https://arxiv.org/abs/2511.10691",
    "source": "Arxiv AI"
  },
  {
    "title": "AI Kill Switch for malicious web-based LLM agent",
    "summary": "arXiv:2511.13725v3 Announce Type: replace-cross Abstract: Recently, web-based Large Language Model (LLM) agents autonomously perform increasingly complex tasks, thereby bringing significant convenience. However, they also amplify the risks of malicious misuse cases such as unauthorized collection of personally identifiable information (PII), genera",
    "url": "https://arxiv.org/abs/2511.13725",
    "source": "Arxiv AI"
  },
  {
    "title": "Geometric-disentangelment Unlearning",
    "summary": "arXiv:2511.17100v3 Announce Type: replace-cross Abstract: Large language models (LLMs) can internalize private or harmful content, motivating unlearning that removes a forget set while preserving retaining knowledge. However, forgetting updates often cause collateral degradation on retaining knowledge, creating a persistent trade-off. Existing LLM ",
    "url": "https://arxiv.org/abs/2511.17100",
    "source": "Arxiv AI"
  },
  {
    "title": "TALES: A Taxonomy and Analysis of Cultural Representations in LLM-generated Stories",
    "summary": "arXiv:2511.21322v2 Announce Type: replace-cross Abstract: Millions of users across the globe turn to AI chatbots for their creative needs, inviting widespread interest in understanding how they represent diverse cultures. However, evaluating cultural representations in open-ended tasks remains challenging and underexplored. In this work, we present",
    "url": "https://arxiv.org/abs/2511.21322",
    "source": "Arxiv AI"
  },
  {
    "title": "Structured Spectral Reasoning for Frequency-Adaptive Multimodal Recommendation",
    "summary": "arXiv:2512.01372v3 Announce Type: replace-cross Abstract: Multimodal recommendation aims to integrate collaborative signals with heterogeneous content such as visual and textual information, but remains challenged by modality-specific noise, semantic inconsistency, and unstable propagation over user-item graphs. These issues are often exacerbated b",
    "url": "https://arxiv.org/abs/2512.01372",
    "source": "Arxiv AI"
  },
  {
    "title": "AlignGemini: Generalizable AI-Generated Image Detection Through Task-Model Alignment",
    "summary": "arXiv:2512.06746v2 Announce Type: replace-cross Abstract: Vision Language Models (VLMs) are increasingly used for detecting AI-generated images (AIGI). However, converting VLMs into reliable detectors is resource-intensive, and the resulting models often suffer from hallucination and poor generalization. To investigate the root cause, we conduct an",
    "url": "https://arxiv.org/abs/2512.06746",
    "source": "Arxiv AI"
  },
  {
    "title": "From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs",
    "summary": "arXiv:2512.06776v2 Announce Type: replace-cross Abstract: Diffusion Language Models (DLMs) enable fast generation, yet training large DLMs from scratch is costly. As a practical shortcut, adapting off-the-shelf Auto-Regressive (AR) model weights into a DLM could quickly equip the DLM with strong long-context generation capabilies. Prior \"adaptation",
    "url": "https://arxiv.org/abs/2512.06776",
    "source": "Arxiv AI"
  },
  {
    "title": "Experience-Evolving Multi-Turn Tool-Use Agent with Hybrid Episodic-Procedural Memory",
    "summary": "arXiv:2512.07287v2 Announce Type: replace-cross Abstract: As intents unfold and environments change, multi-turn agents face continuously shifting decision contexts. Although reusing past experience is intuitively appealing, existing approaches remain limited: full trajectories are often too context-specific to transfer, while tool-level reuse ignor",
    "url": "https://arxiv.org/abs/2512.07287",
    "source": "Arxiv AI"
  },
  {
    "title": "Model-Based Diffusion Sampling for Predictive Control in Offline Decision Making",
    "summary": "arXiv:2512.08280v2 Announce Type: replace-cross Abstract: Offline decision-making via diffusion models often produces trajectories that are misaligned with system dynamics, limiting their reliability for control. We propose Model Predictive Diffuser (MPDiffuser), a compositional diffusion framework that combines a diffusion planner with a dynamics ",
    "url": "https://arxiv.org/abs/2512.08280",
    "source": "Arxiv AI"
  },
  {
    "title": "Geometric Dynamics of Agentic Loops in Large Language Models",
    "summary": "arXiv:2512.10350v5 Announce Type: replace-cross Abstract: Iterative LLM systems(self-refinement, chain-of-thought, autonomous agents) are increasingly deployed, yet their temporal dynamics remain uncharacterized. Prior work evaluates task performance at convergence but ignores the trajectory: how does semantic content evolve across iterations? Does",
    "url": "https://arxiv.org/abs/2512.10350",
    "source": "Arxiv AI"
  },
  {
    "title": "Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols",
    "summary": "arXiv:2512.11614v2 Announce Type: replace-cross Abstract: Retrieval-augmented generation (RAG) relies on retrieved context to guide large language models (LLM), yet treats retrieval as a weak heuristic rather than verifiable evidence -- leading to unsupported answers, hallucinations, and reliance on spurious context. We introduce a novel training f",
    "url": "https://arxiv.org/abs/2512.11614",
    "source": "Arxiv AI"
  },
  {
    "title": "Pretrained Battery Transformer (PBT): A battery life prediction foundation model",
    "summary": "arXiv:2512.16334v4 Announce Type: replace-cross Abstract: Early prediction of battery cycle life is essential for accelerating battery research, manufacturing, and deployment. Although machine learning methods have shown encouraging results, progress is hindered by data scarcity and heterogeneity arising from diverse aging conditions. In other fiel",
    "url": "https://arxiv.org/abs/2512.16334",
    "source": "Arxiv AI"
  },
  {
    "title": "Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs",
    "summary": "arXiv:2512.17131v2 Announce Type: replace-cross Abstract: We propose Generalized Primal Averaging (GPA), an extension of Nesterov's method that unifies and generalizes recent averaging-based optimizers like single-worker DiLoCo and Schedule-Free, within a non-distributed setting. While DiLoCo relies on a memory-intensive two-loop structure to perio",
    "url": "https://arxiv.org/abs/2512.17131",
    "source": "Arxiv AI"
  },
  {
    "title": "Multi-agent Adaptive Mechanism Design",
    "summary": "arXiv:2512.21794v2 Announce Type: replace-cross Abstract: We study a sequential mechanism design problem in which a principal seeks to elicit truthful reports from multiple rational agents while starting with no prior knowledge of agents' beliefs. We introduce Distributionally Robust Adaptive Mechanism (DRAM), a general framework combining insights",
    "url": "https://arxiv.org/abs/2512.21794",
    "source": "Arxiv AI"
  },
  {
    "title": "SB-TRPO: Towards Safe Reinforcement Learning with Hard Constraints",
    "summary": "arXiv:2512.23770v2 Announce Type: replace-cross Abstract: In safety-critical domains, reinforcement learning (RL) agents must often satisfy strict, zero-cost safety constraints while accomplishing tasks. Existing model-free methods frequently either fail to achieve near-zero safety violations or become overly conservative. We introduce Safety-Biase",
    "url": "https://arxiv.org/abs/2512.23770",
    "source": "Arxiv AI"
  },
  {
    "title": "Deep Delta Learning",
    "summary": "arXiv:2601.00417v2 Announce Type: replace-cross Abstract: The effectiveness of deep residual networks hinges on the identity shortcut connection. While this mechanism alleviates the vanishing-gradient problem, it also has a strictly additive inductive bias on feature transformations, limiting the network's ability to model complex hidden state tran",
    "url": "https://arxiv.org/abs/2601.00417",
    "source": "Arxiv AI"
  },
  {
    "title": "IRPM: Intergroup Relative Preference Modeling for Pointwise Generative Reward Models",
    "summary": "arXiv:2601.00677v2 Announce Type: replace-cross Abstract: Generative Reward Models (GRMs) have demonstrated strong performance in reward modeling, due to their interpretability and potential for refinement through reinforcement learning (RL). However, widely used pairwise GRMs create a computational bottleneck in reinforcement learning from human f",
    "url": "https://arxiv.org/abs/2601.00677",
    "source": "Arxiv AI"
  },
  {
    "title": "CAOS: Conformal Aggregation of One-Shot Predictors",
    "summary": "arXiv:2601.05219v2 Announce Type: replace-cross Abstract: One-shot prediction enables rapid adaptation of pretrained foundation models to new tasks using only one labeled example, but lacks principled uncertainty quantification. While conformal prediction provides finite-sample coverage guarantees, standard split conformal methods are inefficient i",
    "url": "https://arxiv.org/abs/2601.05219",
    "source": "Arxiv AI"
  },
  {
    "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset",
    "summary": "arXiv:2601.10305v2 Announce Type: replace-cross Abstract: Vision-Language Pre-training (VLP) models have achieved remarkable success by leveraging large-scale image-text pairs. While English-centric models like CLIP and SigLIP benefit from massive datasets (e.g., LAION-400M), the development of Chinese VLP remains bottlenecked by the lack of high-q",
    "url": "https://arxiv.org/abs/2601.10305",
    "source": "Arxiv AI"
  },
  {
    "title": "Large Language Model Agent for User-friendly Chemical Process Simulations",
    "summary": "arXiv:2601.11650v2 Announce Type: replace-cross Abstract: Modern process simulators enable detailed process design, simulation, and optimization; however, constructing and interpreting simulations is time-consuming and requires expert knowledge. This limits early exploration by inexperienced users. To address this, a large language model (LLM) agen",
    "url": "https://arxiv.org/abs/2601.11650",
    "source": "Arxiv AI"
  },
  {
    "title": "ATOD: An Evaluation Framework and Benchmark for Agentic Task-Oriented Dialogue Systems",
    "summary": "arXiv:2601.11854v2 Announce Type: replace-cross Abstract: Recent advances in task-oriented dialogue (TOD) systems, driven by large language models (LLMs) with extensive API and tool integration, have enabled conversational agents to coordinate interleaved goals, maintain long-horizon context, and act proactively through asynchronous execution. Thes",
    "url": "https://arxiv.org/abs/2601.11854",
    "source": "Arxiv AI"
  },
  {
    "title": "Less is More: Label-Guided Summarization of Procedural and Instructional Videos",
    "summary": "arXiv:2601.12243v2 Announce Type: replace-cross Abstract: Video summarization helps turn long videos into clear, concise representations that are easier to review, document, and analyze, especially in high-stakes domains like surgical training. Prior work has progressed from using basic visual features like color, motion, and structural changes to ",
    "url": "https://arxiv.org/abs/2601.12243",
    "source": "Arxiv AI"
  },
  {
    "title": "Diffusion-Driven Synthetic Tabular Data Generation for Enhanced DoS/DDoS Attack Classification",
    "summary": "arXiv:2601.13197v2 Announce Type: replace-cross Abstract: Class imbalance refers to a situation where certain classes in a dataset have significantly fewer samples than oth- ers, leading to biased model performance. Class imbalance in network intrusion detection using Tabular Denoising Diffusion Probability Models (TabDDPM) for data augmentation is",
    "url": "https://arxiv.org/abs/2601.13197",
    "source": "Arxiv AI"
  },
  {
    "title": "Diff-MN: Diffusion Parameterized MoE-NCDE for Continuous Time Series Generation with Irregular Observations",
    "summary": "arXiv:2601.13534v2 Announce Type: replace-cross Abstract: Time series generation (TSG) is widely used across domains, yet most existing methods assume regular sampling and fixed output resolutions. These assumptions are often violated in practice, where observations are irregular and sparse, while downstream applications require continuous and high",
    "url": "https://arxiv.org/abs/2601.13534",
    "source": "Arxiv AI"
  },
  {
    "title": "Vulnerability of LLMs' Belief Systems? LLMs Belief Resistance Check Through Strategic Persuasive Conversation Interventions",
    "summary": "arXiv:2601.13590v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) are increasingly employed in various question-answering tasks. However, recent studies showcase that LLMs are susceptible to persuasion and could adopt counterfactual beliefs. We present a systematic evaluation of LLM susceptibility to persuasion under the Source",
    "url": "https://arxiv.org/abs/2601.13590",
    "source": "Arxiv AI"
  },
  {
    "title": "Quantum Super-resolution by Adaptive Non-local Observables",
    "summary": "arXiv:2601.14433v2 Announce Type: replace-cross Abstract: Super-resolution (SR) seeks to reconstruct high-resolution (HR) data from low-resolution (LR) observations. Classical deep learning methods have advanced SR substantially, but require increasingly deeper networks, large datasets, and heavy computation to capture fine-grained correlations. In",
    "url": "https://arxiv.org/abs/2601.14433",
    "source": "Arxiv AI"
  },
  {
    "title": "BibAgent: An Agentic Framework for Traceable Miscitation Detection in Scientific Literature",
    "summary": "arXiv:2601.16993v2 Announce Type: replace-cross Abstract: Citations are the bedrock of scientific authority, yet their integrity is compromised by widespread miscitations: ranging from nuanced distortions to fabricated references. Systematic citation verification is currently unfeasible; manual review cannot scale to modern publishing volumes, whil",
    "url": "https://arxiv.org/abs/2601.16993",
    "source": "Arxiv AI"
  },
  {
    "title": "Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis",
    "summary": "arXiv:2601.17687v2 Announce Type: replace-cross Abstract: Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language m",
    "url": "https://arxiv.org/abs/2601.17687",
    "source": "Arxiv AI"
  },
  {
    "title": "LLM-42: Enabling Determinism in LLM Inference with Verified Speculation",
    "summary": "arXiv:2601.17768v2 Announce Type: replace-cross Abstract: In LLM inference, the same prompt may yield different outputs across different runs. At the system level, this non-determinism arises from floating-point non-associativity combined with dynamic batching and GPU kernels whose reduction orders vary with batch size. A straightforward way to eli",
    "url": "https://arxiv.org/abs/2601.17768",
    "source": "Arxiv AI"
  },
  {
    "title": "MalURLBench: A Benchmark Evaluating Agents' Vulnerabilities When Processing Web URLs",
    "summary": "arXiv:2601.18113v2 Announce Type: replace-cross Abstract: LLM-based web agents have become increasingly popular for their utility in daily life and work. However, they exhibit critical vulnerabilities when processing malicious URLs: accepting a disguised malicious URL enables subsequent access to unsafe webpages, which can cause severe damage to se",
    "url": "https://arxiv.org/abs/2601.18113",
    "source": "Arxiv AI"
  },
  {
    "title": "TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment",
    "summary": "arXiv:2601.18292v2 Announce Type: replace-cross Abstract: In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving ",
    "url": "https://arxiv.org/abs/2601.18292",
    "source": "Arxiv AI"
  },
  {
    "title": "AACR-Bench: Evaluating Automatic Code Review with Holistic Repository-Level Context",
    "summary": "arXiv:2601.19494v3 Announce Type: replace-cross Abstract: High-quality evaluation benchmarks are pivotal for deploying Large Language Models (LLMs) in Automated Code Review (ACR). However, existing benchmarks suffer from two critical limitations: first, the lack of multi-language support in repository-level contexts, which restricts the generalizab",
    "url": "https://arxiv.org/abs/2601.19494",
    "source": "Arxiv AI"
  },
  {
    "title": "Generalizable Multimodal Large Language Model Editing via Invariant Trajectory Learning",
    "summary": "arXiv:2601.19700v2 Announce Type: replace-cross Abstract: Knowledge editing emerges as a crucial technique for efficiently correcting incorrect or outdated knowledge in large language models (LLM). Existing editing methods rely on a rigid mapping from parameter or module modifications to output, which causes the generalization limitation in Multimo",
    "url": "https://arxiv.org/abs/2601.19700",
    "source": "Arxiv AI"
  },
  {
    "title": "M-SGWR: Multiscale Similarity and Geographically Weighted Regression",
    "summary": "arXiv:2601.19888v2 Announce Type: replace-cross Abstract: The first law of geography is a cornerstone of spatial analysis, emphasizing that nearby and related locations tend to be more similar, however, defining what constitutes \"near\" and \"related\" remains challenging, as different phenomena exhibit distinct spatial patterns. Traditional local reg",
    "url": "https://arxiv.org/abs/2601.19888",
    "source": "Arxiv AI"
  },
  {
    "title": "CiMRAG: CiM-Aware Domain-Adaptive and Noise-Resilient Retrieval-Augmented Generation for Edge-Based LLMs",
    "summary": "arXiv:2601.20041v2 Announce Type: replace-cross Abstract: Personalized virtual assistants powered by large language models (LLMs) on edge devices are attracting growing attention, with Retrieval-Augmented Generation (RAG) emerging as a key method for personalization by retrieving relevant profile data and generating tailored responses. However, dep",
    "url": "https://arxiv.org/abs/2601.20041",
    "source": "Arxiv AI"
  },
  {
    "title": "How Much Progress Has There Been in NVIDIA Datacenter GPUs?",
    "summary": "arXiv:2601.20115v2 Announce Type: replace-cross Abstract: Graphics Processing Units (GPUs) are the state-of-the-art architecture for essential tasks, ranging from rendering 2D/3D graphics to accelerating workloads in supercomputing centers and, of course, Artificial Intelligence (AI). As GPUs continue improving to satisfy ever-increasing performanc",
    "url": "https://arxiv.org/abs/2601.20115",
    "source": "Arxiv AI"
  },
  {
    "title": "Inequality in Congestion Games with Learning Agents",
    "summary": "arXiv:2601.20578v2 Announce Type: replace-cross Abstract: Who benefits from expanding transport networks? While designed to improve mobility, such interventions can also create inequality. In this paper, we show that disparities arise not only from the structure of the network itself but also from differences in how commuters adapt to it. We model ",
    "url": "https://arxiv.org/abs/2601.20578",
    "source": "Arxiv AI"
  },
  {
    "title": "GNN Explanations that do not Explain and How to find Them",
    "summary": "arXiv:2601.20815v2 Announce Type: replace-cross Abstract: Explanations provided by Self-explainable Graph Neural Networks (SE-GNNs) are fundamental for understanding the model's inner workings and for identifying potential misuse of sensitive attributes. Although recent works have highlighted that these explanations can be suboptimal and potentiall",
    "url": "https://arxiv.org/abs/2601.20815",
    "source": "Arxiv AI"
  },
  {
    "title": "Open-Vocabulary Functional 3D Human-Scene Interaction Generation",
    "summary": "arXiv:2601.20835v2 Announce Type: replace-cross Abstract: Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses requir",
    "url": "https://arxiv.org/abs/2601.20835",
    "source": "Arxiv AI"
  },
  {
    "title": "NEXUS: Bit-Exact ANN-to-SNN Equivalence via Neuromorphic Gate Circuits with Surrogate-Free Training",
    "summary": "arXiv:2601.21279v2 Announce Type: replace-cross Abstract: Spiking Neural Networks (SNNs) promise energy-efficient computing through event-driven sparsity, yet all existing approaches sacrifice accuracy by approximating continuous values with discrete spikes. We propose NEXUS, a framework that achieves bit-exact ANN-to-SNN equivalence -- not approxi",
    "url": "https://arxiv.org/abs/2601.21279",
    "source": "Arxiv AI"
  },
  {
    "title": "L$^3$: Large Lookup Layers",
    "summary": "arXiv:2601.21461v2 Announce Type: replace-cross Abstract: Modern sparse language models typically achieve sparsity through Mixture-of-Experts (MoE) layers, which dynamically route tokens to dense MLP \"experts.\" However, dynamic hard routing has a number of drawbacks, such as potentially poor hardware efficiency and needing auxiliary losses for stab",
    "url": "https://arxiv.org/abs/2601.21461",
    "source": "Arxiv AI"
  },
  {
    "title": "Bi-Anchor Interpolation Solver for Accelerating Generative Modeling",
    "summary": "arXiv:2601.21542v2 Announce Type: replace-cross Abstract: Flow Matching (FM) models have emerged as a leading paradigm for high-fidelity synthesis. However, their reliance on iterative Ordinary Differential Equation (ODE) solving creates a significant latency bottleneck. Existing solutions face a dichotomy: training-free solvers suffer from signifi",
    "url": "https://arxiv.org/abs/2601.21542",
    "source": "Arxiv AI"
  },
  {
    "title": "Toward Culturally Aligned LLMs through Ontology-Guided Multi-Agent Reasoning",
    "summary": "arXiv:2601.21700v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) increasingly support culturally sensitive decision making, yet often exhibit misalignment due to skewed pretraining data and the absence of structured value representations. Existing methods can steer outputs, but often lack demographic grounding and treat values",
    "url": "https://arxiv.org/abs/2601.21700",
    "source": "Arxiv AI"
  },
  {
    "title": "CoFrGeNet: Continued Fraction Architectures for Language Generation",
    "summary": "arXiv:2601.21766v2 Announce Type: replace-cross Abstract: Transformers are arguably the preferred architecture for language generation. In this paper, inspired by continued fractions, we introduce a new function class for generative modeling. The architecture family implementing this function class is named CoFrGeNets - Continued Fraction Generativ",
    "url": "https://arxiv.org/abs/2601.21766",
    "source": "Arxiv AI"
  },
  {
    "title": "Effective LoRA Adapter Routing using Task Representations",
    "summary": "arXiv:2601.21795v2 Announce Type: replace-cross Abstract: Low-rank adaptation (LoRA) enables parameter efficient specialization of large language models (LLMs) through modular adapters, resulting in rapidly growing public adapter pools spanning diverse tasks. Effectively using these adapters requires routing: selecting and composing the appropriate",
    "url": "https://arxiv.org/abs/2601.21795",
    "source": "Arxiv AI"
  },
  {
    "title": "Moral Outrage Shapes Commitments Beyond Attention: Multimodal Moral Emotions on YouTube in Korea and the US",
    "summary": "arXiv:2601.21815v2 Announce Type: replace-cross Abstract: Understanding how media rhetoric shapes audience engagement is crucial in the attention economy. This study examines how moral emotional framing by mainstream news channels on YouTube influences user behavior across Korea and the United States. To capture the platform's multimodal nature, co",
    "url": "https://arxiv.org/abs/2601.21815",
    "source": "Arxiv AI"
  },
  {
    "title": "Token-Guard: Towards Token-Level Hallucination Control via Self-Checking Decoding",
    "summary": "arXiv:2601.21969v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) often hallucinate, generating content inconsistent with the input. Retrieval-Augmented Generation (RAG) and Reinforcement Learning with Human Feedback (RLHF) can mitigate hallucinations but require resource-intensive retrieval or large-scale fine-tuning. Decoding",
    "url": "https://arxiv.org/abs/2601.21969",
    "source": "Arxiv AI"
  },
  {
    "title": "UniRG: Scaling medical imaging report generation with multimodal reinforcement learning",
    "summary": "AI can help generate medical image reports, but today’s models struggle with varying reporting schemes. Learn how UniRG uses reinforcement learning to boost performance of medical vision-language models. The post UniRG: Scaling medical imaging report generation with multimodal reinforcement learning appeared first on Microsoft Research.",
    "url": "https://www.microsoft.com/en-us/research/blog/unirg-scaling-medical-imaging-report-generation-with-multimodal-reinforcement-learning/",
    "source": "Microsoft Research"
  },
  {
    "title": "Multimodal reinforcement learning with agentic verifier for AI agents",
    "summary": "Argos improves multimodal RL by evaluating whether an agent’s reasoning aligns with what it observes over time. The approach reduces visual hallucinations and produces more reliable, data-efficient agents for real-world applications. The post Multimodal reinforcement learning with agentic verifier for AI agents appeared first on Microsoft Research.",
    "url": "https://www.microsoft.com/en-us/research/blog/multimodal-reinforcement-learning-with-agentic-verifier-for-ai-agents/",
    "source": "Microsoft Research"
  },
  {
    "title": "OptiMind: A small language model with optimization expertise",
    "summary": "OptiMind is a small language model that converts business operation challenges, described naturally, into mathematical formulations that optimization software can solve. It reduces formulation time &#038; errors &#038; enables fast, privacy-preserving local use. The post OptiMind: A small language model with optimization expertise appeared first on",
    "url": "https://www.microsoft.com/en-us/research/blog/optimind-a-small-language-model-with-optimization-expertise/",
    "source": "Microsoft Research"
  },
  {
    "title": "Agent Lightning: Adding reinforcement learning to AI agents without code rewrites",
    "summary": "By decoupling how agents work from how they’re trained, Agent Lightning turns each step an agent takes into data for reinforcement learning. This makes it easy for developers to improve agent performance with almost zero code changes. The post Agent Lightning: Adding reinforcement learning to AI agents without code rewrites appeared first on Micros",
    "url": "https://www.microsoft.com/en-us/research/blog/agent-lightning-adding-reinforcement-learning-to-ai-agents-without-code-rewrites/",
    "source": "Microsoft Research"
  },
  {
    "title": "Promptions helps make AI prompting more precise with dynamic UI controls",
    "summary": "Promptions helps developers add dynamic, context-aware controls to chat interfaces so users can guide generative AI responses. It lets users shape outputs quickly without writing long instructions. The post Promptions helps make AI prompting more precise with dynamic UI controls appeared first on Microsoft Research.",
    "url": "https://www.microsoft.com/en-us/research/blog/promptions-helps-make-ai-prompting-more-precise-with-dynamic-ui-controls/",
    "source": "Microsoft Research"
  },
  {
    "title": "GigaTIME: Scaling tumor microenvironment modeling using virtual population generated by multimodal AI",
    "summary": "Using AI-generated virtual populations, Microsoft researchers uncovered hidden cellular patterns that could reshape how we understand and treat cancer. The post GigaTIME: Scaling tumor microenvironment modeling using virtual population generated by multimodal AI appeared first on Microsoft Research.",
    "url": "https://www.microsoft.com/en-us/research/blog/gigatime-scaling-tumor-microenvironment-modeling-using-virtual-population-generated-by-multimodal-ai/",
    "source": "Microsoft Research"
  },
  {
    "title": "Ideas: Community building, machine learning, and the future of AI",
    "summary": "As the Women in Machine Learning Workshop (WiML) marks its 20th annual gathering, cofounders, friends, and collaborators Jenn Wortman Vaughan and Hanna Wallach reflect on WiML’s evolution, navigating the field of ML, and their work in responsible AI. The post Ideas: Community building, machine learning, and the future of AI appeared first on Micros",
    "url": "https://www.microsoft.com/en-us/research/podcast/ideas-community-building-machine-learning-and-the-future-of-ai/",
    "source": "Microsoft Research"
  },
  {
    "title": "Reducing Privacy leaks in AI: Two approaches to contextual integrity",
    "summary": "New research explores two ways to give AI agents stronger privacy safeguards grounded in contextual integrity. One adds lightweight, inference-time checks; the other builds contextual awareness directly into models through reasoning and RL. The post Reducing Privacy leaks in AI: Two approaches to contextual integrity appeared first on Microsoft Res",
    "url": "https://www.microsoft.com/en-us/research/blog/reducing-privacy-leaks-in-ai-two-approaches-to-contextual-integrity/",
    "source": "Microsoft Research"
  },
  {
    "title": "Fara-7B: An Efficient Agentic Model for Computer Use",
    "summary": "Fara-7B is our first agentic small language model for computer use. This experimental model includes robust safety measures to aid responsible deployment. Despite its size, Fara-7B holds its own against larger, more resource-intensive agentic systems. The post Fara-7B: An Efficient Agentic Model for Computer Use appeared first on Microsoft Research",
    "url": "https://www.microsoft.com/en-us/research/blog/fara-7b-an-efficient-agentic-model-for-computer-use/",
    "source": "Microsoft Research"
  },
  {
    "title": "MMCTAgent: Enabling multimodal reasoning over large video and image collections",
    "summary": "MMCTAgent enables dynamic multimodal reasoning with iterative planning and reflection. Built on Microsoft’s AutoGen framework, it integrates language, vision, and temporal understanding for complex tasks like long video and image analysis. The post MMCTAgent: Enabling multimodal reasoning over large video and image collections appeared first on Mic",
    "url": "https://www.microsoft.com/en-us/research/blog/mmctagent-enabling-multimodal-reasoning-over-large-video-and-image-collections/",
    "source": "Microsoft Research"
  },
  {
    "title": "Introducing NVIDIA Cosmos Policy for Advanced Robot Control",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/cosmos-policy-for-robot-control",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Daggr: Chain apps programmatically, inspect visually",
    "summary": "",
    "url": "https://huggingface.co/blog/daggr",
    "source": "Hugging Face Blog"
  },
  {
    "title": "We Got Claude to Build CUDA Kernels and teach open models!",
    "summary": "",
    "url": "https://huggingface.co/blog/upskill",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Architectural Choices in China's Open-Source AI Ecosystem: Building Beyond DeepSeek",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment-blog-2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Alyah ⭐️: Toward Robust Evaluation of Emirati Dialect Capabilities in Arabic LLMs",
    "summary": "",
    "url": "https://huggingface.co/blog/tiiuae/emirati-benchmarks",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Unlocking Agentic RL Training for GPT-OSS: A Practical Retrospective",
    "summary": "",
    "url": "https://huggingface.co/blog/LinkedIn/gpt-oss-agentic-rl",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AssetOpsBench: Bridging the Gap Between AI Agent Benchmarks and Industrial Reality",
    "summary": "",
    "url": "https://huggingface.co/blog/ibm-research/assetopsbench-playground-on-hugging-face",
    "source": "Hugging Face Blog"
  },
  {
    "title": "One Year Since the “DeepSeek Moment”",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Differential Transformer V2",
    "summary": "",
    "url": "https://huggingface.co/blog/microsoft/diff-attn-v2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Waypoint-1: Real-time interactive video diffusion from Overworld",
    "summary": "",
    "url": "https://huggingface.co/blog/waypoint-1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open Responses: What you need to know",
    "summary": "",
    "url": "https://huggingface.co/blog/open-responses",
    "source": "Hugging Face Blog"
  },
  {
    "title": "NVIDIA Cosmos Reason 2 Brings Advanced Reasoning To Physical AI",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/nvidia-cosmos-reason-2-brings-advanced-reasoning",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Falcon-H1-Arabic: Pushing the Boundaries of Arabic Language AI with Hybrid Architecture",
    "summary": "",
    "url": "https://huggingface.co/blog/tiiuae/falcon-h1-arabic",
    "source": "Hugging Face Blog"
  },
  {
    "title": "NVIDIA brings agents to life with DGX Spark and Reachy Mini",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia-reachy-mini",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AprielGuard: A Guardrail for Safety and Adversarial Robustness in Modern LLM Systems",
    "summary": "",
    "url": "https://huggingface.co/blog/ServiceNow-AI/aprielguard",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Tokenization in Transformers v5: Simpler, Clearer, and More Modular",
    "summary": "",
    "url": "https://huggingface.co/blog/tokenizers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Open Evaluation Standard: Benchmarking NVIDIA Nemotron 3 Nano with NeMo Evaluator",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe",
    "source": "Hugging Face Blog"
  },
  {
    "title": "CUGA on Hugging Face: Democratizing Configurable AI Agents",
    "summary": "",
    "url": "https://huggingface.co/blog/ibm-research/cuga-on-hugging-face",
    "source": "Hugging Face Blog"
  },
  {
    "title": "New in llama.cpp: Model Management",
    "summary": "",
    "url": "https://huggingface.co/blog/ggml-org/model-management-in-llamacpp",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Codex is Open Sourcing AI models",
    "summary": "",
    "url": "https://huggingface.co/blog/hf-skills-training-codex",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing swift-huggingface: The Complete Swift Client for Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/swift-huggingface",
    "source": "Hugging Face Blog"
  },
  {
    "title": "DeepMath: A lightweight math reasoning Agent with smolagents",
    "summary": "",
    "url": "https://huggingface.co/blog/intel-deepmath",
    "source": "Hugging Face Blog"
  },
  {
    "title": "We Got Claude to Fine-Tune an Open Source LLM",
    "summary": "",
    "url": "https://huggingface.co/blog/hf-skills-training",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Transformers v5: Simple model definitions powering the AI ecosystem",
    "summary": "",
    "url": "https://huggingface.co/blog/transformers-v5",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Diffusers welcomes FLUX-2",
    "summary": "",
    "url": "https://huggingface.co/blog/flux-2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Continuous batching from first principles",
    "summary": "",
    "url": "https://huggingface.co/blog/continuous_batching",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Building Deep Research: How we Achieved State of the Art",
    "summary": "",
    "url": "https://huggingface.co/blog/Tavily/tavily-deep-research",
    "source": "Hugging Face Blog"
  },
  {
    "title": "OVHcloud on Hugging Face Inference Providers 🔥",
    "summary": "",
    "url": "https://huggingface.co/blog/OVHcloud/inference-providers-ovhcloud",
    "source": "Hugging Face Blog"
  },
  {
    "title": "20x Faster TRL Fine-tuning with RapidFire AI",
    "summary": "",
    "url": "https://huggingface.co/blog/rapidfireai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open ASR Leaderboard: Trends and Insights with New Multilingual & Long-Form Tracks",
    "summary": "",
    "url": "https://huggingface.co/blog/open-asr-leaderboard",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing AnyLanguageModel: One API for Local and Remote LLMs on Apple Platforms",
    "summary": "",
    "url": "https://huggingface.co/blog/anylanguagemodel",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Apriel-H1: The Surprising Key to Distilling Efficient Reasoning Models",
    "summary": "",
    "url": "https://huggingface.co/blog/ServiceNow-AI/apriel-h1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Easily Build and Share ROCm Kernels with Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/build-rocm-kernels",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Join the AMD Open Robotics Hackathon",
    "summary": "",
    "url": "https://huggingface.co/blog/amd/openroboticshackathon",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Building for an Open Future - our new partnership with Google Cloud",
    "summary": "",
    "url": "https://huggingface.co/blog/google-cloud",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Aligning to What? Rethinking Agent Generalization in MiniMax M2",
    "summary": "",
    "url": "https://huggingface.co/blog/MiniMax-AI/aligning-to-what",
    "source": "Hugging Face Blog"
  },
  {
    "title": "On the Shifting Global Compute Landscape",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface/shifting-compute-landscape",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Building a Healthcare Robot from Simulation to Deployment with NVIDIA Isaac",
    "summary": "",
    "url": "https://huggingface.co/blog/lerobotxnvidia-healthcare",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How to Build a Healthcare Robot from Simulation to Deployment with NVIDIA Isaac for Healthcare",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/nvidia-isaac-for-healthcare",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Granite 4.0 Nano: Just how small can you go?",
    "summary": "",
    "url": "https://huggingface.co/blog/ibm-granite/granite-4-nano",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Voice Cloning with Consent",
    "summary": "",
    "url": "https://huggingface.co/blog/voice-consent-gate",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Streaming datasets: 100x More Efficient",
    "summary": "",
    "url": "https://huggingface.co/blog/streaming-datasets",
    "source": "Hugging Face Blog"
  },
  {
    "title": "huggingface_hub v1.0: Five Years of Building the Foundation of Open Machine Learning",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface-hub-v1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "LeRobot v0.4.0: Supercharging OSS Robot Learning",
    "summary": "",
    "url": "https://huggingface.co/blog/lerobot-release-v040",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Building the Open Agent Ecosystem Together: Introducing OpenEnv",
    "summary": "",
    "url": "https://huggingface.co/blog/openenv",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face and VirusTotal collaborate to strengthen AI security",
    "summary": "",
    "url": "https://huggingface.co/blog/virustotal",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Sentence Transformers is joining Hugging Face!",
    "summary": "",
    "url": "https://huggingface.co/blog/sentence-transformers-joins-hf",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Supercharge your OCR Pipelines with Open Models",
    "summary": "",
    "url": "https://huggingface.co/blog/ocr-open-models",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Unlock the power of images with AI Sheets",
    "summary": "",
    "url": "https://huggingface.co/blog/aisheets-unlock-images",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AI for Food Allergies",
    "summary": "",
    "url": "https://huggingface.co/blog/hugging-science/ai-for-food-allergies",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Google Cloud C4 Brings a 70% TCO improvement on GPT OSS with Intel and Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/gpt-oss-on-intel-xeon",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Get your VLM running in 3 simple steps on Intel CPUs",
    "summary": "",
    "url": "https://huggingface.co/blog/openvino-vlm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Nemotron-Personas-India: Synthesized Data for Sovereign AI",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/nemotron-personas-india",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Arm will be @ PyTorch Conference, Join Us!",
    "summary": "",
    "url": "https://huggingface.co/blog/Arm/arm-at-pytorch-conference",
    "source": "Hugging Face Blog"
  },
  {
    "title": "BigCodeArena: Judging code generations end to end with code executions",
    "summary": "",
    "url": "https://huggingface.co/blog/bigcode/arena",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SOTA OCR with Core ML and dots.ocr",
    "summary": "",
    "url": "https://huggingface.co/blog/dots-ocr-ne",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing RTEB: A New Standard for Retrieval Evaluation",
    "summary": "",
    "url": "https://huggingface.co/blog/rteb",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerating Qwen3-8B Agent on Intel® Core™ Ultra with Depth-Pruned Draft Models",
    "summary": "",
    "url": "https://huggingface.co/blog/intel-qwen3-agent",
    "source": "Hugging Face Blog"
  },
  {
    "title": "VibeGame: Exploring Vibe Coding Games",
    "summary": "",
    "url": "https://huggingface.co/blog/vibegame",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Nemotron-Personas-Japan: ソブリン AI のための合成データセット",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/nemotron-personas-japan-ja",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Swift Transformers Reaches 1.0 – and Looks to the Future",
    "summary": "",
    "url": "https://huggingface.co/blog/swift-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Smol2Operator: Post-Training GUI Agents for Computer Use",
    "summary": "",
    "url": "https://huggingface.co/blog/smol2operator",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SyGra: The One-Stop Framework for Building Data for LLMs and SLMs",
    "summary": "",
    "url": "https://huggingface.co/blog/ServiceNow-AI/sygra-data-gen-framework",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Gaia2 and ARE: Empowering the community to study agents",
    "summary": "",
    "url": "https://huggingface.co/blog/gaia2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Scaleway on Hugging Face Inference Providers 🔥",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-providers-scaleway",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Democratizing AI Safety with RiskRubric.ai",
    "summary": "",
    "url": "https://huggingface.co/blog/riskrubric",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Public AI on Hugging Face Inference Providers 🔥",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-providers-publicai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "`LeRobotDataset:v3.0`: Bringing large-scale datasets to `lerobot`",
    "summary": "",
    "url": "https://huggingface.co/blog/lerobot-datasets-v3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Visible Watermarking with Gradio",
    "summary": "",
    "url": "https://huggingface.co/blog/watermarking-with-gradio",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Palmyra-mini family: Powerful, lightweight, and ready to reason!",
    "summary": "",
    "url": "https://huggingface.co/blog/Writer/announcing-palmyra-mini",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Tricks from OpenAI gpt-oss YOU 🫵 can use with transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/faster-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-tune Any LLM from the Hugging Face Hub with Together AI",
    "summary": "",
    "url": "https://huggingface.co/blog/togethercomputer/together-ft",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Jupyter Agents: training LLMs to reason with notebooks",
    "summary": "",
    "url": "https://huggingface.co/blog/jupyter-agent-2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "mmBERT: ModernBERT goes Multilingual",
    "summary": "",
    "url": "https://huggingface.co/blog/mmbert",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome EmbeddingGemma, Google's new efficient embedding model",
    "summary": "",
    "url": "https://huggingface.co/blog/embeddinggemma",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SAIR: Accelerating Pharma R&D with AI-Powered Structural Intelligence",
    "summary": "",
    "url": "https://huggingface.co/blog/SandboxAQ/sair-data-accelerating-drug-discovery-with-ai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Make your ZeroGPU Spaces go brrr with ahead-of-time compilation",
    "summary": "",
    "url": "https://huggingface.co/blog/zerogpu-aoti",
    "source": "Hugging Face Blog"
  },
  {
    "title": "NVIDIA Releases 6 Million Multi-Lingual Reasoning Dataset",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/multilingual-reasoning-v1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Generate Images with Claude and Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/claude-and-mcp",
    "source": "Hugging Face Blog"
  },
  {
    "title": "From Zero to GPU: A Guide to Building and Scaling Production-Ready CUDA Kernels",
    "summary": "",
    "url": "https://huggingface.co/blog/kernel-builder",
    "source": "Hugging Face Blog"
  },
  {
    "title": "MCP for Research: How to Connect AI to Research Tools",
    "summary": "",
    "url": "https://huggingface.co/blog/mcp-for-research",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Kimina-Prover-RL",
    "summary": "",
    "url": "https://huggingface.co/blog/AI-MO/kimina-prover-rl",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Arm & ExecuTorch 0.7: Bringing Generative AI to the masses",
    "summary": "",
    "url": "https://huggingface.co/blog/Arm/executorch-0-dot-7",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Neural Super Sampling is here!",
    "summary": "",
    "url": "https://huggingface.co/blog/Arm/neural-super-sampling",
    "source": "Hugging Face Blog"
  },
  {
    "title": "TextQuests: How Good are LLMs at Text-Based Video Games?",
    "summary": "",
    "url": "https://huggingface.co/blog/textquests",
    "source": "Hugging Face Blog"
  },
  {
    "title": "🇵🇭 FilBench - Can LLMs Understand and Generate Filipino?",
    "summary": "",
    "url": "https://huggingface.co/blog/filbench",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing AI Sheets: a tool to work with datasets using open AI models!",
    "summary": "",
    "url": "https://huggingface.co/blog/aisheets",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerate ND-Parallel: A guide to Efficient Multi-GPU Training",
    "summary": "",
    "url": "https://huggingface.co/blog/accelerate-nd-parallel",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Vision Language Model Alignment in TRL ⚡️",
    "summary": "",
    "url": "https://huggingface.co/blog/trl-vlm-alignment",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome GPT OSS, the new open-source model family from OpenAI!",
    "summary": "",
    "url": "https://huggingface.co/blog/welcome-openai-gpt-oss",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Measuring Open-Source Llama Nemotron Models on DeepResearch Bench",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/ai-q-top-ranking-open-portable-deep-research-agent",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Implementing MCP Servers in Python: An AI Shopping Assistant with Gradio",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-vton-mcp",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Trackio: A Lightweight Experiment Tracking Library from Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/trackio",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Say hello to `hf`: a faster, friendlier Hugging Face CLI ✨",
    "summary": "",
    "url": "https://huggingface.co/blog/hf-cli",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Parquet Content-Defined Chunking",
    "summary": "",
    "url": "https://huggingface.co/blog/parquet-cdc",
    "source": "Hugging Face Blog"
  },
  {
    "title": "TimeScope: How Long Can Your Video Large Multimodal Model Go?",
    "summary": "",
    "url": "https://huggingface.co/blog/timescope-video-lmm-benchmark",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fast LoRA inference for Flux with Diffusers and PEFT",
    "summary": "",
    "url": "https://huggingface.co/blog/lora-fast",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerate a World of LLMs on Hugging Face with NVIDIA NIM",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/multi-llm-nim",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Arc Virtual Cell Challenge: A Primer",
    "summary": "",
    "url": "https://huggingface.co/blog/virtual-cell-challenge",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Consilium: When Multiple LLMs Collaborate",
    "summary": "",
    "url": "https://huggingface.co/blog/consilium-multi-llm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Back to The Future: Evaluating AI Agents on Predicting Future Events",
    "summary": "",
    "url": "https://huggingface.co/blog/futurebench",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Five Big Improvements to Gradio MCP Servers",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-mcp-updates",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Ettin Suite: SoTA Paired Encoders and Decoders",
    "summary": "",
    "url": "https://huggingface.co/blog/ettin",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Migrating the Hub from Git LFS to Xet",
    "summary": "",
    "url": "https://huggingface.co/blog/migrating-the-hub-to-xet",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Kimina-Prover: Applying Test-time RL Search on Large Formal Reasoning Models",
    "summary": "",
    "url": "https://huggingface.co/blog/AI-MO/kimina-prover",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Asynchronous Robot Inference: Decoupling Action Prediction and Execution",
    "summary": "",
    "url": "https://huggingface.co/blog/async-robot-inference",
    "source": "Hugging Face Blog"
  },
  {
    "title": "ScreenEnv: Deploy your full stack Desktop Agent",
    "summary": "",
    "url": "https://huggingface.co/blog/screenenv",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Building the Hugging Face MCP Server",
    "summary": "",
    "url": "https://huggingface.co/blog/building-hf-mcp",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Reachy Mini - The Open-Source Robot for Today's and Tomorrow's AI Builders",
    "summary": "",
    "url": "https://huggingface.co/blog/reachy-mini",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Creating custom kernels for the AMD MI300",
    "summary": "",
    "url": "https://huggingface.co/blog/mi300kernels",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Upskill your LLMs With Gradio MCP Servers",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-mcp-servers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SmolLM3: smol, multilingual, long-context reasoner",
    "summary": "",
    "url": "https://huggingface.co/blog/smollm3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Three Mighty Alerts Supporting Hugging Face’s Production Infrastructure",
    "summary": "",
    "url": "https://huggingface.co/blog/infrastructure-alerting",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Efficient MultiModal Data Pipeline",
    "summary": "",
    "url": "https://huggingface.co/blog/mmdp",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Announcing  NeurIPS 2025 E2LM Competition: Early Training Evaluation of Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/tiiuae/e2lm-competition",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Training and Finetuning Sparse Embedding Models with Sentence Transformers v5",
    "summary": "",
    "url": "https://huggingface.co/blog/train-sparse-encoder",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome the NVIDIA Llama Nemotron Nano VLM to Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/llama-nemotron-nano-vl",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Gemma 3n fully available in the open-source ecosystem!",
    "summary": "",
    "url": "https://huggingface.co/blog/gemma3n",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Transformers backend integration in SGLang",
    "summary": "",
    "url": "https://huggingface.co/blog/transformers-backend-sglang",
    "source": "Hugging Face Blog"
  },
  {
    "title": "(LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware",
    "summary": "",
    "url": "https://huggingface.co/blog/flux-qlora",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Groq on Hugging Face Inference Providers 🔥",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-providers-groq",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How Long Prompts Block Other Requests - Optimizing LLM Performance",
    "summary": "",
    "url": "https://huggingface.co/blog/tngtech/llm-performance-blocked-by-long-prompts",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Learn the Hugging Face Kernel Hub in 5 Minutes",
    "summary": "",
    "url": "https://huggingface.co/blog/hello-hf-kernels",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Featherless AI on Hugging Face Inference Providers 🔥",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-providers-featherless",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Post-Training Isaac GR00T N1.5 for LeRobot SO-101 Arm",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/gr00t-n1-5-so101-tuning",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Training Cluster as a Service - a new collaboration with NVIDIA",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia-training-cluster",
    "source": "Hugging Face Blog"
  },
  {
    "title": "ScreenSuite - The most comprehensive evaluation suite for GUI Agents!",
    "summary": "",
    "url": "https://huggingface.co/blog/screensuite",
    "source": "Hugging Face Blog"
  },
  {
    "title": "KV Cache from scratch in nanoVLM",
    "summary": "",
    "url": "https://huggingface.co/blog/kv-cache",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Real-Time AI Sound Generation on Arm: A Personal Tool for Creative Freedom",
    "summary": "",
    "url": "https://huggingface.co/blog/Arm/ai-sound-gen-on-arm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Holo1: New family of GUI automation VLMs powering GUI agent Surfer-H",
    "summary": "",
    "url": "https://huggingface.co/blog/Hcompany/holo1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SmolVLA: Efficient Vision-Language-Action Model trained on Lerobot Community Data",
    "summary": "",
    "url": "https://huggingface.co/blog/smolvla",
    "source": "Hugging Face Blog"
  },
  {
    "title": "No GPU left behind: Unlocking Efficiency with Co-located vLLM in TRL",
    "summary": "",
    "url": "https://huggingface.co/blog/vllm-colocate",
    "source": "Hugging Face Blog"
  },
  {
    "title": "CodeAgents + Structure: A Better Way to Execute Actions",
    "summary": "",
    "url": "https://huggingface.co/blog/structured-codeagent",
    "source": "Hugging Face Blog"
  },
  {
    "title": "🐯 Liger GRPO meets TRL",
    "summary": "",
    "url": "https://huggingface.co/blog/liger-grpo",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Dell Enterprise Hub is all you need to build AI on premises",
    "summary": "",
    "url": "https://huggingface.co/blog/dell-ai-applications",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Tiny Agents in Python: a MCP-powered agent in ~70 lines of code",
    "summary": "",
    "url": "https://huggingface.co/blog/python-tiny-agents",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance",
    "summary": "",
    "url": "https://huggingface.co/blog/tiiuae/falcon-h1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Falcon-Arabic: A Breakthrough in Arabic Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/tiiuae/falcon-arabic",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Exploring Quantization Backends in Diffusers",
    "summary": "",
    "url": "https://huggingface.co/blog/diffusers-quantization",
    "source": "Hugging Face Blog"
  },
  {
    "title": "nanoVLM: The simplest repository to train your VLM in pure PyTorch",
    "summary": "",
    "url": "https://huggingface.co/blog/nanovlm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Microsoft and Hugging Face expand collaboration",
    "summary": "",
    "url": "https://huggingface.co/blog/azure-ai-foundry",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Falcon-Edge: A series of powerful, universal, fine-tunable 1.58bit language models.",
    "summary": "",
    "url": "https://huggingface.co/blog/tiiuae/falcon-edge",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Transformers Library: standardizing model definitions",
    "summary": "",
    "url": "https://huggingface.co/blog/transformers-model-definition",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Improving Hugging Face Model Access for Kaggle Users",
    "summary": "",
    "url": "https://huggingface.co/blog/kaggle-integration",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Blazingly fast whisper transcriptions with Inference Endpoints",
    "summary": "",
    "url": "https://huggingface.co/blog/fast-whisper-endpoints",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Vision Language Models (Better, faster, stronger)",
    "summary": "",
    "url": "https://huggingface.co/blog/vlms-2025",
    "source": "Hugging Face Blog"
  },
  {
    "title": "LeRobot Community Datasets: The “ImageNet” of Robotics — When and How?",
    "summary": "",
    "url": "https://huggingface.co/blog/lerobot-datasets",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How to Build an MCP Server with Gradio",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-mcp",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The 4 Things Qwen-3’s Chat Template Teaches Us",
    "summary": "",
    "url": "https://huggingface.co/blog/qwen-3-chat-template-deep-dive",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcoming Llama Guard 4 on Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/llama-guard-4",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing AutoRound: Intel’s Advanced Quantization for LLMs and VLMs",
    "summary": "",
    "url": "https://huggingface.co/blog/autoround",
    "source": "Hugging Face Blog"
  },
  {
    "title": "PipelineRL",
    "summary": "",
    "url": "https://huggingface.co/blog/ServiceNow/pipelinerl",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Tiny Agents: an MCP-powered agent in 50 lines of code",
    "summary": "",
    "url": "https://huggingface.co/blog/tiny-agents",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Finetuning olmOCR to be a faithful OCR-Engine",
    "summary": "",
    "url": "https://huggingface.co/blog/tngtech/finetuning-olmocr-to-be-a-faithful-ocr-engine",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Prefill and Decode for Concurrent Requests - Optimizing LLM Performance",
    "summary": "",
    "url": "https://huggingface.co/blog/tngtech/llm-performance-prefill-decode-concurrent-requests",
    "source": "Hugging Face Blog"
  },
  {
    "title": "17 Reasons Why Gradio Isn't Just Another UI Library",
    "summary": "",
    "url": "https://huggingface.co/blog/why-gradio-stands-out",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Cohere on Hugging Face Inference Providers 🔥",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-providers-cohere",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing HELMET: Holistically Evaluating Long-context Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/helmet",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face to sell open-source robots thanks to Pollen Robotics acquisition 🤖",
    "summary": "",
    "url": "https://huggingface.co/blog/hugging-face-pollen-robotics-acquisition",
    "source": "Hugging Face Blog"
  },
  {
    "title": "4M Models Scanned: Protect AI + Hugging Face 6 Months In",
    "summary": "",
    "url": "https://huggingface.co/blog/pai-6-month",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Visual Salamandra: Pushing the Boundaries of Multimodal Understanding",
    "summary": "",
    "url": "https://huggingface.co/blog/BSC-LT/visualsalamandra7b",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face and Cloudflare Partner to Make Real-Time Speech and Video Seamless with FastRTC",
    "summary": "",
    "url": "https://huggingface.co/blog/fastrtc-cloudflare",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Arabic Leaderboards: Introducing Arabic Instruction Following, Updating AraGen, and More",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-3c3h-aragen-ifeval",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome Llama 4 Maverick & Scout on Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/llama4-release",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Journey to 1 Million Gradio Users!",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-1m",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The NLP Course is becoming the LLM Course",
    "summary": "",
    "url": "https://huggingface.co/blog/llm-course",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Efficient Request Queueing – Optimizing LLM Performance",
    "summary": "",
    "url": "https://huggingface.co/blog/tngtech/llm-performance-request-queueing",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How Hugging Face Scaled Secrets Management for AI Infrastructure",
    "summary": "",
    "url": "https://huggingface.co/blog/scaling-secrets-management",
    "source": "Hugging Face Blog"
  },
  {
    "title": "🚀 Accelerating LLM Inference with TGI on Intel Gaudi",
    "summary": "",
    "url": "https://huggingface.co/blog/intel-gaudi-backend-for-tgi",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open R1: Update #4",
    "summary": "",
    "url": "https://huggingface.co/blog/open-r1/update-4",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Training and Finetuning Reranker Models with Sentence Transformers v4",
    "summary": "",
    "url": "https://huggingface.co/blog/train-reranker",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Gradio's new Dataframe!",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-dataframe-upgrade",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The New and Fresh analytics in Inference Endpoints",
    "summary": "",
    "url": "https://huggingface.co/blog/endpoint-analytics",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open R1: How to use OlympicCoder locally for coding",
    "summary": "",
    "url": "https://huggingface.co/blog/olympic-coder-lmstudio",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AI Policy @🤗: Response to the White House AI Action Plan RFI",
    "summary": "",
    "url": "https://huggingface.co/blog/ai-action-wh-2025",
    "source": "Hugging Face Blog"
  },
  {
    "title": "NVIDIA's GTC 2025 Announcement for Physical AI Developers: New Open Models and Datasets",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia-physical-ai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Xet is on the Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/xet-on-the-hub",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome Gemma 3: Google's all new multimodal, multilingual, long context open LLM",
    "summary": "",
    "url": "https://huggingface.co/blog/gemma3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open R1: Update #3",
    "summary": "",
    "url": "https://huggingface.co/blog/open-r1/update-3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "LeRobot goes to driving school: World’s largest open-source self-driving dataset",
    "summary": "",
    "url": "https://huggingface.co/blog/lerobot-goes-to-driving-school",
    "source": "Hugging Face Blog"
  },
  {
    "title": "LLM Inference on Edge: A Fun and Easy Guide to run LLMs via React Native on your Phone!",
    "summary": "",
    "url": "https://huggingface.co/blog/llm-inference-on-edge",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face and JFrog partner to make AI Security more transparent",
    "summary": "",
    "url": "https://huggingface.co/blog/jfrog",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A Deepdive into Aya Vision: Advancing the Frontier of Multilingual Multimodality",
    "summary": "",
    "url": "https://huggingface.co/blog/aya-vision",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Trace & Evaluate your Agent with Arize Phoenix",
    "summary": "",
    "url": "https://huggingface.co/blog/smolagents-phoenix",
    "source": "Hugging Face Blog"
  },
  {
    "title": "HuggingFace, IISc partner to supercharge model building on India's diverse languages",
    "summary": "",
    "url": "https://huggingface.co/blog/iisc-huggingface-collab",
    "source": "Hugging Face Blog"
  },
  {
    "title": "FastRTC: The Real-Time Communication Library for Python",
    "summary": "",
    "url": "https://huggingface.co/blog/fastrtc",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Remote VAEs for decoding with Inference Endpoints 🤗",
    "summary": "",
    "url": "https://huggingface.co/blog/remote_vae",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SigLIP 2: A better multilingual vision language encoder",
    "summary": "",
    "url": "https://huggingface.co/blog/siglip2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SmolVLM2: Bringing Video Understanding to Every Device",
    "summary": "",
    "url": "https://huggingface.co/blog/smolvlm2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "PaliGemma 2 Mix - New Instruction Vision Language Models by Google",
    "summary": "",
    "url": "https://huggingface.co/blog/paligemma2mix",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Three New Serverless Inference Providers: Hyperbolic, Nebius AI Studio, and Novita 🔥",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-providers-nebius-novita-hyperbolic",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome Fireworks.ai on the Hub 🎆",
    "summary": "",
    "url": "https://huggingface.co/blog/fireworks-ai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fixing Open LLM Leaderboard with Math-Verify",
    "summary": "",
    "url": "https://huggingface.co/blog/math_verify_leaderboard",
    "source": "Hugging Face Blog"
  },
  {
    "title": "1 Billion Classifications",
    "summary": "",
    "url": "https://huggingface.co/blog/billion-classifications",
    "source": "Hugging Face Blog"
  },
  {
    "title": "From Chunks to Blocks: Accelerating Uploads and Downloads on the Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/from-chunks-to-blocks",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Build awesome datasets for video generation",
    "summary": "",
    "url": "https://huggingface.co/blog/vid_ds_scripts",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open R1: Update #2",
    "summary": "",
    "url": "https://huggingface.co/blog/open-r1/update-2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Open Arabic LLM Leaderboard 2",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-arabic-v2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open-source DeepResearch – Freeing our search agents",
    "summary": "",
    "url": "https://huggingface.co/blog/open-deep-research",
    "source": "Hugging Face Blog"
  },
  {
    "title": "π0 and π0-FAST: Vision-Language-Action Models for General Robot Control",
    "summary": "",
    "url": "https://huggingface.co/blog/pi0",
    "source": "Hugging Face Blog"
  },
  {
    "title": "DABStep: Data Agent Benchmark for Multi-step Reasoning",
    "summary": "",
    "url": "https://huggingface.co/blog/dabstep",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open-R1: Update #1",
    "summary": "",
    "url": "https://huggingface.co/blog/open-r1/update-1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Mini-R1: Reproduce Deepseek R1 „aha moment“ a RL tutorial",
    "summary": "",
    "url": "https://huggingface.co/blog/open-r1/mini-r1-contdown-game",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The AI tools for Art Newsletter - Issue 1",
    "summary": "",
    "url": "https://huggingface.co/blog/ai-art-newsletter-jan-25",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How to deploy and fine-tune DeepSeek models on AWS",
    "summary": "",
    "url": "https://huggingface.co/blog/deepseek-r1-aws",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome to Inference Providers on the Hub 🔥",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-providers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open-R1: a fully open reproduction of DeepSeek-R1",
    "summary": "",
    "url": "https://huggingface.co/blog/open-r1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "State of open video generation models in Diffusers",
    "summary": "",
    "url": "https://huggingface.co/blog/video_gen",
    "source": "Hugging Face Blog"
  },
  {
    "title": "We now support VLMs in smolagents!",
    "summary": "",
    "url": "https://huggingface.co/blog/smolagents-can-see",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Mastering Long Contexts in LLMs with KVPress",
    "summary": "",
    "url": "https://huggingface.co/blog/nvidia/kvpress",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SmolVLM Grows Smaller – Introducing the 256M & 500M Models!",
    "summary": "",
    "url": "https://huggingface.co/blog/smolervlm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face and FriendliAI partner to supercharge model deployment on the Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/friendliai-partnership",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Yay! Organizations can now publish blog Articles",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface/blog-articles-for-orgs",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Timm ❤️ Transformers: Use any timm model with transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/timm-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing multi-backends (TRT-LLM, vLLM) support for Text Generation Inference",
    "summary": "",
    "url": "https://huggingface.co/blog/tgi-multi-backend",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Train 400x faster Static Embedding Models with Sentence Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/static-embeddings",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AI Agents Are Here. What Now?",
    "summary": "",
    "url": "https://huggingface.co/blog/ethics-soc-7",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Visual Document Retrieval Goes Multilingual",
    "summary": "",
    "url": "https://huggingface.co/blog/vdr-2b-multilingual",
    "source": "Hugging Face Blog"
  },
  {
    "title": "CO₂ Emissions and Models Performance: Insights from the Open LLM Leaderboard",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-emissions-analysis",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing smolagents: simple agents that write actions in code.",
    "summary": "",
    "url": "https://huggingface.co/blog/smolagents",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Visualize and understand GPU memory in PyTorch",
    "summary": "",
    "url": "https://huggingface.co/blog/train_memory",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Controlling Language Model Generation with NVIDIA's LogitsProcessorZoo",
    "summary": "",
    "url": "https://huggingface.co/blog/logits-processor-zoo",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Evaluating Audio Reasoning with Big Bench Audio",
    "summary": "",
    "url": "https://huggingface.co/blog/big-bench-audio-release",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Finally, a Replacement for BERT: Introducing ModernBERT",
    "summary": "",
    "url": "https://huggingface.co/blog/modernbert",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Bamba: Inference-Efficient Hybrid Mamba2 Model",
    "summary": "",
    "url": "https://huggingface.co/blog/bamba",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome to the Falcon 3 Family of Open Models!",
    "summary": "",
    "url": "https://huggingface.co/blog/falcon3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Benchmarking Language Model Performance on 5th Gen Xeon at GCP",
    "summary": "",
    "url": "https://huggingface.co/blog/intel-gcp-c4",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Synthetic Data Generator - Build Datasets with Natural Language",
    "summary": "",
    "url": "https://huggingface.co/blog/synthetic-data-generator",
    "source": "Hugging Face Blog"
  },
  {
    "title": "LeMaterial: an open source initiative to accelerate materials discovery and research",
    "summary": "",
    "url": "https://huggingface.co/blog/lematerial",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face models in Amazon Bedrock",
    "summary": "",
    "url": "https://huggingface.co/blog/bedrock-marketplace",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open Preference Dataset for Text-to-Image Generation by the 🤗 Community",
    "summary": "",
    "url": "https://huggingface.co/blog/image-preferences",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome PaliGemma 2 – New vision language models by Google",
    "summary": "",
    "url": "https://huggingface.co/blog/paligemma2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How good are LLMs at fixing their mistakes? A chatbot arena experiment with Keras and TPUs",
    "summary": "",
    "url": "https://huggingface.co/blog/keras-chatbot-arena",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Rethinking LLM Evaluation with 3C3H: AraGen Benchmark and Leaderboard",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-3c3h-aragen",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Investing in Performance: Fine-tune small models with LLM insights  - a CFM case study",
    "summary": "",
    "url": "https://huggingface.co/blog/cfm-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open Source Developers Guide to the EU AI Act",
    "summary": "",
    "url": "https://huggingface.co/blog/eu-ai-act-for-oss-developers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Rearchitecting Hugging Face Uploads and Downloads",
    "summary": "",
    "url": "https://huggingface.co/blog/rearchitecting-uploads-and-downloads",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SmolVLM - small yet mighty Vision Language Model",
    "summary": "",
    "url": "https://huggingface.co/blog/smolvlm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "You could have designed state of the art positional encoding",
    "summary": "",
    "url": "https://huggingface.co/blog/designing-positional-encoding",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Letting Large Models Debate: The First Multilingual LLM Debate Competition",
    "summary": "",
    "url": "https://huggingface.co/blog/debate",
    "source": "Hugging Face Blog"
  },
  {
    "title": "From Files to Chunks: Improving HF Storage Efficiency",
    "summary": "",
    "url": "https://huggingface.co/blog/from-files-to-chunks",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Faster Text Generation with Self-Speculative Decoding",
    "summary": "",
    "url": "https://huggingface.co/blog/layerskip",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Open Leaderboard for Japanese LLMs!",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-japanese",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Judge Arena: Benchmarking LLMs as Evaluators",
    "summary": "",
    "url": "https://huggingface.co/blog/arena-atla",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Share your open ML datasets on Hugging Face Hub!",
    "summary": "",
    "url": "https://huggingface.co/blog/researcher-dataset-sharing",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face + PyCharm",
    "summary": "",
    "url": "https://huggingface.co/blog/pycharm-integration",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Argilla 2.4: Easily Build Fine-Tuning and Evaluation Datasets on the Hub — No Code Required",
    "summary": "",
    "url": "https://huggingface.co/blog/argilla-ui-hub",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Universal Assisted Generation: Faster Decoding with Any Assistant Model",
    "summary": "",
    "url": "https://huggingface.co/blog/universal_assisted_generation",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Expert Support case study: Bolstering a RAG app with LLM-as-a-Judge",
    "summary": "",
    "url": "https://huggingface.co/blog/digital-green-llm-judge",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A Deepdive into Aya Expanse: Advancing the Frontier of Multilinguality",
    "summary": "",
    "url": "https://huggingface.co/blog/aya-expanse",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing SynthID Text",
    "summary": "",
    "url": "https://huggingface.co/blog/synthid-text",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing HUGS - Scale your AI with Open Models",
    "summary": "",
    "url": "https://huggingface.co/blog/hugs",
    "source": "Hugging Face Blog"
  },
  {
    "title": "CinePile 2.0 - making stronger datasets with adversarial refinement",
    "summary": "",
    "url": "https://huggingface.co/blog/cinepile2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face Teams Up with Protect AI: Enhancing Model Security for the ML Community",
    "summary": "",
    "url": "https://huggingface.co/blog/protectai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Transformers.js v3: WebGPU Support, New Models & Tasks, and More…",
    "summary": "",
    "url": "https://huggingface.co/blog/transformersjs-v3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Diffusers welcomes Stable Diffusion 3.5 Large",
    "summary": "",
    "url": "https://huggingface.co/blog/sd3-5",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Releasing Outlines-core 0.1.0: structured generation in Rust and Python",
    "summary": "",
    "url": "https://huggingface.co/blog/outlines-core",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploying Speech-to-Speech on Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/s2s_endpoint",
    "source": "Hugging Face Blog"
  },
  {
    "title": "“Llama 3.2 in Keras”",
    "summary": "",
    "url": "https://huggingface.co/blog/keras-llama-32",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fixing Gradient Accumulation",
    "summary": "",
    "url": "https://huggingface.co/blog/gradient_accumulation",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the AMD 5th Gen EPYC™ CPU",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface-amd-turin",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A Security Review of Gradio 5",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-5-security",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome, Gradio 5",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-5",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Scaling AI-based Data Processing with Hugging Face + Dask",
    "summary": "",
    "url": "https://huggingface.co/blog/dask-scaling",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Faster Assisted Generation with Dynamic Speculation",
    "summary": "",
    "url": "https://huggingface.co/blog/dynamic_speculation_lookahead",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Improving Parquet Dedupe on Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/improve_parquet_dedupe",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Open FinLLM Leaderboard",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-finbench",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A Short Summary of Chinese AI Global Expansion",
    "summary": "",
    "url": "https://huggingface.co/blog/chinese-ai-expansion",
    "source": "Hugging Face Blog"
  },
  {
    "title": "🇨🇿 BenCzechMark - Can your LLM Understand Czech?",
    "summary": "",
    "url": "https://huggingface.co/blog/benczechmark",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Converting Vertex-Colored Meshes to Textured Meshes",
    "summary": "",
    "url": "https://huggingface.co/blog/vertex-colored-to-textured-mesh",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Llama can now see and run on your device - welcome Llama 3.2",
    "summary": "",
    "url": "https://huggingface.co/blog/llama32",
    "source": "Hugging Face Blog"
  },
  {
    "title": "FineVideo: behind the scenes",
    "summary": "",
    "url": "https://huggingface.co/blog/fine-video",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Exploring the Daily Papers Page on Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/daily-papers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Optimize and deploy with Optimum-Intel and OpenVINO GenAI",
    "summary": "",
    "url": "https://huggingface.co/blog/deploy-with-openvino",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-tuning LLMs to 1.58bit: extreme quantization made easy",
    "summary": "",
    "url": "https://huggingface.co/blog/1_58_llm_extreme_quantization",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the SQL Console on Datasets",
    "summary": "",
    "url": "https://huggingface.co/blog/sql-console",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Community Tools on HuggingChat",
    "summary": "",
    "url": "https://huggingface.co/blog/community-tools",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerate 1.0.0",
    "summary": "",
    "url": "https://huggingface.co/blog/accelerate-v1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face partners with TruffleHog to Scan for Secrets",
    "summary": "",
    "url": "https://huggingface.co/blog/trufflesecurity-partnership",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Scaling robotics datasets with video encoding",
    "summary": "",
    "url": "https://huggingface.co/blog/video-encoding",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The 5 Most Under-Rated Tools on Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/unsung-heroes",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Improving Hugging Face Training Efficiency Through Packing with Flash Attention 2",
    "summary": "",
    "url": "https://huggingface.co/blog/packing-with-FA2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploy Meta Llama 3.1 405B on Google Cloud Vertex AI",
    "summary": "",
    "url": "https://huggingface.co/blog/llama31-on-vertex-ai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A failed experiment: Infini-Attention, and why we should keep trying?",
    "summary": "",
    "url": "https://huggingface.co/blog/infini-attention",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introduction to ggml",
    "summary": "",
    "url": "https://huggingface.co/blog/introduction-to-ggml",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome Falcon Mamba: The first strong attention-free 7B model",
    "summary": "",
    "url": "https://huggingface.co/blog/falconmamba",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Tool Use, Unified",
    "summary": "",
    "url": "https://huggingface.co/blog/unified-tool-use",
    "source": "Hugging Face Blog"
  },
  {
    "title": "XetHub is joining Hugging Face!",
    "summary": "",
    "url": "https://huggingface.co/blog/xethub-joins-hf",
    "source": "Hugging Face Blog"
  },
  {
    "title": "2024 Security Feature Highlights",
    "summary": "",
    "url": "https://huggingface.co/blog/2024-security-features",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing TextImage Augmentation for Document Images",
    "summary": "",
    "url": "https://huggingface.co/blog/doc_aug_hf_alb",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Google releases Gemma 2 2B, ShieldGemma and Gemma Scope",
    "summary": "",
    "url": "https://huggingface.co/blog/gemma-july-update",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Memory-efficient Diffusion Transformers with Quanto and Diffusers",
    "summary": "",
    "url": "https://huggingface.co/blog/quanto-diffusers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Serverless Inference with Hugging Face and NVIDIA NIM",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-dgx-cloud",
    "source": "Hugging Face Blog"
  },
  {
    "title": "LAVE: Zero-shot VQA Evaluation on Docmatix with LLMs - Do We Still Need Fine-Tuning?",
    "summary": "",
    "url": "https://huggingface.co/blog/zero-shot-vqa-docmatix",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Llama 3.1 - 405B, 70B & 8B with multilinguality and long context",
    "summary": "",
    "url": "https://huggingface.co/blog/llama31",
    "source": "Hugging Face Blog"
  },
  {
    "title": "WWDC 24: Running Mistral 7B with Core ML",
    "summary": "",
    "url": "https://huggingface.co/blog/mistral-coreml",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Docmatix - a huge dataset for Document Visual Question Answering",
    "summary": "",
    "url": "https://huggingface.co/blog/docmatix",
    "source": "Hugging Face Blog"
  },
  {
    "title": "TGI Multi-LoRA: Deploy Once, Serve 30 Models",
    "summary": "",
    "url": "https://huggingface.co/blog/multi-lora-serving",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SmolLM - blazingly fast and remarkably powerful",
    "summary": "",
    "url": "https://huggingface.co/blog/smollm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How we leveraged distilabel to create an Argilla 2.0 Chatbot",
    "summary": "",
    "url": "https://huggingface.co/blog/argilla-chatbot",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How NuminaMath Won the 1st AIMO Progress Prize",
    "summary": "",
    "url": "https://huggingface.co/blog/winning-aimo-progress-prize",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Announcing New Hugging Face and KerasHub integration",
    "summary": "",
    "url": "https://huggingface.co/blog/keras-hub-integration",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Experimenting with Automatic PII Detection on the Hub using Presidio",
    "summary": "",
    "url": "https://huggingface.co/blog/presidio-pii-detection",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Preference Optimization for Vision Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/dpo_vlm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Google Cloud TPUs made available to Hugging Face users",
    "summary": "",
    "url": "https://huggingface.co/blog/tpu-inference-endpoints-spaces",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Banque des Territoires (CDC Group) x Polyconseil x Hugging Face: Enhancing a Major French Environmental Program with a Sovereign Data Solution",
    "summary": "",
    "url": "https://huggingface.co/blog/sovereign-data-solution-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Announcing New Dataset Search Features",
    "summary": "",
    "url": "https://huggingface.co/blog/datasets-filters",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerating Protein Language Model ProtST on Intel Gaudi 2",
    "summary": "",
    "url": "https://huggingface.co/blog/intel-protein-language-model-protst",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Our Transformers Code Agent beats the GAIA benchmark 🏅",
    "summary": "",
    "url": "https://huggingface.co/blog/beating-gaia",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome Gemma 2 - Google’s new open LLM",
    "summary": "",
    "url": "https://huggingface.co/blog/gemma2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "XLSCOUT Unveils ParaEmbed 2.0: a Powerful Embedding Model Tailored for Patents and IP with Expert Support from Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/xlscout-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-tuning Florence-2 - Microsoft's Cutting-edge Vision Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/finetune-florence2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Ethics and Society Newsletter #6: Building Better AI: The Importance of Data Quality",
    "summary": "",
    "url": "https://huggingface.co/blog/ethics-soc-6",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Data Is Better Together: A Look Back and Forward",
    "summary": "",
    "url": "https://huggingface.co/blog/dibt",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Going multimodal: How Prezi is leveraging the Hub and the Expert Support Program to accelerate their ML roadmap",
    "summary": "",
    "url": "https://huggingface.co/blog/prezi-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "BigCodeBench: The Next Generation of HumanEval",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-bigcodebench",
    "source": "Hugging Face Blog"
  },
  {
    "title": "From DeepSpeed to FSDP and Back Again with Hugging Face Accelerate",
    "summary": "",
    "url": "https://huggingface.co/blog/deepspeed-to-fsdp-and-back",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Diffusers welcomes Stable Diffusion 3",
    "summary": "",
    "url": "https://huggingface.co/blog/sd3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Putting RL back in RLHF",
    "summary": "",
    "url": "https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Making sense of this mess",
    "summary": "",
    "url": "https://huggingface.co/blog/transformers-docs-redesign",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Hugging Face Embedding Container for Amazon SageMaker",
    "summary": "",
    "url": "https://huggingface.co/blog/sagemaker-huggingface-embedding",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Launching the Artificial Analysis Text to Image Leaderboard & Arena",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-artificial-analysis2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing NPC-Playground, a 3D playground to interact with LLM-powered NPCs",
    "summary": "",
    "url": "https://huggingface.co/blog/npc-gigax-cubzh",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Faster assisted generation support for Intel Gaudi",
    "summary": "",
    "url": "https://huggingface.co/blog/assisted-generation-support-gaudi",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Space secrets security update",
    "summary": "",
    "url": "https://huggingface.co/blog/space-secrets-disclosure",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Benchmarking Text Generation Inference",
    "summary": "",
    "url": "https://huggingface.co/blog/tgi-benchmarking",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Training and Finetuning Embedding Models with Sentence Transformers v3",
    "summary": "",
    "url": "https://huggingface.co/blog/train-sentence-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages",
    "summary": "",
    "url": "https://huggingface.co/blog/falcon2-11b",
    "source": "Hugging Face Blog"
  },
  {
    "title": "CyberSecEval 2 - A Comprehensive Evaluation Framework for Cybersecurity Risks and Capabilities of Large Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-llamaguard",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploy models on AWS Inferentia2 from Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/inferentia-inference-endpoints",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Spaces Dev Mode for a seamless developer experience",
    "summary": "",
    "url": "https://huggingface.co/blog/spaces-dev-mode",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Build AI on premise with Dell Enterprise Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/dell-enterprise-hub",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face on AMD Instinct MI300 GPU",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface-amd-mi300",
    "source": "Hugging Face Blog"
  },
  {
    "title": "From cloud to developers: Hugging Face and Microsoft Deepen Collaboration",
    "summary": "",
    "url": "https://huggingface.co/blog/microsoft-collaboration",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Unlocking Longer Generation with Key-Value Cache Quantization",
    "summary": "",
    "url": "https://huggingface.co/blog/kv-cache-quantization",
    "source": "Hugging Face Blog"
  },
  {
    "title": "PaliGemma – Google's Cutting-Edge Open Vision Language Model",
    "summary": "",
    "url": "https://huggingface.co/blog/paligemma",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face x LangChain : A new partner package",
    "summary": "",
    "url": "https://huggingface.co/blog/langchain",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Open Arabic LLM Leaderboard",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-arabic",
    "source": "Hugging Face Blog"
  },
  {
    "title": "License to Call: Introducing Transformers Agents 2.0",
    "summary": "",
    "url": "https://huggingface.co/blog/agents",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Subscribe to Enterprise Hub with your AWS Account",
    "summary": "",
    "url": "https://huggingface.co/blog/enterprise-hub-aws-marketplace",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Building Cost-Efficient Enterprise RAG applications with Intel Gaudi 2 and Intel Xeon",
    "summary": "",
    "url": "https://huggingface.co/blog/cost-efficient-rag-applications-with-intel",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Open Leaderboard for Hebrew LLMs!",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-hebrew",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Bringing the Artificial Analysis LLM Performance Leaderboard to Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-artificial-analysis",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Powerful ASR + diarization + speculative decoding with Hugging Face Inference Endpoints",
    "summary": "",
    "url": "https://huggingface.co/blog/asr-diarization",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Improving Prompt Consistency with Structured Generations",
    "summary": "",
    "url": "https://huggingface.co/blog/evaluation-structured-outputs",
    "source": "Hugging Face Blog"
  },
  {
    "title": "StarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for Code Generation",
    "summary": "",
    "url": "https://huggingface.co/blog/sc2-instruct",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Open Chain of Thought Leaderboard",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-cot",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent",
    "summary": "",
    "url": "https://huggingface.co/blog/jat",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Open Medical-LLM Leaderboard: Benchmarking Large Language Models in Healthcare",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-medicalllm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome Llama 3 - Meta's new open LLM",
    "summary": "",
    "url": "https://huggingface.co/blog/llama3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AI Apps in a Flash with Gradio's Reload Mode",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-reload",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the LiveCodeBench Leaderboard - Holistic and Contamination-Free Evaluation of Code LLMs",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-livecodebench",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Running Privacy-Preserving Inferences on Hugging Face Endpoints",
    "summary": "",
    "url": "https://huggingface.co/blog/fhe-endpoints",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Ryght’s Journey to Empower Healthcare and Life Sciences with Expert Support from Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/ryght-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Idefics2: A Powerful 8B Vision-Language Model for the community",
    "summary": "",
    "url": "https://huggingface.co/blog/idefics2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Vision Language Models Explained",
    "summary": "",
    "url": "https://huggingface.co/blog/vlms",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Making thousands of open LLMs bloom in the Vertex AI Model Garden",
    "summary": "",
    "url": "https://huggingface.co/blog/google-cloud-model-garden",
    "source": "Hugging Face Blog"
  },
  {
    "title": "CodeGemma - an official Google release for code LLMs",
    "summary": "",
    "url": "https://huggingface.co/blog/codegemma",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Public Policy at Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/policy-blog",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face partners with Wiz Research to Improve AI Security",
    "summary": "",
    "url": "https://huggingface.co/blog/hugging-face-wiz-security-blog",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Text2SQL using Hugging Face Dataset Viewer API and Motherduck DuckDB-NSQL-7B",
    "summary": "",
    "url": "https://huggingface.co/blog/duckdb-nsql-7b",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Blazing Fast SetFit Inference with 🤗 Optimum Intel on Xeon",
    "summary": "",
    "url": "https://huggingface.co/blog/setfit-optimum-intel",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Bringing serverless GPU inference to Hugging Face users",
    "summary": "",
    "url": "https://huggingface.co/blog/cloudflare-workers-ai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Pollen-Vision: Unified interface for Zero-Shot vision models in robotics",
    "summary": "",
    "url": "https://huggingface.co/blog/pollen-vision",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Total noob’s intro to Hugging Face Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/noob_intro_transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Binary and Scalar Embedding Quantization for Significantly Faster & Cheaper Retrieval",
    "summary": "",
    "url": "https://huggingface.co/blog/embedding-quantization",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Chatbot Guardrails Arena",
    "summary": "",
    "url": "https://huggingface.co/blog/arena-lighthouz",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A Chatbot on your Laptop: Phi-2 on Intel Meteor Lake",
    "summary": "",
    "url": "https://huggingface.co/blog/phi2-intel-meteor-lake",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Cosmopedia: how to create large-scale synthetic data for pre-training Large Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/cosmopedia",
    "source": "Hugging Face Blog"
  },
  {
    "title": "GaLore: Advancing Large Model Training on Consumer-grade Hardware",
    "summary": "",
    "url": "https://huggingface.co/blog/galore",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Easily Train Models with H100 GPUs on NVIDIA DGX Cloud",
    "summary": "",
    "url": "https://huggingface.co/blog/train-dgx-cloud",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Quanto: a PyTorch quantization backend for Optimum",
    "summary": "",
    "url": "https://huggingface.co/blog/quanto-introduction",
    "source": "Hugging Face Blog"
  },
  {
    "title": "CPU Optimized Embeddings with 🤗 Optimum Intel and fastRAG",
    "summary": "",
    "url": "https://huggingface.co/blog/intel-fast-embedding",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset",
    "summary": "",
    "url": "https://huggingface.co/blog/websight",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing ConTextual: How well can your Multimodal model jointly reason over text and image in text-rich scenes?",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-contextual",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Data is better together: Enabling communities to collectively build better datasets together using Argilla and Hugging Face Spaces",
    "summary": "",
    "url": "https://huggingface.co/blog/community-datasets",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Text-Generation Pipeline on Intel® Gaudi® 2 AI Accelerator",
    "summary": "",
    "url": "https://huggingface.co/blog/textgen-pipe-gaudi",
    "source": "Hugging Face Blog"
  },
  {
    "title": "StarCoder2 and The Stack v2",
    "summary": "",
    "url": "https://huggingface.co/blog/starcoder2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "TTS Arena: Benchmarking Text-to-Speech Models in the Wild",
    "summary": "",
    "url": "https://huggingface.co/blog/arena-tts",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AI Watermarking 101: Tools and Techniques",
    "summary": "",
    "url": "https://huggingface.co/blog/watermarking",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-Tuning Gemma Models in Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/gemma-peft",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Red-Teaming Resistance Leaderboard",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-haizelab",
    "source": "Hugging Face Blog"
  },
  {
    "title": "🪆 Introduction to Matryoshka Embedding Models",
    "summary": "",
    "url": "https://huggingface.co/blog/matryoshka",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome Gemma - Google’s new open LLM",
    "summary": "",
    "url": "https://huggingface.co/blog/gemma",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Open Ko-LLM Leaderboard: Leading the Korean LLM Evaluation Ecosystem",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-upstage",
    "source": "Hugging Face Blog"
  },
  {
    "title": "🤗 PEFT welcomes new merging methods",
    "summary": "",
    "url": "https://huggingface.co/blog/peft_merging",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Synthetic data: save money, time and carbon with open source",
    "summary": "",
    "url": "https://huggingface.co/blog/synthetic-data-save-costs",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AMD Pervasive AI Developer Contest!",
    "summary": "",
    "url": "https://huggingface.co/blog/amd_pervasive_developer_ai_contest",
    "source": "Hugging Face Blog"
  },
  {
    "title": "From OpenAI to Open LLMs with Messages API on Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/tgi-messages-api",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SegMoE: Segmind Mixture of Diffusion Experts",
    "summary": "",
    "url": "https://huggingface.co/blog/segmoe",
    "source": "Hugging Face Blog"
  },
  {
    "title": "NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexity Classes and Dynamic Updates",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-nphardeval",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Constitutional AI with Open LLMs",
    "summary": "",
    "url": "https://huggingface.co/blog/constitutional_ai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face Text Generation Inference available for AWS Inferentia2",
    "summary": "",
    "url": "https://huggingface.co/blog/text-generation-inference-on-inferentia2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Patch Time Series Transformer in Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/patchtst",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Enterprise Scenarios Leaderboard: a Leaderboard for Real World Use Cases",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-patronus",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerate StarCoder with 🤗 Optimum Intel on Xeon: Q8/Q4 and Speculative Decoding",
    "summary": "",
    "url": "https://huggingface.co/blog/intel-starcoder-quantization",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Hallucinations Leaderboard, an Open Effort to Measure Hallucinations in Large Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-hallucinations",
    "source": "Hugging Face Blog"
  },
  {
    "title": "An Introduction to AI Secure LLM Safety Leaderboard",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-decodingtrust",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face and Google partner for open AI collaboration",
    "summary": "",
    "url": "https://huggingface.co/blog/gcp-partnership",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open-source LLMs as LangChain Agents",
    "summary": "",
    "url": "https://huggingface.co/blog/open-source-llms-as-agents",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-Tune W2V2-Bert for low-resource ASR with 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/fine-tune-w2v2-bert",
    "source": "Hugging Face Blog"
  },
  {
    "title": "PatchTSMixer in HuggingFace",
    "summary": "",
    "url": "https://huggingface.co/blog/patchtsmixer",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Preference Tuning LLMs with Direct Preference Optimization Methods",
    "summary": "",
    "url": "https://huggingface.co/blog/pref-tuning",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerating SD Turbo and SDXL Turbo Inference with ONNX Runtime and Olive",
    "summary": "",
    "url": "https://huggingface.co/blog/sdxl_ort_inference",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Run ComfyUI workflows for free with Gradio on Hugging Face Spaces",
    "summary": "",
    "url": "https://huggingface.co/blog/run-comfyui-workflows-on-spaces",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A guide to setting up your own Hugging Face leaderboard: an end-to-end example with Vectara's hallucination leaderboard",
    "summary": "",
    "url": "https://huggingface.co/blog/leaderboard-vectara",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Make LLM Fine-tuning 2x faster with Unsloth and 🤗 TRL",
    "summary": "",
    "url": "https://huggingface.co/blog/unsloth-trl",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome aMUSEd: Efficient Text-to-Image Generation",
    "summary": "",
    "url": "https://huggingface.co/blog/amused",
    "source": "Hugging Face Blog"
  },
  {
    "title": "LoRA training scripts of the world, unite!",
    "summary": "",
    "url": "https://huggingface.co/blog/sdxl_lora_advanced_script",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Speculative Decoding for 2x Faster Whisper Inference",
    "summary": "",
    "url": "https://huggingface.co/blog/whisper-speculative-decoding",
    "source": "Hugging Face Blog"
  },
  {
    "title": "2023, year of open LLMs",
    "summary": "",
    "url": "https://huggingface.co/blog/2023-in-llms",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome Mixtral - a SOTA Mixture of Experts on Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/mixtral",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Mixture of Experts Explained",
    "summary": "",
    "url": "https://huggingface.co/blog/moe",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SetFitABSA: Few-Shot Aspect Based Sentiment Analysis using SetFit",
    "summary": "",
    "url": "https://huggingface.co/blog/setfit-absa",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AMD + 🤗: Large Language Models Out-of-the-Box Acceleration with AMD GPU",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface-and-optimum-amd",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Optimum-NVIDIA Unlocking blazingly fast LLM inference in just 1 line of code",
    "summary": "",
    "url": "https://huggingface.co/blog/optimum-nvidia",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Goodbye cold boot - how we made LoRA Inference 300% faster",
    "summary": "",
    "url": "https://huggingface.co/blog/lora-adapters-dynamic-loading",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open LLM Leaderboard: DROP deep dive",
    "summary": "",
    "url": "https://huggingface.co/blog/open-llm-leaderboard-drop",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SDXL in 4 steps with Latent Consistency LoRAs",
    "summary": "",
    "url": "https://huggingface.co/blog/lcm_lora",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Make your llama generation time fly with AWS Inferentia2",
    "summary": "",
    "url": "https://huggingface.co/blog/inferentia-llama2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Prodigy-HF: a direct integration with Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/prodigy-hf",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tweets Analysis with Lora",
    "summary": "",
    "url": "https://huggingface.co/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Storage Regions on the HF Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/regions",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Personal Copilot: Train Your Own Coding Assistant",
    "summary": "",
    "url": "https://huggingface.co/blog/personal-copilot",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Interactively explore your Huggingface dataset with one line of code",
    "summary": "",
    "url": "https://huggingface.co/blog/scalable-data-inspection",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploy Embedding Models with Hugging Face Inference Endpoints",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-endpoints-embeddings",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The N Implementation Details of RLHF with PPO",
    "summary": "",
    "url": "https://huggingface.co/blog/the_n_implementation_details_of_rlhf_with_ppo",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Exploring simple optimizations for SDXL",
    "summary": "",
    "url": "https://huggingface.co/blog/simple_sdxl_optimizations",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Gradio-Lite: Serverless Gradio Running Entirely in Your Browser",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-lite",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerating over 130,000 Hugging Face models with ONNX Runtime",
    "summary": "",
    "url": "https://huggingface.co/blog/ort-accelerating-hf-models",
    "source": "Hugging Face Blog"
  },
  {
    "title": "🧨 Accelerating Stable Diffusion XL Inference with JAX on Cloud TPU v5e",
    "summary": "",
    "url": "https://huggingface.co/blog/sdxl_jax",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Chat Templates: An End to the Silent Performance Killer",
    "summary": "",
    "url": "https://huggingface.co/blog/chat-templates",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploying the AI Comic Factory using the Inference API",
    "summary": "",
    "url": "https://huggingface.co/blog/ai-comic-factory",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Ethics and Society Newsletter #5: Hugging Face Goes To Washington and Other Summer 2023 Musings",
    "summary": "",
    "url": "https://huggingface.co/blog/ethics-soc-5",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Finetune Stable Diffusion Models with DDPO via TRL",
    "summary": "",
    "url": "https://huggingface.co/blog/trl-ddpo",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Non-engineers guide: Train a LLaMA 2 chatbot",
    "summary": "",
    "url": "https://huggingface.co/blog/Llama2-for-non-engineers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Llama 2 on Amazon SageMaker a Benchmark",
    "summary": "",
    "url": "https://huggingface.co/blog/llama-sagemaker-benchmark",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Inference for PROs",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-pro",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Rocket Money x Hugging Face: Scaling Volatile ML Models in Production​",
    "summary": "",
    "url": "https://huggingface.co/blog/rocketmoney-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introduction to 3D Gaussian Splatting",
    "summary": "",
    "url": "https://huggingface.co/blog/gaussian-splatting",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Object Detection Leaderboard",
    "summary": "",
    "url": "https://huggingface.co/blog/object-detection-leaderboard",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Optimizing your LLM in production",
    "summary": "",
    "url": "https://huggingface.co/blog/optimize-llm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Würstchen: Fast Diffusion for Image Generation",
    "summary": "",
    "url": "https://huggingface.co/blog/wuerstchen",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-tuning Llama 2 70B using PyTorch FSDP",
    "summary": "",
    "url": "https://huggingface.co/blog/ram-efficient-pytorch-fsdp",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Overview of natively supported quantization schemes in 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/overview-quantization-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SafeCoder vs. Closed-source Code Assistants",
    "summary": "",
    "url": "https://huggingface.co/blog/safecoder-vs-closed-source-code-assistants",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Efficient Controllable Generation for SDXL with T2I-Adapters",
    "summary": "",
    "url": "https://huggingface.co/blog/t2i-sdxl-adapters",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Spread Your Wings: Falcon 180B is here",
    "summary": "",
    "url": "https://huggingface.co/blog/falcon-180b",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fetch Cuts ML Processing Latency by 50% Using Amazon SageMaker & Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/fetch-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AudioLDM 2, but faster ⚡️",
    "summary": "",
    "url": "https://huggingface.co/blog/audioldm2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Code Llama: Llama 2 learns to code",
    "summary": "",
    "url": "https://huggingface.co/blog/codellama",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deprecation of Git Authentication using password",
    "summary": "",
    "url": "https://huggingface.co/blog/password-git-deprecation",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Making LLMs lighter with AutoGPTQ and transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/gptq-integration",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing SafeCoder",
    "summary": "",
    "url": "https://huggingface.co/blog/safecoder",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing IDEFICS: An Open Reproduction of State-of-the-art Visual Langage Model",
    "summary": "",
    "url": "https://huggingface.co/blog/idefics",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face Hub on the AWS Marketplace: Pay with your AWS Account",
    "summary": "",
    "url": "https://huggingface.co/blog/aws-marketplace",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Optimizing Bark using 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/optimizing-bark",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploying Hugging Face Models with BentoML: DeepFloyd IF in Action",
    "summary": "",
    "url": "https://huggingface.co/blog/deploy-deepfloydif-using-bentoml",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-tune Llama 2 with DPO",
    "summary": "",
    "url": "https://huggingface.co/blog/dpo-trl",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Releasing Swift Transformers: Run On-Device LLMs in Apple Devices",
    "summary": "",
    "url": "https://huggingface.co/blog/swift-coreml-llm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploy MusicGen in no time with Inference Endpoints",
    "summary": "",
    "url": "https://huggingface.co/blog/run-musicgen-as-an-api",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/huggy-lingo",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Towards Encrypted Large Language Models with FHE",
    "summary": "",
    "url": "https://huggingface.co/blog/encrypted-llm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Practical 3D Asset Generation: A Step-by-Step Guide",
    "summary": "",
    "url": "https://huggingface.co/blog/3d-assets",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open-sourcing Knowledge Distillation Code and Weights of SD-Small and SD-Tiny",
    "summary": "",
    "url": "https://huggingface.co/blog/sd_distillation",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Stable Diffusion XL on Mac with Advanced Core ML Quantization",
    "summary": "",
    "url": "https://huggingface.co/blog/stable-diffusion-xl-coreml",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AI Policy @🤗: Open ML Considerations in the EU AI Act",
    "summary": "",
    "url": "https://huggingface.co/blog/eu-ai-act-oss",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Agents.js: Give tools to your LLMs using JavaScript",
    "summary": "",
    "url": "https://huggingface.co/blog/agents-js",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Results of the Open Source AI Game Jam",
    "summary": "",
    "url": "https://huggingface.co/blog/game-jam-first-edition-results",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Happy 1st anniversary 🤗 Diffusers!",
    "summary": "",
    "url": "https://huggingface.co/blog/diffusers-turns-1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Llama 2 is here - get it on Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/llama2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Building an AI WebTV",
    "summary": "",
    "url": "https://huggingface.co/blog/ai-webtv",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Open-Source Text Generation & LLM Ecosystem at Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/os-llms",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-tuning Stable Diffusion models on Intel CPUs",
    "summary": "",
    "url": "https://huggingface.co/blog/stable-diffusion-finetuning-intel",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Making ML-powered web games with Transformers.js",
    "summary": "",
    "url": "https://huggingface.co/blog/ml-web-games",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploy LLMs with Hugging Face Inference Endpoints",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-endpoints-llm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Making a web app generator with open ML models",
    "summary": "",
    "url": "https://huggingface.co/blog/text-to-webapp",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Leveraging Hugging Face for complex generative AI use cases",
    "summary": "",
    "url": "https://huggingface.co/blog/writer-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerating Vision-Language Models: BridgeTower on Habana Gaudi2",
    "summary": "",
    "url": "https://huggingface.co/blog/bridgetower",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Ethics and Society Newsletter #4: Bias in Text-to-Image Models",
    "summary": "",
    "url": "https://huggingface.co/blog/ethics-soc-4",
    "source": "Hugging Face Blog"
  },
  {
    "title": "What's going on with the Open LLM Leaderboard?",
    "summary": "",
    "url": "https://huggingface.co/blog/open-llm-leaderboard-mmlu",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Panel on Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/panel-on-hugging-face",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-Tune MMS Adapter Models for low-resource ASR",
    "summary": "",
    "url": "https://huggingface.co/blog/mms_adapters",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer)",
    "summary": "",
    "url": "https://huggingface.co/blog/autoformer",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Faster Stable Diffusion with Core ML on iPhone, iPad, and Mac",
    "summary": "",
    "url": "https://huggingface.co/blog/fast-diffusers-coreml",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploy Livebook notebooks as apps to Hugging Face Spaces",
    "summary": "",
    "url": "https://huggingface.co/blog/livebook-app-deployment",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Announcing our new Content Guidelines and Policy",
    "summary": "",
    "url": "https://huggingface.co/blog/content-guidelines-update",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face and AMD partner on accelerating state-of-the-art models for CPU and GPU platforms",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface-and-amd",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Can foundation models label data like humans?",
    "summary": "",
    "url": "https://huggingface.co/blog/open-llm-leaderboard-rlhf",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Hugging Face Hub for Galleries, Libraries, Archives and Museums",
    "summary": "",
    "url": "https://huggingface.co/blog/hf-hub-glam-guide",
    "source": "Hugging Face Blog"
  },
  {
    "title": "DuckDB: analyze 50,000+ datasets stored on the Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/hub-duckdb",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome fastText to the Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/fasttext",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Falcon has landed in the Hugging Face ecosystem",
    "summary": "",
    "url": "https://huggingface.co/blog/falcon",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AI Speech Recognition in Unity",
    "summary": "",
    "url": "https://huggingface.co/blog/unity-asr",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Announcing the Open Source AI Game Jam 🎮",
    "summary": "",
    "url": "https://huggingface.co/blog/game-jam",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Hugging Face LLM Inference Container for Amazon SageMaker",
    "summary": "",
    "url": "https://huggingface.co/blog/sagemaker-huggingface-llm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing BERTopic Integration with the Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/bertopic",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Optimizing Stable Diffusion for Intel CPUs with NNCF and 🤗 Optimum",
    "summary": "",
    "url": "https://huggingface.co/blog/train-optimize-sd-intel",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA",
    "summary": "",
    "url": "https://huggingface.co/blog/4bit-transformers-bitsandbytes",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure",
    "summary": "",
    "url": "https://huggingface.co/blog/hugging-face-endpoints-on-azure",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face and IBM partner on watsonx.ai, the next-generation enterprise studio for AI builders",
    "summary": "",
    "url": "https://huggingface.co/blog/huggingface-and-ibm",
    "source": "Hugging Face Blog"
  },
  {
    "title": "🐶Safetensors audited as really safe and becoming the default",
    "summary": "",
    "url": "https://huggingface.co/blog/safetensors-security-audit",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Instruction-tuning Stable Diffusion with InstructPix2Pix",
    "summary": "",
    "url": "https://huggingface.co/blog/instruction-tuning-sd",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Large-scale Near-deduplication Behind BigCode",
    "summary": "",
    "url": "https://huggingface.co/blog/dedup",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Smaller is better: Q8-Chat, an efficient generative AI experience on Xeon",
    "summary": "",
    "url": "https://huggingface.co/blog/generative-ai-models-on-intel-cpu",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face Selected for the French Data Protection Agency Enhanced Support Program",
    "summary": "",
    "url": "https://huggingface.co/blog/cnil",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Run a Chatgpt-like Chatbot on a Single GPU with ROCm",
    "summary": "",
    "url": "https://huggingface.co/blog/chatbot-amd-gpu",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing RWKV - An RNN with the advantages of a transformer",
    "summary": "",
    "url": "https://huggingface.co/blog/rwkv",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Assisted Generation: a new direction toward low-latency text generation",
    "summary": "",
    "url": "https://huggingface.co/blog/assisted-generation",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Creating a Coding Assistant with StarCoder",
    "summary": "",
    "url": "https://huggingface.co/blog/starchat-alpha",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A Dive into Text-to-Video Models",
    "summary": "",
    "url": "https://huggingface.co/blog/text-to-video",
    "source": "Hugging Face Blog"
  },
  {
    "title": "StarCoder: A State-of-the-Art LLM for Code",
    "summary": "",
    "url": "https://huggingface.co/blog/starcoder",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How to Install and Use the Hugging Face Unity API",
    "summary": "",
    "url": "https://huggingface.co/blog/unity-api",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Training a language model with 🤗 Transformers using TensorFlow and TPUs",
    "summary": "",
    "url": "https://huggingface.co/blog/tf_tpu",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Running IF with 🧨 diffusers on a Free Tier Google Colab",
    "summary": "",
    "url": "https://huggingface.co/blog/if",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Databricks ❤️ Hugging Face: up to 40% faster training and tuning of Large Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/databricks-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing HuggingFace blog for Chinese speakers: Fostering Collaboration with the Chinese AI community",
    "summary": "",
    "url": "https://huggingface.co/blog/chinese-language-blog",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How to host a Unity game in a Space",
    "summary": "",
    "url": "https://huggingface.co/blog/unity-in-spaces",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerating Hugging Face Transformers with AWS Inferentia2",
    "summary": "",
    "url": "https://huggingface.co/blog/accelerate-transformers-with-inferentia2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Graph Classification with Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/graphml-classification",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Creating Privacy Preserving AI with Substra",
    "summary": "",
    "url": "https://huggingface.co/blog/owkin-substra",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Snorkel AI x Hugging Face: unlock foundation models for enterprises",
    "summary": "",
    "url": "https://huggingface.co/blog/snorkel-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "StackLLaMA: A hands-on guide to train LLaMA with RLHF",
    "summary": "",
    "url": "https://huggingface.co/blog/stackllama",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Ethics and Society Newsletter #3: Ethical Openness at Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/ethics-soc-3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator",
    "summary": "",
    "url": "https://huggingface.co/blog/habana-gaudi-2-bloom",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerating Stable Diffusion Inference on Intel CPUs",
    "summary": "",
    "url": "https://huggingface.co/blog/stable-diffusion-inference-intel",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Federated Learning using Hugging Face and Flower",
    "summary": "",
    "url": "https://huggingface.co/blog/fl-with-flower",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Train your ControlNet with diffusers",
    "summary": "",
    "url": "https://huggingface.co/blog/train-your-controlnet",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Jupyter X Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/notebooks-hub",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Multivariate Probabilistic Time Series Forecasting with Informer",
    "summary": "",
    "url": "https://huggingface.co/blog/informer",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU",
    "summary": "",
    "url": "https://huggingface.co/blog/trl-peft",
    "source": "Hugging Face Blog"
  },
  {
    "title": "New ViT and ALIGN Models From Kakao Brain",
    "summary": "",
    "url": "https://huggingface.co/blog/vit-align",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Using Machine Learning to Aid Survivors and Race through Time",
    "summary": "",
    "url": "https://huggingface.co/blog/using-ml-for-disasters",
    "source": "Hugging Face Blog"
  },
  {
    "title": "ControlNet in 🧨 Diffusers",
    "summary": "",
    "url": "https://huggingface.co/blog/controlnet",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Ethical Guidelines for developing the Diffusers library",
    "summary": "",
    "url": "https://huggingface.co/blog/ethics-diffusers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How Hugging Face Accelerated Development of Witty Works Writing Assistant",
    "summary": "",
    "url": "https://huggingface.co/blog/classification-use-cases",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Red-Teaming Large Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/red-teaming",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Swift 🧨Diffusers - Fast Stable Diffusion for Mac",
    "summary": "",
    "url": "https://huggingface.co/blog/fast-mac-diffusers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fetch Consolidates AI Tools and Saves 30% Development Time with Hugging Face on AWS",
    "summary": "",
    "url": "https://huggingface.co/blog/fetch-eap-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face and AWS partner to make AI more accessible",
    "summary": "",
    "url": "https://huggingface.co/blog/aws-partnership",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Zero-shot image-to-text generation with BLIP-2",
    "summary": "",
    "url": "https://huggingface.co/blog/blip-2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Why we’re switching to Hugging Face Inference Endpoints, and maybe you should too",
    "summary": "",
    "url": "https://huggingface.co/blog/mantis-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Parameter-Efficient Fine-Tuning using 🤗 PEFT",
    "summary": "",
    "url": "https://huggingface.co/blog/peft",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Speech Synthesis, Recognition, and More With SpeechT5",
    "summary": "",
    "url": "https://huggingface.co/blog/speecht5",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Generating Stories: AI for Game Development #5",
    "summary": "",
    "url": "https://huggingface.co/blog/ml-for-games-5",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing ⚔️ AI vs. AI ⚔️ a deep reinforcement learning multi-agents competition system",
    "summary": "",
    "url": "https://huggingface.co/blog/aivsai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerating PyTorch Transformers with Intel Sapphire Rapids - part 2",
    "summary": "",
    "url": "https://huggingface.co/blog/intel-sapphire-rapids-inference",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A Dive into Vision-Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/vision_language_pretraining",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The State of Computer Vision at Hugging Face 🤗",
    "summary": "",
    "url": "https://huggingface.co/blog/cv_state",
    "source": "Hugging Face Blog"
  },
  {
    "title": "2D Asset Generation: AI for Game Development #4",
    "summary": "",
    "url": "https://huggingface.co/blog/ml-for-games-4",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Using LoRA for Efficient Stable Diffusion Fine-Tuning",
    "summary": "",
    "url": "https://huggingface.co/blog/lora",
    "source": "Hugging Face Blog"
  },
  {
    "title": "What Makes a Dialog Agent Useful?",
    "summary": "",
    "url": "https://huggingface.co/blog/dialog-agents",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Optimum+ONNX Runtime - Easier, Faster training for your Hugging Face models",
    "summary": "",
    "url": "https://huggingface.co/blog/optimum-onnxruntime-training",
    "source": "Hugging Face Blog"
  },
  {
    "title": "3D Asset Generation: AI for Game Development #3",
    "summary": "",
    "url": "https://huggingface.co/blog/ml-for-games-3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Universal Image Segmentation with Mask2Former and OneFormer",
    "summary": "",
    "url": "https://huggingface.co/blog/mask2former",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome PaddlePaddle to the Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/paddlepaddle",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Image Similarity with Hugging Face Datasets and Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/image-similarity",
    "source": "Hugging Face Blog"
  },
  {
    "title": "AI for Game Development: Creating a Farming Game in 5 Days. Part 2",
    "summary": "",
    "url": "https://huggingface.co/blog/ml-for-games-2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introduction to Graph Machine Learning",
    "summary": "",
    "url": "https://huggingface.co/blog/intro-graphml",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Zero-shot image segmentation with CLIPSeg",
    "summary": "",
    "url": "https://huggingface.co/blog/clipseg-zero-shot",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Model Cards",
    "summary": "",
    "url": "https://huggingface.co/blog/model-cards",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Let's talk about biases in machine learning! Ethics and Society Newsletter #2",
    "summary": "",
    "url": "https://huggingface.co/blog/ethics-soc-2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A Complete Guide to Audio Datasets",
    "summary": "",
    "url": "https://huggingface.co/blog/audio-datasets",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Faster Training and Inference: Habana Gaudi®2 vs Nvidia A100 80GB",
    "summary": "",
    "url": "https://huggingface.co/blog/habana-gaudi-2-benchmark",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Illustrating Reinforcement Learning from Human Feedback (RLHF)",
    "summary": "",
    "url": "https://huggingface.co/blog/rlhf",
    "source": "Hugging Face Blog"
  },
  {
    "title": "From GPT2 to Stable Diffusion: Hugging Face arrives to the Elixir community",
    "summary": "",
    "url": "https://huggingface.co/blog/elixir-bumblebee",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deep Learning with Proteins",
    "summary": "",
    "url": "https://huggingface.co/blog/deep-learning-with-proteins",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Using Stable Diffusion with Core ML on Apple Silicon",
    "summary": "",
    "url": "https://huggingface.co/blog/diffusers-coreml",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Probabilistic Time Series Forecasting with 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/time-series-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "VQ-Diffusion",
    "summary": "",
    "url": "https://huggingface.co/blog/vq-diffusion",
    "source": "Hugging Face Blog"
  },
  {
    "title": "We are hiring interns!",
    "summary": "",
    "url": "https://huggingface.co/blog/interns-2023",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Diffusion Models Live Event",
    "summary": "",
    "url": "https://huggingface.co/blog/diffusion-models-event",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Director of Machine Learning Insights [Part 4]",
    "summary": "",
    "url": "https://huggingface.co/blog/ml-director-insights-4",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerating Document AI",
    "summary": "",
    "url": "https://huggingface.co/blog/document-ai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "An overview of inference solutions on Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-update",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face Machine Learning Demos on arXiv",
    "summary": "",
    "url": "https://huggingface.co/blog/arxiv",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Sentiment Analysis on Encrypted Data with Homomorphic Encryption",
    "summary": "",
    "url": "https://huggingface.co/blog/sentiment-analysis-fhe",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Generating Human-level Text with Contrastive Search in Transformers 🤗",
    "summary": "",
    "url": "https://huggingface.co/blog/introducing-csearch",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing our new pricing",
    "summary": "",
    "url": "https://huggingface.co/blog/pricing-update",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Training Stable Diffusion with Dreambooth using Diffusers",
    "summary": "",
    "url": "https://huggingface.co/blog/dreambooth",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-Tune Whisper For Multilingual ASR with 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/fine-tune-whisper",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerate your models with 🤗 Optimum Intel and OpenVINO",
    "summary": "",
    "url": "https://huggingface.co/blog/openvino",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Evaluating Language Model Bias with 🤗 Evaluate",
    "summary": "",
    "url": "https://huggingface.co/blog/evaluating-llm-bias",
    "source": "Hugging Face Blog"
  },
  {
    "title": "From PyTorch DDP to Accelerate to Trainer, mastery of distributed training with ease",
    "summary": "",
    "url": "https://huggingface.co/blog/pytorch-ddp-accelerate-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "MTEB: Massive Text Embedding Benchmark",
    "summary": "",
    "url": "https://huggingface.co/blog/mteb",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Getting Started with Hugging Face Inference Endpoints",
    "summary": "",
    "url": "https://huggingface.co/blog/inference-endpoints",
    "source": "Hugging Face Blog"
  },
  {
    "title": "🧨 Stable Diffusion  in JAX / Flax !",
    "summary": "",
    "url": "https://huggingface.co/blog/stable_diffusion_jax",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Optimization story: Bloom inference",
    "summary": "",
    "url": "https://huggingface.co/blog/bloom-inference-optimization",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing DOI: the Digital Object Identifier to Datasets and Models",
    "summary": "",
    "url": "https://huggingface.co/blog/introducing-doi",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Japanese Stable Diffusion",
    "summary": "",
    "url": "https://huggingface.co/blog/japanese-stable-diffusion",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Very Large Language Models and How to Evaluate Them",
    "summary": "",
    "url": "https://huggingface.co/blog/zero-shot-eval-on-the-hub",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Image Classification with AutoTrain",
    "summary": "",
    "url": "https://huggingface.co/blog/autotrain-image-classification",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How 🤗 Accelerate runs very large models thanks to PyTorch",
    "summary": "",
    "url": "https://huggingface.co/blog/accelerate-large-models",
    "source": "Hugging Face Blog"
  },
  {
    "title": "SetFit: Efficient Few-Shot Learning Without Prompts",
    "summary": "",
    "url": "https://huggingface.co/blog/setfit",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Ethics and Society Newsletter #1",
    "summary": "",
    "url": "https://huggingface.co/blog/ethics-soc-1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate",
    "summary": "",
    "url": "https://huggingface.co/blog/bloom-inference-pytorch-scripts",
    "source": "Hugging Face Blog"
  },
  {
    "title": "What's new in Diffusers? 🎨",
    "summary": "",
    "url": "https://huggingface.co/blog/diffusers-2nd-month",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Train your first Decision Transformer",
    "summary": "",
    "url": "https://huggingface.co/blog/train-decision-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How to train a Language Model with Megatron-LM",
    "summary": "",
    "url": "https://huggingface.co/blog/megatron-training",
    "source": "Hugging Face Blog"
  },
  {
    "title": "OpenRAIL: Towards open and responsible AI licensing frameworks",
    "summary": "",
    "url": "https://huggingface.co/blog/open_rail",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Visualize proteins on Hugging Face Spaces",
    "summary": "",
    "url": "https://huggingface.co/blog/spaces_3dmoljs",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Stable Diffusion with 🧨 Diffusers",
    "summary": "",
    "url": "https://huggingface.co/blog/stable_diffusion",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Pre-Train BERT with Hugging Face Transformers and Habana Gaudi",
    "summary": "",
    "url": "https://huggingface.co/blog/pretraining-bert",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploying 🤗 ViT on Vertex AI",
    "summary": "",
    "url": "https://huggingface.co/blog/deploy-vertex-ai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore",
    "summary": "",
    "url": "https://huggingface.co/blog/vision-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using transformers, accelerate and bitsandbytes",
    "summary": "",
    "url": "https://huggingface.co/blog/hf-bitsandbytes-integration",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Skops",
    "summary": "",
    "url": "https://huggingface.co/blog/skops",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face's TensorFlow Philosophy",
    "summary": "",
    "url": "https://huggingface.co/blog/tensorflow-philosophy",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploying 🤗 ViT on Kubernetes with TF Serving",
    "summary": "",
    "url": "https://huggingface.co/blog/deploy-tfserving-kubernetes",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Train and Fine-Tune Sentence Transformers Models",
    "summary": "",
    "url": "https://huggingface.co/blog/how-to-train-sentence-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Proximal Policy Optimization (PPO)",
    "summary": "",
    "url": "https://huggingface.co/blog/deep-rl-ppo",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Private Hub: A New Way to Build With Machine Learning",
    "summary": "",
    "url": "https://huggingface.co/blog/introducing-private-hub",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method",
    "summary": "",
    "url": "https://huggingface.co/blog/nystromformer",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Comments on U.S. National AI Research Resource Interim Report",
    "summary": "",
    "url": "https://huggingface.co/blog/us-national-ai-research-resource",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing new audio and vision documentation in 🤗 Datasets",
    "summary": "",
    "url": "https://huggingface.co/blog/datasets-docs-update",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Faster Text Generation with TensorFlow and XLA",
    "summary": "",
    "url": "https://huggingface.co/blog/tf-xla-generate",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploying TensorFlow Vision Models in Hugging Face with TF Serving",
    "summary": "",
    "url": "https://huggingface.co/blog/tf-serving-vision",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Advantage Actor Critic (A2C)",
    "summary": "",
    "url": "https://huggingface.co/blog/deep-rl-a2c",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How to train your model dynamically using adversarial data",
    "summary": "",
    "url": "https://huggingface.co/blog/mnist-adversarial",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Technology Behind BLOOM Training",
    "summary": "",
    "url": "https://huggingface.co/blog/bloom-megatron-deepspeed",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Building a Playlist Generator with Sentence Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/playlist-generator",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing The World's Largest Open Multilingual Language Model: BLOOM",
    "summary": "",
    "url": "https://huggingface.co/blog/bloom",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Getting Started with Sentiment Analysis on Twitter",
    "summary": "",
    "url": "https://huggingface.co/blog/sentiment-analysis-twitter",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Policy Gradient with PyTorch",
    "summary": "",
    "url": "https://huggingface.co/blog/deep-rl-pg",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Liftoff! How to get started with your first ML project 🚀",
    "summary": "",
    "url": "https://huggingface.co/blog/your-first-ml-project",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerate Large Model Training using DeepSpeed",
    "summary": "",
    "url": "https://huggingface.co/blog/accelerate-deepspeed",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Announcing Evaluation on the Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/eval-on-the-hub",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Getting Started With Embeddings",
    "summary": "",
    "url": "https://huggingface.co/blog/getting-started-with-embeddings",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Convert Transformers to ONNX with Hugging Face Optimum",
    "summary": "",
    "url": "https://huggingface.co/blog/convert-transformers-to-onnx",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration",
    "summary": "",
    "url": "https://huggingface.co/blog/intel",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Annotated Diffusion Model",
    "summary": "",
    "url": "https://huggingface.co/blog/annotated-diffusion",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deep Q-Learning with Space Invaders",
    "summary": "",
    "url": "https://huggingface.co/blog/deep-rl-dqn",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Graphcore and Hugging Face Launch New Lineup of IPU-Ready Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/graphcore-update",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Pull Requests and Discussions 🥳",
    "summary": "",
    "url": "https://huggingface.co/blog/community-update",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Efficient Table Pre-training without Real Data: An Introduction to TAPEX",
    "summary": "",
    "url": "https://huggingface.co/blog/tapex",
    "source": "Hugging Face Blog"
  },
  {
    "title": "An Introduction to Q-Learning Part 2/2",
    "summary": "",
    "url": "https://huggingface.co/blog/deep-rl-q-part2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How Sempre Health is leveraging the Expert Acceleration Program to accelerate their ML roadmap",
    "summary": "",
    "url": "https://huggingface.co/blog/sempre-health-eap-case-study",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Putting ethical principles at the core of the research lifecycle",
    "summary": "",
    "url": "https://huggingface.co/blog/ethical-charter-multimodal",
    "source": "Hugging Face Blog"
  },
  {
    "title": "An Introduction to Q-Learning Part 1",
    "summary": "",
    "url": "https://huggingface.co/blog/deep-rl-q-part1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Machine Learning Experts - Sasha Luccioni",
    "summary": "",
    "url": "https://huggingface.co/blog/sasha-luccioni-interview",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Announcing the Hugging Face Fellowship Program",
    "summary": "",
    "url": "https://huggingface.co/blog/fellowship",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Gradio 3.0 is Out!",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-blocks",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Student Ambassador Program’s call for applications is open!",
    "summary": "",
    "url": "https://huggingface.co/blog/ambassadors",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerated Inference with Optimum and Transformers Pipelines",
    "summary": "",
    "url": "https://huggingface.co/blog/optimum-inference",
    "source": "Hugging Face Blog"
  },
  {
    "title": "We Raised $100 Million for Open & Collaborative Machine Learning 🚀",
    "summary": "",
    "url": "https://huggingface.co/blog/series-c",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome fastai to the Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/fastai",
    "source": "Hugging Face Blog"
  },
  {
    "title": "An Introduction to Deep Reinforcement Learning",
    "summary": "",
    "url": "https://huggingface.co/blog/deep-rl-intro",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel",
    "summary": "",
    "url": "https://huggingface.co/blog/pytorch-fsdp",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Opinion Classification with Kili and HuggingFace AutoTrain",
    "summary": "",
    "url": "https://huggingface.co/blog/opinion-classification-with-kili",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Director of Machine Learning Insights",
    "summary": "",
    "url": "https://huggingface.co/blog/ml-director-insights",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Getting Started with Transformers on Habana Gaudi",
    "summary": "",
    "url": "https://huggingface.co/blog/getting-started-habana",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Hugging Face for Education 🤗",
    "summary": "",
    "url": "https://huggingface.co/blog/education",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Supercharged Customer Service with Machine Learning",
    "summary": "",
    "url": "https://huggingface.co/blog/supercharge-customer-service-with-machine-learning",
    "source": "Hugging Face Blog"
  },
  {
    "title": "CO2 Emissions and the 🤗 Hub: Leading the Charge",
    "summary": "",
    "url": "https://huggingface.co/blog/carbon-emissions-on-the-hub",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Machine Learning Experts - Lewis Tunstall",
    "summary": "",
    "url": "https://huggingface.co/blog/lewis-tunstall-interview",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training",
    "summary": "",
    "url": "https://huggingface.co/blog/habana",
    "source": "Hugging Face Blog"
  },
  {
    "title": "~Don't~ Repeat Yourself",
    "summary": "",
    "url": "https://huggingface.co/blog/transformers-design-philosophy",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Decision Transformers on Hugging Face 🤗",
    "summary": "",
    "url": "https://huggingface.co/blog/decision-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Machine Learning Experts - Margaret Mitchell",
    "summary": "",
    "url": "https://huggingface.co/blog/meg-mitchell-interview",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Announcing the 🤗 AI Research Residency Program",
    "summary": "",
    "url": "https://huggingface.co/blog/ai-residency",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-Tune a Semantic Segmentation Model with a Custom Dataset",
    "summary": "",
    "url": "https://huggingface.co/blog/fine-tune-segformer",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerate BERT inference with Hugging Face Transformers and AWS Inferentia",
    "summary": "",
    "url": "https://huggingface.co/blog/bert-inferentia-sagemaker",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Image search with 🤗 datasets",
    "summary": "",
    "url": "https://huggingface.co/blog/image-search-datasets",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Guiding Text Generation with Constrained Beam Search in 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/constrained-beam-search",
    "source": "Hugging Face Blog"
  },
  {
    "title": "BERT 101 - State Of The Art NLP Model Explained",
    "summary": "",
    "url": "https://huggingface.co/blog/bert-101",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-Tune ViT for Image Classification with 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/fine-tune-vit",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Getting Started with Sentiment Analysis using Python",
    "summary": "",
    "url": "https://huggingface.co/blog/sentiment-analysis-python",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Making automatic speech recognition work on large files with Wav2Vec2 in 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/asr-chunking",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Supercharged Searching on the 🤗 Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/searching-the-hub",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome Stable-baselines3 to the Hugging Face Hub 🤗",
    "summary": "",
    "url": "https://huggingface.co/blog/sb3",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs",
    "summary": "",
    "url": "https://huggingface.co/blog/infinity-cpu-performance",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Boosting Wav2Vec2 with n-grams in 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/wav2vec2-with-ngram",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploy GPT-J 6B for inference using  Hugging Face Transformers and Amazon SageMaker",
    "summary": "",
    "url": "https://huggingface.co/blog/gptj-sagemaker",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Active Learning with AutoNLP and Prodigy",
    "summary": "",
    "url": "https://huggingface.co/blog/autonlp-prodigy",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Gradio is joining Hugging Face!",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-joins-hf",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Perceiver IO: a scalable, fully-attentional model that works on any modality",
    "summary": "",
    "url": "https://huggingface.co/blog/perceiver",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Training CodeParrot 🦜 from Scratch",
    "summary": "",
    "url": "https://huggingface.co/blog/codeparrot",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Snowball Fight ☃️, our first ML-Agents environment",
    "summary": "",
    "url": "https://huggingface.co/blog/snowball-fight",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Getting Started with Hugging Face Transformers for IPUs with Optimum",
    "summary": "",
    "url": "https://huggingface.co/blog/graphcore-getting-started",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing the Data Measurements Tool: an Interactive Tool for Looking at Datasets",
    "summary": "",
    "url": "https://huggingface.co/blog/data-measurements-tool",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Accelerating PyTorch distributed fine-tuning with Intel technologies",
    "summary": "",
    "url": "https://huggingface.co/blog/accelerating-pytorch",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-Tune XLSR-Wav2Vec2 for low-resource ASR with 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/fine-tune-xlsr-wav2vec2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Scaling up BERT-like model Inference on modern CPU  - Part 2",
    "summary": "",
    "url": "https://huggingface.co/blog/bert-cpu-scaling-part-2",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Course Launch Community Event",
    "summary": "",
    "url": "https://huggingface.co/blog/course-launch-event",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Large Language Models: A New Moore's Law?",
    "summary": "",
    "url": "https://huggingface.co/blog/large-language-models",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Train a Sentence Embedding Model with 1B Training Pairs",
    "summary": "",
    "url": "https://huggingface.co/blog/1b-sentence-embeddings",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Age of Machine Learning As Code Has Arrived",
    "summary": "",
    "url": "https://huggingface.co/blog/the-age-of-ml-as-code",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine tuning CLIP with Remote Sensing (Satellite) images and captions",
    "summary": "",
    "url": "https://huggingface.co/blog/fine-tune-clip-rsicd",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hosting your Models and Datasets on Hugging Face Spaces using Streamlit",
    "summary": "",
    "url": "https://huggingface.co/blog/streamlit-spaces",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Showcase Your Projects in Spaces using Gradio",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio-spaces",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Summer at Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/summer-at-huggingface",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face and Graphcore partner for IPU-optimized Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/graphcore",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing Optimum: The Optimization Toolkit for Transformers at Scale",
    "summary": "",
    "url": "https://huggingface.co/blog/hardware-partners-program",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deep Learning over the Internet: Training Language Models Collaboratively",
    "summary": "",
    "url": "https://huggingface.co/blog/collaborative-training",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Welcome spaCy to the Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/spacy",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Deploy Hugging Face models easily with Amazon SageMaker",
    "summary": "",
    "url": "https://huggingface.co/blog/deploy-hugging-face-models-easily-with-amazon-sagemaker",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Sentence Transformers in the Hugging Face Hub",
    "summary": "",
    "url": "https://huggingface.co/blog/sentence-transformers-in-the-hub",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Few-shot learning in practice: GPT-Neo and the 🤗 Accelerated Inference API",
    "summary": "",
    "url": "https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Using & Mixing Hugging Face Models with Gradio 2.0",
    "summary": "",
    "url": "https://huggingface.co/blog/gradio",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Scaling-up BERT Inference on CPU (Part 1)",
    "summary": "",
    "url": "https://huggingface.co/blog/bert-cpu-scaling-part-1",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Introducing 🤗 Accelerate",
    "summary": "",
    "url": "https://huggingface.co/blog/accelerate-library",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Distributed Training: Train BART/T5 for Summarization using 🤗 Transformers and Amazon SageMaker",
    "summary": "",
    "url": "https://huggingface.co/blog/sagemaker-distributed-training-seq2seq",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Understanding BigBird's Block Sparse Attention",
    "summary": "",
    "url": "https://huggingface.co/blog/big-bird",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Partnership: Amazon SageMaker and Hugging Face",
    "summary": "",
    "url": "https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face",
    "source": "Hugging Face Blog"
  },
  {
    "title": "My Journey to a serverless transformers pipeline on Google Cloud",
    "summary": "",
    "url": "https://huggingface.co/blog/how-to-deploy-a-pipeline-to-google-clouds",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fine-Tune Wav2Vec2 for English ASR in Hugging Face with 🤗 Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/fine-tune-wav2vec2-english",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face Reads, Feb. 2021 - Long-range Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/long-range-transformers",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Simple considerations for simple people building fancy neural networks",
    "summary": "",
    "url": "https://huggingface.co/blog/simple-considerations",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Retrieval Augmented Generation with Huggingface Transformers and Ray",
    "summary": "",
    "url": "https://huggingface.co/blog/ray-rag",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hugging Face on PyTorch / XLA TPUs",
    "summary": "",
    "url": "https://huggingface.co/blog/pytorch-xla",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Faster TensorFlow models in Hugging Face Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/tf-serving",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Fit More and Train Faster With ZeRO via DeepSpeed and FairScale",
    "summary": "",
    "url": "https://huggingface.co/blog/zero-deepspeed-fairscale",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How we sped up transformer inference 100x for 🤗 API customers",
    "summary": "",
    "url": "https://huggingface.co/blog/accelerated-inference",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Leveraging Pre-trained Language Model Checkpoints for Encoder-Decoder Models",
    "summary": "",
    "url": "https://huggingface.co/blog/warm-starting-encoder-decoder",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Porting fairseq wmt19 translation system to transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/porting-fsmt",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Hyperparameter Search with Transformers and Ray Tune",
    "summary": "",
    "url": "https://huggingface.co/blog/ray-tune",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Transformer-based Encoder-Decoder Models",
    "summary": "",
    "url": "https://huggingface.co/blog/encoder-decoder",
    "source": "Hugging Face Blog"
  },
  {
    "title": "Block Sparse Matrices for Smaller and Faster Language Models",
    "summary": "",
    "url": "https://huggingface.co/blog/pytorch_block_sparse",
    "source": "Hugging Face Blog"
  },
  {
    "title": "The Reformer - Pushing the limits of language modeling",
    "summary": "",
    "url": "https://huggingface.co/blog/reformer",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How to generate text: using different decoding methods for language generation with Transformers",
    "summary": "",
    "url": "https://huggingface.co/blog/how-to-generate",
    "source": "Hugging Face Blog"
  },
  {
    "title": "How to train a new language model from scratch using Transformers and Tokenizers",
    "summary": "",
    "url": "https://huggingface.co/blog/how-to-train",
    "source": "Hugging Face Blog"
  }
]